# MISS-QA: A Multimodal Scientific Information-Seeking QA Benchmark

MISS-QA (Multimodal Information-Seeking over Scientific papers â€“ Question Answering) is the **first benchmark** specifically designed to evaluate the ability of multimodal foundation models to **interpret schematic diagrams** and answer **information-seeking questions** within scientific literature.

> ğŸ”¬ â€œCan Multimodal Foundation Models Understand Schematic Diagrams?â€

------

## ğŸŒŸ Highlights

- ğŸ“š **1500 QA pairs** annotated by **expert researchers**
- ğŸ“„ Covers **465 AI-related papers** from arXiv
- ğŸ¯ Focuses on **schematic diagrams**, not just charts or tables
- ğŸ¤– Evaluates **18 frontier vision-language models** (o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL)
- ğŸ§  Automatic evaluation protocol trained on **human-scored data**

------

## ğŸ§© Benchmark Structure

Each example in MISS-QA includes:

- A **schematic diagram** from a scientific paper
- A **highlighted visual element** (bounding box)
- A **free-form information-seeking question**
- The corresponding **scientific context**
- A human-annotated **answer** (or marked as unanswerable)

### ğŸ” Information-Seeking Scenarios

- **Design Rationale**
- **Implementation Details**
- **Literature Background**
- **Experimental Results**
- **Other** (e.g., limitations, ethics)

------

## ğŸ“Š Model Evaluation

MISS-QA is used to benchmark proprietary and open-source **multimodal foundation models**. Performance is automatically scored using a custom evaluation protocol aligned with human judgment.

------

## ğŸ› ï¸ How to Use

### ğŸ” Step 0: Installation

```bash
git clone https://github.com/QDRhhhh/MISSQA.git
cd MISSQA
conda create --name missqa python=3.10
conda activate missqa
pip install -r requirements.txt
```

### ğŸ” Step 1: Run Model Inference

Use the provided bash script to run inference with your multimodal model:

```bash
bash scripts/vllm_large.sh
```

This will generate model responses and save them to:

```swift
./outputs/
```

### âœ… Step 2: Evaluate Model Accuracy

Once inference is complete, run the accuracy evaluation script:

```bash
python acc_evaluation.py
```

The processed and scored outputs will be saved to:

```swift
./processed_outputs/
```
