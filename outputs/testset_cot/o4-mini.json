[
    {
        "question": "Why is minimizing 2D SE important for SEGA?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Under the federated framework described in Section 3.1, personalized global aggregation aims to provide clients with maximum external information by producing global models that can benefit individual clients more. The server needs an aggregation strategy that considers client heterogeneity and individual characteristics to maximize external knowledge for all clients. To achieve this objective, we construct a client graph Gclient based on clients’ similarity. By minimizing the two-dimensional Structural Entropy (2DSE) of Gclient, a graph capturing the internal similarities among clients is obtained, finalizing the Global Aggregation strategy for each client (SEGA).Gclient is an undirected, fully connected, weighted graph consisting of K nodes corresponding to K clients, with their similarities as edge weights. The similarity between client models can be estimated by providing them with the same input and measuring the similarity between their respective outputs. On this basis, the server first generates a random graph Grandom as input to all client models. With graph pooling, the server obtains different client models’ representations of the same graph."
        ],
        "final_answer": "Minimizing 2D structural entropy is important for global aggregation because it enables the creation of a client graph that captures the internal similarities among clients, which is used to finalize the Global Aggregation strategy, maximizing the external knowledge available to each client while considering client heterogeneity and individualcharacteristics.",
        "relevant_elements": [
            "Minimizing 2D SE",
            "SEGA"
        ],
        "id": 4001,
        "masked_question": "Why is [mask1] important for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Minimizing 2D SE",
            "SEGA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.00614v1_figure_1.png",
        "paperid": "2409.00614v1",
        "paper_path": "./papers/2409.00614v1.json",
        "figure_id": "2409.00614v1_figure_1.png",
        "caption": "Figure 1. The overall framework of DAMe.",
        "qtype": "Design_Rationale",
        "response": "Here’s the high‐level intuition, unpacked step by step:\n\n1.  In SEGA (“Structural Entropy–based Global Aggregation”, the blue‐boxed module) we want to build a personalized global model for each client by pooling only from the clients most similar to it.  \n2.  To discover those “natural” groups of similar clients, we take the fully connected, weighted client‐similarity graph and apply the red‐boxed step: we minimise its two‐dimensional structural entropy (2DSE).  \n3.  Why minimising 2DSE matters: a low 2DSE partitioning finds the clustering of clients that most compresses the graph—that is, it groups together clients whose models behave similarly on a reference graph.  \n4.  Once we have those clusters, SEGA aggregates each client’s model only with its cluster­mates (using their similarity weights).  \n5.  In short, minimising the 2DSE is the key that exposes the right client partitions, and without that partitioning SEGA could not do its personalized, heterogeneity‐aware global aggregation."
    },
    {
        "question": "What is the motivation behind combining the content of common and private stream in this framework?",
        "relevant_section_ids": [
            "1.2"
        ],
        "relevant_context": [
            "Under the federated framework described in Section 3.1, personalized global aggregation aims to provide clients with maximum external information by producing global models that can benefit individual clients more. The server needs an aggregation strategy that considers client heterogeneity and individual characteristics to maximize external knowledge for all clients. To achieve this objective, we construct a client graph Gclient based on clients’ similarity. By minimizing the two-dimensional Structural Entropy (2DSE) of Gclient, a graph capturing the internal similarities among clients is obtained, finalizing the Global Aggregation strategy for each client (SEGA).Gclient is an undirected, fully connected, weighted graph consisting of K nodes corresponding to K clients, with their similarities as edge weights. The similarity between client models can be estimated by providing them with the same input and measuring the similarity between their respective outputs. On this basis, the server first generates a random graph Grandom as input to all client models. With graph pooling, the server obtains different client models’ representations of the same graph"
        ],
        "final_answer": "Minimizing 2D structural entropy is important for global aggregation because it enables the creation of a client graph that captures the internal similarities among clients, which is used to finalize the Global Aggregation strategy, maximizing the external knowledge available to each client while considering client heterogeneity and individual characteristics.",
        "relevant_elements": [
            "common and private stream"
        ],
        "id": 4002,
        "masked_question": "What is the motivation behind combining the content of [mask1] in this framework?",
        "masked_number": 1,
        "masked_elements": [
            "common and private stream"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.02872v1_figure_1.png",
        "paperid": "2408.02872v1",
        "paper_path": "./papers/2408.02872v1.json",
        "figure_id": "2408.02872v1_figure_1.png",
        "caption": "Figure 1. System model of the proposed RSMA-based NOUM transmission.",
        "qtype": "Design_Rationale",
        "response": "In this paper the little red‐boxed “common” stream is exactly that: we actually pack the multicast message (and any “common” pieces of the unicast messages) into a single RSMA common stream rather than sending it in its own time‐frequency slot.  The whole point of doing that is two–fold:\n\n 1.  It allows the SIC‐layer in a standard RSMA receiver to both  \n     a) strip off the multicast information first (treating all the private/unicast streams as noise)  \n     b) then remove it and continue on to decode each user’s private stream.  \n\n 2.  By doing so we reuse the same power and spectral resources for unicast and multicast, manage the inter‐stream interference (unicast vs. unicast and unicast vs. multicast) with one SIC stage, and—critically—do all of this without increasing receiver complexity.  \n\nIn short, “combining the multicast message into the common stream” lets us  \n– superimpose unicast/multicast on the same resource blocks,  \n– exploit RSMA’s built‐in SIC to separate them, and  \n– remain robust even under imperfect CSIT,  \nall while matching very uneven (heterogeneous) traffic demands."
    },
    {
        "question": "How does MACL achieve real subject similarity using multi-view data processing?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "The key idea of Multi-scale Appearance Similarity Contrastive Learning (MACL) is to ensure that the distance relationships between multiscale features are consistent with those of real subjects. This means the features of the same subject with different situations should be as close as possible (intra-consistency), while the distances between different samples’ features should match those between real subjects (inter-distinctiveness). As shown in Fig. 2(b)(right), we achieve intra-consistency by pulling positive samples of the reference subject closer, and inter-distinctiveness by introducing scaling factors to align the feature distances with negative samples to real subject distances. In this section, we will introduce the S+Space and MACL in the S+Space. As shown in Fig. 2(b) (right), We select frames different from the reference images as MACL positive samples. By aligning images of the same subject, CustomContrast effectively decouples irrelevant features of the subject. The processing details of positive samples are in Appendix B."
        ],
        "final_answer": "MACL achieves real subject similarity using multiview data by ensuring that the distance relationships between multiscale features are consistent with those of real subjects. This is done by maintaining intra-consistency, where features of the same subject with different situations are as close as possible, and inter-distinctiveness, where the distances between different samples' features match those between real subjects. MACL preserves the multi-scale similarity structure, ensuring that the similarities of learned features are positively correlated with those of real subjects.",
        "relevant_elements": [
            "MACL",
            "multi-view data processing"
        ],
        "id": 4003,
        "masked_question": "How does [mask1] achieve real subject similarity using [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "MACL",
            "multi-view data processing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.05606v2_figure_2.png",
        "paperid": "2409.05606v2",
        "paper_path": "./papers/2409.05606v2.json",
        "figure_id": "2409.05606v2_figure_2.png",
        "caption": "Overview of the proposed CustomContrast. (a) Training pipeline. The consistency between textual and visual features is accurately learned by the MFI-Encoder, which includes a Textual-Visual (TV) Fusion module to enhance feature consistency from visual and textual Qformers. (b) The MCL paradigm includes CSCL, aligning high-level semantics by contrasting visual and textual embeddings via CLS tokens, and MACL, which is applied to text embeddings from different cross-attention layers. MACL decouples redundant subject features by aligning positive samples (segmented images of the same subject from various views, positions, and sizes), while preserving relative distances by contrasting with other subjects.",
        "qtype": "Implementation_Details",
        "response": "First, let’s pin down what each mask is pointing at:\n\n  • [mask1] is the little red‐boxed subdiagram under “Positive Samples” in the MACL figure—that is, the multi‐view data‐processing pipeline that picks out different frames (i, j, k) of the same object and turns them into positive pairs.  \n  • [mask2] is the blue‐boxed region as a whole, namely the Multiscale Appearance Contrastive Learning (MACL) module.\n\nStep-by-step, here’s how the red‐boxed multi-view data processing ([mask1]) uses MACL ([mask2]) to enforce “real subject similarity”:\n\n1. **Sample positive views**  \n   During training, the red block selects three different frames (i, j, k) all containing the same physical subject. Frame i becomes the “positive sample,” frame j the “reference,” and frame k is reconstructed by the diffusion model. This gives us multiple “views” of one object.\n\n2. **Extract learned features**  \n   Each of these frames is passed through the MFI‐Encoder and into the UNet’s cross‐attention layers.  We collect, at every attention layer, the injected token embeddings (across all samples in the batch).\n\n3. **Measure real‐world similarity**  \n   Separately, a frozen CLIP image encoder is run on the *segmented* subject crops from the original frames to produce an “appearance similarity matrix” S_real.  S_real[i, j] is the cosine similarity between the *true* appearances of subject i and subject j.\n\n4. **Compute multiscale learned similarities**  \n   Within MACL, we compute the cosine similarities of the learned token embeddings at each cross‐attention layer for *every* pair of samples in the batch.  Call this S_learned.\n\n5. **Scale by real‐subject distances**  \n   A learnable scaling factor γ[i,j] is derived so that whenever two *different* objects in the real world are very dissimilar, we push their learned features apart proportionally; and whenever two views of the *same* object are nearly identical, we pull their features together.  Concretely, the MACL loss is a contrastive objective that uses γ[i,j]⋅S_learned[i,j] in its logits, with γ[i,j] itself a function of S_real[i,j].\n\n6. **Contrast and align**  \n   By minimizing that contrastive loss across all layers and both positive (same‐object) and negative (different‐object) pairs, MACL *aligns* the geometry of the learned feature space with the geometry of the real‐subject space measured by CLIP.  \n\nIn short: the red‐boxed “multi‐view data processing” generates the *positive* pairs for the same object, and then the blue‐boxed MACL module enforces that their learned features obey the *actual* similarity structure of real subjects (as computed by a frozen vision encoder), thereby achieving “real subject similarity.”"
    },
    {
        "question": "What role does energy scores play in seen and unseen categorie estimation for online models?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In Figure 3, we found that the two-stage split of the CGCD struggled to identify both known and unknown categories in our proposed online learning scenario. Online learning with batch-wise data led to severe forgetting in the split network, resulting in poor detection of unknown samples. In contrast, energy-based discovery showed better performance in novel category discovery without extra parameters compared to CGCD. Moreover, as it does not require an additional learning phase, energy-based discovery enables end-to-end training. Inspired by this observation, we propose an energy-guided discovery approach for novel category discovery. To the best of our knowledge, this is the first work to utilize the energy score for novel category discovery. The process in the second stage is the same as the first stage, where we split the unknown data into seen and unseen categories based on their energy scores. For the initial batch of incremental sessions, the online model is identical to the offline model. Additionally, since the initial batch data is the first data of the incremental session, any data classified as unknown is assumed to be unseen data. The proposed energy-guided discovery splits unlabeled data into known, seen, and unseen categories effectively. Unlike prior methods which require sufficient data, it can identify novel categories with batch-wise data."
        ],
        "final_answer": "Energy scores are used to classify unlabeled data into known and unknown categories in the first stage, and then further split unknown data into seen and unseen categories in the second stage. This is done by calculating the energy scores using a Gaussian Mixture Model to identify which cluster a sample belongs to, ultimately facilitating the estimation of seen and unseen data in the online model.",
        "relevant_elements": [
            "energy scores",
            "seen and unseen categorie estimation"
        ],
        "id": 4004,
        "masked_question": "What role does [mask1] play in [mask2] estimation for online models?",
        "masked_number": 2,
        "masked_elements": [
            "energy scores",
            "seen and unseen categorie estimation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.13492v1_figure_2.png",
        "paperid": "2408.13492v1",
        "paper_path": "./papers/2408.13492v1.json",
        "figure_id": "2408.13492v1_figure_2.png",
        "caption": "Overall process of the proposed DEAN framework. The energy-guided discovery splits unlabeled data into known, seen, and unseen data for better novel category discovery, while variance-based feature augmentation enhances the clustering of unseen data. Lce facilitates better discriminative learning in the online continual learning.",
        "qtype": "Implementation_Details",
        "response": "Here’s what’s going on, step by step:\n\n1.  In the second stage of discovery we take all of the “unknown” samples (i.e. those ruled out as “known” in stage 1) and run them through the online model to get an energy score for each example.  \n2.  We then fit a two‐component Gaussian Mixture Model (GMM) to those energy scores.  One component corresponds to low‐energy (seen) examples and the other to high‐energy (unseen) examples.  \n3.  The red‐boxed figure is simply the visualisation of that GMM cut—two clusters in feature space, one colored green (“seen”) and one colored red (“unseen”).  This split (the red box) is exactly what tells us which examples the online model thinks are truly novel.  \n4.  The “unseen estimation” (the blue‐boxed energy plots) is nothing more than reading off the high‐energy cluster from that same GMM.  In other words, the red‐boxed split is the mechanism by which we form the high‐energy (unseen) group, and that high‐energy group is what the blue box calls the unseen estimation.  \n\nSo in one sentence:\n\n•  The red‐box clustering is the GMM‐based partition of unknown samples into “seen” (low energy) and “unseen” (high energy); that partition is exactly what the online model uses to produce its unseen estimation in the blue box."
    },
    {
        "question": "What is the relationship between Photonic Processing Unit and eDRAMs?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The computation process of our R&B architecture contains three stages, as illustrated in Fig. 2(a). Initially, inputs are retrieved from eDRAMs, and corresponding weights are allocated to MRRs. Subsequently, following the PRM configuration, the weights are fixed and reused, allowing the inputs to pass through the MRRs to be optically weighted. The intermediate MVM results generated by the PPUs are then detected by BPDs, where they are converted into summed currents and digitized by ADCs. In the final stage, OBUs transform these outputs to generate the layer-wise results, which are then stored back in eDRAMs in preparation for the next computational layer.A critical aspect of this architecture is the role of the OBU during inference, mirroring its function during training and inference by executing essential shuffle and transpose operations. Subsequently, following the PRM configuration, the weights are fixed and reused. Along with PRM, these two technologies constitute the primary innovation of our R&B architecture. By leveraging one MRR array to represent multiple weight matrices, the architecture dramatically reduces the frequency of MRR writing operations, along with power consumption and latency, all while sustaining high performance."
        ],
        "final_answer": "The Photonic Processing Unit (PPU) plays a role in the R&B architecture's computation process where inputs are initially retrieved from eDRAMs to be processed by PPUs. After processing, the outputs are stored back into eDRAMs for the next layer computation.",
        "relevant_elements": [
            "Photonic Processing Unit",
            "eDRAMs"
        ],
        "id": 4005,
        "masked_question": "What is the relationship between [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Photonic Processing Unit",
            "eDRAMs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.01836v3_figure_2.png",
        "paperid": "2409.01836v3",
        "paper_path": "./papers/2409.01836v3.json",
        "figure_id": "2409.01836v3_figure_2.png",
        "caption": "(a) Overview of the R & B architecture. Each PPU contains a photonic MVM unit and a sampling and hold (S&H) unit. (b) Photonic Reuse Method (PRM). Block-wise reuse allows weight sharing among blocks (a block typically contains multiple layers). Layer-wise reuse enables weight sharing between individual layers. (c) Opto-electronic Blend Unit (OBU). OBUs handle shuffle operations via the peripheral circuit and perform transpose operations directly in the optical domain. (d) Computing pipeline of our R&B architecture.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the entire Photonic Processing Unit (the block of MRR‐based matrix–vector multipliers plus sample-and-hold), and Mask₂ is the very last “eDRAM write” stage in the on-chip pipeline.  In other words, the PPU (Mask₁) sits upstream in the R&B computing flow, performs the optical MVMs (and holds their analog outputs), and then, after O/E conversion and digitization, its results are finally written back into eDRAM in Mask₂.  Mask₂ simply consumes and stores the outputs produced by Mask₁."
    },
    {
        "question": "How does the Verification Strategy ensure high-quality data output?",
        "relevant_section_ids": [
            "2.1",
            "3.1.1"
        ],
        "relevant_context": [
            "The first module in our framework is Quality Verification Agent, which ensures that the generated questions and answers meet a certain standard of quality. This component involves two main processes: \newline Verification Strategy:This includes additional heuristic strategies to judge which samples should be contained as high-quality data. This includes additional heuristic strategies to judge which samples should be contained as high-quality data. Specifically, we utilize two wide-used verification strategies:\newline • Scoring: We prompt LLMs to generate continuous scores, manually set a more reliable threshold score based on the validation set, and set those exceeding the threshold score as high-quality data.\newline • Classification: We prompt LLMs to generate binary classification and select those classified as high-quality data.Verification Condition:\newline Verification Condition:This involves setting specific conditions that both questions and answers must meet to be considered high-quality verification.The process includes:\newline • Criteria Perspectives: Criteria include relevance to the document, clarity, factual accuracy, logical coherence, and complexity of the question and answer.\newline • Auxiliary Context Information: We integrate additional contextual instructions to enhance the model’s accuracy and robustness, like guidelines. \newline • Auxiliary Generation Information: We enable the model to provide more reasoning rationale during output generation and observe whether this improves the robustness and accuracy of the verification process.",
            "Scoring is a Better Verification Strategy Compared with Classification. As shown in Figure 3 (a), the scoring strategy shows significantly higher kappa and precision scores compared to binary quality classification. This statistical improvement suggests that scoring better captures the nuances of human judgments. This observation aligns with findings in short-context scenarios (Fu et al., 2024a), reinforcing the generalizability of scoring strategies across different lengths of textual data."
        ],
        "final_answer": "The Verification Strategy ensures high-quality data output by employing scoring and classification strategies. Scoring involves prompting LLMs to generate continuous scores and setting a threshold to determine high-quality data. This strategy captures the nuances of human judgments better than binary classification, which simply classifies samples as high-quality or not. This process ensures consistency, precision, and alignment with human judgment, thus improving data quality.",
        "relevant_elements": [
            "Verification Strategy",
            "high-quality data output"
        ],
        "id": 4007,
        "masked_question": "How does the [mask1] ensure [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Verification Strategy",
            "high-quality data output"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.01893v1_figure_2.png",
        "paperid": "2409.01893v1",
        "paper_path": "./papers/2409.01893v1.json",
        "figure_id": "2409.01893v1_figure_2.png",
        "caption": "The overall process of our Multi-agent Interactive Multi-hop Generation (MIMG) data synthesis framework.",
        "qtype": "Experimental_Results",
        "response": "The two highlighted regions in the figure are (1) the red‐boxed “Verification Strategy” module in the Quality Verification Agent, and (2) the blue‐boxed “High Quality” output of that same agent.  In plain English the question reads:\n\n“How does the Verification Strategy ensure High Quality?”\n\nChain-of-Thought (step-by-step):\n\n1.  The Verification Strategy is the subroutine inside the Quality Verification Agent (§2.1) that decides which generated Q&A pairs are good enough to keep.  \n2.  According to the text, it does this in two complementary ways:  \n   a.  Scoring:  the LLM is prompted to assign each sample a continuous quality score.  We pick a threshold (tuned on a held-out set) and only accept samples whose score exceeds that threshold.  \n   b.  Classification:  the LLM is also asked to make a binary good/bad judgment, and we accept only those marked “good.”  \n3.  Underneath both of those strategies sit a set of concrete quality criteria—relevance, clarity, factual accuracy, logical coherence and appropriate difficulty—as well as optional “auxiliary” prompts (guidelines, domain context, even chain-of-thought rationales) designed to steer the LLM’s judgment closer to human annotation.  \n4.  By filtering out everything that either scores too low or is classified as “poor,” the Verification Strategy guarantees that only high-quality samples pass through—hence the blue‐boxed “High Quality” bucket.  \n\nAnswer­ (directly):  \nThe Verification Strategy applies two LLM-based filters—a continuous scoring filter (keep only samples whose score ≥ threshold) and a binary classification filter (keep only samples labeled “high-quality”)—each guided by multiple human-inspired criteria (relevance, clarity, accuracy, coherence, complexity) and enriched with auxiliary instructions and optional rationales.  Anything that fails these checks is discarded, so only truly high‐quality Q&A pairs emerge."
    },
    {
        "question": "How does the reinforcement learning algorithm contribute to the updates of policy group?",
        "relevant_section_ids": [
            "4",
            "4.1"
        ],
        "relevant_context": [
            "In Section 3, we introduce the concept of environment agent to realize the adversarial policy search by combining logic rules with reinforcement learning. However, due to the black-box nature of data-driven methods, while adversarial actions can be generated, the difficulty of generating adversarial actions is difficult to quantify accurately, which limits the rationality of adversarial scenario generation. In this section, a data generation method based on scenarios with varying difficulty is presented. The method uses the performance of different stages in the policy search convergence process as a reference to quantify the adversarial intensity, thereby achieving a quantitative representation of scenario difficulty. The model parameters of the environment agent trained on different stages are updated and saved, and then output to the constructed policy group. The policy group is used to generate data that forms the basis for training the scenario difficulty quantitative representation model",
            "A reinforcement learning training process with stable convergence can be divided into two phases, i.e., the performance improvement phase and the convergence stabilization phase. In the performance improvement phase, the average return is still continuously increasing, which indicates that the policy search is still ongoing and the model parameters are still being updated to peruse better performance. In the convergence stabilization phase, however, the average return remains basically unchanged, indicating that the policy search is basically over, and the obtained policy is already the optimal policy that the current algorithm can achieve."
        ],
        "final_answer": "The reinforcement learning algorithm contributes to policy group updates by providing a systematic approach to search for optimal policies through the performance improvement phase, where model parameters are updated to pursue better performance, and the convergence stabilization phase, where the optimal policy is obtained. The performance at different stages is used to update and save model parameters to the constructed policy group.",
        "relevant_elements": [
            "reinforcement learning algorithm",
            "policy group"
        ],
        "id": 4008,
        "masked_question": "How does the [mask1] contribute to the updates of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "reinforcement learning algorithm",
            "policy group"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.14000v1_figure_1.png",
        "paperid": "2408.14000v1",
        "paper_path": "./papers/2408.14000v1.json",
        "figure_id": "2408.14000v1_figure_1.png",
        "caption": "Overall architecture of data driven quantitative representation method of scenario difficulty for autonomous driving based on environment agent policy search.",
        "qtype": "Experimental_Results",
        "response": "First, let’s be very concrete about what each box is:\n\n  • [mask1] (red) is the “Reinforcement Learning Algorithm” that is training the environment‐agent.  \n  • [mask2] (blue) is the “Policy Groups”—in other words, the set of stored policy snapshots at different points in training.\n\nHow the RL‐algorithm ([mask1]) feeds into and updates the policy‐group ([mask2]) can be broken down in three steps:\n\n  1. **Learning and parameter updates**  \n     – The RL algorithm repeatedly interacts with the driving simulator, collecting observations and rewards.  \n     – At each training step it applies its update rule (e.g. policy‐gradient, Q‐learning, or whatever update is specified in Eq. 4 of the paper) to its network weights.\n\n  2. **Periodic model snapshotting**  \n     – Every \\( \\Delta \\) training steps (see Lines 8–12 in Alg. 1), the current network parameters of the RL agent are “snapshotted” and placed into a temporary set \\( \\Psi \\).  \n     – Each snapshot comes tagged with its training‐step index \\( t_j \\).\n\n  3. **Filtering & forming the policy group**  \n     – Once training is far enough along, the paper proposes splitting the agent’s maximum average return into \\( N \\) equal intervals (see Lines 13–16).  \n     – For each interval they pick the snapshot whose return is closest to the interval boundary.  \n     – Those selected snapshots are pulled out of \\( \\Psi \\) and deposited into the final “Policy Groups” box.  \n\nIn short: the red RL module is continuously learning and adjusting the agent parameters; at fixed intervals it hands off those parameters (plus their performance statistics) to a buffer; once enough snapshots are in that buffer, the algorithm filters and “promotes” a handful of them into the blue Policy‐Group.  Those stored policies then serve as the adversaries of different strengths."
    },
    {
        "question": "What are potential limitations of using Lipschitz optimization in neural subspace training?",
        "relevant_section_ids": [
            "5.4"
        ],
        "relevant_context": [
            "In this work, Lipschitz optimization is only applied to the elastic potential term of eq. 2. Since the nonlinear mapping is also involved in the inertia term, this may lower the convergence speed of the simulation involving dynamics. Considering that the inertia term is in quadratic form, the Hessian Lipschitz of the inertia term can be optimized by minimizing or bounding the Lipschitz constant of the network’s input-output Jacobian . This is a promising direction for future work to further accelerate the simulation with dynamics. Another limitation of our method is the extended training time introduced by incorporating Lipschitz optimization into the pipeline. As shown in Table 1, even with cubature acceleration, the training time is still increased by approximately five times compared to the conventional method. This issue can be addressed by employing fast approximate methods to estimate Lipschitz energy."
        ],
        "final_answer": "Potential limitations of using Lipschitz optimization in neural subspace training include the intractability of directly optimizing the Lipschitz constant due to the need to traverse all possible point pairs, sparse gradients that could damage Lipschitz characteristics in certain areas, increased memory usage, and potential memory shortages when training high-resolution meshes.",
        "relevant_elements": [
            "Lipschitz optimization"
        ],
        "id": 4009,
        "masked_question": "What are potential limitations of using [mask1] in neural subspace training?",
        "masked_number": 1,
        "masked_elements": [
            "Lipschitz optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.03807v1_figure_2.png",
        "paperid": "2409.03807v1",
        "paper_path": "./papers/2409.03807v1.json",
        "figure_id": "2409.03807v1_figure_2.png",
        "caption": "Network training settings for effective neural subspace construction. (a) The supervised setting. (b) The unsupervised setting. Conventional methods only consider the loss shown in blue but do not optimize the Lipschitz loss (shown in orange) to control the landscape of simulation objective in the subspace.",
        "qtype": "Others",
        "response": "The red‐boxed term in both (a) and (b) is the “Lipschitz loss” — i.e. the regularizer that tries to directly drive down the Hessian‐Lipschitz constant of the reduced potential.  In our experience the main drawbacks of adding that term into a neural-subspace training pipeline are:  \n\n•  Substantial extra training cost.  Even with cubature acceleration the Lipschitz loss evaluation (it requires Hessian or Jacobian spectral bounds) makes the wall-clock for one training run easily 3–5× longer than the vanilla, reconstruction-only baseline.  \n\n•  Cubature / sampling hyperparameters.  You must pick and generate a cubature set (or other quadrature) to estimate the Lipschitz loss cheaply.  Too few cubature points ⇒ poor Lipschitz estimates, too many ⇒ cubature-generation and per-step cost blow up.  \n\n•  Partial coverage of the physics.  In the paper we only applied Lipschitz regularization to the elastic (potential) term.  The kinetic/inertia term still “rides” on a non-controlled mapping, so for fully dynamic simulations the per‐step convergence speed can still lag if the network’s Jacobian isn’t also bounded.\n\n•  Sensitivity at higher subspace dimensions.  As the latent dimension grows, balancing the Lipschitz loss against reconstruction (and/or potential) losses becomes more delicate; in practice we found it harder to keep training stable without extensive tuning."
    },
    {
        "question": "What are the potential challenges of combining local SOP and global SOP in extracting meaningful image features?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "The state s is defined based on the ultrasound image. We have adopted an image quality classification network from our previous work , which used ResNet50 as a base network with multi-scale and higher-order processing of the image for conducting the holistic assessment of the image quality. The block diagram of this network is shown in Fig. 2. This classifier first extracts features at multiple scales to encode the inter-patient anatomical variations. Then, it uses second-order pooling (SoP) in the intermediate layers (local) and at the end of the network (global) to exploit the second-order statistical dependency of features. The local-to-global SoP will capture the higher-order relationships between different spatial locations and provide the seed for correlating local patches. This network encodes the image into a feature vector of size 2048, which represents the state of the policy."
        ],
        "final_answer": "Combining local and global second-order pooling (SoP) poses challenges such as increased computational complexity, potential feature redundancy, and the need for careful hyperparameter tuning. It demands substantial data to effectively handle multi-scale features while ensuring the model’s robustness. Additionally, balancing local and global information without conflicts can complicate optimization, particularly in real-time medical applications.",
        "relevant_elements": [
            "local SOP",
            "global SOP"
        ],
        "id": 4010,
        "masked_question": "What are the potential challenges of combining [mask1] and [mask2] in extracting meaningful image features?",
        "masked_number": 2,
        "masked_elements": [
            "local SOP",
            "global SOP"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.02337v1_figure_2.png",
        "paperid": "2409.02337v1",
        "paper_path": "./papers/2409.02337v1.json",
        "figure_id": "2409.02337v1_figure_2.png",
        "caption": "State space representation using a deep convolution neural network",
        "qtype": "Others",
        "response": "First, let me restate what the two masked modules are, by aligning the picture with the text:\n\n  • [mask1] (red box) is the “Local SoP” block – a second‐order pooling layer applied in the intermediate stages to capture patch‐level (local) feature statistics.  \n  • [mask2] (blue box) is the “Global SoP” block – a second‐order pooling layer applied at the very end of the network to capture whole‐image (global) feature statistics.\n\nNow: what difficulties arise when you try to combine a local SoP and a global SoP in the same feature‐extraction pipeline?\n\nChain-of-Thought:\n\n1. Second‐order pooling by itself is computationally heavy: it requires forming (and often normalizing or even decomposing) covariance matrices of the incoming feature maps.  \n2. If you do it locally (at multiple spatial patches) *and* again globally (over the entire feature map), you end up with two very high‐dimensional representations.  Even if you reduce each with, say, a matrix square‐root or PCA, the raw covariance dimension grows quadratically in the number of channels.  \n3. Fusing or concatenating a local‐SoP descriptor and a global‐SoP descriptor is nontrivial: they live in different “spaces” (one aggregates over small windows, the other over the whole map), so you need careful normalization, weighting or gating to prevent one from swamping the other.  \n4. From an optimization standpoint, back‐propagating through two separate second‐order operations can be numerically delicate—gradients can explode or vanish unless you use special normalization tricks (e.g. matrix log or iterative normalization).  \n5. Finally, having two separate high‐capacity SoP modules risks overfitting, especially if your dataset isn’t huge: you’ve effectively doubled the number of covariance parameters your network must learn to handle.\n\nPutting it all together, the key challenges are:\n\n  1. **Computational and memory cost**: forming and storing two sets of large covariance descriptors.  \n  2. **High dimensionality / feature explosion**: the quadratic growth in representation size for both modules.  \n  3. **Fusion difficulty**: aligning, normalizing, and weighting local vs. global statistics so that neither dominates.  \n  4. **Numerical stability**: ensuring stable gradients through back‐prop of two separate second‐order operations.  \n  5. **Risk of overfitting**: doubled parameter complexity from two SoP layers without a correspondingly large dataset."
    },
    {
        "question": "How does tree attention mask interact with merged sequence?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The traditional causal attention masks are designed for linear sequences, where each token attends to all previous tokens, restricting speculative decoding to verifying one sequence at a time. However, as the sequence lengthens during draft token generation, the number of potential continuations increases. For example, in the draft tree in Figure 2, the token following 'guest' could be 'speaker' or 'speak', while both 'at' and 'for' could follow 'speaker'. This creates a need to verify multiple draft sequences simultaneously. Tree attention modifies the attention mask to address this by compressing multiple sequences into a single merged sequence, such as ['guest', 'speaker', 'speak', 'at', 'for', 'ings'], while preserving a tree structure in the tree attention mask. Each child node attends only to its parent nodes, preventing sibling tokens from interfering with each other. After the LLM processes the merged sequence, all possible sequences such as 'guest speaker', 'guest speaker at', 'guest speaker for', and 'guest speak', along with their corresponding output tokens, are extracted based on the tree structure and verified in parallel."
        ],
        "final_answer": "The tree attention mask compresses multiple sequences into a single merged sequence while preserving a tree structure. Within this structure, each child node attends only to its parent nodes, preventing sibling tokens from interfering with each other. This allows the LLM to process and verify all possible sequences in parallel.",
        "relevant_elements": [
            "tree attention mask",
            "merged sequence"
        ],
        "id": 4011,
        "masked_question": "How does [mask1] interact with [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "tree attention mask",
            "merged sequence"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.08696v1_figure_2.png",
        "paperid": "2408.08696v1",
        "paper_path": "./papers/2408.08696v1.json",
        "figure_id": "2408.08696v1_figure_2.png",
        "caption": "An overview of Token Recycling. The adjacency matrix, initialized by inheriting from the previous query, stores candidate tokens. Token Recycling first retrieves a draft tree from the matrix based on the last token of the current content. The tree is then compressed into a merged sequence with a corresponding tree attention mask and sent to the LLM for a forward pass. After processing, all possible draft sequences are extracted and verified. The longest correct sequence is selected and added to the content. Finally, the top-k candidate tokens are used to update the matrix for the next iteration.",
        "qtype": "Literature_Background",
        "response": "Here is the step-by-step of how the red-boxed “Tree Attention Mask” feeds into the blue-boxed “Merged Sequence/Model-Forward” stage:\n\n1. Draft-Tree construction (Stage 2, left):  \n   – We pull out a small tree of candidate continuations (e.g. guest → speaker or speak → at/for → ings/…).\n\n2. Mask creation (Stage 2, right – red box):  \n   – From that draft tree we build a “tree attention mask,” a matrix that tells us exactly which token may attend to which other token (each child only to its parent, no sibling cross-talk).\n\n3. Sequence merging (Stage 3, blue box):  \n   – We flatten all of those tree paths into one “Merged Sequence” (e.g. [guest, speaker, speak, at, for, ings]).  \n\n4. Forward pass with tree mask (Stage 3, blue box):  \n   – When we run that merged sequence through the LLM, we do *not* use the normal left-to-right causal mask. Instead we *apply* the red-boxed tree attention mask over the blue-boxed merged inputs.  \n\n5. Effect:  \n   – The mask gates the self-attention so every token in that big merged batch only “sees” its allowed tree ancestors (parents), exactly as defined by the red-boxed mask.  \n\nIn short, the red-boxed Tree-Attention-Mask is overlaid on the blue-boxed merged sequence during the model’s forward‐pass, enforcing the parent‐only attention pattern you need to verify all k draft paths in parallel."
    },
    {
        "question": "What are the benefits of using channel-wise concatenation in the processing of vision feature?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "We notice that existing popular fusion strategies, despite their variations in designs, can be broadly represented by the following several categories: (1) Sequence Append: directly appending the visual tokens from different backbones as a longer sequence; (2) Channel Concatenation: concatenating the visual tokens along the channel dimension without increasing the sequence length; (3) LLaVA-HR: injecting high-resolution features into low-resolution vision encoders using mixture-of-resolution adapter; (4) Mini-Gemini: using the CLIP tokens as the low resolution queries to cross-attend another high-resolution vision encoder in the co-located local windows.  Although sequence append shows comparable performance to channel concatenation, it faces the challenge to handle more vision encoders due to the increasing sequence length. Hence, we choose direct channel concatenation as our fusion strategy considering its performance, expandability, and efficiency."
        ],
        "final_answer": "The benefits of using channel-wise concatenation in vision feature processing include achieving the best average performance, maintaining better throughput compared to sequence append, and offering performance, expandability, and efficiency.",
        "relevant_elements": [
            "vision feature"
        ],
        "id": 4012,
        "masked_question": "What are the benefits of using channel-wise concatenation in the processing of [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "vision feature"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.15998v1_figure_2.png",
        "paperid": "2408.15998v1",
        "paper_path": "./papers/2408.15998v1.json",
        "figure_id": "2408.15998v1_figure_2.png",
        "caption": "Overview of the Eagle exploration pipeline.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1.  In the figure, the red‐boxed module labeled “Vision Feature” is exactly where the outputs of each vision expert (flattened and interpolated feature maps) are being fused.  \n2.  In the text this is described as fusion strategy (2) “Channel Concatenation,” i.e. stacking the different experts’ token vectors along the channel dimension rather than simply tacking them on end as a longer sequence.  \n3.  From Table 4 and its discussion we know that channel‐wise concatenation  \n    •   yields the highest average accuracy across the benchmarks,  \n    •   avoids blowing up the sequence length (so throughput stays high),  \n    •   is far more scalable when you want to add still more experts (sequence-length-based methods get prohibitively slow), and  \n    •   remains computationally and memory-efficient.  \n\nSo, by concatenating along the channel axis you get better overall performance, maintain faster inference (higher throughput), and keep the design easily expandable to more vision encoders—all without the quadratic costs that come from longer token sequences."
    },
    {
        "question": "How does the tailored zero-shot score contribute to the efficiency of neural architecture search?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To enable a more accurate assessment of our hybrid networks, we integrate two selected zero-shot metrics. Given the significant difference in score magnitudes between these metrics, as shown in Figures 3(b) and 3(c), we focus on relative rankings rather than score magnitudes. Specifically, for a group of networks, the score of our tailored zero-shot metric for a specific network is determined by the relative ranking of its Zen-Score within the group. For instance, if a network exhibits the highest Zen-Score, its term yields a value of 1. The effectiveness of our tailored metric is validated through Table II and Figure 3, which demonstrate the highest Kendall-Tau Correlation. Additionally, this metric contributes to enhanced search efficiency due to the swift computational speed of both NN-Degree and Zen-Score. For example, assessing accuracy for an individual hybrid model from our supernet takes an average of several seconds, whereas computing our tailored zero-shot metric requires less time, making it over X times faster when tested on CIFAR100 and profiled on an NVIDIA GeForce RTX 2080Ti."
        ],
        "final_answer": "The tailored zero-shot score contributes to neural architecture search efficiency by enabling faster assessment due to its swift computational speed. The computation of the tailored zero-shot metric is significantly faster than assessing the accuracy of individual hybrid models derived from the supernet, leading to enhanced search efficiency.",
        "relevant_elements": [
            "tailored zero-shot score",
            "neural architecture search"
        ],
        "id": 4013,
        "masked_question": "How does the [mask1] contribute to the efficiency of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "tailored zero-shot score",
            "neural architecture search"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.04829v1_figure_2.png",
        "paperid": "2409.04829v1",
        "paper_path": "./papers/2409.04829v1.json",
        "figure_id": "2409.04829v1_figure_2.png",
        "caption": "The overview of our NASH framework, where we integrate both the neural architecture search (NAS) and coarse-to-fine accelerator search to directly obtain optimal pairing of models and accelerators. Specifically, the NAS consists of a tailored zero-shot metric to pre-identify promising multiplication-reduce hybrid models before supernet training. Besides, the accelerator search involves a novel coarse-to-fine search strategy to expedite the accelerator search process.",
        "qtype": "Experimental_Results",
        "response": "In our co‐search loop (see the left half of the big figure), the red-boxed “Tailored Zero-Shot Score” is used to rapidly triage hundreds or thousands of possible hybrid networks before we ever touch the chip simulator.  Concretely:\n\n  1.   We compute our zero-shot score (a fusion of NN-Degree and Zen-Score ranking) for every candidate sub-network in the supernet.  \n  2.   We sort them by that score and only keep the top-K proposals for the next stage.  \n  3.   Only those K sub-networks are then sent into the blue-boxed “Hardware Performance” evaluator (the cycle-accurate simulator on our accelerator) to measure latency, throughput, energy, etc.  \n\nBecause the zero-shot score is virtually free (no back-prop, no full training, no expensive region-counting), it prunes away the vast majority of bad designs up front.  That means we run the slow hardware simulator only on a few dozen (rather than hundreds or thousands of) candidates.  The net effect is a 10×–100× reduction in end-to-end NAS cost and huge savings in hardware evaluation time."
    },
    {
        "question": "How does Recursive Token Merging interact with Self Attention module to enhance video consistency?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "TALO strategy perturbs each benign frame of video separately. This per-frame optimization makes the frames likely optimized along different adversarial directions resulting in motion discontinuity and temporal inconsistency. Furthermore, separately perturbing each benign frame reduces the monotonous gradients because the interactions among the frames are not exploited. To this end, we introduce a recursive token merging (ReToMe) strategy that recursively matches and merges similar tokens across frames together enabling the self-attention module to extract consistent features. In the following, we first provide the basic operation of token merging and token unmerging and then our recursive token merging algorithm.Token Merging (ToMe) is first applied to speed up diffusion models through several diffusion-specific improvements . Generally, tokens T are partitioned into a source (s⁢r⁢c) and destination (d⁢s⁢t) set. Then, tokens in s⁢r⁢c are matched to their most similar token in d⁢s⁢t, and r most similar edges are selected subsequently. Next, we merge the connected r most similar tokens in s⁢r⁢c to d⁢s⁢t by replacing them as the linked d⁢s⁢t tokens. To keep the token number unchanged, we divide merged tokens after self-attention by assigning their values to merged tokens in s⁢r⁢c.A self-attention module takes a sequence of input and output tokens across all frames. To partition tokens across frames into src and dst, we define stride as B. We randomly choose one out of the first B frames (e.g., the g-th frame), and select the subsequent frames every B interval into the dst set.Nevertheless, during the merging process expressed above, tokens in dst are not merged and compressed. To maximally fuse the inter-frame information, we recursively apply the above merging process to tokens in dst until they contain only one frame. Our ReToMe has three advantages. Firstly, ReToMe ensures that the most similar tokens share identical outputs, maximizing the compression of tokens. This approach fosters internal uniformity of features across frames and preserves temporal consistency, thereby effectively achieving temporal imperceptibility. Secondly, the merged tokens decrease interaction inside adversarial perturbations, effectively preventing overfitting on the surrogate model. Furthermore, the tokens linked to merged tokens facilitate inter-frame interaction in gradient calculation, which may induce more robust and diverse gradients. Therefore, ReToMe can effectively boost adversarial transferability."
        ],
        "final_answer": "Recursive Token Merging interacts with the Self Attention module by recursively matching and merging similar tokens across frames, enabling the Self Attention module to extract consistent features. This method ensures that the most similar tokens share identical outputs, which maximizes internal uniformity of features across frames and preserves temporal consistency, thereby enhancing video consistency.",
        "relevant_elements": [
            "Recursive Token Merging",
            "Self Attention module"
        ],
        "id": 4014,
        "masked_question": "How does [mask1] interact with [mask2] to enhance video consistency?",
        "masked_number": 2,
        "masked_elements": [
            "Recursive Token Merging",
            "Self Attention module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.05479v1_figure_2.png",
        "paperid": "2408.05479v1",
        "paper_path": "./papers/2408.05479v1.json",
        "figure_id": "2408.05479v1_figure_2.png",
        "caption": "Framework overview of the proposed ReToMe-VA. For a video clip, DDIM inversion is applied to map the benign frames into the latent space. Timestep-wise Adversarial Latent Optimization is employed during the DDIM sampling process to optimize the latents. Throughout the whole pipeline, Recursive Token Merging and Recursive Token Unmerging Modules are integrated into the diffusion model to enhance its effectiveness. Additionally, structure loss is utilized to maintain the structural consistency of video frames. Ultimately, the resulting adversarial video clip is capable of deceiving the target model.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the Recursive Token Merging block (the red‐boxed module) and Mask₂ is the Self-Attention layer (the blue‐boxed module).  What the paper does is first use Recursive Token Merging to collapse (i.e. merge) semantically similar tokens from different frames into a shared set of tokens.  Those fused tokens are then fed into the Self-Attention layer, which now computes its queries, keys, and values over a mixture of frames rather than treating each frame in isolation.  Because Self-Attention is operating on merged, cross-frame tokens, it learns updates that are inherently synchronized across time.  After attention is applied, the tokens are “unmerged” back into their per-frame positions, carrying with them the consistent, jointly computed features.  In this way, the merge → self-attention → unmerge pipeline forces the network to process and update frames in lockstep, which enforces temporal coherence and produces adversarial videos without frame‐to‐frame flicker."
    },
    {
        "question": "What is the importance of iterative parameter updating in retraining scheduling?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "It asynchronously reuses learned features from different subtasks and incorporates dynamic switching and incremental parameter updating to optimize the limited representation capacity of compressed mobile DNNs"
        ],
        "final_answer": "The importance of iterative parameter updating in retraining scheduling is to optimize the limited representation capacity of compressed mobile DNNs by incorporating dynamic switching and incremental parameter updating.",
        "relevant_elements": [
            "iterative parameter updating",
            "retraining scheduling"
        ],
        "id": 4015,
        "masked_question": "What is the importance of [mask1] in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "iterative parameter updating",
            "retraining scheduling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2407.00016v1_figure_1.png",
        "paperid": "2407.00016v1",
        "paper_path": "./papers/2407.00016v1.json",
        "figure_id": "2407.00016v1_figure_1.png",
        "caption": "Illustration of AdaBridge s system workflow.",
        "qtype": "Design_Rationale",
        "response": "Let’s first unpack what you’re pointing to in the picture:\n\n  1. The red‐outlined area (mask1) is the little “Task module” block together with its “iterative parameter updating” loop.  \n  2. The blue‐outlined region (mask2) is the entire “Asynchronous multi‐task retraining computation scheduling” component on the edge server.  \n\nChain of thought:  \n- In the text they describe this blue area as an adapter‐based, plug-and-play multi‐task learning framework that must (a) reuse features from different subtasks, (b) switch services dynamically, and (c) incrementally update parameters.  \n- Within that framework, the red‐boxed Task module is exactly where those per‐task parameter updates happen, in an asynchronous, iterative fashion.  \n- Without that piece, you couldn’t actually refine each task’s weights on the newly gathered data or fuse/separate tasks on the fly.  \n\nAnswer:\nThe Task module with its iterative parameter‐updating loop (the red box) is the core mechanism inside the asynchronous multi‐task retraining scheduler (the blue box) that actually carries out each subtask’s learning steps.  It ensures that, even though features are being shared and reused across tasks via adapters, each task can still incrementally refine its own parameters in an asynchronous, plug-and-play fashion—enabling the whole scheduler to adapt quickly, efficiently reuse computation, and keep multiple tasks in sync without retraining from scratch."
    },
    {
        "question": "What impact does incorporating physical constraint loss have on the predictions of LSTM block?",
        "relevant_section_ids": [
            "2.5"
        ],
        "relevant_context": [
            "Energy conservation asserts that in a conservative system, the total energy remains constant over time. This concept is particularly relevant in systems where external energy exchanges are absent. To quantify alignment with energy conservation principles, we define an energy conservation loss function,ℒenergy, which measures the discrepancy between the energy states of the input and output fields. This function is integrated into the overall loss function to enhance the adherence of the model to energy conservation."
        ],
        "final_answer": "The importance of iterative parameter updating in retraining scheduling is to optimize the limited representation capacity of compressed mobile DNNs by incorporating dynamic switching and incremental parameter updating.",
        "relevant_elements": [
            "physical constraint loss",
            "LSTM block"
        ],
        "id": 4016,
        "masked_question": "What impact does incorporating [mask1] have on the predictions of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "physical constraint loss",
            "LSTM block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.00458v1_figure_1.png",
        "paperid": "2409.00458v1",
        "paper_path": "./papers/2409.00458v1.json",
        "figure_id": "2409.00458v1_figure_1.png",
        "caption": "Schematic representation of physics-constrained CED-LSTM model employing Voronoi tessellation for enhanced state field mapping from sparse observations.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify the two masked items in the schematic:  \n  1. The red-boxed term ([mask1]) is the “Physical Constraint Loss” (in our case the energy‐conservation loss).  \n  2. The blue-boxed modules ([mask2]) are the LSTM blocks that generate the future latent representations.  \n\nChain of thought:  \n- In the Physics-constrained CED-LSTM, the LSTM blocks predict a sequence of future latent vectors, which are then decoded back into full state fields.  \n- If we train with only the MSE loss, the LSTM will learn to minimize point-wise differences but may drift from physically admissible states (e.g. it may create or destroy energy).  \n- By adding the Physical Constraint Loss, we explicitly penalize any mismatch in total energy between the input and the decoded output.  \n- During back-propagation this extra loss term alters the LSTM’s weight updates so that its predictions, once decoded, also satisfy the energy‐conservation law.  \n- The net effect is that the LSTM’s forecasts become both more accurate (lower MSE) and more physically consistent, with much less long-term drift in energy and better stability in rolling forecasts.  \n\nAnswer:  \nIncorporating the Physical Constraint Loss forces the LSTM blocks to produce latent predictions whose decoded fields obey energy conservation.  This additional penalty leads to lower overall error, greatly reduced drift in total energy, and more stable, physically realistic long‐term forecasts."
    },
    {
        "question": "What are the specific functions of the RPN and the ROIHead in the Detector",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Similarly, Meta R-CNN combines a two-stage detector and reweights RoI features in the detection head. Attention-RPN exploits matching relationship between the few-shot support set and query set with a contrastive training scheme, which can then be applied to detect novel objects without retraining and fine-tuning."
        ],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "the RPN and the ROIHead",
            "Detector"
        ],
        "id": 4017,
        "masked_question": "What are the specific functions of the [mask1] in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "the RPN and the ROIHead",
            "Detector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2408.05674v1_figure_2.png",
        "paperid": "2408.05674v1",
        "paper_path": "./papers/2408.05674v1.json",
        "figure_id": "2408.05674v1_figure_2.png",
        "caption": "The overview of the proposed Prototype-based Soft-labels and Test-Time Learning (PS-TTL) framework for FSOD. Both the student and teacher networks are first initialized by the few-shot detector and then fine-tuned on test data. The teacher network takes test data as input to generate pseudo-labels, while the student model is trained using these pseudo-labels after post-processing with N-way K-shot data as supervision signals and updates the teacher net- work through EMA. A Prototype-based Soft-labels (PS) strategy is adopted to maintain class prototypes and compute the feature similarity between low-confidence pseudo-labels and class prototypes to replace them with soft-labels.",
        "qtype": "Implementation_Details",
        "response": "In the figure, the large blue‐outlined block (“[mask2]”) is the two‐stage detector in the teacher network (essentially a Faster-R-CNN style head), and the little red box inside it (“[mask1]”) is the Region Proposal Network (RPN).  Concretely, the RPN’s job is to take the backbone’s convolutional feature map and  \n  \n  1. slide a small network over it to predict, at each spatial location and for a set of predefined anchors,  \n     – an “objectness” score (is there *some* object here?)  \n     – a set of bounding‐box regressions (to refine those anchors into tighter boxes)  \n  2. propose the top-scoring box candidates (usually after non-maximum suppression)  \n  3. hand those candidate regions (the “proposals”) off to the downstream ROI head for final classification and box-refinement."
    },
    {
        "question": "How are EEG and adversarial example integrated into the model training process?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Adversarial perturbations are image transformations capable of fooling ANNs while remaining imperceptible for humans. To assess the adversarial robustness of our models, we employed Foolbox to create adversarial versions of the 1654 original validation images under different attack strengths."
        ],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "EEG",
            "adversarial example"
        ],
        "id": 4018,
        "masked_question": "How are [mask1] and [mask2] integrated into the model training process?",
        "masked_number": 2,
        "masked_elements": [
            "EEG",
            "adversarial example"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.03646v1_figure_1.png",
        "paperid": "2409.03646v1",
        "paper_path": "./papers/2409.03646v1.json",
        "figure_id": "2409.03646v1_figure_1.png",
        "caption": "Paradigm for improving adversarial robustness via co-training with human EEG: We first trained dual-task learning (DTL) models with original and shuffled EEG data and then evaluated their robustness against various adversarial attacks. We trained four clusters of ResNet50 backbone models, each incorporating a different independent EEG predictor: Dense Layers (CNN), Recurrent Neural Networks (RNN), Transformer, and Attention layers. Finally, we measured the relationship between adversarial robustness gain and EEG prediction accuracy.",
        "qtype": "Literature_Background",
        "response": "In our setup “mask₁” is nothing exotic – it is simply the EEG‐prediction head that we hang off of the ResNet50 feature extractor – and “mask₂” is just the adversarially perturbed image.  Concretely:\n\n1. Mask₁ (the red‐boxed EEG trace)  \n   • During training we take the intermediate feature map from ResNet-50 and feed it into a small EEG‐predictor network (we experimented with CNN, RNN, Transformer, Attention heads, etc.).  \n   • We jointly optimize two losses:  \n     – the usual cross‐entropy for image‐classification, and  \n     – a regression (or MSE) loss pushing the EEG‐head to reproduce the human EEG waveform recorded while subjects viewed that very same image.  \n   • Because both heads share the same backbone, the network learns features that are good both for classifying the image and for predicting its neural signature.\n\n2. Mask₂ (the blue‐boxed “adversarial example”)  \n   • After training that dual‐task model on clean data + EEG, we freeze it and then generate adversarial examples by adding small, norm‐bounded perturbations to held‐out test images.  \n   • Those perturbed images (mask₂) are fed into the trained model’s classification branch **only** to measure how much the classification accuracy degrades.  We do **not** re‐train on those adversarial inputs – they are purely used for post-hoc robustness evaluation."
    },
    {
        "question": "How are MLP and attention mechanism utilized to process utterance and description embeddings?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "This architecture is designed with a straightforward target that injects the personality information of each speaker into their corresponding utterances by a multi-layer perceptron network.Through this mechanism, all the utterances from the same speaker are shared in the unified speaker vector representation, while the weights are updated in the training process. Finally, the utterance vector is fused with the speaker vector which supports emotional classification.We consider a variant of our BiosERC model, which is engineered to dynamically incorporate the speaker’s information into each utterance via the attention mechanism. The relationship between the current utterance and all individual speakers is integrated to enrich the utterance vector representation."
        ],
        "final_answer": "In BiosERC, a multi-layer perceptron (MLP) network injects personality information of speakers into their corresponding utterances, creating a unified speaker vector representation. The attention mechanism dynamically incorporates speaker information into each utterance, modeling the relationship between the utterance and all speakers in a conversation to enrich the utterance vector representation.",
        "relevant_elements": [
            "MLP",
            "attention mechanism"
        ],
        "id": 4019,
        "masked_question": "How are [mask1] and [mask2] utilized to process utterance and description embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "MLP",
            "attention mechanism"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2407.04279v1_figure_2.png",
        "paperid": "2407.04279v1",
        "paper_path": "./papers/2407.04279v1.json",
        "figure_id": "2407.04279v1_figure_2.png",
        "caption": "Overview of our BiosERC model architecture.",
        "qtype": "Literature_Background",
        "response": "Let’s unpack the little red‐ and blue‐boxed modules on the right of Fig. 2.  Both are simply two different ways of “injecting” a speaker’s biography embedding into the utterance embedding before classification:\n\n1.  [mask1] is the red‐boxed MLP.  \n    – We take the current utterance vector h_i^utt and the corresponding speaker description vector h_j^desc, concatenate (or otherwise stack) them, and feed them into a small feed‐forward network.  \n    – The MLP learns to project that [h_i^utt; h_j^desc] pair into a single fused vector which then replaces the plain speaker vector in the downstream classification layer.  \n\n2.  [mask2] is the blue‐boxed Attention.  \n    – Here we treat h_i^utt as the “query” and all speaker description vectors {h_k^desc} as “keys” and “values.”  \n    – A standard attention (scaled dot‐product) computes weights over the descriptions and forms a weighted sum: the attended fusion vector.  \n    – That attended vector then takes the role of the speaker embedding in the final emotion classifier.  \n\nIn short, the red box ([mask1]) is the MLP‐fusion variant, and the blue box ([mask2]) is the cross‐attention fusion variant for marrying utterance and biography embeddings."
    },
    {
        "question": "What role does FM play in shared decoder?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Nevertheless, while multi-stage guidance proves beneficial in extracting valuable information from features at various levels, it is more challenging to maximize the mutual information between the conditional contrasts and the target MR contrast distributions. This is mainly due to intricate dependencies between multi-contrast imaging and finding more common and mutually adaptive feature representation.To overcome this challenge, we propose an adaptive feature maximize (FM) within the denoising network, unifying feature distributions as shown in Fig. 1(C).The distinction between local and global feature contrasts derived from the denoising and conditional feature distributions aids in adaptively assigning weights to more pertinent features. This adaptive weighting facilitates the selection of mutually dependent and highly effective shared representations within the latent distribution. Consequently, these representations can be leveraged to achieve more precise denoised target contrast."
        ],
        "final_answer": "The adaptive feature maximizer unifies feature distributions by utilizing encoded features from the Semantic Encoder and Diffusive Encoder, which undergo separate local and global feature extraction processes. It assigns weights based on feature relevance to facilitate the selection of mutually adaptive and effective shared representations, ultimately leading to more precise denoised target contrast.",
        "relevant_elements": [
            "FM",
            "shared decoderm"
        ],
        "id": 4020,
        "masked_question": "What role does [mask1] play in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "FM",
            "shared decoderm"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2409.00585v1_figure_1.png",
        "paperid": "2409.00585v1",
        "paper_path": "./papers/2409.00585v1.json",
        "figure_id": "2409.00585v1_figure_1.png",
        "caption": "Network architecture of McCaD. A: Overall Architecture, B: Multi-scale Feature Guided Denoising Network to incorporate feature characteristics from conditional MRI contrasts at various stages to guide the reverse diffusion process, C: Adaptive Feature Maximizer, to weights more pertinent features within the latent space D: Feature Attentive Loss to improve the perceptual quality of the synthetic results.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-Thought Reasoning:\n\n1. In the figure, the red-boxed module is labeled “FM” and in the text is referred to as the “Adaptive Feature Maximizer (FM).”  \n2. The blue-boxed module is the “Multi-contrast Synthesizer (Shared Decoder D),” i.e. the common decoder that actually reconstructs the denoised MR contrast.  \n3. Text (Sec. 3.4) explains that the two encoder branches (the Denoising Encoder and the Semantic Encoder) each produce features at multiple scales.  \n4. These features go into FM, which separately extracts local (via convolution) and global (via global pooling) feature statistics, then uses small adaptive (“feature adaptive”) sub‐modules to compute a weight for each feature stream.  \n5. FM then fuses (concatenates) the weighted local and global features into a single latent representation Z_I (eq. 6).  \n6. This fused representation Z_I is what the shared decoder (“Multi-contrast Synthesizer”) consumes, via skip connections and channel‐wise concatenation, to guide the final image synthesis.  \n\nAnswer:\n\nThe Adaptive Feature Maximizer sits immediately before the shared decoder and its job is to take the multi-scale features from both encoders, compute per‐feature relevance weights (local vs. global), and fuse them into a single latent conditioning vector (Z_I). That fused, adaptively weighted feature map is then fed into the Multi-contrast Synthesizer (the shared decoder), ensuring that the decoder has the most informative, mutually relevant features to produce high-quality multi-contrast MR images."
    },
    {
        "question": "How does the self-attention module contribute to the global alignment loss based on the results?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Self-attention",
            "Global alignment loss"
        ],
        "id": 572,
        "masked_question": "How does the [mask1] module contribute to the global alignment loss based on the results?",
        "masked_number": 1,
        "masked_elements": [
            "Self-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00609v1_figure_1.png",
        "paperid": "2411.00609v1",
        "paper_path": "./papers/2411.00609v1.json",
        "figure_id": "2411.00609v1_figure_1.png",
        "caption": "Figure 1: The Proposed MRI-Report Contrastive Learning Framework",
        "qtype": "Experimental_Results",
        "response": "Mask1 in Figure 1 is the self‐attention layer inserted on top of the last 3D‐ResNet block.  Its sole role in the global‐alignment loss is to take the raw convolutional feature–map and turn it into a single “globally attended” MRI embedding.  Concretely:\n\n1.  The output of the fourth ResNet module (a 512‐channel feature map) is fed through the self‐attention head (query/key/value 3D convolutions, scaled‐dot‐product, residual add).  \n2.  That attention-weighted feature map is pooled and passed through the small MLP “projection” head, producing a 512-dimensional global MRI vector.  \n3.  This vector is treated as the image anchor in the triplet‐based global contrastive loss and is pulled close to its paired text CLS vector (and pushed away from unpaired ones, with the tumor‐location weighting applied).\n\nIn the ablation studies, removing or crippling this self‐attention output (e.g. dropping the global‐alignment term entirely) causes the downstream AUC and attention‐mask Dice scores to drop sharply.  Thus the self‐attention module is exactly what produces the “global” image representation that the model then aligns—via triplet loss—with the report’s CLS embedding, and its inclusion is responsible for the marked gains in both global alignment and final classification performance."
    },
    {
        "question": "What motivates attention-based Modal Fusion for integrating diverse modal-specific representations?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Specifically, for each entity, we measure the distinct importance of its modality information with attention mechanism, and employ the attention weights to integrate modal-specific feature variables (sampled from Eq. (6)) as follows:",
            "where α_m is the attention weight for modality m, taking the different nature of entities into consideration.",
            "In this way, we obtain modal-hybrid feature variables considering the distinct modality importance of the entity and leverage the IB-refined modal-specific feature variables."
        ],
        "final_answer": "Because different entities rely on their modalities to varying degrees, the model uses an attention mechanism to dynamically measure and weight each modality’s contribution. This attention-based fusion ensures that the modal-hybrid representation integrates modal-specific features in proportion to their importance for each entity.",
        "relevant_elements": [
            "Modal Fusion",
            "Modal-specific Features"
        ],
        "id": 601,
        "masked_question": "What motivates attention-based [mask1] for integrating diverse modal-specific representations?",
        "masked_number": 1,
        "masked_elements": [
            "Modal Fusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19302v1_figure_2.png",
        "paperid": "2407.19302v1",
        "paper_path": "./papers/2407.19302v1.json",
        "figure_id": "2407.19302v1_figure_2.png",
        "caption": "Figure 2. The framework of the proposed IBMEA for the multi-modal entity alignment task.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1. After the variational encoder step, we end up with one feature vector per modality (graph, image, relation, attribute).  \n2. However, not every modality is equally informative for every entity pair—some images might be low-quality, some relations sparse, etc.  \n3. We therefore need a way to “mix” these four vectors into a single entity embedding that up-weights the strong modalities and down-weights the noisy or less relevant ones.  \n4. That is exactly what the red-boxed module does: it is an attention-based fusion (sometimes called an attention-based modal aggregator or modal fusion layer).  \n5. Concretely, it learns a per-modality attention weight αₘ for each entity, and then forms the hybrid vector as the weighted sum  \n   z_h = ∑ₘ αₘ · zₘ.  \n6. In short, the heterogeneity and varying informativeness of the four modalities motivates using an attention-based fusion so that the final representation automatically emphasizes the most alignment-relevant modality(ies)."
    },
    {
        "question": "How does the Submodular function integrate Clue Score and Collaboration Score to rank sub-regions iteratively?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Clue Score: An essential aspect of interpretability is enabling the object-level foundation model to accurately locate and identify objects while using fewer regions. … Then, the clue score of sub-region s is defined as: (Equation for f_clue).",
            "Collaboration Score: Some regions may exhibit strong combination effects, meaning they contribute effectively to model decisions only when paired with multiple specific sub-regions. … Therefore, we introduce the collaboration score f_collab to assess sub-regions with high sensitivity to decision outcomes.",
            "Submodular Function: The scores above are combined to construct a submodular function f, as follows:",
            "Saliency Map Generation: Using the above submodular function, a greedy search algorithm is applied to sort all sub-regions in V, yielding an ordered subset S. Introducing the submodular function enables the search algorithm to more precisely identify key visual regions for interpretation.",
            "Since the saliency map requires all sub-regions to be ranked, k can be set to |V| to compute ordered subsets. When the set function f satisfies the properties of diminishing returns and monotonic non-negative, a greedy search guarantees an approximate optimal solution [7]."
        ],
        "final_answer": "The method first computes two scores per super-pixel: a Clue Score (how much that region alone supports detecting the target) and a Collaboration Score (how much removal of that region degrades detection in combination with others). It then defines a single submodular set function f by combining these two scores over any subset of regions. Because f is monotonic and has diminishing returns, a standard greedy algorithm can be used: at each iteration the region whose addition yields the largest marginal increase in f is selected next. Repeating this until all regions are chosen produces a ranked list of sub-regions by importance.",
        "relevant_elements": [
            "Submodular function",
            "Clue Score",
            "Colla. Score"
        ],
        "id": 604,
        "masked_question": "How does the [mask1] integrate [mask2] and Collaboration Score to rank sub-regions iteratively?",
        "masked_number": 2,
        "masked_elements": [
            "Submodular function",
            "Clue Score"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16198v1_figure_2.png",
        "paperid": "2411.16198v1",
        "paper_path": "./papers/2411.16198v1.json",
        "figure_id": "2411.16198v1_figure_2.png",
        "caption": "Figure 2: Framework of the proposed Visual Precision Search method for interpreting an object-level foundation model. The input is first sparsified into a set of sub-regions and then interpreted across different instances. A submodular function guides the search for significant sub-regions, updating the ordered subset iteratively, and ultimately generating the instance-level attribution map.",
        "qtype": "Implementation_Details",
        "response": "Here’s the high-level picture:\n\n1.  The red-boxed “submodular function” f(·) is defined as a weighted combination of the blue-boxed Clue Score C(·) and the Collaboration Score D(·).  In practice one simply sets  \n   \n   f(S) = α · ∑_{v∈S} C(v)   +   (1−α) · ∑_{v∈S} D(v)  \n\n   for an appropriate trade-off weight α∈[0,1].\n\n2.  Because f is non-negative, monotonic and exhibits diminishing returns, we can run the classic greedy submodular maximization:  \n   • Start with S₀=∅.  \n   • At iteration t, for each not-yet-picked sub-region v compute its marginal gain  \n     Δ_f(v|S_{t−1}) = f(S_{t−1} ∪ {v}) − f(S_{t−1}).  \n   • Pick the v with the largest Δ_f and set S_t = S_{t−1} ∪ {v}.  \n\n3.  Repeating this until all regions are ordered gives you an approximate optimum ranking of sub-regions under the joint clue¬+¬collaboration objective.   The sequence in which regions enter S exactly defines their saliency ordering."
    },
    {
        "question": "How does the attribution score assessment compute marginal effects over the Ordered Subset to produce saliency weights?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Saliency Map Generation: Using the above submodular function, a greedy search algorithm is applied to sort all sub-regions in V, yielding an ordered subset S. ... We evaluate the salient difference between the two sub-regions by the marginal effect.",
            "The attribution score α(s_i) for each sub-region s_i in S is assessed by: α(s_i) = f(S_i) − f(S_{i−1}) (+ β for the first sub-region), where β represents a baseline attribution score for the first sub-region, and f denotes the set function.",
            "When a new sub-region is added, a small marginal increase suggests comparable importance to the previous sub-region. A negative marginal effect indicates a counterproductive impact, which can be assessed by its absolute value. Finally, α is normalized to obtain the saliency map of the sub-region."
        ],
        "final_answer": "The attribution score for each sub-region is computed as the marginal gain in the submodular scoring function when adding that region to the set of previously selected regions—i.e., α(s_i)=f(S_i)−f(S_{i−1}), with a small baseline β added for the very first region. These per‐region marginal gains are then normalized across the ordered subset to yield the final saliency weights.",
        "relevant_elements": [
            "Attribution score assessment",
            "Ordered Subset S"
        ],
        "id": 605,
        "masked_question": "How does the [mask1] compute marginal effects over the Ordered Subset to produce saliency weights?",
        "masked_number": 1,
        "masked_elements": [
            "Attribution score assessment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16198v1_figure_2.png",
        "paperid": "2411.16198v1",
        "paper_path": "./papers/2411.16198v1.json",
        "figure_id": "2411.16198v1_figure_2.png",
        "caption": "Figure 2: Framework of the proposed Visual Precision Search method for interpreting an object-level foundation model. The input is first sparsified into a set of sub-regions and then interpreted across different instances. A submodular function guides the search for significant sub-regions, updating the ordered subset iteratively, and ultimately generating the instance-level attribution map.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module is nothing mystical – it is simply computing the “marginal gain” of each super‐pixel in the greedy ordering and then turning those gains into a normalized saliency score.  Concretely, once you have run your submodular greedy search over the set of all sub‐regions V to produce an ordered list  \nS = [v₁, v₂, …, vₖ],  \nyou form the growing prefixes  \nSᵢ = {v₁, …, vᵢ}.  The attribution module then does, for i = 1…k:  \n\n Aᵢ = F(Sᵢ) − F(Sᵢ₋₁)  \n\n(with F(S₀) defined as a small baseline constant for the very first region).  These Aᵢ are the raw “marginal effects” of adding vᵢ; if any Aᵢ is negative one takes its absolute value (to mark a counterproductive effect), and finally all Aᵢ’s are L₁–normalized to sum to 1.  The result is exactly the per‐region saliency weights you see in the output map."
    },
    {
        "question": "How does the submodular function integrate clue and collaboration scores during Visual Precision Search?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Clue Score: An essential aspect of interpretability is enabling the object-level foundation model to accurately locate and identify objects while using fewer regions. … Given a subregion r, the object-level model outputs T instances … Then, the clue score of sub-region r is defined as: (definition of clue score).",
            "Collaboration Score: Some regions may exhibit strong combination effects, meaning they contribute effectively to model decisions only when paired with multiple specific sub-regions. Therefore, we introduce the collaboration score S_collab(r) to assess sub-regions with high sensitivity to decision outcomes: (definition of collaboration score).",
            "Submodular Function: The scores above are combined to construct a submodular function f(S), as follows:"
        ],
        "final_answer": "The submodular function f(S) is formed by directly combining (i.e. summing) the per-region clue scores and collaboration scores. In other words, for any candidate set of superpixel sub-regions S, f(S) aggregates each region’s clue score (measuring how strongly it supports correct detection) together with its collaboration score (measuring how critical it is in combination with other regions), yielding a single submodular objective that balances both individual and joint contributions.",
        "relevant_elements": [
            "Submodular function",
            "Clue Score",
            "Collaboration Score"
        ],
        "id": 606,
        "masked_question": "How does the [mask1] integrate [mask2] and collaboration scores during Visual Precision Search?",
        "masked_number": 2,
        "masked_elements": [
            "Submodular function",
            "Clue Score"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.16198v1_figure_2.png",
        "paperid": "2411.16198v1",
        "paper_path": "./papers/2411.16198v1.json",
        "figure_id": "2411.16198v1_figure_2.png",
        "caption": "Figure 2: Framework of the proposed Visual Precision Search method for interpreting an object-level foundation model. The input is first sparsified into a set of sub-regions and then interpreted across different instances. A submodular function guides the search for significant sub-regions, updating the ordered subset iteratively, and ultimately generating the instance-level attribution map.",
        "qtype": "Literature_Background",
        "response": "Here’s what’s going on, step by step:\n\n1.  During Visual Precision Search we treat every candidate super‐pixel (sub‐region) as carrying two scores  \n    – a “clue score” (how much that piece by itself helps the model re-localize and re-classify the target)  \n    – a “collaboration score” (how much the model’s performance collapses if you remove that piece in the context of the others).  \n\n2.  The thing in the red box—the Submodular function—is simply an additive objective that fuses those two scores into one.  Concretely, if S is the current set of chosen regions, then  \n   \n   F(S)  =  λ · ∑_{i∈S}  ClueScore(i)   +   (1–λ) · ∑_{i∈S}  CollabScore(i)  \n   \n   for some weighting parameter λ∈[0,1].  \n\n3.  Because both ClueScore and CollabScore are non-negative and we combine them linearly, F(·) remains submodular (i.e. it has the diminishing-returns property).  \n\n4.  At each greedy step of Visual Precision Search we pick the new region i that gives the largest marginal gain  \n   \n   F(S ∪ {i}) – F(S)  =  λ·ClueScore(i)  +  (1–λ)·CollabScore(i).  \n\n5.  This single combined score drives the ranking of sub‐regions, ensuring the method picks patches that are both strong individual “clues” to the object and critical collaborators in the full detection decision."
    },
    {
        "question": "How does sparse sub-region segmentation impact submodular function scoring in Visual Precision Search?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "We apply the SLICO superpixel segmentation algorithm [1] to divide the input into n sub-regions, V.",
            "The quality of the search space is determined by sparse division, meaning that both the method of partitioning the input and the number of sub-regions play a crucial role in the faithfulness of the Visual Precision Search."
        ],
        "final_answer": "By breaking the image into a sparse set of super-pixel sub-regions, we define the atomic elements over which our submodular scoring function (combining clue and collaboration scores) operates. The choice of segmentation method and the number of sub-regions directly controls the search space: finer or semantically appropriate sparsification leads to more faithful and accurate region scores under the greedy submodular optimization, whereas poor or overly coarse segmentation degrades the quality and faithfulness of the resulting saliency map.",
        "relevant_elements": [
            "Sparse Sub-region Set V",
            "Submodular function"
        ],
        "id": 607,
        "masked_question": "How does [mask1] segmentation impact submodular function scoring in Visual Precision Search?",
        "masked_number": 1,
        "masked_elements": [
            "Sparse Sub-region Set V"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.16198v1_figure_2.png",
        "paperid": "2411.16198v1",
        "paper_path": "./papers/2411.16198v1.json",
        "figure_id": "2411.16198v1_figure_2.png",
        "caption": "Figure 2: Framework of the proposed Visual Precision Search method for interpreting an object-level foundation model. The input is first sparsified into a set of sub-regions and then interpreted across different instances. A submodular function guides the search for significant sub-regions, updating the ordered subset iteratively, and ultimately generating the instance-level attribution map.",
        "qtype": "Literature_Background",
        "response": "In “Visual Precision Search,” that red‐boxed patch is nothing mystical – it’s simply one of the many superpixel–based sub‐regions into which we’ve chopped the original bounding‐box (using SLICO).  The key point is that our entire submodular scoring machinery treats each of these superpixels as an atomic “element” whose inclusion (or removal) we can test against the detector.  \n\nConcretely:\n\n1.  Mask1 → one superpixel (a small, coherent patch of pixels)  \n2.  Clue score for mask1 measures how much bringing that patch back into the model’s input raises the target object’s IoU × confidence.  \n3.  Collaboration score for mask1 measures how badly the detector collapses (confidence or localization drop) when that patch alone is omitted.  \n4.  Our submodular function f(S) simply adds these two per‐patch scores, but exploits the “diminishing returns” property: as you already include more patches, the marginal gain from mask1 shrinks.  \n\nBecause the segmentation decides (a) how many elements you get and (b) how well each patch aligns with real object parts, it has a direct impact on the size and shape of those clue/collaboration scores.  In practice:\n\n–  If segmentation is too coarse, you’ll get only big blobs, so your submodular scores are blunt, and your saliency map is low resolution.  \n–  If it’s too fine, you get lots of tiny patches, you burn more compute searching, but you can localize very precisely.  \n\nEither way, every patch (mask1 included) feeds straight into the same submodular function, and its final ranking (and attribution weight) is determined by the combined clue + collaboration score subject to diminishing returns."
    },
    {
        "question": "How does conditioning on user-defined SCM impact denoising diffusion in the Semantic Conditional Module?",
        "relevant_section_ids": [
            "4.2.1"
        ],
        "relevant_context": [
            "In Semantic Conditional Module, the parameters θ_sem are composed of the object’s contact map parameters. We use a conditional generation model to infer probable contact maps ε_{θ_sem} based on user-specified or algorithmically predicted Semantic Contact Maps."
        ],
        "final_answer": "By feeding the user-defined Semantic Contact Map (SCM) into the diffusion model as a conditioning signal, each denoising step in the Semantic Conditional Module is guided to reconstruct contact-map samples that adhere to the user’s specified finger–object contact patterns. In other words, the SCM is concatenated as a condition at every noise level, steering the diffusion-based generator to output contact maps consistent with the fine-grained, user-defined semantics and thereby enabling controllable contact-map inference.",
        "relevant_elements": [
            "Semantic Conditional Module",
            "SCM"
        ],
        "id": 612,
        "masked_question": "How does conditioning on user-defined [mask1] impact denoising diffusion in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "SCM",
            "Semantic Conditional Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19370v1_figure_2.png",
        "paperid": "2407.19370v1",
        "paper_path": "./papers/2407.19370v1.json",
        "figure_id": "2407.19370v1_figure_2.png",
        "caption": "Figure 2. Overview of ClickDiff: The model initially takes an object s point cloud as input and predicts the contact map conditioned on the Semantic Contact Map within the Semantic Conditional Module. Subsequently, the predicted contact map is fed into the Contact Conditional Module, where grasping is generated under the guidance of TGC and contact map.",
        "qtype": "Experimental_Results",
        "response": "First, let us fix the two masked terms:\n\n– [mask1] is the “user-defined Semantic Contact Map (SCM)” (the red‐boxed heatmap of which finger should touch which object region).  \n– [mask2] is the “Point Encoder” (the blue-boxed PointNet that featurizes the object point cloud for the diffusion model).\n\n—\n\nChain-of-thought:\n\n1.  In our dual‐stage diffusion pipeline we have two conditional denoisers: one that predicts an object contact map from noise, and a second that predicts MANO hand parameters.  Both of them are *conditional* diffusion models—that is, at every timestep they not only consume the current noisy sample \\(x^n\\) but also a conditioning signal \\(C\\).\n\n2.  In the *Semantic* Conditional Module the conditioning signal \\(C\\) is exactly the SCM: a per-point heatmap that tells the network “these object points must end up being touched by finger 1, those by finger 2,” etc.  \n\n3.  During testing (and training) each denoising step looks roughly like\n   \\[\n     x^{n-1} \\;=\\; \\text{Denoiser}\\bigl(x^n,\\; C\\bigr)\n     \\quad\\text{where}\\quad C = \\text{PointEncoder}(\\mathcal{O}) \\,\\oplus\\, \\text{SCM},\n   \\]\n   i.e. we first run the object point‐cloud \\(\\mathcal{O}\\) through the Point Encoder to get a feature tensor, then *concatenate* (⊕) the user-specified SCM to that feature, and feed the result into the U-Net denoiser.\n\n4.  In practice this means that at *every* diffusion timestep the network sees not only “here is a noisy contact map” but also “and here is exactly where I, the user, clicked and want contacts.”  The denoiser thus learns to pull probability mass toward those clicked regions and away from untargeted regions.\n\n5.  The net effect is that the noisy contact map (and downstream hand‐pose parameters) are *steered*—through the concatenated SCM condition—toward a final grasp that respects your clicks.  \n\nAnswer:\n\nBy concatenating the user-provided Semantic Contact Map into the Point Encoder’s features at each denoising step, the conditional diffusion U-Net is explicitly reminded which object points *must* be touched by which fingers.  This extra channel of information biases the network’s noise removal so that the intermediate samples \\(x^n\\) are progressively refined into a contact map (and ultimately a hand pose) that fulfills exactly the user-clicked regions."
    },
    {
        "question": "How does enforcing Tactile-Guided Constraint within the Contact Conditional Module refine grasp alignment?",
        "relevant_section_ids": [
            "4.3",
            "5.3.3"
        ],
        "relevant_context": [
            "The Tactile-Guided Constraint loss (L_TGC) specifically targets the vertices within the finger sets proximal to the object's surface, ensuring that fingers accurately align with the designated ground-truth contact areas by accurately indexing the point pairs in the SCM and calculating the distance between the centroid of each finger’s predefined set of points and the contact point on the object.",
            "Applying the Tactile-Guided Constraint effectively ensures that the fingers align with the designated ground-truth contact regions. Notably, the introduction of L_TGC results in a significant reduction in joint displacement and improvements in contact metrics, exemplified by a 6.11 mm decrease in Contact Deviation (CDev). Experiments demonstrate that our TGC constrains the contact position of fingers in the Contact Conditional Module, which solves the contact ambiguity problem well."
        ],
        "final_answer": "By adding the Tactile-Guided Constraint during Contact Conditional Module training, the model explicitly pulls finger vertices near the object’s surface toward the SCM-specified contact points. This is done by computing L2 distances between finger-centroids (from pre-weighted finger point sets) and their corresponding object contact points, which 1) resolves the ambiguity of ‘‘which part of the hand’’ should touch, 2) forces the fingertips to align with the true contact regions, and 3) yields a measurable drop in contact deviation (over 6 mm) and joint displacement.",
        "relevant_elements": [
            "Contact Conditional Module",
            "Tactile-Guided Constraint"
        ],
        "id": 613,
        "masked_question": "How does enforcing [mask1] within the [mask2] refine grasp alignment?",
        "masked_number": 2,
        "masked_elements": [
            "Tactile-Guided Constraint",
            "Contact Conditional Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.19370v1_figure_2.png",
        "paperid": "2407.19370v1",
        "paper_path": "./papers/2407.19370v1.json",
        "figure_id": "2407.19370v1_figure_2.png",
        "caption": "Figure 2. Overview of ClickDiff: The model initially takes an object s point cloud as input and predicts the contact map conditioned on the Semantic Contact Map within the Semantic Conditional Module. Subsequently, the predicted contact map is fed into the Contact Conditional Module, where grasping is generated under the guidance of TGC and contact map.",
        "qtype": "Experimental_Results",
        "response": "By “dropping in” the red‐boxed Tactile‐Guided Constraint (Lₜgc) as an extra loss term inside the diffusion network (the blue‐boxed Conditional Module), the model is forced not just to hit the right overall contact map, but to position each fingertip exactly where the Semantic Contact Map says it should be.  Concretely, during denoising the network computes, for each finger, a small set of candidate touch‐points on its inner surface, takes their weighted centroid, and then adds an L₂ penalty between that centroid and the ground‐truth SCM contact point on the object.  That penalty “pulls” the generated hand mesh into perfect alignment with the desired contact locations, sharply reducing ambiguity and tightening up the final grasp."
    },
    {
        "question": "What potential limitations arise when using user-specified Semantic Contact Map for diverse object geometries?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Semantic Contact Map"
        ],
        "id": 614,
        "masked_question": "What potential limitations arise when using user-specified [mask1] for diverse object geometries?",
        "masked_number": 1,
        "masked_elements": [
            "Semantic Contact Map"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.19370v1_figure_2.png",
        "paperid": "2407.19370v1",
        "paper_path": "./papers/2407.19370v1.json",
        "figure_id": "2407.19370v1_figure_2.png",
        "caption": "Figure 2. Overview of ClickDiff: The model initially takes an object s point cloud as input and predicts the contact map conditioned on the Semantic Contact Map within the Semantic Conditional Module. Subsequently, the predicted contact map is fed into the Contact Conditional Module, where grasping is generated under the guidance of TGC and contact map.",
        "qtype": "Others",
        "response": "Here is a step-by-step reasoning:\n\n1.  Mask₁ is the “Semantic Contact Map” (SCM) block, i.e. the little red-boxed module into which the user “clicks” on the object and the system “traverses around” those clicks to build a per-point finger-indexed heatmap.\n\n2.  That SCM is entirely driven by whatever clicks the user provides (and a simple neighbourhood expansion of those clicks).  In exchange for this fine-grained control, two things happen when you expose it to wildly different object shapes:\n\n    •   Manual effort explodes.  Every new object (or every change in shape) forces the user to re-click all the desired contact regions by hand.  If you have a dozen mugs, bowls, drills, pliers, etc., the user must painstakingly place clicks on each one.\n\n    •   Heuristic spreading breaks down on complex geometry.  The SCM’s “traverse-around” step typically just does a distance-threshold flood fill in the point cloud.  On highly curved, concave or very high‐resolution meshes that flood can be too small (missing grooves or rims) or too large (spilling onto adjoining surfaces), leading to contact maps that no longer correspond to physically plausible grasp regions.\n\n3.  In practice this means\n\n    –  It does not scale to large libraries of objects without retracing clicks.  \n    –  It can produce wildly inconsistent contact regions if the underlying point‐cloud sampling density or local curvature varies from object to object.\n    –  It may mis-assign which fingers should touch where because the “spread‐around” neighborhood ignores local surface normals and topology.\n\n4.  All of these issues are inherent to any user-specified, click-based SCM when you try to cover a very diverse set of geometries."
    },
    {
        "question": "What limitations arise from Hop Fuse’s reliance on content-aware dynamic sampling under sudden scene changes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Hop Fuse",
            "dynamic sampling"
        ],
        "id": 616,
        "masked_question": "What limitations arise from [mask1]’s reliance on content-aware [mask2] under sudden scene changes?",
        "masked_number": 2,
        "masked_elements": [
            "Hop Fuse",
            "dynamic sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00608v1_figure_2.png",
        "paperid": "2411.00608v1",
        "paper_path": "./papers/2411.00608v1.json",
        "figure_id": "2411.00608v1_figure_2.png",
        "caption": "Figure 2: System overview of HopTrack. Hop Fuse associates active tracks with detections from dynamically sampled frames. Hop Update updates tracks’ positions and suppresses inaccurate tracks.",
        "qtype": "Others",
        "response": "Here “mask1” is the Hop Fuse stage, and “mask2” is the content-aware dynamic sampling module.  By tying Hop Fuse so tightly to that sampler, it can suffer whenever the scene “jumps” faster than the sampler can react.  In practice this means:\n\n  1.  If a sudden influx of objects or an abrupt camera motion occurs, the sampling rate may still be low (because it hasn’t yet “seen” the change), so you go many frames without a fresh detection.  \n  2.  During that gap, the Kalman‐predicted tracks drift farther from reality, leading to missed or mis-matched objects (more false negatives and ID swaps).  \n  3.  Occlusions or rapid appearance changes can slip through uncorrected until the sampler finally ups its rate—by which point Hop Fuse has already accumulated error.  \n\nIn short, under very sudden scene changes the content-aware sampler reacts too slowly, causing Hop Fuse to work with stale detections and degrading overall tracking quality."
    },
    {
        "question": "How might discretized dynamic matching in Hop Update struggle with varying object textures or illumination shifts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Hop Update",
            "dynamic matching"
        ],
        "id": 617,
        "masked_question": "How might discretized [mask1] in Hop Update struggle with varying object textures or illumination shifts?",
        "masked_number": 1,
        "masked_elements": [
            "dynamic matching"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.00608v1_figure_2.png",
        "paperid": "2411.00608v1",
        "paper_path": "./papers/2411.00608v1.json",
        "figure_id": "2411.00608v1_figure_2.png",
        "caption": "Figure 2: System overview of HopTrack. Hop Fuse associates active tracks with detections from dynamically sampled frames. Hop Update updates tracks’ positions and suppresses inaccurate tracks.",
        "qtype": "Others",
        "response": "Chain‐of‐thought:\n\n1. In the Hop Update stage, the system uses a “discretized dynamic matching” step to decide whether a tracked box from the last frame still corresponds to an object in the current frame.  \n\n2. In that step each bounding‐box is chopped into a small grid of patches (“cells”), and for each cell the tracker computes a very simple feature vector: the average R, G, B intensities plus a couple of Kalman‐filter motion terms.  \n\n3. Matching then reduces to comparing those per‐cell feature vectors via cosine similarity and counting how many patch‐pairs pass a fixed threshold.  \n\n4. Because the only appearance cue is average color per cell, the matcher assumes that an object’s patch‐level color distribution remains roughly constant between updates.  \n\n5. If the object’s texture is highly non-uniform (e.g. a patterned shirt or vehicle decal), different parts of the pattern may fall into different cells from one frame to the next, causing the average‐color features to change unpredictably.  \n\n6. Likewise, if the overall illumination shifts (cloud passes overhead, camera auto-exposure adjusts, or shadows move), even a uniformly colored object will register a different average intensity in each cell.  \n\n7. In both cases the cosine similarity score can fall below the match threshold even though the object actually remains the same, so the tracker may fail to associate the track correctly.\n\nAnswer:\n\nDiscretized dynamic matching in Hop Update relies on coarse average-color features per image “cell.” When an object’s local texture is highly varied or the scene lighting shifts between frames, those average intensities can change enough to push the cosine similarity below the fixed threshold. As a result, the tracker may fail to re-associate the true object under different textures or illumination."
    },
    {
        "question": "What drives using both static matching and dynamic matching for identity association?",
        "relevant_section_ids": [
            "3.4",
            "3.5"
        ],
        "relevant_context": [
            "Section 3.4: \"The discretized static and dynamic matching is meant to use appearance features that can be extracted efficiently with the CPU, in order to associate objects with large inter-frame displacement across multiple frames and to suppress inaccurate tracks.\"",
            "Section 3.5: \"The issue with static matching is that during the Hop Update phase, depending on the accuracy of the Kalman filter, the tracked objects might not be in the center of the bounding box or the bounding box might not be tight. Therefore, we propose a lightweight, dynamic discretized matching method to be run on each hopping frame, to check if the bounding boxes are accurately tracking the objects, and suppress tracks when occlusion happens.\""
        ],
        "final_answer": "Static matching is used at detection frames to efficiently associate objects that have large inter-frame displacements using CPU-friendly appearance cues, while dynamic matching is run on each hopping (non-detection) frame to correct for bounding‐box misalignments and occlusions when the Kalman filter’s predictions become inaccurate.",
        "relevant_elements": [
            "Static matching",
            "Dynamic matching",
            "Association"
        ],
        "id": 618,
        "masked_question": "What drives using both [mask1] and [mask2] for identity association?",
        "masked_number": 2,
        "masked_elements": [
            "Static matching",
            "Dynamic matching"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.00608v1_figure_2.png",
        "paperid": "2411.00608v1",
        "paper_path": "./papers/2411.00608v1.json",
        "figure_id": "2411.00608v1_figure_2.png",
        "caption": "Figure 2: System overview of HopTrack. Hop Fuse associates active tracks with detections from dynamically sampled frames. Hop Update updates tracks’ positions and suppresses inaccurate tracks.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module is our “static” discretized image match, which uses only the raw RGB cell‐wise histograms to link detections on full detection frames.  The blue‐boxed module is our “dynamic” discretized match, which augments those same RGB cell features with the Kalman‐filter state (i.e. the KF update) on the intermediate (“hopping”) frames.  \n\nWhat drives using both of them is simply that neither pure appearance nor pure motion alone is reliable over long intervals or under occlusion.  By (1) doing a fast, CPU‐cheap RGB–only match whenever we actually run the detector, and (2) doing an RGB+KF match in between detections, we get the best of both worlds – robust identity association without the expense of deep feature extraction on every frame."
    },
    {
        "question": "What motivates introducing community-level hetero-meta-path alongside node-level hetero-meta-path for dual-modal integration?",
        "relevant_section_ids": [
            "4.1.2"
        ],
        "relevant_context": [
            "In constructing node-level hetero-meta-path, we measure the similarity of connection patterns of cross-modal node pairs as the strength of their structure-function coupling.",
            "As for community-level hetero-meta-path, we suggest that brain regions with cooperative interactions may form a closed induced subgraph in both Gf and Gd."
        ],
        "final_answer": "While node-level hetero-meta-paths capture pairwise structure–function coupling between individual regions, community-level hetero-meta-paths are introduced to model higher-order, cooperative interactions among sets of brain regions that form closed subgraphs in both functional and structural networks.",
        "relevant_elements": [
            "node-level hetero-meta-path",
            "community-level hetero-meta-path"
        ],
        "id": 620,
        "masked_question": "What motivates introducing [mask1] alongside node-level hetero-meta-path for dual-modal integration?",
        "masked_number": 1,
        "masked_elements": [
            "community-level hetero-meta-path"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.08424v1_figure_1.png",
        "paperid": "2411.08424v1",
        "paper_path": "./papers/2411.08424v1.json",
        "figure_id": "2411.08424v1_figure_1.png",
        "caption": "Figure 1: Overview of our proposed method. a) We extract node features, Φ1subscriptΦ1\\Phi_{1}roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and Φ2subscriptΦ2\\Phi_{2}roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT from each modality to establish Gf={𝒩f,Φ1}subscript𝐺𝑓subscript𝒩𝑓subscriptΦ1G_{f}=\\left\\{\\mathcal{N}_{f},\\Phi_{1}\\right\\}italic_G start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }, Gd={𝒩d,Φ2}subscript𝐺𝑑subscript𝒩𝑑subscriptΦ2G_{d}=\\left\\{\\mathcal{N}_{d},\\Phi_{2}\\right\\}italic_G start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = { caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT }. b) Node-level and community-level hetero-meta-paths are combined as meta-path Φ3:𝒩f→𝒩d:subscriptΦ3→subscript𝒩𝑓subscript𝒩𝑑\\Phi_{3}:\\mathcal{N}_{f}\\rightarrow\\mathcal{N}_{d}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT : caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT → caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, and Φ4subscriptΦ4\\Phi_{4}roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is a reversal of Φ3subscriptΦ3\\Phi_{3}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. The subject-level HG is denoted as GH={(𝒩f,𝒩d),(Φ1,Φ2,Φ3,Φ4)}subscript𝐺𝐻subscript𝒩𝑓subscript𝒩𝑑subscriptΦ1subscriptΦ2subscriptΦ3subscriptΦ4G_{H}=\\left\\{\\left(\\mathcal{N}_{f},\\mathcal{N}_{d}\\right),\\left(\\Phi_{1},\\Phi_%\n{2},\\Phi_{3},\\Phi_{4}\\right)\\right\\}italic_G start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = { ( caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , ( roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) },. c) We preserve Φ2subscriptΦ2\\Phi_{2}roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and dynamically reconstruct FC to obtain Φ^1subscript^Φ1\\hat{\\Phi}_{1}over^ start_ARG roman_Φ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, then update Φ3subscriptΦ3\\Phi_{3}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and Φ4subscriptΦ4\\Phi_{4}roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT to generate augmented G^Hsubscript^𝐺𝐻\\hat{G}_{H}over^ start_ARG italic_G end_ARG start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT. d) Both GHsubscript𝐺𝐻G_{H}italic_G start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT and G^Hsubscript^𝐺𝐻\\hat{G}_{H}over^ start_ARG italic_G end_ARG start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT are fed into backbone consisted of HAN, HG pooling and readout layers to extract dual-modal features.",
        "qtype": "Design_Rationale",
        "response": "The mystery “red‐box” [mask1] in fact isn’t a fourth homo‐ or hetero‐path at all but the “community‐level hetero‐meta‐path” that the authors build in parallel with their node‐level one.  In their words, the node‐level hetero‐path simply links individual ROIs across FC and SC by how similar their connection profiles are; the community‐level hetero‐path, by contrast, links small subgraphs (triangles of ROIs) that co-exist in both the functional and structural graphs.  \n\nWhy introduce it?  Because real brain function is not just about isolated pairs of regions talking to one another, but about groups of regions cooperating as a unit.  By mining those closed‐triangle motifs that appear in both FC and SC, the model picks up higher-order, community-wide structure–function couplings.  In practice this gives the heterogeneous graph a much richer, more holistic cross-modal backbone, which in turn improves the dual-modal feature fusion and downstream classification."
    },
    {
        "question": "How does structural constraint preserve Φ2 during augmented GH generation?",
        "relevant_section_ids": [
            "4",
            "4.2"
        ],
        "relevant_context": [
            "In present work, we propose a novel HGNN to fuse dual-modal information. We define meta-paths in the fused  as Φ1, Φ2, Φ3, and Φ4, where homo-meta-paths Φ1, Φ2 are edges of FC or SC, and hetero-meta-paths Φ3, Φ4 are edges between FC and SC.",
            "The abundant heterogeneity of the HG provides ample possibilities from the perspective of construction, which provides convenience for augmentation. Therefore, we propose to dynamically reconstruct FC to obtain , then ,  will naturally update along with . While  is fixed as structural constraint to maintain the semantic consistency of HGs before and after augmentation.",
            "Finally, we consider edges in  as  corresponding to Φ1. With  fixed, we can update  and  following (5)–(7). Then the augmented  can be constructed following (8)–(9). We sent  and  in pair into the backbone to avoid data leakage."
        ],
        "final_answer": "During augmentation only the functional‐connectivity meta-path Φ1 is re-estimated from sliding-window correlation, while the structural‐connectivity meta-path Φ2 is held fixed as a ‘‘structural constraint.’’ In other words, the adjacency matrix corresponding to Φ2 (SC) is not changed during augmentation, preserving Φ2 in the augmented heterogeneous graph.",
        "relevant_elements": [
            "structural constraint",
            "Φ2",
            "augmented GH"
        ],
        "id": 622,
        "masked_question": "How does [mask1] preserve [mask2] during augmented GH generation?",
        "masked_number": 2,
        "masked_elements": [
            "structural constraint",
            "Φ2"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.08424v1_figure_1.png",
        "paperid": "2411.08424v1",
        "paper_path": "./papers/2411.08424v1.json",
        "figure_id": "2411.08424v1_figure_1.png",
        "caption": "Figure 1: Overview of our proposed method. a) We extract node features, Φ1subscriptΦ1\\Phi_{1}roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and Φ2subscriptΦ2\\Phi_{2}roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT from each modality to establish Gf={𝒩f,Φ1}subscript𝐺𝑓subscript𝒩𝑓subscriptΦ1G_{f}=\\left\\{\\mathcal{N}_{f},\\Phi_{1}\\right\\}italic_G start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT }, Gd={𝒩d,Φ2}subscript𝐺𝑑subscript𝒩𝑑subscriptΦ2G_{d}=\\left\\{\\mathcal{N}_{d},\\Phi_{2}\\right\\}italic_G start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = { caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT }. b) Node-level and community-level hetero-meta-paths are combined as meta-path Φ3:𝒩f→𝒩d:subscriptΦ3→subscript𝒩𝑓subscript𝒩𝑑\\Phi_{3}:\\mathcal{N}_{f}\\rightarrow\\mathcal{N}_{d}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT : caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT → caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, and Φ4subscriptΦ4\\Phi_{4}roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT is a reversal of Φ3subscriptΦ3\\Phi_{3}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT. The subject-level HG is denoted as GH={(𝒩f,𝒩d),(Φ1,Φ2,Φ3,Φ4)}subscript𝐺𝐻subscript𝒩𝑓subscript𝒩𝑑subscriptΦ1subscriptΦ2subscriptΦ3subscriptΦ4G_{H}=\\left\\{\\left(\\mathcal{N}_{f},\\mathcal{N}_{d}\\right),\\left(\\Phi_{1},\\Phi_%\n{2},\\Phi_{3},\\Phi_{4}\\right)\\right\\}italic_G start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT = { ( caligraphic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT , caligraphic_N start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , ( roman_Φ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT ) },. c) We preserve Φ2subscriptΦ2\\Phi_{2}roman_Φ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT and dynamically reconstruct FC to obtain Φ^1subscript^Φ1\\hat{\\Phi}_{1}over^ start_ARG roman_Φ end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, then update Φ3subscriptΦ3\\Phi_{3}roman_Φ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT and Φ4subscriptΦ4\\Phi_{4}roman_Φ start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT to generate augmented G^Hsubscript^𝐺𝐻\\hat{G}_{H}over^ start_ARG italic_G end_ARG start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT. d) Both GHsubscript𝐺𝐻G_{H}italic_G start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT and G^Hsubscript^𝐺𝐻\\hat{G}_{H}over^ start_ARG italic_G end_ARG start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT are fed into backbone consisted of HAN, HG pooling and readout layers to extract dual-modal features.",
        "qtype": "Implementation_Details",
        "response": "Here the red‐boxed “structural constraint” is nothing more than the decision to keep the community-level meta-path (Φ₂) exactly the same while we rebuild the FC edges.  Concretely, during augmentation we only re‐compute Φ₁ (the FC adjacency) but we leave Φ₂ (the community-level SC adjacency) entirely untouched.  In other words, by “fixing” Φ₂ as a structural constraint, we guarantee that all of the original community-level connections are preserved in the augmented graph."
    },
    {
        "question": "How does instance normalization in latent map fusion remove style while preserving source content?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "In the top branch, an instance normalization (IN) layer standardizes Z_x across spatial dimensions using channel-wise mean and variance, producing Z^c_x. Previous studies show that channel-wise statistics in latent feature maps can encapsulate the style of images [40,41,42,43]. By standardizing each feature channel to zero mean and unit variance, the IN layer removes instance-specific style from an image while retaining essential content features in Z^c_x [44]."
        ],
        "final_answer": "Instance normalization (IN) computes the mean and variance of each latent feature channel across spatial locations and re-scales the channel to zero mean and unit variance. Because style information is largely encoded in these channel-wise statistics, IN effectively removes the original style, while the remaining normalized spatial patterns continue to carry the anatomical (content) information of the source MRI.",
        "relevant_elements": [
            "Instance Normalization",
            "Latent Map Fusion"
        ],
        "id": 624,
        "masked_question": "How does [mask1] in [mask2] remove style while preserving source content?",
        "masked_number": 2,
        "masked_elements": [
            "Instance Normalization",
            "Latent Map Fusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.09315v1_figure_1.png",
        "paperid": "2408.09315v1",
        "paper_path": "./papers/2408.09315v1.json",
        "figure_id": "2408.09315v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed\nHCLD framework.\nDuring training, it extracts latent feature maps from source and target MRIs using an encoder 𝑬𝑬\\bm{E}bold_italic_E, fuses latent representations, and trains a conditional latent diffusion model (cLDM) to estimate the translated latent maps.\nDuring inference, it applies the trained cLDM to generate the final translated latent map by iterative denoising Tssubscript𝑇𝑠T_{s}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT steps and then utilizes a decoder 𝑫𝑫\\bm{D}bold_italic_D to reconstruct the translated MRI.\nBoth 𝑬𝑬\\bm{E}bold_italic_E and 𝑫𝑫\\bm{D}bold_italic_D are derived from an autoencoder pre-trained on 3,500 T1-weighted brain\nMRIs.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In the latent-map fusion block (the blue-outlined region) we have two parallel branches processing the source code zₓ and the target code zᵧ.  \n2. The top branch applies an “IN” operator to zₓ – this is exactly the module boxed in red.  \n3. Instance Normalization (IN) works by computing, for each channel k of the feature map, the mean μₖ and standard deviation σₖ over all spatial positions in that channel (i.e. across the H×W×D volume).  \n4. It then subtracts μₖ and divides by σₖ so that each channel has zero mean and unit variance.  \n5. Prior work has shown that those per-channel means and variances in a deep feature map encode “style” information (contrast, texture statistics, scanner‐specific signatures, etc.).  \n6. By forcing every channel to have the same (zero, one) statistics, IN strips out the style cues while leaving intact the relative activation patterns across space – which correspond to the underlying anatomical content.  \n7. The normalized output zₓᶜ is therefore a content-only latent map: its style has been erased (all channels share the same statistics) but its spatial structure (the brain anatomy) is unchanged.  \n\nAnswer  \nThe red-boxed “IN” is an Instance-Normalization layer in the latent-map fusion module (the blue box).  It removes style by computing each channel’s mean and variance over the spatial dimensions, then normalizing to zero mean/unit variance—thereby discarding the style information carried in those statistics—while preserving the spatial activation patterns that encode the source’s anatomical content."
    },
    {
        "question": "How are timestep encodings integrated into ResBlock and AttnBlock within the cLDM?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "ResBlock",
            "AttnBlock",
            "cLDM"
        ],
        "id": 625,
        "masked_question": "How are timestep encodings integrated into [mask1] and AttnBlock within the cLDM?",
        "masked_number": 1,
        "masked_elements": [
            "ResBlock"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.09315v1_figure_1.png",
        "paperid": "2408.09315v1",
        "paper_path": "./papers/2408.09315v1.json",
        "figure_id": "2408.09315v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed\nHCLD framework.\nDuring training, it extracts latent feature maps from source and target MRIs using an encoder 𝑬𝑬\\bm{E}bold_italic_E, fuses latent representations, and trains a conditional latent diffusion model (cLDM) to estimate the translated latent maps.\nDuring inference, it applies the trained cLDM to generate the final translated latent map by iterative denoising Tssubscript𝑇𝑠T_{s}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT steps and then utilizes a decoder 𝑫𝑫\\bm{D}bold_italic_D to reconstruct the translated MRI.\nBoth 𝑬𝑬\\bm{E}bold_italic_E and 𝑫𝑫\\bm{D}bold_italic_D are derived from an autoencoder pre-trained on 3,500 T1-weighted brain\nMRIs.",
        "qtype": "Implementation_Details",
        "response": "In our cLDM (the little U-Net on the right of Fig. 1) every convolutional or attention block is “time-conditioned,” i.e. the current diffusion step t is turned into a dense vector and injected into both the ResBlock (your Mask 1) and into the self-attention blocks.  Concretely, we do exactly the same thing that most modern diffusion UNets do:\n\n  1.  We take the scalar timestep t and run it through a small sinusoidal-positional embedding layer (usually 128–512 dim), followed by a two-layer MLP to get a “t-embedding” of the same width as the feature maps.  \n  2.  In each ResBlock we add that t-embedding – after appropriate reshaping and broadcast – directly to the convolutional activations (typically immediately after the first GroupNorm and before the second convolution).  Equivalently you can view it as a simple FiLM‐style modulation (scale–shift) on the normalized features.  \n  3.  In each AttnBlock the very same t-embedding is likewise added to the block’s key/value projections (or to the post-normed features before attention), so that the attention scores themselves become a function of t as well as of the spatial features.\n\nBy sharing exactly the same t-embedding MLP across all ResBlocks and AttnBlocks, and simply adding it into each block’s internal activations, the network learns to “know” which diffusion step it is at, at every layer of the U-Net."
    },
    {
        "question": "How does latent map fusion's IN branch repurpose instance normalization to differentiate content from style?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "In the top branch, an instance normalization (IN) layer standardizes z_x across spatial dimensions using channel-wise mean and variance, producing z^c_x.",
            "Previous studies show that channel-wise statistics in latent feature maps can encapsulate the style of images [40, 41, 42, 43]. By standardizing each feature channel to zero mean and unit variance, the IN layer removes instance-specific style from an image while retaining essential content features in z^c_x."
        ],
        "final_answer": "The IN branch applies instance normalization to the source latent map by normalizing each channel to zero mean and unit variance. Since channel-wise mean and variance encode style, this procedure strips away instance-specific style information, yielding a latent map that retains content features while discarding style.",
        "relevant_elements": [
            "Latent Map Fusion",
            "IN"
        ],
        "id": 626,
        "masked_question": "How does [mask1]'s [mask2] branch repurpose instance normalization to differentiate content from style?",
        "masked_number": 2,
        "masked_elements": [
            "Latent Map Fusion",
            "IN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.09315v1_figure_1.png",
        "paperid": "2408.09315v1",
        "paper_path": "./papers/2408.09315v1.json",
        "figure_id": "2408.09315v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed\nHCLD framework.\nDuring training, it extracts latent feature maps from source and target MRIs using an encoder 𝑬𝑬\\bm{E}bold_italic_E, fuses latent representations, and trains a conditional latent diffusion model (cLDM) to estimate the translated latent maps.\nDuring inference, it applies the trained cLDM to generate the final translated latent map by iterative denoising Tssubscript𝑇𝑠T_{s}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT steps and then utilizes a decoder 𝑫𝑫\\bm{D}bold_italic_D to reconstruct the translated MRI.\nBoth 𝑬𝑬\\bm{E}bold_italic_E and 𝑫𝑫\\bm{D}bold_italic_D are derived from an autoencoder pre-trained on 3,500 T1-weighted brain\nMRIs.",
        "qtype": "Literature_Background",
        "response": "In the Latent Map Fusion module (the big red‐out­lined box), the “IN” branch (the small blue box) simply applies instance normalization to the source feature map.  Concretely, for each channel it subtracts that channel’s mean and divides by its standard deviation, forcing every channel to have zero mean and unit variance.  Since prior work has shown that channel‐wise mean / variance encodes “style,” zeroing them out strips away style cues and leaves behind only the spatial/content information.  Hence, this branch “reuses” vanilla instance normalization as a cheap content extractor by using it to wash out style statistics."
    },
    {
        "question": "How does pre-trained encoder reuse enable cLDM's efficient two-stage latent diffusion optimization?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "Notably, only the cLDM undergoes updates during the training stage.",
            "In the second training stage, the pre-trained autoencoder networks E and D are reused with their network parameters frozen. Only the cLDM is updated to reconstruct the translated source latent map with the target domain style, which is computationally efficient as it operates in low-dimensional latent space."
        ],
        "final_answer": "By first training a 3D autoencoder (encoder E and decoder D) to compress MRIs into a low-dimensional latent space, then freezing its weights in the second stage, the framework only needs to update the conditional latent diffusion model (cLDM). This two-stage training—autoencoder pre-training followed by cLDM optimization in latent space—dramatically reduces computational cost (because diffusion operates in low-dimensional space), avoids the instability of jointly updating multiple networks, and improves generalizability on unseen data.",
        "relevant_elements": [
            "Encoder E",
            "Conditional Latent Diffusion Model (cLDM)"
        ],
        "id": 627,
        "masked_question": "How does pre-trained [mask1] reuse enable cLDM's efficient two-stage latent diffusion optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Encoder E"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.09315v1_figure_1.png",
        "paperid": "2408.09315v1",
        "paper_path": "./papers/2408.09315v1.json",
        "figure_id": "2408.09315v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed\nHCLD framework.\nDuring training, it extracts latent feature maps from source and target MRIs using an encoder 𝑬𝑬\\bm{E}bold_italic_E, fuses latent representations, and trains a conditional latent diffusion model (cLDM) to estimate the translated latent maps.\nDuring inference, it applies the trained cLDM to generate the final translated latent map by iterative denoising Tssubscript𝑇𝑠T_{s}italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT steps and then utilizes a decoder 𝑫𝑫\\bm{D}bold_italic_D to reconstruct the translated MRI.\nBoth 𝑬𝑬\\bm{E}bold_italic_E and 𝑫𝑫\\bm{D}bold_italic_D are derived from an autoencoder pre-trained on 3,500 T1-weighted brain\nMRIs.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In our framework we actually split training into two stages – first we train a 3D autoencoder (encoder E and decoder D) on large unlabeled MRI data, and then we “freeze” those autoencoder weights and only train the conditional latent diffusion module (cLDM).  \n2. The red box in Fig. 1 highlights the pre‐trained encoder (the “mask1”), which is part of that frozen autoencoder.  At inference time we likewise use the same frozen encoder to map any new MRI into its compact latent code.  \n3. By reusing this pre‐trained encoder (and decoder) instead of learning all the image representations from scratch in the diffusion model, we restrict the expensive diffusion learning to a much smaller, low‐dimensional latent space.  \n4. Concretely, this means that during the second training stage we only back‐propagate through the cLDM, not through the full high‐resolution 3D encoder/decoder.  The result is far fewer parameters to update, far lower memory and compute requirements, faster convergence, and a more stable training process.  \n5. In short, the “pre‐trained encoder” reuse decouples representation learning from style‐transfer learning and makes our two‐stage latent diffusion optimization dramatically more efficient.  \n\nAnswer  \nMask1 refers to the pre-trained encoder.  By freezing that encoder (and its paired decoder) after stage 1 and only training the cLDM in the resulting low-dimensional latent space in stage 2, the model (a) collapses the diffusion problem into a much smaller space, (b) avoids back-prop through expensive 3D convnets during diffusion training, and (c) therefore achieves efficient, stable two-stage optimization."
    },
    {
        "question": "How does feature extraction inform multi-relational text graph construction differently than single-view construction?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Existing methods treat words and documents as nodes and construct a heterogeneous text graph based on the point-wise mutual information (PMI) relationships between words and the TF-IDF relationships between words and documents. Despite such methods having achieved promising results, they neglect the rich and deep semantics, which is pivotal for capturing the core intent of the text. (Section 1)",
            "To forge links between texts that are otherwise unconnected, we extract various core features: titles, keywords, and events. Each of these is embedded via a pre-trained encoder to yield vector representations that will later define semantic relations. (Section 3.1)",
            "Rather than relying on a single, undifferentiated graph, we calculate the semantic similarity between the extracted features to construct multiple semantic relationships between document nodes, corresponding to title relationships, keyword relationships, and event relationships. Based on the rich features inherent in the text, the constructed text graph can maximize the connections between similar documents. (Section 3.2)"
        ],
        "final_answer": "Traditional single-view graph construction builds one graph—typically using PMI for word–word edges and TF-IDF for word–document edges—thus ignoring deeper semantics. In contrast, ConNHS’s feature extraction first pulls out titles, keywords, and events and embeds each via a pre-trained encoder. Then, in multi-relational graph construction, these distinct features are used to compute separate similarity scores, producing three parallel subgraphs (title-based, keyword-based, event-based). This multi-view approach captures richer semantic connections than a single undifferentiated graph.",
        "relevant_elements": [
            "Feature extraction",
            "Multi-relational text graph construction"
        ],
        "id": 628,
        "masked_question": "How does [mask1] inform [mask2] differently than single-view construction?",
        "masked_number": 2,
        "masked_elements": [
            "Feature extraction",
            "Multi-relational text graph construction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16787v1_figure_1.png",
        "paperid": "2411.16787v1",
        "paper_path": "./papers/2411.16787v1.json",
        "figure_id": "2411.16787v1_figure_1.png",
        "caption": "Figure 1: Flow chart of the proposed ConNHS. Initially, we construct a multi-relational text graph by leveraging inherent core features (titles, keywords, events) to establish semantic connections among texts while encoding textual content as initial node representations. Subsequently, relational separation yields distinct subgraphs, upon which intra-graph and inter-graph propagation are performed to obtain contrastive samples and similarity score matrix. During Contrastive learning with NHS, negative selection is optimized to encourage more explicit cluster boundaries (minimizing intra-class distances while maximizing inter-class distances; distinct colors indicate different clusters). Ultimately, predicted labels are assigned to document nodes via a logical classifier.",
        "qtype": "Literature_Background",
        "response": "In our pipeline the red‐boxed “feature extraction” stage doesn’t simply spit out one big document embedding and then build one homogeneous graph on top of it (as a single‐view method would).  Instead, it pulls three orthogonal “views” of each text – its title, its salient keywords and its extracted events – and encodes each of those separately.  In the blue‐boxed “multi‐relational graph construction” we then take each of those three feature embeddings in turn, compute pairwise cosine‐similarities, threshold them, and so build three distinct edge-sets (one for titles, one for keywords, one for events).  In other words, rather than collapsing all the text into one similarity metric and one graph, we use the multiple semantically‐focused feature vectors produced in [mask1] to drive a three-layered graph in [mask2]."
    },
    {
        "question": "How does inter-graph propagation improve upon equal-weight fusion in earlier multi-graph frameworks?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Secondly, they assign equal weights to different features during the inter-graph propagation, ignoring the intrinsic differences inherent in these features.",
            "After intra-graph propagation, each document node learns unique feature information under different semantic relationships. Therefore, we design a cross-graph attention network to coordinate and integrate diverse feature information."
        ],
        "final_answer": "Inter-graph propagation improves upon equal-weight fusion by introducing a cross-graph attention network (CGAN) that learns attention weights for each semantic subgraph’s node representations, rather than averaging them equally. This attention mechanism harmonizes and coordinates diverse feature information across graphs, capturing their intrinsic differences and leading to more nuanced fused representations.",
        "relevant_elements": [
            "Inter-Graph propagation"
        ],
        "id": 629,
        "masked_question": "How does [mask1] improve upon equal-weight fusion in earlier multi-graph frameworks?",
        "masked_number": 1,
        "masked_elements": [
            "Inter-Graph propagation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16787v1_figure_1.png",
        "paperid": "2411.16787v1",
        "paper_path": "./papers/2411.16787v1.json",
        "figure_id": "2411.16787v1_figure_1.png",
        "caption": "Figure 1: Flow chart of the proposed ConNHS. Initially, we construct a multi-relational text graph by leveraging inherent core features (titles, keywords, events) to establish semantic connections among texts while encoding textual content as initial node representations. Subsequently, relational separation yields distinct subgraphs, upon which intra-graph and inter-graph propagation are performed to obtain contrastive samples and similarity score matrix. During Contrastive learning with NHS, negative selection is optimized to encourage more explicit cluster boundaries (minimizing intra-class distances while maximizing inter-class distances; distinct colors indicate different clusters). Ultimately, predicted labels are assigned to document nodes via a logical classifier.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module is in fact our relation‐aware GCN (RW-GCN) intra‐graph propagator.  In all of the older multi-graph methods you simply average (i.e. give equal weight to) every neighboring node (or every sub-graph) when you fuse features – treating every neighbor (or every view) as equally important.  RW-GCN replaces that blunt, uniform averaging with a tiny, learnable “attention” step:\n\n  1.  For each edge (i, j) we run the pair’s current features through a small MLP + sigmoid to produce a weight αij ∈ [0,1].  \n  2.  We multiply each neighbor’s feature vector by its αij, so that strong, semantically relevant links get up-weighted and weak ones get down-weighted.  \n  3.  We sum (rather than average) those weighted vectors and then pass them through a linear transformation to make our next layer’s node embedding.\n\nBecause those αij are learned end-to-end (and take the edge features into account), RW-GCN “fuses” its neighborhood in a fully adaptive way—rather than giving every neighbor (or every subgraph) the same fixed weight."
    },
    {
        "question": "How does regressing post-D rewards on binary features quantify feature imprint methodology?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "We can now quantify the extent to which target and spoiler features imprint on the RMs by regressing rewards (or reward shifts) against the boolean feature indicators:",
            "… The coefficient β_j estimates the point increase in reward between an entry t_i (or t_i′) containing feature j compared to an entry without it, holding all other features constant. We refer to this as the post-D imprint for value j."
        ],
        "final_answer": "By performing a linear regression of the post-D reward scores on binary feature indicators, the method assigns each feature j a coefficient β_j. This coefficient directly measures the point increase in the reward model’s score when that feature is present (versus absent), thereby quantifying the strength of the feature’s imprint on the trained reward model.",
        "relevant_elements": [
            "post-D reward vectors",
            "feature imprint"
        ],
        "id": 632,
        "masked_question": "How does regressing [mask1] on binary features quantify feature imprint methodology?",
        "masked_number": 1,
        "masked_elements": [
            "post-D reward vectors"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.10270v1_figure_1.png",
        "paperid": "2408.10270v1",
        "paper_path": "./papers/2408.10270v1.json",
        "figure_id": "2408.10270v1_figure_1.png",
        "caption": "Figure 1: Summary of the paper s background, setup and contributions. [1] AI Alignment Pipeline: This section illustrates the sequence of events during RLHF, highlighting the interactions between the alignment dataset, human preferences, the RM and the base-model being aligned. [2] Alignment Dataset Taxonomization: The alignment dataset 𝒟𝒟\\mathcal{D}caligraphic_D comprises pairs of text (tic,tirsuperscriptsubscript𝑡𝑖𝑐superscriptsubscript𝑡𝑖𝑟t_{i}^{c},t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT) where ticsuperscriptsubscript𝑡𝑖𝑐t_{i}^{c}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT is preferred by the human over tirsuperscriptsubscript𝑡𝑖𝑟t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT presumably because it is more aligned with a set of defined target values. (Top) The alignment dataset is featurized using an LM-labeler based on a set of target features (intended for alignment, in black) and spoiler features (learned inadvertently, in grey). (Bottom) The alignment dataset is rewritten and re-featurized accordingly. [3] Reward Models (RMs): (Top) An RM maps a user input-model output pair t𝑡titalic_t to a score r (t).𝑟𝑡r(t).italic_r ( italic_t ) . We compare the RM before (pre-𝒟𝒟\\mathcal{D}caligraphic_D model ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG) and after (post-𝒟𝒟\\mathcal{D}caligraphic_D model ℛℛ\\mathcal{R}caligraphic_R) it is trained on the alignment dataset. (Bottom) The pair of rewards awarded by ℛℛ\\mathcal{R}caligraphic_R (r (tic),r (tir))𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{0,.5,.5}r}(t_{i}^{c}),{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}))( italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) is interpreted as vectors. The sign of r (tic)−r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})-{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) - italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) indicates whether the RM s scores are aligned or not with human preferences in the dataset. (r¯ (tic),r¯ (tir))¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c}),{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r}))( under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) denotes the reward vectors assigned by ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG. [4] Evaluation Report for Anthropic/hh Alignment Dataset x OpenAssistant RM Alignment Pipeline: Results of the SEAL methodology applied to an open-source alignment pipeline purposed to render base models more helpful and harmless. (Feature Imprint) By regressing rewards against binary features indicators, we estimate that top features driving rewards are harmlessness, privacy-preserving, helpfulness, eloquence and sentiment. A feature imprint of β (harmlessness)=2.09𝛽harmlessness2.09\\beta(\\text{harmlessness})=2.09italic_β ( harmlessness ) = 2.09 implies that harmless text has a reward 2.092.092.092.09 points higher than harmful text. (Alignment Resistance) More than one out of four pairs in the alignment dataset have r (tic)<r (tir),𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}),italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) , indicating that ℛℛ\\mathcal{R}caligraphic_R rewards the entry least preferred by the human (the teal arrow is in the misaligned space). Additionally, ℛℛ\\mathcal{R}caligraphic_R reverses alignment 8%percent88\\%8 % of the time (r¯ (tic)>r¯ (tir)¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c})>{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r})under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) > under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) and r (tic)<r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT )). (Robustness Scores) Rewriting entries to sound more positive increases the risks of misalignment.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the pieces fit together:\n\n1.  **What gets regressed?**  \n    In §2.1 the authors write down two regressions:  \n    (a)  rᵢ = α + ∑ⱼ βⱼ fᵢⱼ + prompt fixed effect + εᵢ  \n    (b)  Δθᵢ = α′ + ∑ⱼ γⱼ fᵢⱼ + prompt fixed effect + ε′ᵢ  \n    where  \n    - rᵢ is the reward score that the (post-training) RM assigns to example i,  \n    - Δθᵢ is the angle “reward shift” between pre- and post-training reward vectors on example i,  \n    - fᵢⱼ ∈ {0,1} are the binary feature indicators (helpfulness, harmlessness, eloquence, etc.),  \n    - βⱼ (resp. γⱼ) is the coefficient on feature j.  \n\n2.  **What does the coefficient measure?**  \n    Take the first regression as the archetype: βⱼ is literally “the point increase in reward when feature j is present versus absent, holding everything else constant (including other features and prompt).”  \n    - A large positive βⱼ says “whenever feature j appears in the text, the RM—after training—gives it a much higher score.”  \n    - A βⱼ near zero says the model ignores that feature.  \n\n3.  **Why is this a measure of “feature imprint”?**  \n    - By comparing the post-training coefficients βⱼ to the pre-training coefficients (the same regression run on the old RM), you see exactly how much more (or less) the model now cares about feature j.  \n    - That difference is the “imprint” of that feature onto the reward model during RLHF.  \n\n4.  **In plain English:**  \n    Regressing the RM’s **assigned rewards** (or the **reward‐shift angles**) on a set of binary feature flags boils down each feature’s effect to a single number, βⱼ.  That βⱼ is interpreted as “how many reward‐points the model adds (or subtracts) whenever this feature is present.”  By inspecting those β’s before and after training, we get a direct, quantitative read-out of which values the RM has learned (the “feature imprints”)."
    },
    {
        "question": "How does label-flip logistic regression isolate robustness scores using rewritten alignment dataset methodology?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The robustness score is computed as the coefficient of a logistic regression that measures the impact of label flipping on misalignment incidence.",
            "The indicator variable Δ_i equals 1 when the RM was aligned with human preferences before rewriting and not after.",
            "We set 0 (the absence of label flip) as the baseline, resulting in two coefficients R_{f,chosen} and R_{f,rejected}.",
            "Thus, R_{f,k} measures the extent to which alignment is robust to rewriting, isolating the effects of each feature and each event type."
        ],
        "final_answer": "They fit a logistic regression predicting whether a pair flips from aligned to misaligned after rewriting (Δ_i=1), with categorical indicators for each feature flip event (e.g. feature f flipped in the chosen or rejected entry). By using “no flip” as the baseline, the model yields two coefficients per feature—R_{f,chosen} and R_{f,rejected}—whose sizes (or exponentials) are the odds-multipliers for misalignment caused by that specific flip. These coefficients therefore isolate each feature’s robustness score to mild perturbations in the rewritten dataset.",
        "relevant_elements": [
            "rewritten alignment dataset",
            "robustness scores"
        ],
        "id": 633,
        "masked_question": "How does label-flip logistic regression isolate [mask1] using rewritten alignment dataset methodology?",
        "masked_number": 1,
        "masked_elements": [
            "robustness scores"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.10270v1_figure_1.png",
        "paperid": "2408.10270v1",
        "paper_path": "./papers/2408.10270v1.json",
        "figure_id": "2408.10270v1_figure_1.png",
        "caption": "Figure 1: Summary of the paper s background, setup and contributions. [1] AI Alignment Pipeline: This section illustrates the sequence of events during RLHF, highlighting the interactions between the alignment dataset, human preferences, the RM and the base-model being aligned. [2] Alignment Dataset Taxonomization: The alignment dataset 𝒟𝒟\\mathcal{D}caligraphic_D comprises pairs of text (tic,tirsuperscriptsubscript𝑡𝑖𝑐superscriptsubscript𝑡𝑖𝑟t_{i}^{c},t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT) where ticsuperscriptsubscript𝑡𝑖𝑐t_{i}^{c}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT is preferred by the human over tirsuperscriptsubscript𝑡𝑖𝑟t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT presumably because it is more aligned with a set of defined target values. (Top) The alignment dataset is featurized using an LM-labeler based on a set of target features (intended for alignment, in black) and spoiler features (learned inadvertently, in grey). (Bottom) The alignment dataset is rewritten and re-featurized accordingly. [3] Reward Models (RMs): (Top) An RM maps a user input-model output pair t𝑡titalic_t to a score r (t).𝑟𝑡r(t).italic_r ( italic_t ) . We compare the RM before (pre-𝒟𝒟\\mathcal{D}caligraphic_D model ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG) and after (post-𝒟𝒟\\mathcal{D}caligraphic_D model ℛℛ\\mathcal{R}caligraphic_R) it is trained on the alignment dataset. (Bottom) The pair of rewards awarded by ℛℛ\\mathcal{R}caligraphic_R (r (tic),r (tir))𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{0,.5,.5}r}(t_{i}^{c}),{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}))( italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) is interpreted as vectors. The sign of r (tic)−r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})-{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) - italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) indicates whether the RM s scores are aligned or not with human preferences in the dataset. (r¯ (tic),r¯ (tir))¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c}),{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r}))( under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) denotes the reward vectors assigned by ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG. [4] Evaluation Report for Anthropic/hh Alignment Dataset x OpenAssistant RM Alignment Pipeline: Results of the SEAL methodology applied to an open-source alignment pipeline purposed to render base models more helpful and harmless. (Feature Imprint) By regressing rewards against binary features indicators, we estimate that top features driving rewards are harmlessness, privacy-preserving, helpfulness, eloquence and sentiment. A feature imprint of β (harmlessness)=2.09𝛽harmlessness2.09\\beta(\\text{harmlessness})=2.09italic_β ( harmlessness ) = 2.09 implies that harmless text has a reward 2.092.092.092.09 points higher than harmful text. (Alignment Resistance) More than one out of four pairs in the alignment dataset have r (tic)<r (tir),𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}),italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) , indicating that ℛℛ\\mathcal{R}caligraphic_R rewards the entry least preferred by the human (the teal arrow is in the misaligned space). Additionally, ℛℛ\\mathcal{R}caligraphic_R reverses alignment 8%percent88\\%8 % of the time (r¯ (tic)>r¯ (tir)¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c})>{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r})under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) > under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) and r (tic)<r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT )). (Robustness Scores) Rewriting entries to sound more positive increases the risks of misalignment.",
        "qtype": "Experimental_Results",
        "response": "Label‐flip logistic regression “zeroes in” on exactly the numbers you see boxed in red by turning each possible feature‐flip into its own dummy variable in a logistic model of post‐rewrite misalignment.  Concretely, after rewriting a held‐out subset of the alignment pairs, we\n\n 1. compute δᵢ = 1 whenever the reward model was aligned before rewriting and misaligned after,  \n 2. annotate each pair i with binary flags of the form “chosen entry’s positivity flipped” or “rejected entry’s positivity flipped” (and similarly for other features),  \n 3. fit  \n      logit Pr[δᵢ=1] = α + ∑_{features j}∑_{events e∈{chosen,rejected}} γ_{j}^{e} · 1[{feature j flipped in event e}],  \n\nusing “no flip” as the baseline.  Each fitted γ_{j}^{e} is exactly the robustness score for feature j in event e—it tells you by what factor the odds of misalignment rise (or fall) when that one label flips and everything else stays the same.  The red‐boxed “> 1.12×” you see for positivity is just the exponentiated γ for chosen‐less‐positive and rejected‐more‐positive flips."
    },
    {
        "question": "What ethical concerns arise from using an LM-labeler for dataset featurization in Alignment Dataset Taxonomization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Alignment Dataset Taxonomization",
            "LM-labeler"
        ],
        "id": 634,
        "masked_question": "What ethical concerns arise from using an LM-labeler for dataset featurization in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Alignment Dataset Taxonomization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.10270v1_figure_1.png",
        "paperid": "2408.10270v1",
        "paper_path": "./papers/2408.10270v1.json",
        "figure_id": "2408.10270v1_figure_1.png",
        "caption": "Figure 1: Summary of the paper s background, setup and contributions. [1] AI Alignment Pipeline: This section illustrates the sequence of events during RLHF, highlighting the interactions between the alignment dataset, human preferences, the RM and the base-model being aligned. [2] Alignment Dataset Taxonomization: The alignment dataset 𝒟𝒟\\mathcal{D}caligraphic_D comprises pairs of text (tic,tirsuperscriptsubscript𝑡𝑖𝑐superscriptsubscript𝑡𝑖𝑟t_{i}^{c},t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT) where ticsuperscriptsubscript𝑡𝑖𝑐t_{i}^{c}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT is preferred by the human over tirsuperscriptsubscript𝑡𝑖𝑟t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT presumably because it is more aligned with a set of defined target values. (Top) The alignment dataset is featurized using an LM-labeler based on a set of target features (intended for alignment, in black) and spoiler features (learned inadvertently, in grey). (Bottom) The alignment dataset is rewritten and re-featurized accordingly. [3] Reward Models (RMs): (Top) An RM maps a user input-model output pair t𝑡titalic_t to a score r (t).𝑟𝑡r(t).italic_r ( italic_t ) . We compare the RM before (pre-𝒟𝒟\\mathcal{D}caligraphic_D model ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG) and after (post-𝒟𝒟\\mathcal{D}caligraphic_D model ℛℛ\\mathcal{R}caligraphic_R) it is trained on the alignment dataset. (Bottom) The pair of rewards awarded by ℛℛ\\mathcal{R}caligraphic_R (r (tic),r (tir))𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{0,.5,.5}r}(t_{i}^{c}),{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}))( italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) is interpreted as vectors. The sign of r (tic)−r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})-{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) - italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) indicates whether the RM s scores are aligned or not with human preferences in the dataset. (r¯ (tic),r¯ (tir))¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c}),{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r}))( under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) denotes the reward vectors assigned by ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG. [4] Evaluation Report for Anthropic/hh Alignment Dataset x OpenAssistant RM Alignment Pipeline: Results of the SEAL methodology applied to an open-source alignment pipeline purposed to render base models more helpful and harmless. (Feature Imprint) By regressing rewards against binary features indicators, we estimate that top features driving rewards are harmlessness, privacy-preserving, helpfulness, eloquence and sentiment. A feature imprint of β (harmlessness)=2.09𝛽harmlessness2.09\\beta(\\text{harmlessness})=2.09italic_β ( harmlessness ) = 2.09 implies that harmless text has a reward 2.092.092.092.09 points higher than harmful text. (Alignment Resistance) More than one out of four pairs in the alignment dataset have r (tic)<r (tir),𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}),italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) , indicating that ℛℛ\\mathcal{R}caligraphic_R rewards the entry least preferred by the human (the teal arrow is in the misaligned space). Additionally, ℛℛ\\mathcal{R}caligraphic_R reverses alignment 8%percent88\\%8 % of the time (r¯ (tic)>r¯ (tir)¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c})>{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r})under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) > under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) and r (tic)<r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT )). (Robustness Scores) Rewriting entries to sound more positive increases the risks of misalignment.",
        "qtype": "Others",
        "response": "In that red‐boxed “Alignment Dataset Taxonomization” module the authors in fact outsource all of the feature‐labeling to a black‐box LLM (in their case GPT-4).  That shortcut carries with it a host of ethical alarms:\n\n  •  Bias amplification.  Whatever systematic blind spots or cultural prejudices the LLM picked up in pre-training will now be frozen into your “ground truth” featurization.  If, say, the model tends to associate elaborate phrasing with “harmlessness,” you risk encoding a style bias that penalizes voices or dialects it doesn’t favor.  \n  •  Opaqueness and lack of accountability.  Because GPT-4 is neither transparent nor easily interrogated, you lose any real audit trail for *why* a given example was tagged as “helpful,” “harmless,” “eloquent,” etc.  This makes it nearly impossible to trace or correct errors or to answer “who is responsible?” when the system goes off the rails.  \n  •  Over-reliance on automated moral judgments.  A single model’s notion of “good” or “safe” may diverge in subtle but important ways from human values, and by letting the LLM define your feature taxonomy you effectively turn over key ethical judgments to a statistical oracle.  \n  •  Privacy and data‐leakage risks.  Any sensitive or proprietary content that the LLM sees during featurization could inadvertently be memorized and regurgitated, compounding confidentiality concerns.  \n\nIn short, leaning on an LM‐labeler for dataset featurization trades away human oversight, opens the door to hidden bias, and makes the entire alignment pipeline both less transparent and harder to audit."
    },
    {
        "question": "What limitations arise from relying on Pre- and Post-D Reward Vectors to interpret nuanced human preferences?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pre- and Post-D Reward Vectors"
        ],
        "id": 635,
        "masked_question": "What limitations arise from relying on [mask1] to interpret nuanced human preferences?",
        "masked_number": 1,
        "masked_elements": [
            "Pre- and Post-D Reward Vectors"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.10270v1_figure_1.png",
        "paperid": "2408.10270v1",
        "paper_path": "./papers/2408.10270v1.json",
        "figure_id": "2408.10270v1_figure_1.png",
        "caption": "Figure 1: Summary of the paper s background, setup and contributions. [1] AI Alignment Pipeline: This section illustrates the sequence of events during RLHF, highlighting the interactions between the alignment dataset, human preferences, the RM and the base-model being aligned. [2] Alignment Dataset Taxonomization: The alignment dataset 𝒟𝒟\\mathcal{D}caligraphic_D comprises pairs of text (tic,tirsuperscriptsubscript𝑡𝑖𝑐superscriptsubscript𝑡𝑖𝑟t_{i}^{c},t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT , italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT) where ticsuperscriptsubscript𝑡𝑖𝑐t_{i}^{c}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT is preferred by the human over tirsuperscriptsubscript𝑡𝑖𝑟t_{i}^{r}italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT presumably because it is more aligned with a set of defined target values. (Top) The alignment dataset is featurized using an LM-labeler based on a set of target features (intended for alignment, in black) and spoiler features (learned inadvertently, in grey). (Bottom) The alignment dataset is rewritten and re-featurized accordingly. [3] Reward Models (RMs): (Top) An RM maps a user input-model output pair t𝑡titalic_t to a score r (t).𝑟𝑡r(t).italic_r ( italic_t ) . We compare the RM before (pre-𝒟𝒟\\mathcal{D}caligraphic_D model ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG) and after (post-𝒟𝒟\\mathcal{D}caligraphic_D model ℛℛ\\mathcal{R}caligraphic_R) it is trained on the alignment dataset. (Bottom) The pair of rewards awarded by ℛℛ\\mathcal{R}caligraphic_R (r (tic),r (tir))𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{0,.5,.5}r}(t_{i}^{c}),{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}))( italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) is interpreted as vectors. The sign of r (tic)−r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})-{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) - italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) indicates whether the RM s scores are aligned or not with human preferences in the dataset. (r¯ (tic),r¯ (tir))¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟({\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c}),{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r}))( under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) , under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) ) denotes the reward vectors assigned by ℛ¯¯ℛ\\underline{\\mathcal{R}}under¯ start_ARG caligraphic_R end_ARG. [4] Evaluation Report for Anthropic/hh Alignment Dataset x OpenAssistant RM Alignment Pipeline: Results of the SEAL methodology applied to an open-source alignment pipeline purposed to render base models more helpful and harmless. (Feature Imprint) By regressing rewards against binary features indicators, we estimate that top features driving rewards are harmlessness, privacy-preserving, helpfulness, eloquence and sentiment. A feature imprint of β (harmlessness)=2.09𝛽harmlessness2.09\\beta(\\text{harmlessness})=2.09italic_β ( harmlessness ) = 2.09 implies that harmless text has a reward 2.092.092.092.09 points higher than harmful text. (Alignment Resistance) More than one out of four pairs in the alignment dataset have r (tic)<r (tir),𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r}),italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) , indicating that ℛℛ\\mathcal{R}caligraphic_R rewards the entry least preferred by the human (the teal arrow is in the misaligned space). Additionally, ℛℛ\\mathcal{R}caligraphic_R reverses alignment 8%percent88\\%8 % of the time (r¯ (tic)>r¯ (tir)¯𝑟superscriptsubscript𝑡𝑖𝑐¯𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{.75,.5,.25}\\underline{r}}(t_{i}^{c})>{\\color[rgb]{.75,.5,.25}%\n\\underline{r}}(t_{i}^{r})under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) > under¯ start_ARG italic_r end_ARG ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT ) and r (tic)<r (tir)𝑟superscriptsubscript𝑡𝑖𝑐𝑟superscriptsubscript𝑡𝑖𝑟{\\color[rgb]{0,.5,.5}r}(t_{i}^{c})<{\\color[rgb]{0,.5,.5}r}(t_{i}^{r})italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) < italic_r ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT )). (Robustness Scores) Rewriting entries to sound more positive increases the risks of misalignment.",
        "qtype": "Others",
        "response": "Here the thing boxed in red is the pair of “pre‐ and post‐reward vectors” that the reward model assigns to each (chosen, rejected) response.  By themselves, these two‐dimensional vectors are a very crude proxy for what humans actually mean when they say one answer is better than another.  In particular:\n\n  1.  They collapse all of the rich, often conflicting aspects of a human judgment (tone, style, values, safety, relevance, etc.) into just two numbers, losing almost all of the nuance.  \n  2.  Small changes in those scores (or small perturbations of the text) can swing the vector from the “aligned” half‐plane to the “misaligned” half‐plane, even if the human distinction between the two answers was minimal or noisy.  \n  3.  Because the boundary between aligned/misaligned is just the zero‐line of r(c)–r(r), the interpretation is hypersensitive to minor score shifts—so you get flips in “alignment” that don’t correspond to any real change in human preference.  \n  4.  Finally, these vectors themselves are learned from a noisy, biased human dataset, so treating their geometry as ground truth can mask deeper failures (e.g. concept confusion or overfitting to spurious “spoiler” features)."
    },
    {
        "question": "What limitations arise in CAP when compounding a fixed normal prompt with multiple abnormal prompts?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "One issue in ZSAD is that the fine-grained abnormality patterns can be very different from the auxiliary dataset to test datasets. In addition to the learning of a set of complementary fine-grained abnormality prompts, it is important to ensure that the learned fine-grained abnormality patterns are generalized to target testing datasets."
        ],
        "final_answer": "Because CAP compounds a fixed normal prompt with a static set of abnormal prompts, the resulting fine-grained abnormality semantics remain fixed and do not adapt to new test domains. In other words, CAP alone may fail to generalize its learned abnormal prompts when the target dataset exhibits different fine-grained anomaly patterns.",
        "relevant_elements": [
            "Compound Abnormality Prompting (CAP)"
        ],
        "id": 636,
        "masked_question": "What limitations arise in [mask1] when compounding a fixed normal prompt with multiple abnormal prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Compound Abnormality Prompting (CAP)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.10289v1_figure_2.png",
        "paperid": "2410.10289v1",
        "paper_path": "./papers/2410.10289v1.json",
        "figure_id": "2410.10289v1_figure_2.png",
        "caption": "Figure 2: Overview of FAPrompt. It consists of two novel modules, including the Compound Abnormality Prompting (CAP) module and the Data-dependent Abnormality Prior (DAP) module detailed in the top-right and bottom-right corners respectively. CAP is devised to learn fine-grained abnormality semantics without relying on detailed human annotations or text descriptions, while DAP is designed to adaptively select the most abnormal features from each query/test image as a sample-wise abnormality prior to enhance the cross-dataset generalizability of the abnormality prompts in CAP.",
        "qtype": "Others",
        "response": "Because all of the abnormal tokens get tacked onto exactly the same fixed “normal” context, they tend to collapse onto the same direction in CLIP’s embedding space and become semantically redundant.  In other words, simply compounding many abnormal‐token sets on top of one immutable normal prompt severely limits their diversity (they all stay too close to one another), so you end up with multiple “abnormal” prompts that really all say the same thing.  That is why the paper adds an orthogonality (diversity) loss to force each abnormal prompt apart."
    },
    {
        "question": "What alternative strategies could enhance DAP’s sample-wise abnormality prior beyond top M patch selection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Data-dependent Abnormality Prior (DAP)",
            "sample-wise abnormality prior"
        ],
        "id": 637,
        "masked_question": "What alternative strategies could enhance DAP’s [mask1] beyond top M patch selection?",
        "masked_number": 1,
        "masked_elements": [
            "sample-wise abnormality prior"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.10289v1_figure_2.png",
        "paperid": "2410.10289v1",
        "paper_path": "./papers/2410.10289v1.json",
        "figure_id": "2410.10289v1_figure_2.png",
        "caption": "Figure 2: Overview of FAPrompt. It consists of two novel modules, including the Compound Abnormality Prompting (CAP) module and the Data-dependent Abnormality Prior (DAP) module detailed in the top-right and bottom-right corners respectively. CAP is devised to learn fine-grained abnormality semantics without relying on detailed human annotations or text descriptions, while DAP is designed to adaptively select the most abnormal features from each query/test image as a sample-wise abnormality prior to enhance the cross-dataset generalizability of the abnormality prompts in CAP.",
        "qtype": "Others",
        "response": "First, let’s be clear about what “[mask1]” is pointing at in the figure.  In the bottom‐right inset you see a yellow block labeled “Sample‐wise Abnormality Prior,” that is exactly what DAP is producing by picking the top M patch‐token embeddings.  So the question is really:\n\n   “Beyond just taking the M highest‐scoring patches as your per‐image abnormality prior, what other strategies could you use to build that sample‐wise prior?”\n\nHere is a short chain of thought:\n\n1.  In DAP as described, you compute a similarity score between each patch token and the abnormality‐prompt prototype, then you hard‐select the top M patches and average them (plus a tiny MLP) to form a fixed‐length prior vector.  \n2.  Hard selection of the top M can be noisy (what if anomalies spread over many small regions?) or too coarse (it throws away the rest of the image entirely).  \n3.  Instead you could build a more flexible, softer or structural prior by:  \n   •  Soft‐attention over all patches: compute a softmax over patch scores and take the weighted sum of all patch embeddings rather than a hard top M.  \n   •  Learned patch‐gating module: train a small CNN or transformer over the grid of patch tokens to predict a continuous “anomaly mask,” then pool the masked tokens.  \n   •  Clustering or superpixel grouping: cluster the patch tokens (or operate on superpixels in pixel‐space) and select the most “anomalous” clusters or segments, which may aggregate spatially coherent cues better than isolated patches.  \n   •  Region‐proposal/ROI pooling: plug in an off‐the‐shelf unsupervised proposal method (e.g. selective search, RPN) to generate regions of interest, score each region against the abnormal prototype, and pool from the top regions.  \n   •  Multi‐scale pyramid pooling: extract priors not just from the base‐grid patches but also from coarser/finer feature maps, then fuse them.  This can catch anomalies of wildly different sizes.  \n   •  Self‐ or cross‐attention modules: let the text‐prompt features attend over the whole set of patch tokens (via a small transformer block), producing a dynamic per‐image prior without any manual top‐K.  \n\nAny of these would replace or augment the “top M patch selection” in DAP with a more graded, spatially coherent, or learned selection mechanism, and in practice often leads to more robust sample‐wise priors."
    },
    {
        "question": "What reasoning underlies applying an orthogonal constraint among abnormality prompts in CAP?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To capture complementary fine-grained abnormalities and reduce redundant information captured by the abnormality prompts, it is essential to maximize the diversity among the fine-grained abnormalities.",
            "A straightforward approach would be to train distinct abnormal prompts on separate, annotated subsets with samples from different anomalous types. However, this would require extensive human annotations. To address this issue, we propose to add an orthogonal constraint loss L_oc into the abnormality prompts in CAP as a alternative method to encourage this diversity."
        ],
        "final_answer": "The orthogonal constraint is applied to encourage maximum diversity among the fine-grained abnormality prompts. By enforcing orthogonality, the prompts capture complementary, non-redundant abnormal semantics without requiring separate annotations for each anomaly type.",
        "relevant_elements": [
            "Compound Abnormality Prompting (CAP)",
            "orthogonal constraint"
        ],
        "id": 638,
        "masked_question": "What reasoning underlies applying an orthogonal constraint among abnormality prompts in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Compound Abnormality Prompting (CAP)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.10289v1_figure_2.png",
        "paperid": "2410.10289v1",
        "paper_path": "./papers/2410.10289v1.json",
        "figure_id": "2410.10289v1_figure_2.png",
        "caption": "Figure 2: Overview of FAPrompt. It consists of two novel modules, including the Compound Abnormality Prompting (CAP) module and the Data-dependent Abnormality Prior (DAP) module detailed in the top-right and bottom-right corners respectively. CAP is devised to learn fine-grained abnormality semantics without relying on detailed human annotations or text descriptions, while DAP is designed to adaptively select the most abnormal features from each query/test image as a sample-wise abnormality prior to enhance the cross-dataset generalizability of the abnormality prompts in CAP.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed module in the CAP section is nothing mystical – it is simply the orthogonality penalty we put on the learned “abnormal” prompt vectors so that they don’t all collapse onto the same direction in embedding space.  In more pedestrian terms, because CAP is using K separate “abnormality” tokens to try to cover potentially very different fine‐grained defects, we explicitly push their CLIP embeddings to be mutually orthogonal (i.e. zero inner product).  This “orthogonal constraint” (denoted Lortho in the text) therefore guarantees that each abnormality prompt learns a distinct slice of the anomaly manifold, rather than redundantly overlapping with its siblings."
    },
    {
        "question": "What is the motivation for deriving a sample-wise abnormality prior in DAP for prompt adaptation?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "One issue in ZSAD is that the fine-grained abnormality patterns can be very different from the auxiliary dataset to test datasets.",
            "In addition to the learning of a set of complementary fine-grained abnormality prompts, it is important to ensure that the learned fine-grained abnormality patterns are generalized to target testing datasets.",
            "Inspired by the instance-conditional information design in CoCoOp (Zhou et al., 2022a ###reference_b62###), we introduce the DAP module to enhance the cross-dataset generalizability of the abnormal tokens in CAP by adaptively selecting the embeddings of the most abnormal regions to serve as a sample-wise abnormality prior for each image input."
        ],
        "final_answer": "Because fine-grained abnormality patterns can differ substantially between the auxiliary (training) data and a new test dataset, DAP derives a sample-wise abnormality prior (by selecting the most anomalous patches in each image) so that the learned abnormality prompts can dynamically adapt to the specific characteristics of each target image and thus generalize better across datasets.",
        "relevant_elements": [
            "Data-dependent Abnormality Prior (DAP)",
            "sample-wise abnormality prior"
        ],
        "id": 639,
        "masked_question": "What is the motivation for deriving a [mask1] in DAP for prompt adaptation?",
        "masked_number": 1,
        "masked_elements": [
            "sample-wise abnormality prior"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.10289v1_figure_2.png",
        "paperid": "2410.10289v1",
        "paper_path": "./papers/2410.10289v1.json",
        "figure_id": "2410.10289v1_figure_2.png",
        "caption": "Figure 2: Overview of FAPrompt. It consists of two novel modules, including the Compound Abnormality Prompting (CAP) module and the Data-dependent Abnormality Prior (DAP) module detailed in the top-right and bottom-right corners respectively. CAP is devised to learn fine-grained abnormality semantics without relying on detailed human annotations or text descriptions, while DAP is designed to adaptively select the most abnormal features from each query/test image as a sample-wise abnormality prior to enhance the cross-dataset generalizability of the abnormality prompts in CAP.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step‐by‐step:\n\n1.  The goal of CAP is to learn a small set of “abnormality prompts” that capture fine‐grained deviations from normal, but those learned prompts are fixed once trained on the auxiliary data.\n\n2.  In zero‐shot anomaly detection, the kinds of anomalies you see at test time can look very different (in appearance or location) from anything in your training set.  A single static prompt for “abnormal” therefore may fail to highlight the right features when you move to a new dataset.\n\n3.  To address this, the authors introduce the Data‐dependent Abnormality Prior (DAP).  DAP looks at each test image, selects the top–M most “abnormal” patch embeddings (via cosine similarity to the learned prompt prototype), and uses those embeddings to form a small “prior” vector.\n\n4.  That “prior” vector is then added into the abnormal‐token prompts of CAP, so that the model’s notion of “abnormal” is nudged towards the kinds of deviations actually present in the current image.\n\n5.  In other words, **the motivation for deriving a “sample‐wise abnormality prior” in DAP is to inject instance‐specific abnormal features into the prompts so they can adapt on‐the‐fly to the particular anomalies of a new test image**, greatly improving cross‐dataset generalization."
    },
    {
        "question": "What motivates incorporating MoE routing into MLP modules rather than using dense MLP processing?",
        "relevant_section_ids": [
            "3",
            "4.1"
        ],
        "relevant_context": [
            "Skipping a small number of heads or layers has negligible impact on model performance, with faster performance deterioration observed when skipping more MLP layers than removing attention heads. Importantly, the performance scaling differ between GSM8K and HumanEval datasets, indicating that the redundancy in the pretrained LLM is data-dependent. These results motivated us to explore learned, data-dependent routing modules that not only skip MLP layers and attention heads in a learnable manner, but also skip attention layers and subsets of MLP weights.",
            "As many pre-trained transformer models have dense MLP layers (no experts), ElastiFormer converts a dense MLP module to a MoE counterpart losslessly by breaking parameters into block matrices."
        ],
        "final_answer": "The empirical finding that dense MLP layers exhibit significant, data-dependent redundancy motivates converting them into Mixture-of-Experts modules. By decomposing a dense MLP into expert sub-networks, ElastiFormer can learn to route each input through only a subset of experts—thereby reducing the number of active parameters and computation without degrading model performance.",
        "relevant_elements": [
            "MLP",
            "MoE"
        ],
        "id": 640,
        "masked_question": "What motivates incorporating [mask1] routing into MLP modules rather than using dense MLP processing?",
        "masked_number": 1,
        "masked_elements": [
            "MoE"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15281v1_figure_1.png",
        "paperid": "2411.15281v1",
        "paper_path": "./papers/2411.15281v1.json",
        "figure_id": "2411.15281v1_figure_1.png",
        "caption": "Figure 1: \nOverview of ElastiFormer for language, visual, and multi-modal transformers.\n(Left) Illustration of learned routing modules around Multi-Head Attention (MHA) and Multi-layer Perceptron (MLP) modules of a pretrained transformer model.\n(Middle) Illustration of learned routing modules inside MLP and MHA modules, and learned routing that selects a subset of image tokens that provide multi-modal input to language decoder in VLMs.\n(Right) Illustration of self-distillation training objectives across modalities. Note that for Visual Transformers (ViT), the example provided here is from Masked Auto Encoding (MAE) ViT.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through the reasoning:\n\n1.  In Figure 2 the authors show that if you simply drop (“skip”) whole MLP layers at random in a pretrained transformer, you pay almost no loss in quality until you drop quite a lot of them.  That tells you the dense MLPs are *highly redundant* in a data‐dependent way.\n\n2.  Rather than statically pruning layers once and for all, ElastiFormer “breaks” each dense MLP into several smaller expert blocks (an MoE), and *learns* at run‐time which subset of those blocks to actually execute for any given input token.  \n\n3.  Because many inputs only need a few of those blocks, you save FLOPs and memory bandwidth—and you still recover the original dense MLP if you ever route all experts equally.\n\n4.  In short, the *data‐dependent redundancy* of dense MLPs motivates replacing them with a tiny router plus an MoE inside the MLP, so that each token only pays for the experts it actually needs, rather than always paying for the full dense matrix multiply."
    },
    {
        "question": "What reasoning supports selecting only certain visual encoder tokens for language decoder input?",
        "relevant_section_ids": [
            "1",
            "4"
        ],
        "relevant_context": [
            "Applying ElastiFormer to visual-language models, we show that 40% of image tokens can be dropped before being decoded by the language decoder without significantly impacting performance.",
            "Input Subset Selection: For this type of subset selection, given a sequence of N tokens, we select k to be processed by a given module (e.g. MHA, MLP). This routing scheme saves computational cost by reducing the total number of input tokens processed. Examples of routing in ElastiFormer that employ this scheme are routing around MHA, around MLP (Figure 1 (Left)), and visual token selection in VLM (Figure 1 (Mid-Bottom))."
        ],
        "final_answer": "Since many visual‐encoder tokens are redundant for generating the language output, ElastiFormer’s learned input-subset routing drops a fraction of image tokens—only the most informative k of N are sent to the language decoder—thereby cutting compute by not processing tokens that contribute little to performance.",
        "relevant_elements": [
            "Visual Encoder",
            "Language Decoder"
        ],
        "id": 641,
        "masked_question": "What reasoning supports selecting only certain [mask1] tokens for [mask2] input?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Encoder",
            "Language Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15281v1_figure_1.png",
        "paperid": "2411.15281v1",
        "paper_path": "./papers/2411.15281v1.json",
        "figure_id": "2411.15281v1_figure_1.png",
        "caption": "Figure 1: \nOverview of ElastiFormer for language, visual, and multi-modal transformers.\n(Left) Illustration of learned routing modules around Multi-Head Attention (MHA) and Multi-layer Perceptron (MLP) modules of a pretrained transformer model.\n(Middle) Illustration of learned routing modules inside MLP and MHA modules, and learned routing that selects a subset of image tokens that provide multi-modal input to language decoder in VLMs.\n(Right) Illustration of self-distillation training objectives across modalities. Note that for Visual Transformers (ViT), the example provided here is from Masked Auto Encoding (MAE) ViT.",
        "qtype": "Design_Rationale",
        "response": "Here’s the logic, step by step:\n\n1.  Context in the paper shows that many of the raw image‐encoder tokens (“mask1,” the red‐boxed Visual Encoder outputs) carry redundant information for the downstream task of text decoding.  \n2.  To exploit that, ElastiFormer wraps a tiny “input‐subset” router around the Visual Encoder.  That router looks at each visual token, emits a scalar score (logit), and then only passes the top-k tokens on to the Language Decoder (“mask2,” the blue box) while the rest are simply bypassed via the residual connection.  \n3.  Those top-k tokens are chosen because the router is trained by self-distillation: it minimizes a KL divergence (forward, top-50 in practice) between the student (elastic) and teacher (original) model outputs, plus—if it’s an autoregressive setting—a small binary‐classification loss that teaches it exactly which tokens the teacher would have used at generation time.  \n4.  Empirically, the authors find that up to 40 % of the image tokens can be dropped with no measurable loss in downstream language performance, which validates that only a subset of visual tokens need ever reach the language decoder."
    },
    {
        "question": "How does Route compute and apply its scalar weight to modulate Multihead Self-Attention outputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Route",
            "Multihead Self-Attention (MHA)"
        ],
        "id": 642,
        "masked_question": "How does [mask1] compute and apply its scalar weight to modulate Multihead Self-Attention outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Route"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15281v1_figure_1.png",
        "paperid": "2411.15281v1",
        "paper_path": "./papers/2411.15281v1.json",
        "figure_id": "2411.15281v1_figure_1.png",
        "caption": "Figure 1: \nOverview of ElastiFormer for language, visual, and multi-modal transformers.\n(Left) Illustration of learned routing modules around Multi-Head Attention (MHA) and Multi-layer Perceptron (MLP) modules of a pretrained transformer model.\n(Middle) Illustration of learned routing modules inside MLP and MHA modules, and learned routing that selects a subset of image tokens that provide multi-modal input to language decoder in VLMs.\n(Right) Illustration of self-distillation training objectives across modalities. Note that for Visual Transformers (ViT), the example provided here is from Masked Auto Encoding (MAE) ViT.",
        "qtype": "Implementation_Details",
        "response": "First, let me restate the piece of the diagram you circled in red: it is the little “flame” icon and the word “Route” directly below the Multi‐Head Self‐Attention box.  This is the learned, input‐dependent gate that we insert around the frozen MHA sub‐layer.  \n\nHere is what that router actually does, step by step:\n\n1.  **Linear projection to a logit.**  \n    For each token embedding xᶩᵢ entering the MHA block, we run a tiny “router” network (in practice just a single linear layer, though one can also use a one–hidden–layer MLP).  That router produces a single scalar logit gᵢ.\n\n2.  **(Top-k selection during training.)**  \n    We collect all of the gᵢ in a sequence of length L, pick the top-k of them, and mark those tokens as “active.”  \n    –  The selected tokens go on to be fed through the standard attention heads;  \n    –  the un-selected tokens are simply routed around the attention block (they get a zero contribution from it).\n\n3.  **Scalar gating weight.**  \n    Once we have decided which tokens are active, we normalize the corresponding gᵢ (either by a simple sigmoid or by a tiny softmax over the k selected tokens) to turn them into weights wᵢ∈[0,1], and set the other wᵢ to zero.  \n\n4.  **Modulation of the MHA output.**  \n    If Aᵢ is the usual MHA output for token i, we then compute  \n      ▷  yᵢ = xᶩᵢ + wᵢ · Aᵢ.  \n    In other words, wᵢ simply re‐scales (“gates”) how much of the full self‐attention output Aᵢ actually makes it into the residual sum.\n\n5.  **Auxiliary loss for the router.**  \n    In training we add a small binary‐cross‐entropy term to teach the router which tokens end up in the top-k of the final sequence (this mimics the “will this token affect the final output?” question).  \n    Together with the main distillation loss, this trains the router to turn on attention only for those tokens that really matter, and to gate out the rest.\n\nAt inference time you run the same tiny linear projector, pick the tokens whose logits are in the top-k, turn all the other wᵢ to zero, and multiply each chosen Aᵢ by its learned scalar wᵢ before adding back to xᶩ.  That is how the “Route” module computes and applies its per‐token scalar weight to modulate the frozen MHA outputs."
    },
    {
        "question": "How does Route operate on Projector outputs to select image tokens for the Language Decoder?",
        "relevant_section_ids": [
            "4",
            "4.1"
        ],
        "relevant_context": [
            "Input Subset Selection: For this type of subset selection, given a sequence of n tokens, we select k to be processed by a given module (e.g. MHA, MLP). This routing scheme saves computational cost by reducing the total number of input tokens processed. Examples of routing in ElastiFormer that employ this scheme are routing around MHA, around MLP (Figure 1 (Left)), and visual token selection in VLM (Figure 1 (Mid-Bottom)).",
            "As opposed to parameter subset selection where the router outputs a m-dimensional logits for m sub-networks, the routers in input subset selection output scalar-valued logits for each input token. The top-k input tokens are then processed by the subsequent module (MHA or MLP), and the output is added to a zero-initialized output tensor with the same shape as the un-selected input x."
        ],
        "final_answer": "Route takes the Projector’s sequence of image token embeddings, computes a scalar logit for each token, ranks them, and then selects the top-k tokens (highest logits) to pass into the Language Decoder. All other tokens are dropped (i.e., contribute zeros), so only the chosen subset of image tokens is decoded.",
        "relevant_elements": [
            "Route",
            "Projector"
        ],
        "id": 643,
        "masked_question": "How does [mask1] operate on [mask2] outputs to select image tokens for the Language Decoder?",
        "masked_number": 2,
        "masked_elements": [
            "Route",
            "Projector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.15281v1_figure_1.png",
        "paperid": "2411.15281v1",
        "paper_path": "./papers/2411.15281v1.json",
        "figure_id": "2411.15281v1_figure_1.png",
        "caption": "Figure 1: \nOverview of ElastiFormer for language, visual, and multi-modal transformers.\n(Left) Illustration of learned routing modules around Multi-Head Attention (MHA) and Multi-layer Perceptron (MLP) modules of a pretrained transformer model.\n(Middle) Illustration of learned routing modules inside MLP and MHA modules, and learned routing that selects a subset of image tokens that provide multi-modal input to language decoder in VLMs.\n(Right) Illustration of self-distillation training objectives across modalities. Note that for Visual Transformers (ViT), the example provided here is from Masked Auto Encoding (MAE) ViT.",
        "qtype": "Implementation_Details",
        "response": "The trick here is that Elasti-VLM simply re-uses the same “input-subset” router we’ve already seen around MHA/MLP, but now hooked up to the visual-projector outputs.  Concretely:\n\n 1. The projector (blue box) spits out a D-dimensional embedding for every image token.  \n 2. The red-boxed routing module is just a tiny linear head (one weight vector + bias) that takes each of those D-dim embeddings and produces a single scalar “importance” logit.  \n 3. We then take the top-k tokens by that logit score, drop the rest (or zero-pad them), and only those k embeddings go into the language decoder.  \n\nBecause it’s the exact same router used at training and inference, no extra parameters are needed at generation time, and we backpropagate through it with a straight-through/top-k estimator (plus an auxiliary BCE loss when distilling causal LMs)."
    },
    {
        "question": "How does probability quantization preserve relative likelihood magnitudes when assigning discrete FeFET I_DS states?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We initially convert the original probabilities into logarithmic values, then truncate very small probabilities to manage quantization precision efficiently.",
            "After quantization, we apply column normalization to the likelihoods corresponding to the same evidence value (i.e., the likelihoods stored in the same column) and priors: where each column of the normalized probabilities is added with a constant, with their maximum values scaled to 1. This normalization enhances the differences among posteriors of multiple events without altering their order of magnitude, thus mitigating the accuracy degradation after quantization.",
            "Finally, the normalized logarithmic probabilities are linearly mapped to discrete FeFET states with corresponding I_DS values and respective FeFET write configuration."
        ],
        "final_answer": "By first converting probabilities to logarithmic values and truncating very small ones, then normalizing each column so its maximum is scaled to 1 (preserving the order of magnitudes), and finally linearly mapping those normalized log-probabilities to discrete FeFET I_DS levels, the quantization process maintains the relative differences among likelihoods while assigning specific current states.",
        "relevant_elements": [
            "Probability Quantization",
            "Discrete FeFET I_DS"
        ],
        "id": 644,
        "masked_question": "How does [mask1] preserve relative likelihood magnitudes when assigning [mask2] states?",
        "masked_number": 2,
        "masked_elements": [
            "Probability Quantization",
            "Discrete FeFET I_DS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.19356v1_figure_2.png",
        "paperid": "2410.19356v1",
        "paper_path": "./papers/2410.19356v1.json",
        "figure_id": "2410.19356v1_figure_2.png",
        "caption": "Figure 2. \nThe overall workflow of FeBiM.\nTrained probabilities of the Bayesian model are quantized and mapped to discrete FeFET states.\nGiven observed evidence values, the FeFET-based crossbar outputs maximum posterior.",
        "qtype": "Implementation_Details",
        "response": "Mask 1 is the “Probability Quantization and Mapping” block (the red‐boxed panel 2b), and Mask 2 is the set of discrete FeFET I₍DS₎ states (the blue‐boxed panel 2c).  The way the quantization block preserves the relative magnitudes of the original likelihoods when it hands off values to those discrete FeFET states is as follows:\n\n 1.  Log‐Domain Conversion and Truncation  \n    – The continuous likelihoods are first converted into log-probabilities and any very small values are truncated.  \n    – Working in the log domain spreads out small differences and prevents tiny probabilities from vanishing.  \n\n 2.  Column Normalization  \n    – Within each evidence column, the log-likelihoods are shifted and scaled so that the largest value in that column becomes 1.  \n    – This operation amplifies the differences between levels without changing their rank order or relative spacing.  \n\n 3.  Uniform Linear Mapping to FeFET Levels  \n    – The normalized log-probabilities (all now between 0 and 1) are then uniformly quantized into K discrete steps.  \n    – Each quantization step is associated with a unique target I₍DS₎ conductance state.  \n    – Because the quantization is linear over the [0,1] interval, two values that differ by Δ before mapping will still differ by Δ′ (proportional to Δ) in the assigned FeFET state.  \n\nBy doing “log → normalize → linear-quantize,” the framework guarantees that (a) larger likelihoods map to higher conductance states, (b) the gap between any two probabilities is preserved up to the chosen quantization resolution, and (c) the ordering of likelihoods (which event is most likely, second most likely, etc.) remains exactly the same once they have been programmed into the discrete FeFET I₍DS₎ states."
    },
    {
        "question": "How are activated quantized likelihood columns aggregated into row currents for posterior determination?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "During the inference, discretized evidence values of the test samples activate corresponding crossbar columns. The stored likelihoods are accumulated along each row, as shown in Fig. 2(c). In this way, the posterior probabilities for each event are yielded as the crossbar outputs without extra calculation circuitry.",
            "One column of each likelihood block is activated according to the input evidence value on BLs, and other unselected columns are inhibited with V_READ = –0.5 V. The activated FeFET cells’ I_DS accumulate along each sourceline as I_SL, representing the calculated posteriors (denoted in green)."
        ],
        "final_answer": "During inference, one quantized likelihood column per evidence node is driven ON; each FeFET in those activated columns sources a current proportional to its stored log-likelihood, and these currents sum along each row (sourceline). The resulting total row current (I_SL) directly encodes the posterior probability for each event.",
        "relevant_elements": [
            "Quantized Likelihoods",
            "Row currents"
        ],
        "id": 645,
        "masked_question": "How are activated [mask1] columns aggregated into row currents for posterior determination?",
        "masked_number": 1,
        "masked_elements": [
            "Quantized Likelihoods"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.19356v1_figure_2.png",
        "paperid": "2410.19356v1",
        "paper_path": "./papers/2410.19356v1.json",
        "figure_id": "2410.19356v1_figure_2.png",
        "caption": "Figure 2. \nThe overall workflow of FeBiM.\nTrained probabilities of the Bayesian model are quantized and mapped to discrete FeFET states.\nGiven observed evidence values, the FeFET-based crossbar outputs maximum posterior.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed label in Fig. 2(b) is simply the “Quantized Likelihoods” that have been mapped into the FeFET crossbar.  During inference (see Sec. 3.2), exactly one column in each likelihood “block” is driven high (VBL=+0.5 V) according to the observed evidence, while all other columns sit at −0.5 V to keep them off.  Because every FeFET in a given column is tied to the same gate (the bit-line) and every FeFET in a given row shares the same source (the source-line), the drain currents of all the turned-on devices in that row simply add together by Kirchhoff’s law onto the row line.  That summed current on row i is thus\n\n   I row,i  =  ∑ (Gij × 0.5 V)\n\nwhere Gij is the conductance (i.e. the stored, quantized log-likelihood) of the cell at row i, column j.  If you also drive the “prior” column, its current is added in the same way.  In this fashion each row’s total current directly encodes the (log-domain) posterior for one hypothesis.  A simple WTA sense amplifier then picks the row with the highest current as the final decision."
    },
    {
        "question": "How do discrete FeFET I_DS outputs emulate posterior accumulation in in-memory Bayesian inference?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "During inference, the prior column is activated with 0.5 V on SL. One column of each likelihood block is activated according to the input evidence value on WLs, and other unselected columns are inhibited with −0.5 V. The activated FeFET cells’ I_DS accumulate along each SL as I_SL, representing the calculated posteriors (denoted in green).",
            "Finally, the normalized logarithmic probabilities L are linearly mapped to discrete FeFET states with corresponding I_DS values and respective FeFET write configuration."
        ],
        "final_answer": "Each FeFET cell is programmed to one of several discrete I_DS levels that encode quantized logarithmic priors and likelihoods. During inference, only the columns corresponding to the observed evidence are driven, so the cells whose I_DS represent the chosen likelihoods (and the prior column) turn on. Their drain currents then sum along each sourceline, effectively performing a sum of log-probabilities (i.e., the log of the product of prior and likelihoods). The total current on each line therefore directly maps to the posterior for each event, with higher accumulated I_DS indicating a larger posterior probability.",
        "relevant_elements": [
            "Discrete FeFET I_DS",
            "In-Memory Bayesian Inference"
        ],
        "id": 647,
        "masked_question": "How do [mask1] outputs emulate posterior accumulation in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Discrete FeFET I_DS",
            "In-Memory Bayesian Inference"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.19356v1_figure_2.png",
        "paperid": "2410.19356v1",
        "paper_path": "./papers/2410.19356v1.json",
        "figure_id": "2410.19356v1_figure_2.png",
        "caption": "Figure 2. \nThe overall workflow of FeBiM.\nTrained probabilities of the Bayesian model are quantized and mapped to discrete FeFET states.\nGiven observed evidence values, the FeFET-based crossbar outputs maximum posterior.",
        "qtype": "Literature_Background",
        "response": "Step-by-step reasoning (chain of thought):\n\n1. In our design each cell in the crossbar is a multi-bit FeFET whose conductance has been programmed to one of a small number of discrete states.  Those states encode the quantized log-probabilities (prior or likelihood) for a given hypothesis/evidence pair.  \n2. During inference we present a one-hot voltage on the gate lines of the selected “evidence” columns (and on the prior column).  FeFETs in those activated columns turn on and source a drain current Iₙ proportional to their stored log-probability weight.  \n3. Kirchhoff’s current law then forces all of those individual FeFET currents in a given row to sum together onto the common sourceline for that row.  \n4. Because each FeFET’s current is proportional to its stored log-probability, that row-sum current exactly equals  \n   (log prior) + (log likelihood₁) + … + (log likelihood_N)  \n   i.e. the un-normalized log‐posterior for that hypothesis.  \n5. Thus the “Discrete FeFET I₍DS₎” outputs (the red-boxed currents) physically accumulate along each row in the in-memory Bayesian inference block (the blue-boxed crossbar), yielding an analog current that encodes the posterior.  \n\nAnswer:\n\nThe discrete FeFET I₍DS₎ outputs act as analog current sources whose amplitudes represent individual log‐probability terms.  When you activate the appropriate columns for your evidence and prior, those per-cell currents simply sum along each row (Kirchhoff’s law), so that the total row current is the sum of log-prior plus log-likelihoods—that is, the posterior—directly in memory."
    },
    {
        "question": "How do multi-scale feature maps influence cost volume formation in depth estimation from sparse views?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "These two views are then fed into a shared image encoder ε_img to generate multiscale image features f_l^s and f_r^s where s is the feature scale.",
            "From the feature maps f_l^s of each source view, a cost volume C is generated by correlating the two feature maps."
        ],
        "final_answer": "The method first encodes each sparse source view into a set of multi-scale feature maps. It then constructs the cost volume by correlating the corresponding feature maps at each scale between the two views, thereby allowing the depth estimator to leverage matching cues across multiple resolutions.",
        "relevant_elements": [
            "Multi Scale Feature Maps",
            "Cost Volume"
        ],
        "id": 648,
        "masked_question": "How do [mask1] influence cost volume formation in depth estimation from sparse views?",
        "masked_number": 1,
        "masked_elements": [
            "Multi Scale Feature Maps"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.03086v1_figure_1.png",
        "paperid": "2411.03086v1",
        "paper_path": "./papers/2411.03086v1.json",
        "figure_id": "2411.03086v1_figure_1.png",
        "caption": "Figure 1: \nThe HFGaussian pipeline: Given a target view, the nearest source views Ilsubscript𝐼𝑙I_{l}italic_I start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Irsubscript𝐼𝑟I_{r}italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT are selected, and passed through an image encoder ϵi⁢m⁢gsubscriptitalic-ϵ𝑖𝑚𝑔\\epsilon_{img}italic_ϵ start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT to generate feature maps flssuperscriptsubscript𝑓𝑙𝑠f_{l}^{s}italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT and frssuperscriptsubscript𝑓𝑟𝑠f_{r}^{s}italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT for depth maps Dlsubscript𝐷𝑙D_{l}italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Drsubscript𝐷𝑟D_{r}italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT estimation.\nThe depth maps are then encoded using a ϵd⁢e⁢p⁢t⁢hsubscriptitalic-ϵ𝑑𝑒𝑝𝑡ℎ\\epsilon_{depth}italic_ϵ start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT encoder and combined with the image features before passing through a U-Net based decoder ϵp⁢a⁢r⁢a⁢m⁢ssubscriptitalic-ϵ𝑝𝑎𝑟𝑎𝑚𝑠\\epsilon_{params}italic_ϵ start_POSTSUBSCRIPT italic_p italic_a italic_r italic_a italic_m italic_s end_POSTSUBSCRIPT to predict Gaussian feature maps ℳrsubscriptℳ𝑟\\mathcal{M}_{r}caligraphic_M start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, ℳssubscriptℳ𝑠\\mathcal{M}_{s}caligraphic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, ℳαsubscriptℳ𝛼\\mathcal{M}_{\\alpha}caligraphic_M start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT, and ℳfsubscriptℳ𝑓\\mathcal{M}_{f}caligraphic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT.\nFinally, the predicted Gaussians are splatted and rasterized to generate the novel view and human features, which are further processed by a smaller MLP ϵf⁢e⁢a⁢t⁢u⁢r⁢esubscriptitalic-ϵ𝑓𝑒𝑎𝑡𝑢𝑟𝑒\\epsilon_{feature}italic_ϵ start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t italic_u italic_r italic_e end_POSTSUBSCRIPT to obtain the final human features.",
        "qtype": "Literature_Background",
        "response": "Here “[mask1]” are the multi‐scale feature maps fₗˢ and fᵣˢ produced by the image encoder for the two source views.  In the pipeline they are fed directly into the cost‐volume construction block.  Concretely, at each scale s you take the left and right feature maps fₗˢ, fᵣˢ and—for a discrete set of depth (or disparity) hypotheses—compute a matching score (e.g. via correlation or concatenation) between them.  Stacking these matching scores over all depth levels (and across all scales) yields the 4D cost volume that the depth‐estimator network then refines.  Because you supply features at multiple resolutions, the cost volume captures both fine, local image structure and coarser, contextual cues, which makes the subsequent depth estimation from only two or three sparse views both more accurate and more robust."
    },
    {
        "question": "How does MLP ε_feature extend Gaussian rasterizer outputs using feature splatting concepts?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Inspired by feature splatting [36], we apply a similar technique that estimates human feature vectors h by splatting Gaussian features f in the image plane, and then blending the feature vectors using alpha composition:",
            "The blended feature vectors h are decoded by a MLP consisting of two linear layers with ReLU activation functions, followed by a final layer with a sigmoid activation function, to render the continuous surface embeddings."
        ],
        "final_answer": "MLP ε_feature takes the per‐Gaussian feature maps produced by the Gaussian rasterizer, which have been 'splat' onto the image plane and alpha-blended, and decodes these composite feature vectors through two ReLU‐activated linear layers plus a final sigmoid layer. In this way, it extends the raw rasterizer outputs into smooth, continuous surface embeddings (e.g. dense-pose features) using the principle of feature splatting.",
        "relevant_elements": [
            "Gaussian Rasterizer",
            "MLP ε_feature"
        ],
        "id": 649,
        "masked_question": "How does [mask1] extend Gaussian rasterizer outputs using feature splatting concepts?",
        "masked_number": 1,
        "masked_elements": [
            "MLP ε_feature"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.03086v1_figure_1.png",
        "paperid": "2411.03086v1",
        "paper_path": "./papers/2411.03086v1.json",
        "figure_id": "2411.03086v1_figure_1.png",
        "caption": "Figure 1: \nThe HFGaussian pipeline: Given a target view, the nearest source views Ilsubscript𝐼𝑙I_{l}italic_I start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Irsubscript𝐼𝑟I_{r}italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT are selected, and passed through an image encoder ϵi⁢m⁢gsubscriptitalic-ϵ𝑖𝑚𝑔\\epsilon_{img}italic_ϵ start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT to generate feature maps flssuperscriptsubscript𝑓𝑙𝑠f_{l}^{s}italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT and frssuperscriptsubscript𝑓𝑟𝑠f_{r}^{s}italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT for depth maps Dlsubscript𝐷𝑙D_{l}italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Drsubscript𝐷𝑟D_{r}italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT estimation.\nThe depth maps are then encoded using a ϵd⁢e⁢p⁢t⁢hsubscriptitalic-ϵ𝑑𝑒𝑝𝑡ℎ\\epsilon_{depth}italic_ϵ start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT encoder and combined with the image features before passing through a U-Net based decoder ϵp⁢a⁢r⁢a⁢m⁢ssubscriptitalic-ϵ𝑝𝑎𝑟𝑎𝑚𝑠\\epsilon_{params}italic_ϵ start_POSTSUBSCRIPT italic_p italic_a italic_r italic_a italic_m italic_s end_POSTSUBSCRIPT to predict Gaussian feature maps ℳrsubscriptℳ𝑟\\mathcal{M}_{r}caligraphic_M start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, ℳssubscriptℳ𝑠\\mathcal{M}_{s}caligraphic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, ℳαsubscriptℳ𝛼\\mathcal{M}_{\\alpha}caligraphic_M start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT, and ℳfsubscriptℳ𝑓\\mathcal{M}_{f}caligraphic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT.\nFinally, the predicted Gaussians are splatted and rasterized to generate the novel view and human features, which are further processed by a smaller MLP ϵf⁢e⁢a⁢t⁢u⁢r⁢esubscriptitalic-ϵ𝑓𝑒𝑎𝑡𝑢𝑟𝑒\\epsilon_{feature}italic_ϵ start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t italic_u italic_r italic_e end_POSTSUBSCRIPT to obtain the final human features.",
        "qtype": "Literature_Background",
        "response": "Here the red‐boxed module is simply the small “feature MLP” (ε_feature) that sits on top of the Gaussian rasterizer.  In HFGaussian we don’t stop at splatting and blending RGB + α – we also splat per‐Gaussian feature vectors (the maps ℳ_f), alpha‐blend them in the image plane exactly as in feature‐splatting [36], and then pass the resulting, per‐pixel feature vectors through this two‐layer MLP (with ReLU activations and a final sigmoid).  That MLP decodes the blended features into the continuous surface embeddings (the dense human feature outputs)."
    },
    {
        "question": "How does the cost volume inform depth refinement within the depth estimator to generate D_l and D_r?",
        "relevant_section_ids": [
            "3.2",
            "3.1"
        ],
        "relevant_context": [
            "From the feature maps f^l and f^r of each source view, a cost volume C is generated by correlating the two feature maps. Then, an iterative update mechanism is used to estimate depth maps D_l and D_r corresponding to each source view.",
            "Using the feature maps from each source view (f_i, f_j), a 3D correlation volume C_ij is generated. This correlation volume along with the corresponding camera parameters (P_i, P_j) is used to iteratively estimate depth maps. It can be formulated as: where f_update represents the depth estimation module."
        ],
        "final_answer": "The cost volume is built by correlating the left and right image feature maps to encode pixel‐wise matching costs. This volume is then fed into the depth estimator’s iterative update mechanism (f_update), which uses those matching cues to progressively refine and output the per‐view depth maps D_l and D_r.",
        "relevant_elements": [
            "Cost Volume",
            "Depth Estimator",
            "Depth Maps D_l and D_r"
        ],
        "id": 651,
        "masked_question": "How does the [mask1] inform depth refinement within the [mask2] to generate D_l and D_r?",
        "masked_number": 2,
        "masked_elements": [
            "Cost Volume",
            "Depth Estimator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.03086v1_figure_1.png",
        "paperid": "2411.03086v1",
        "paper_path": "./papers/2411.03086v1.json",
        "figure_id": "2411.03086v1_figure_1.png",
        "caption": "Figure 1: \nThe HFGaussian pipeline: Given a target view, the nearest source views Ilsubscript𝐼𝑙I_{l}italic_I start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Irsubscript𝐼𝑟I_{r}italic_I start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT are selected, and passed through an image encoder ϵi⁢m⁢gsubscriptitalic-ϵ𝑖𝑚𝑔\\epsilon_{img}italic_ϵ start_POSTSUBSCRIPT italic_i italic_m italic_g end_POSTSUBSCRIPT to generate feature maps flssuperscriptsubscript𝑓𝑙𝑠f_{l}^{s}italic_f start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT and frssuperscriptsubscript𝑓𝑟𝑠f_{r}^{s}italic_f start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT for depth maps Dlsubscript𝐷𝑙D_{l}italic_D start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT and Drsubscript𝐷𝑟D_{r}italic_D start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT estimation.\nThe depth maps are then encoded using a ϵd⁢e⁢p⁢t⁢hsubscriptitalic-ϵ𝑑𝑒𝑝𝑡ℎ\\epsilon_{depth}italic_ϵ start_POSTSUBSCRIPT italic_d italic_e italic_p italic_t italic_h end_POSTSUBSCRIPT encoder and combined with the image features before passing through a U-Net based decoder ϵp⁢a⁢r⁢a⁢m⁢ssubscriptitalic-ϵ𝑝𝑎𝑟𝑎𝑚𝑠\\epsilon_{params}italic_ϵ start_POSTSUBSCRIPT italic_p italic_a italic_r italic_a italic_m italic_s end_POSTSUBSCRIPT to predict Gaussian feature maps ℳrsubscriptℳ𝑟\\mathcal{M}_{r}caligraphic_M start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, ℳssubscriptℳ𝑠\\mathcal{M}_{s}caligraphic_M start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT, ℳαsubscriptℳ𝛼\\mathcal{M}_{\\alpha}caligraphic_M start_POSTSUBSCRIPT italic_α end_POSTSUBSCRIPT, and ℳfsubscriptℳ𝑓\\mathcal{M}_{f}caligraphic_M start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT.\nFinally, the predicted Gaussians are splatted and rasterized to generate the novel view and human features, which are further processed by a smaller MLP ϵf⁢e⁢a⁢t⁢u⁢r⁢esubscriptitalic-ϵ𝑓𝑒𝑎𝑡𝑢𝑟𝑒\\epsilon_{feature}italic_ϵ start_POSTSUBSCRIPT italic_f italic_e italic_a italic_t italic_u italic_r italic_e end_POSTSUBSCRIPT to obtain the final human features.",
        "qtype": "Experimental_Results",
        "response": "First, note that the little red‐boxed component is the 3D “cost volume” built by correlating the two source‐view feature maps, and the big blue‐boxed region is the iterative depth‐estimation (or “depth estimator”) module that spits out the two depth maps Dₗ and Dᵣ.\n\nStep by step:  \n1. We encode each source image (Iₗ, Iᵣ) with ϵimg to get multi‐scale feature maps fₗˢ, fᵣˢ.  \n2. Those feature maps are cross–correlated along a discrete set of candidate depths to form the cost volume (mask1).  This volume carries, for every pixel and every depth hypothesis, a measure of how well that pixel lines up between the two views.  \n3. The cost volume is fed into the depth estimator (mask2), together with the image features.  Inside this block a small U-Net‐style (or recurrent‐update) network learns to read off which depth hypotheses have high matching confidence and to produce a refined depth prediction.  \n4. By iteratively “reading” the cost volume and regressing depth residuals, the depth estimator converges on per‐pixel depths, yielding the final Dₗ and Dᵣ.  \n\nIn short: the cost volume tells the depth‐estimator module “where the views agree” at each trial depth, and the depth‐estimator network uses those cues (along with image features) to refine and output the two depth maps."
    },
    {
        "question": "How does the transition from Image Latents to Noisy Latents influence subsequent denoising in the 2D Diffusion model?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Image Latents",
            "Noisy Latents",
            "2D Diffusion model"
        ],
        "id": 652,
        "masked_question": "How does the transition from [mask1] to [mask2] influence subsequent denoising in the 2D Diffusion model?",
        "masked_number": 2,
        "masked_elements": [
            "Image Latents",
            "Noisy Latents"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.06693v1_figure_2.png",
        "paperid": "2408.06693v1",
        "paper_path": "./papers/2408.06693v1.json",
        "figure_id": "2408.06693v1_figure_2.png",
        "caption": "Figure 2:  Methods comparison. We extended the Diffusion Classifier [18] paper to a multi-view [40] settings and we compare with our DC3DO model, based on [54]",
        "qtype": "Experimental_Results",
        "response": "In the top‐row “Multi‐View Diffusion” block the red‐boxed tensor is simply your clean, encoded image–latent (Eφ(x)), and the blue‐boxed tensor is exactly that same latent after you have injected a bit of Gaussian noise according to your forward‐diffusion schedule at time t.  Concretely:\n\n 1.  You start with the red box (call it z₀)  \n 2.  You sample ϵ∼N(0,I) and form the noisy latent  \n       zₜ = √ᾱₜ · z₀ + √(1−ᾱₜ) · ϵ  \n    which is what you see in the blue box.  \n 3.  That blue‐boxed zₜ is the actual input to your 2D U-Net denoiser, which, conditioned on the class embedding c and t, must predict back the injected ϵ.  \n\nBecause zₜ carries exactly the right amount of corruption (determined by t and the α schedule), it defines the difficulty of the denoising step—i.e. how much signal vs.\\ noise the network must learn to separate.  In short, the red→blue transition is the forward‐diffusion corruption step that creates the model’s training and inference signal for every denoising U-Net pass."
    },
    {
        "question": "What limitations stem from using PVCNN Shape Latent Encoder for shape latent extraction in DC3DO?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "PVCNN",
            "Shape latent"
        ],
        "id": 654,
        "masked_question": "What limitations stem from using [mask1] Shape Latent Encoder for shape latent extraction in DC3DO?",
        "masked_number": 1,
        "masked_elements": [
            "PVCNN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.06693v1_figure_2.png",
        "paperid": "2408.06693v1",
        "paper_path": "./papers/2408.06693v1.json",
        "figure_id": "2408.06693v1_figure_2.png",
        "caption": "Figure 2:  Methods comparison. We extended the Diffusion Classifier [18] paper to a multi-view [40] settings and we compare with our DC3DO model, based on [54]",
        "qtype": "Others",
        "response": "The red‐boxed “Shape Latent Encoder” in our DC3DO diagram is in fact a PVCNN (Point–Voxel CNN) that LION uses to turn a raw 3D point cloud into a fixed-size “global” latent vector.  While it gives you a nice, compact code that’s cheap to diffuse over, it comes with two main drawbacks:\n\n 1. Coarse, global‐only representation  \n    – PVCNN is designed to pull out high-level, overall shape cues; by design it collapses away most of the per-point and fine-grain geometry.  In DC3DO we in fact discard the full point-structured “local” LION latents and only diffuse the global code.  That means any subtle surface details (small handles on a mug, thin struts on a chair, subtle curvature differences in car fenders, etc.) have already been averaged out before the diffusion even begins.  As a result the classifier can struggle to tell apart classes that differ only in those fine details, or to generalize to novel out-of-distribution shapes that hinge on precisely those missing local features.\n\n 2. Pretraining and distributional bias  \n    – The PVCNN/LION encoder was pretrained on a finite subset of ShapeNet classes (chairs, cars, airplanes).  Its global latent is optimized to reconstruct or generate shapes from exactly that distribution.  When you freeze it and then run a diffusion-classifier on top, any blind spots or geometric biases it learned in that pretraining get baked into your “shape embedding.”  In practice this can hurt zero-shot or few-shot classification of new classes, atypical topologies (e.g. highly non-manifold point clouds) or different point densities and noise characteristics that PVCNN never saw.\n\nPut simply, by leaning on a PVCNN global encoder you gain speed and compactness at the expense of discarding the bulk of the local, discriminative geometry and inheriting all of the encoder’s own training biases."
    },
    {
        "question": "What biases might Identity Embedding introduce in Inter-Atlas Message-Passing, affecting cross-atlas fusion equity?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Identity Embedding",
            "Inter-Atlas Message-Passing"
        ],
        "id": 656,
        "masked_question": "What biases might [mask1] introduce in Inter-Atlas Message-Passing, affecting cross-atlas fusion equity?",
        "masked_number": 1,
        "masked_elements": [
            "Identity Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.08228v1_figure_1.png",
        "paperid": "2410.08228v1",
        "paper_path": "./papers/2410.08228v1.json",
        "figure_id": "2410.08228v1_figure_1.png",
        "caption": "Figure 1: The framework of AIDFusion for multi-atlas brain network classification.",
        "qtype": "Others",
        "response": "First, let’s be crystal‐clear about what the red box is flagging.  In Fig. 1a (and in Sec. 4.1) the authors introduce a “learnable identity embedding” for every ROI before they ever send anything through their disentangle‐Transformer or the inter‐atlas GCN.  Concretely, each node in atlas a (or atlas b) picks up a d–dimensional vector that says “I am ROI 17 in Schaefer” or “I am ROI 53 in AAL.”  Nothing in that embedding is forced to line up across atlases—you’re simply giving each region its own unique tag.\n\nNow move on to the inter‐atlas message‐passing (Sec. 4.2 and Fig. 1c):  once you have those identity–augmented features you connect each ROI in atlas a to its k–nearest neighbors (by spatial centroid) in atlas b and run a GCN.  The GCN update uses whatever feature each node carries—in our case that heavily includes the identity tag.\n\nChain of thought on the bias that can creep in:\n\n 1.  Identity embeddings are purely atlas–specific tags.  They have no guarantee of being comparable across atlases—ROI 17 in Schaefer might sit next to ROI 53 in AAL in feature–space purely because their learned identity vectors happen to line up, not because they share any true functional or anatomical similarity.\n\n 2.  During the GCN‐style message‐passing, nodes send and receive messages weighted by some function of their features (including identity).  If two nodes happen to have very similar identity‐tags (by accident of initialization or training), they will pass information back and forth far more strongly than two nodes whose tags remain orthogonal—even if in reality the latter pair should carry more complementary information.\n\n 3.  In effect you can end up privileging atlas–specific “celebrity” ROIs whose identity embeddings have drifted towards the bulk of the feature‐space.  Those “popular” nodes get to dump all their information everywhere, starving the other nodes and tilting the fused representation.\n\n 4.  The net result is an inequitable fusion:  some ROIs (and hence some atlases) dominate the cross‐atlas message‐passing simply because their identity embeddings turned out to be “closer” in that learned space, not because they are functionally complementary.\n\nIn plain language: by sticking a freely learned ID‐vector on each node, you risk turning your inter‐atlas GCN into a popularity contest—nodes with similar identity‐tags exchange lots of messages, nodes with dissimilar tags are shut out.  That upsets the goal of equal, spatially‐driven fusion and lets atlas‐specific idiosyncrasies swamp genuine cross‐atlas signals."
    },
    {
        "question": "Why apply identity embedding prior to the disentangle transformer?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Correlation-based brain networks already contain sufficient positional information for ROIs, making general positional embeddings both costly and redundant.",
            "Instead, we propose a learnable identity embedding that adaptively learns a unique identity for each ROI, aligning nodes in the same ROI across the same atlas.",
            "This embedding assigns the same identity to nodes within the same ROI."
        ],
        "final_answer": "Identity embedding is applied before the disentangle transformer because in fully connected fMRI brain networks general positional embeddings are expensive and unnecessary; instead, a learnable identity embedding gives each ROI a unique, consistent identity (aligning nodes within the same ROI) so that the transformer can distinguish and properly process each region.",
        "relevant_elements": [
            "Identity Embedding",
            "Disentangle Transformer"
        ],
        "id": 658,
        "masked_question": "Why apply [mask1] prior to the disentangle transformer?",
        "masked_number": 1,
        "masked_elements": [
            "Identity Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.08228v1_figure_1.png",
        "paperid": "2410.08228v1",
        "paper_path": "./papers/2410.08228v1.json",
        "figure_id": "2410.08228v1_figure_1.png",
        "caption": "Figure 1: The framework of AIDFusion for multi-atlas brain network classification.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-Thought  \n1. In the figure, the red-boxed module is the “Identity Embedding” that takes the raw node features X (the fMRI‐based connectivity of each ROI) and appends a small learnable vector U to every node.   \n2. The paper explains that standard positional encodings (e.g. based on graph distances or eigenvectors) are either too expensive or redundant for a fully‐connected correlation graph.  Instead, they introduce a very cheap, learnable identity embedding that gives each ROI its own “tag.”  \n3. A Transformer has no built-in notion of “which node is which” unless you give it positional or identity cues—otherwise every node looks identical in the attention layer.  \n4. By inserting the identity embedding *before* the Disentangle Transformer, each ROI carries a unique, atlas‐consistent identifier into the self-attention layers.  That makes it possible for the model to (a) align homologous ROIs across atlases, and (b) let the Disentangle Transformer learn which parts of each node’s representation are atlas-specific (to be filtered away) versus shared (to be kept).  \n\nAnswer  \nWe apply the identity embedding prior to the Disentangle Transformer so that each ROI node carries a unique, learnable “identity tag” into the self-attention layers.  This cheaply replaces expensive positional encodings, aligns corresponding ROIs across atlases, and gives the transformer the information it needs to distinguish nodes when it is filtering out atlas-specific (inconsistent) signals."
    },
    {
        "question": "How does inter-atlas message-passing interact with population-level consistency preservation?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Our proposed AIDFusion enables inter-atlas message-passing between neighboring regions in different atlases by considering spatial information. Specifically, we use the spatial distance between the centroids of ROIs in different atlases to construct inter-atlas connections. As shown in Figure 1c, we utilize the k-nearest-neighbor (NN) algorithm to connect each ROI to k ROIs from the other atlas. ... Afterwards, an adjacency matrix A_inter is obtained and used for graph convolution [22]:\nZ = σ(D^{-1/2} A_inter D^{-1/2} H W),\nwhere σ is the activation function, D is the degree matrix of A_inter, H is the combined node representation matrix for the two atlases, and W is the learnable weight matrix of the GCN layer.",
            "Population-level Consistency. The readout function R is an essential component of learning the graph-level representations M for brain network analysis (e.g., classification), which maps a set of learned node-level embeddings to a graph-level embedding. To further constrain the consistency for graph representations across different atlases, we introduce a mean squared error (MSE) loss on the population level. As shown in Figure 1e, a population graph G_pop is constructed by computing the similarity of each two subjects’ graph representations in the same atlas. The intuition here is we aim to maintain the relationship of subjects across atlases, instead of directly enforcing graph representations of two atlases to be the same. Such loss is formulated as follows:\nL_pop = ‖S(M^a) – S(M^b)‖_F^2,\nwhere S(·) computes the pairwise similarity matrix of the graph representations in a batch."
        ],
        "final_answer": "Inter-atlas message-passing first fuses ROI-level features across atlases via spatially grounded connections and GCN updates, producing node embeddings that incorporate complementary information from both parcellations. These enhanced node embeddings are then pooled by a readout function into graph-level representations, whose inter-subject similarity structure is constrained by the population-level MSE loss. In this way, after performing inter-atlas message-passing, the model preserves the relative relationships among subjects’ graph representations across different atlases.",
        "relevant_elements": [
            "Inter-Atlas Message-Passing",
            "Population-level Consistency"
        ],
        "id": 659,
        "masked_question": "How does [mask1] interact with [mask2] preservation?",
        "masked_number": 2,
        "masked_elements": [
            "Inter-Atlas Message-Passing",
            "Population-level Consistency"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.08228v1_figure_1.png",
        "paperid": "2410.08228v1",
        "paper_path": "./papers/2410.08228v1.json",
        "figure_id": "2410.08228v1_figure_1.png",
        "caption": "Figure 1: The framework of AIDFusion for multi-atlas brain network classification.",
        "qtype": "Design_Rationale",
        "response": "Let’s first pin down what the two masked blocks are:\n\n1.  [mask1]  (red box) is the Inter-Atlas Message-Passing module (Sec. 4.2).  \n2.  [mask2]  (blue box) is the Population-Level Consistency preservation (Sec. 4.3).\n\nNow – how do they “talk to” each other?\n\n1.  Inter-Atlas Message-Passing takes the disentangled node features from each atlas and, using a k-NN graph built on ROI centroids, runs a light GCN to let information flow across atlas boundaries.  The output of this step is two refined sets of node representations, which we’ll call Mᵃ and Mᵇ.  \n\n2.  Next, each of these fused node sets is fed through a readout to produce two graph‐level embeddings (one per atlas).  \n\n3.  Population-Level Consistency then computes a subject‐by‐subject similarity matrix for each atlas’s graph embeddings and applies an MSE loss between those two similarity matrices.  In this way, it enforces that the global, pairwise relationships among subjects remain the same, even though the underlying node features have been richly fused by the Inter-Atlas Message-Passing.\n\nIn short: the Inter-Atlas Message-Passing module is what produces the “cross‐atlas fused” node features, and the Population-Level Consistency loss sits on top of the graph‐level summaries of those features, penalizing any mismatch in the subject–subject similarity structure."
    },
    {
        "question": "What justifies optimizing only learnable tokens rather than entire text embeddings during Prompt Optimization?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Furthermore, to preserve the semantic meaning of the original prompt, rather than optimizing the entire text embedding , we follow the approach introduced in um2024minorityprompt, attaching learnable token embeddings to the end of the prompt and optimizing only these embeddings.",
            "By preserving the other token embeddings in the original text prompt, we ensure that essential text information is retained without loss and ensure the diffusion sampling trajectory on the correct manifold."
        ],
        "final_answer": "Optimizing only the appended learnable tokens preserves the semantic meaning of the user’s original prompt—by keeping the original token embeddings fixed, essential content is retained and the diffusion sampling stays on the correct data manifold.",
        "relevant_elements": [
            "learnable tokens",
            "Prompt Optimization"
        ],
        "id": 660,
        "masked_question": "What justifies optimizing only learnable tokens rather than entire text embeddings during [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Prompt Optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15540v1_figure_2.png",
        "paperid": "2411.15540v1",
        "paper_path": "./papers/2411.15540v1.json",
        "figure_id": "2411.15540v1_figure_2.png",
        "caption": "Figure 2: Overall pipeline of MotionPrompt. MotionPrompt enhances temporal consistency in text-to-video diffusion models by combining prompt optimization with an optical flow-based discriminator. Leveraging gradients from a subset of frames and aligning optical flow with real-world motion patterns, MotionPrompt efficiently generates videos with smooth, realistic motion and strong contextual coherence.",
        "qtype": "Design_Rationale",
        "response": "During the red‐boxed “Prompt Optimization” phase we are *not* free to tweak the *entire* text embedding c(J); instead we only optimize a small set of newly appended, learnable token embeddings S.  There are three, tightly coupled reasons for that design choice:\n\n1.  Preserve the original prompt semantics  \n    •  By leaving the embeddings of the user’s actual words frozen and only tweaking a few “dummy” tokens at the end, we guarantee that the lion’s share of the prompt (its nouns, verbs, adjectives, etc.) remains exactly as written.  If we had optimized the whole embedding vector, it would easily drift away from the *correct* manifold of “white fox on a rock” and could lose track of what the user actually asked for.\n\n2.  Keep the diffusion trajectory on the right manifold  \n    •  The entire sampling chain in a latent diffusion model is derived under the assumption that your conditioning c(J) lies on (or very close to) the *manifold* on which the model was trained.  Arbitrarily pushing around every dimension of c(J) risks breaking that assumption and can send you off into untrained regions of latent space, making the model’s denoiser fail or hallucinate.  By only nudging a few extra tokens, we are making a *small, controlled* detour that still stays on a valid diffusion path.\n\n3.  Computational and numerical stability  \n    •  Optimizing hundreds or thousands of floating-point dimensions (the full text embedding) at every time step would be both expensive and prone to catastrophic collapse (the well-known “prompt adversarial” phenomenon).  Restricting the search to a handful of extra tokens dramatically cuts down memory and compute, and we regularize them back toward their *initial* values so they never wander too far.\n\nAll of these considerations come straight from the “MinorityPrompt” idea referenced in the paper: append a few learnable tokens, keep the rest of the prompt fixed, and thus gain *indirect*, *semantically grounded* control over the video while safeguarding both the prompt’s meaning and the diffusion model’s learned manifold."
    },
    {
        "question": "What advantage arises from leveraging Optical Flow Discriminator feedback within Prompt Optimization over full-frame gradient guidance?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Specifically, calculating the gradient of all frames is computationally expensive. Providing guidance for only selected frames may reduce memory usage, but this can disrupt frame-to-frame consistency, resulting in inconsistencies in appearance, motion, and coherence throughout the video.",
            "To address this, we employ the prompt optimization method and extend it to capitalize the text prompt’s influence across the entire video. This approach enables indirect control of the latent video representation by using gradients derived from only a subset of frames, rather than necessitating gradients for every frame.",
            "Note that by optimizing the prompt rather than the latent representation directly, we can design the optical flow discriminator to take a single flow as input, rather than requiring flow from entire video sequences."
        ],
        "final_answer": "By using optical flow discriminator feedback within prompt optimization, MotionPrompt avoids the prohibitive cost of computing gradients over every frame and instead relies on discriminator judgments of single-pair optical flows. This yields computational efficiency and preserves temporal consistency without the need for full‐frame gradient guidance.",
        "relevant_elements": [
            "Optical Flow Discriminator",
            "Prompt Optimization"
        ],
        "id": 661,
        "masked_question": "What advantage arises from leveraging Optical Flow Discriminator feedback within [mask1] over full-frame gradient guidance?",
        "masked_number": 1,
        "masked_elements": [
            "Prompt Optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15540v1_figure_2.png",
        "paperid": "2411.15540v1",
        "paper_path": "./papers/2411.15540v1.json",
        "figure_id": "2411.15540v1_figure_2.png",
        "caption": "Figure 2: Overall pipeline of MotionPrompt. MotionPrompt enhances temporal consistency in text-to-video diffusion models by combining prompt optimization with an optical flow-based discriminator. Leveraging gradients from a subset of frames and aligning optical flow with real-world motion patterns, MotionPrompt efficiently generates videos with smooth, realistic motion and strong contextual coherence.",
        "qtype": "Design_Rationale",
        "response": "By moving the temporal “signal” into an optical‐flow discriminator that only looks at a single flow map (or just a handful of them) and then back‐propagates into the appended prompt tokens, we never have to unroll the entire video and compute per–pixel gradients on every frame.  In practice that means\n\n  1. we only decode and differentiate two (or a few) frames at a time instead of the full T-frame video  \n  2. we only run a tiny flow‐only discriminator rather than a heavy full‐frame loss  \n  3. we optimize a small bundle of new text embeddings, not the whole video latent  \n\nAs a result you get the same strong temporal‐coherence signal that a full video‐gradient would give you, but at a small fraction of the memory and compute cost and without the inter‐frame “jitter” that often comes from raw pixel‐wise guidance."
    },
    {
        "question": "How does Text Transformer integrate learnable tokens S into c(J*) for inference-time prompt optimization?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Furthermore, to preserve the semantic meaning of the original prompt, rather than optimizing the entire text embedding c, we follow the approach introduced in um2024minorityprompt, attaching learnable token embeddings to the end of the prompt and optimizing only these embeddings.",
            "Specifically, we first add new text tokens S to the tokenizer vocabulary and initialize their embeddings with words that can help improve video quality, such as \"authentic\" and \"real\".",
            "We then append these learnable tokens to the end of the given text prompt (e.g., \"White fox on the rock.\" → \"White fox on the rock  …\"). We denote this modified prompt as Ŝ.",
            "This leads to the following modified optimization problem: where e_S denotes the embeddings of tokens S, and c(Ŝ) varies with each timestep."
        ],
        "final_answer": "Text Transformer incorporates the learnable tokens S by first adding them to the tokenizer vocabulary and initializing their embeddings, then appending these tokens to the end of the original prompt to form a modified prompt Ŝ. During inference, the Text Transformer encodes this combined prompt (original tokens plus S) to produce the conditioning vector c(Ŝ), in which only the embeddings of S are updated over time for prompt optimization.",
        "relevant_elements": [
            "Text Transformer",
            "learnable tokens S"
        ],
        "id": 662,
        "masked_question": "How does [mask1] integrate [mask2] into c(J*) for inference-time prompt optimization?",
        "masked_number": 2,
        "masked_elements": [
            "Text Transformer",
            "learnable tokens S"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15540v1_figure_2.png",
        "paperid": "2411.15540v1",
        "paper_path": "./papers/2411.15540v1.json",
        "figure_id": "2411.15540v1_figure_2.png",
        "caption": "Figure 2: Overall pipeline of MotionPrompt. MotionPrompt enhances temporal consistency in text-to-video diffusion models by combining prompt optimization with an optical flow-based discriminator. Leveraging gradients from a subset of frames and aligning optical flow with real-world motion patterns, MotionPrompt efficiently generates videos with smooth, realistic motion and strong contextual coherence.",
        "qtype": "Implementation_Details",
        "response": "Here’s the short story.  In our diagram the red‐boxed module is simply the frozen text transformer; the blue‐boxed bit is the original prompt with your extra, learnable tokens S* tacked on the end.  “How do they get folded together into c(J*)?”  –  Answer:  at inference time we build a single token sequence J*  =  [“White”, “fox”, “on”, “rock”, …, S*], we run *that* whole sequence once through the text transformer, and we take its final hidden–state (or pooled) output as our conditioning vector c(J*).  Since only the S* embeddings are variable, back-propagating the flow/discriminator loss into J* just nudges those S* vectors, which in turn changes c(J*) on the very next denoising step.  That is exactly how the text‐transformer “glues” the blue-boxed prompt into the red-boxed c(J*) without ever touching the rest of the prompt."
    },
    {
        "question": "How does f(e) parameterize T(e) to differentiate treatment assignments across encouragement policies?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In our designs, the large observational data  can optionally serve as a special dataset with no-encouragement , to increase the number of encouragements and samples in the overall dataset .",
            "Within each dataset , the variables  and  are respectively the observable and unmeasured confounders, potentially confounding the analysis of the causal effect of the treatment variables  on the outcome variables .",
            "Therefore, we apply  different encouragement policies to promote treatment adoption without directly manipulating the treatment in certain candidate groups:\n\n    T_i(e) = f(e; X_i(e), U_i(e)),\n\nwhere  denotes different treatment assignment mechanisms, with unknown parameters  for various encouragements  ,  depicts the heterogeneous treatment effect with unknown parameters  , and  embeds the unmeasured confounding effects from  as additive noise that is a common assumption used in causality."
        ],
        "final_answer": "The function f(e) specifies a separate treatment-assignment mechanism for each encouragement policy e.  Concretely, under encouragement e the treatment T(e) is generated as T_i(e)=f(e;X_i(e),U_i(e)), so that f(e) parameterizes how the observed covariates X and unobserved confounders U are mapped into a treatment level differently for each encouragement.",
        "relevant_elements": [
            "f(e)",
            "T(e)",
            "encouragement policies"
        ],
        "id": 664,
        "masked_question": "How does [mask1] parameterize [mask2] to differentiate treatment assignments across encouragement policies?",
        "masked_number": 2,
        "masked_elements": [
            "f(e)",
            "T(e)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.05428v1_figure_1.png",
        "paperid": "2408.05428v1",
        "paper_path": "./papers/2408.05428v1.json",
        "figure_id": "2408.05428v1_figure_1.png",
        "caption": "Figure 1: Overview of the Encouragement Design Framework. For example, in online course platforms like Coursera, edX, and Udacity, using only observational data to control observed confounders X(0)superscript𝑋0X^{(0)}italic_X start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, we can not consistently estimate the causal effects of forum engagement duration T(0)superscript𝑇0T^{(0)}italic_T start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT on exam scores Y(0)superscript𝑌0Y^{(0)}italic_Y start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT due to the presence of unmeasured confounders U(0)superscript𝑈0U^{(0)}italic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Therefore, we use varied encouragement policies (Class A: eA=Nonesubscript𝑒ANone{e}_{\\text{A}}=\\text{None}italic_e start_POSTSUBSCRIPT A end_POSTSUBSCRIPT = None, Class B: eB=Praisesubscript𝑒BPraise{e}_{\\text{B}}=\\text{Praise}italic_e start_POSTSUBSCRIPT B end_POSTSUBSCRIPT = Praise, Class C: eC=Pointssubscript𝑒CPoints{e}_{\\text{C}}=\\text{Points}italic_e start_POSTSUBSCRIPT C end_POSTSUBSCRIPT = Points) to encourage longer forum engagement duration (i.e., treatments T(e)superscript𝑇𝑒T^{(e)}italic_T start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), while these policies do not have a direct impact on exam scores (i.e., outcomes Y(e)superscript𝑌𝑒Y^{(e)}italic_Y start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), which offers new opportunities to identify causal effects.",
        "qtype": "Implementation_Details",
        "response": "Here, the little colored boxes f(e₁), f(e₂), …, f(e_k) in the red‐outline are nothing more than the policy–specific “knobs” that go into the treatment‐assignment node T^(e) (the blue‐outlined circle).  In fact, for each encouragement e we postulate a separate mapping\n\n T^(e) = f(e) ( X^(e), U^(e) )  \n\nso that by choosing a different f(e) (i.e. a different color in the red box) we get a different distribution or mechanism for T^(e).  In this way the family { f(e₁),…,f(e_k) } parameterizes the treatment‐assignment variable T^(e) and lets us cleanly distinguish how each encouragement policy “pushes” subjects into treatment in a different way."
    },
    {
        "question": "How are U(e) accounted for in T(e) generation under f(e) mappings?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Therefore, we apply m different encouragement policies to promote treatment adoption without directly manipulating the treatment in certain candidate groups: where f^{(e)} denotes different treatment assignment mechanisms, with unknown parameters θ_e for various encouragements e, g depicts the heterogeneous treatment effect with unknown parameters ϕ, and U embeds the unmeasured confounding effects from X as additive noise that is a common assumption used in causality (Newey and Powell 2003; Imbens, Rubin et al. 2015; Hartford et al. 2017)."
        ],
        "final_answer": "Under each encouragement mapping f^{(e)}, the latent confounder U^{(e)} is fed into the treatment assignment mechanism as an additive noise term. In other words, the treatment T^{(e)} is generated via T^{(e)} = f^{(e)}(X^{(e)}, U^{(e)}; θ_e), so that U^{(e)} directly influences T^{(e)} through the f^{(e)} mapping.",
        "relevant_elements": [
            "U(e)",
            "T(e)",
            "f(e)"
        ],
        "id": 665,
        "masked_question": "How are [mask1] accounted for in T(e) generation under f(e) mappings?",
        "masked_number": 1,
        "masked_elements": [
            "U(e)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.05428v1_figure_1.png",
        "paperid": "2408.05428v1",
        "paper_path": "./papers/2408.05428v1.json",
        "figure_id": "2408.05428v1_figure_1.png",
        "caption": "Figure 1: Overview of the Encouragement Design Framework. For example, in online course platforms like Coursera, edX, and Udacity, using only observational data to control observed confounders X(0)superscript𝑋0X^{(0)}italic_X start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, we can not consistently estimate the causal effects of forum engagement duration T(0)superscript𝑇0T^{(0)}italic_T start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT on exam scores Y(0)superscript𝑌0Y^{(0)}italic_Y start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT due to the presence of unmeasured confounders U(0)superscript𝑈0U^{(0)}italic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Therefore, we use varied encouragement policies (Class A: eA=Nonesubscript𝑒ANone{e}_{\\text{A}}=\\text{None}italic_e start_POSTSUBSCRIPT A end_POSTSUBSCRIPT = None, Class B: eB=Praisesubscript𝑒BPraise{e}_{\\text{B}}=\\text{Praise}italic_e start_POSTSUBSCRIPT B end_POSTSUBSCRIPT = Praise, Class C: eC=Pointssubscript𝑒CPoints{e}_{\\text{C}}=\\text{Points}italic_e start_POSTSUBSCRIPT C end_POSTSUBSCRIPT = Points) to encourage longer forum engagement duration (i.e., treatments T(e)superscript𝑇𝑒T^{(e)}italic_T start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), while these policies do not have a direct impact on exam scores (i.e., outcomes Y(e)superscript𝑌𝑒Y^{(e)}italic_Y start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), which offers new opportunities to identify causal effects.",
        "qtype": "Implementation_Details",
        "response": "Here the red‐boxed nodes are the latent confounders U(e).  In our structural model for each encouragement e the treatment is generated via\n\n  T(e) = f (e)( X(e), U(e) ) ,\n\nso U(e) is simply carried along as the “additive noise” input to f(e).  In other words, any unobserved confounding is absorbed into the grey U(e) node and fed directly into the treatment‐assignment mapping f(e), ensuring that T(e) depends jointly on the observed covariates and the hidden confounders."
    },
    {
        "question": "How does f(e)-driven variation in T(e) align with non-compliance frameworks for unbiased Y(e) estimation?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.2.x"
        ],
        "relevant_context": [
            {
                "section_id": "3.1",
                "sentence": "we adopt various encouragement policies (e₁, e₂, …, eₖ) to motivate longer forum engagement (i.e., treatments T), which changes the distribution of T given e, in other words, increases time spent on the forum to varying encouragements."
            },
            {
                "section_id": "1",
                "sentence": "As shown in Figure 1, these random encouragements serve as instrumental variables (IVs), which only positively motivate the choice of treatment, while the outcome response remains unaffected by encouragements."
            },
            {
                "section_id": "3.2.x",
                "sentence": "The adopted encouragement policies serve as IVs, which only positively motivate the choice of treatments, without directly affecting the outcome response, which satisfies the following three IV conditions: (a) Relevance: IVs directly affect T; (b) Exclusion: IVs do not directly affect Y; (c) Independence: IVs are conditional independent of the error."
            }
        ],
        "final_answer": "By letting f(e) govern how encouragements shift the distribution of T, the model creates exogenous variation in T(e) exactly as in non-compliance settings.  In other words, each encouragement e induces a predictable change in treatment adoption via f(e), yet does not directly affect the outcome Y(e).  This mirrors the instrumental-variable (non-compliance) framework—where encouragements are ‘instruments’ that satisfy relevance (they change T), exclusion (they don’t change Y except through T), and independence (they’re exogenous).  As a result, f(e)-driven variation in T(e) aligns directly with non-compliance approaches and yields unbiased estimation of Y(e).",
        "relevant_elements": [
            "f(e)",
            "T(e)",
            "Y(e)"
        ],
        "id": 666,
        "masked_question": "How does [mask1]-driven variation in T(e) align with non-compliance frameworks for unbiased Y(e) estimation?",
        "masked_number": 1,
        "masked_elements": [
            "f(e)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.05428v1_figure_1.png",
        "paperid": "2408.05428v1",
        "paper_path": "./papers/2408.05428v1.json",
        "figure_id": "2408.05428v1_figure_1.png",
        "caption": "Figure 1: Overview of the Encouragement Design Framework. For example, in online course platforms like Coursera, edX, and Udacity, using only observational data to control observed confounders X(0)superscript𝑋0X^{(0)}italic_X start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT, we can not consistently estimate the causal effects of forum engagement duration T(0)superscript𝑇0T^{(0)}italic_T start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT on exam scores Y(0)superscript𝑌0Y^{(0)}italic_Y start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT due to the presence of unmeasured confounders U(0)superscript𝑈0U^{(0)}italic_U start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT. Therefore, we use varied encouragement policies (Class A: eA=Nonesubscript𝑒ANone{e}_{\\text{A}}=\\text{None}italic_e start_POSTSUBSCRIPT A end_POSTSUBSCRIPT = None, Class B: eB=Praisesubscript𝑒BPraise{e}_{\\text{B}}=\\text{Praise}italic_e start_POSTSUBSCRIPT B end_POSTSUBSCRIPT = Praise, Class C: eC=Pointssubscript𝑒CPoints{e}_{\\text{C}}=\\text{Points}italic_e start_POSTSUBSCRIPT C end_POSTSUBSCRIPT = Points) to encourage longer forum engagement duration (i.e., treatments T(e)superscript𝑇𝑒T^{(e)}italic_T start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), while these policies do not have a direct impact on exam scores (i.e., outcomes Y(e)superscript𝑌𝑒Y^{(e)}italic_Y start_POSTSUPERSCRIPT ( italic_e ) end_POSTSUPERSCRIPT), which offers new opportunities to identify causal effects.",
        "qtype": "Literature_Background",
        "response": "First, let me restate what lives in that red box.  Those little coloured squares labelled  \n   \n   f(e₁), f(e₂), …, f(eₖ)  \n   \nare nothing more than the k different “encouragement‐to‐treatment” mappings – in other words, the functions that take you from encouragement policy e to a change in your actual forum‐time T(e).  In DAG language they are exactly the arrows\n             \n             e ──f(e)──► T(e)\n             \n\nNow, in a standard *non-compliance* or *encouragement‐design* framework (Angrist–Imbens–Rubin 1996 and its many descendants) you do the following:\n\n1.  **Randomize or exogenously assign** an encouragement instrument e.  \n2.  That instrument e *shifts* the probability or amount of treatment T (this is your “first stage”).  \n3.  You assume e does not otherwise directly affect Y (the “exclusion” assumption) and is independent of any unmeasured confounders U (the “independence” assumption).  \n4.  Some people comply (their T changes in the direction of the encouragement) and some do not.  Under monotonicity you can isolate the “compliers” and recover an unbiased *local average treatment effect* (LATE) or, more generally, a conditional ATE among those whose T was actually moved by e.\n\nIn our picture, *each* f(eₖ) is exactly its own encouragement‐instrument “first stage.”  By having multiple of them – f(e₁), f(e₂), …, f(eₖ) – we introduce a family of exogenous shifts in T(e).  As long as\n\n  •  Relevance:  E[T | eₖ] actually changes when we switch from eⱼ to eₖ,  \n  •  Exclusion:  f(eₖ) only operates through T and never directly nudges Y,  \n  •  Independence:  eₖ remains orthogonal to U,\n\nwe’ve recreated the textbook non-compliance IV set-up.  In other words, those coloured f(e) boxes are the “instruments” that move T without touching Y directly, and by comparing outcomes only where T was shifted, we obtain an unbiased CATE (or LATE) estimate for the compliers."
    },
    {
        "question": "How does F-PET leverage SSF to maintain stability-plasticity balance?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "F-PET",
            "SSF"
        ],
        "id": 668,
        "masked_question": "How does [mask1] leverage [mask2] to maintain stability-plasticity balance?",
        "masked_number": 2,
        "masked_elements": [
            "F-PET",
            "SSF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02175v1_figure_1.png",
        "paperid": "2411.02175v1",
        "paper_path": "./papers/2411.02175v1.json",
        "figure_id": "2411.02175v1_figure_1.png",
        "caption": "Figure 1: \nComparisons of (a) prevailing PTM-based CL methods [52, 23, 2] and our Slow And Fast parameter-Efficient tuning (SAFE).\nThe right part (b) illustrates several parameter-efficient tuning (PET) blocks: Adapter [6], Scale & Shift (SSF) [21], and Visual Prompt Tuning (VPT) [16].",
        "qtype": "Literature_Background",
        "response": "Here, the red‐boxed module is the fast learner’s PET adapter (“F-PET”) and the blue‐boxed module is the Scale & Shift Fine-tuning block (“SSF”).  The way F-PET “leverages” SSF to hold onto old knowledge (stability) while still picking up new classes (plasticity) is simple:\n\n1.  In F-PET we only turn on and update the tiny SSF parameters (i.e. per‐channel scale and shift) in each transformer block, leaving the rest of the backbone frozen.  This keeps the bulk of the network’s representations intact (stability) and confines all the new task learning to a very small set of weights (plasticity).\n\n2.  We further anchor those SSF‐tuned features by (a) aligning them back to the frozen slow learner with a cosine‐distance feature‐alignment loss, and (b) using cross‐classification losses so that new fast‐SSF features remain compatible with the old slow learner’s class prototypes.  \n\nAltogether, by restricting updates to the SSF layers and then guiding those updates with alignment and cross‐classification losses, F-PET achieves a clean trade-off: it can quickly adapt (plasticity) without overwriting the previous session’s knowledge (stability)."
    },
    {
        "question": "How does transferring PTM knowledge to S-PET calibration enhance feature generalizability for subsequent sessions?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "In the first session, the slow learner is tuned to inherit the general knowledge from PTM and is frozen afterward.",
            "For the slow learner, W₁ is learned in the first session and expanded using feature centroids of training samples within the same classes [28] afterward to preserve learned general knowledge.",
            "Intuitively, the joint optimization of three losses makes the adapted model simultaneously acquire distribution-specific knowledge based on D₁ and inherit general knowledge of the PTM using L_corr and L_orth.",
            "As a result, the slow model can better generalize to incoming classes even unseen in the first training session."
        ],
        "final_answer": "By explicitly aligning the S-PET features with PTM features via correlation and orthogonality losses in the first session, and then freezing those parameters—while only expanding its classification head using imprinted class centroids—S-PET inherits PTM’s invariant feature components. This retained general knowledge enables the slow learner to produce representations that generalize well to novel classes in all subsequent sessions.",
        "relevant_elements": [
            "PTM",
            "S-PET"
        ],
        "id": 669,
        "masked_question": "How does transferring [mask1] knowledge to S-PET calibration enhance feature generalizability for subsequent sessions?",
        "masked_number": 1,
        "masked_elements": [
            "PTM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02175v1_figure_1.png",
        "paperid": "2411.02175v1",
        "paper_path": "./papers/2411.02175v1.json",
        "figure_id": "2411.02175v1_figure_1.png",
        "caption": "Figure 1: \nComparisons of (a) prevailing PTM-based CL methods [52, 23, 2] and our Slow And Fast parameter-Efficient tuning (SAFE).\nThe right part (b) illustrates several parameter-efficient tuning (PET) blocks: Adapter [6], Scale & Shift (SSF) [21], and Visual Prompt Tuning (VPT) [16].",
        "qtype": "Experimental_Results",
        "response": "Here’s the reasoning step by step:\n\n1.  In the figure the red‐boxed module is the frozen Pre-Trained Model (PTM).  \n2.  During Session 1 we “squeeze” out its intrinsic, broadly generalizable features by computing a cross-correlation matrix between the PTM’s activations and the new PET-augmented slow learner (S-PET).  \n3.  We then maximize the diagonal entries (encouraging S-PET to reproduce the PTM’s most invariant patterns) and suppress the off-diagonals (to remove redundant components).  \n4.  This calibration forces S-PET to inherit the PTM’s rich, high-level abstractions—rather than simply overfitting to Session 1’s classes.  \n5.  As a result, the S-PET representation remains broadly discriminative and transfers well to novel classes in all later sessions, boosting its generalizability."
    },
    {
        "question": "How do structural variations among Adapter, SSF, and VPT influence parameter placement within transformer layers?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Adapter",
            "SSF",
            "VPT"
        ],
        "id": 670,
        "masked_question": "How do structural variations among [mask1], SSF, and VPT influence parameter placement within transformer layers?",
        "masked_number": 1,
        "masked_elements": [
            "Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02175v1_figure_1.png",
        "paperid": "2411.02175v1",
        "paper_path": "./papers/2411.02175v1.json",
        "figure_id": "2411.02175v1_figure_1.png",
        "caption": "Figure 1: \nComparisons of (a) prevailing PTM-based CL methods [52, 23, 2] and our Slow And Fast parameter-Efficient tuning (SAFE).\nThe right part (b) illustrates several parameter-efficient tuning (PET) blocks: Adapter [6], Scale & Shift (SSF) [21], and Visual Prompt Tuning (VPT) [16].",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what “[mask1]” is.  In the little “classic PET blocks” diagram on the right the red‐boxed module is the Adapter.  So the three variants are\n\n  • Adapter  \n  • SSF (Scale & Shift Fine‐tuning)  \n  • VPT (Visual Prompt Tuning)  \n\nAll three are ways to add only a few learnable parameters to a frozen Transformer, but they differ in *where* those parameters live:\n\n1.  Adapter  \n    – A tiny two‐layer bottleneck MLP (W₍down₎→Act→W₍up₎)  \n    – *Inserted inside every Transformer layer*, usually immediately after a LayerNorm (often in both the attention and the feed‐forward sub-layers).  \n    – Because it sits *in-between* the Residual+Norm blocks, it can reshape intermediate representations but comes with the cost of extra weight matrices in every layer.\n\n2.  SSF (Scale & Shift)  \n    – A per-dimension scale vector γ and shift vector β  \n    – Also *inside each Transformer layer* but much lighter: it simply rescales and recenters (i.e. re-parametrizes) the normalized outputs of attention or feed-forward blocks.  \n    – No MLPs—only 2·d parameters per sub-layer (d is the hidden dim).  \n\n3.  VPT (Visual Prompt Tuning)  \n    – Learns a small set of extra “prompt” tokens  \n    – *Only at the very input* to the model: you prepend or append these tokens to the patch embeddings before any Transformer layer.  \n    – The body of the Transformer is left completely frozen—no extra parameters are inserted in the deeper layers at all.\n\nIn short, the *structural variation* among them boils down to **parameter placement**:\n\n  • Adapter: deep *inside* every layer (small MLPs)  \n  • SSF: deep *inside* every layer but extremely lightweight (just scale & bias)  \n  • VPT: *only* at the model input (learned tokens), leaving all internal layers untouched"
    },
    {
        "question": "How does Sparse Signal Reconstruction influence Data Stratification effectiveness in integrating continuous event-based features?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "By converting meal and insulin events into continuous values, we aim to capture the dynamic relationships between these factors and blood glucose levels to improve the accuracy of our prediction model.",
            "Both X_low and X_high are then combined with effective carbs intake (X_ic) and insulin dosage (X_ins), which are computed in our SSR module (Section 3.2), to create two new datasets."
        ],
        "final_answer": "The Sparse Signal Reconstruction module transforms the inherently sparse, event-based carbohydrate and insulin inputs into continuous time-series representations that capture their physiological onset, peak, and decay. When Data Stratification then combines these continuous event-based features with the decomposed low- and high-frequency glucose signals, it can more effectively integrate the dynamic influence of meals and insulin into each stratified dataset, improving the coherence and predictive power of both the low- and high-frequency forecasting branches.",
        "relevant_elements": [
            "Sparse Signal Reconstruction",
            "Data Stratification"
        ],
        "id": 671,
        "masked_question": "How does [mask1] influence Data Stratification effectiveness in integrating continuous event-based features?",
        "masked_number": 1,
        "masked_elements": [
            "Sparse Signal Reconstruction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10703v1_figure_2.png",
        "paperid": "2411.10703v1",
        "paper_path": "./papers/2411.10703v1.json",
        "figure_id": "2411.10703v1_figure_2.png",
        "caption": "Figure 2. An Overview of GlucoNet includes sensing to measure variables, Sparse signal construction to extract the effective variables, Feature decomposition using the VMD method, Data stratification to combine various variables, and a forecasting module using LSTM and Knowledge Distillation Framework.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought (step by step):\n\n1.  In the big picture, GlucoNet first measures three modalities (CGM blood glucose Xg, carbohydrate events Xc, insulin events Xins).  \n2.  The red box (our [mask1]) is labeled “Transform Xins, Xc  – Effective Feature Transformation f(·)” – this is the Sparse Signal Reconstruction (SSR) module.  \n3.  Its job is to take the two event‐based streams (meals and insulin boluses, both naturally sparse and irregular in time) and turn them into continuous, physiologically plausible time‐series:  \n    •  Carb events → “operative carbohydrate” curve (rises, peaks, decays)  \n    •  Insulin boluses → “active insulin” curve (onset, peak, decay)  \n4.  After SSR, we have four continuous series:  \n    – Low‐frequency BGL (from VMD)  \n    – High‐frequency BGL (from VMD)  \n    – Operative carbohydrate (SSR)  \n    – Active insulin (SSR)  \n5.  In Data Stratification, these continuous time‐series are grouped into two datasets:  \n    – Low-freq set: {low‐freq BGL, operative carbs, active insulin}  \n    – High-freq set: {high‐freq BGL, operative carbs, active insulin}  \n6.  Because SSR has already aligned and filled in the carbohydrate and insulin signals at every timestamp, the stratification step can simply “zip” them onto both the low- and high-frequency BGL modes without any missing data or misalignment.  \n7.  If SSR had not converted the events into continuous curves, the stratification would be forced to deal with wildly sparse, irregular data—leading to dropped samples, interpolation errors, or the inability to join modalities at all.  \n\nAnswer to the question:\n\nBy performing the Sparse Signal Reconstruction (the “Effective Feature Transformation f(·)” in the red box), GlucoNet converts the originally sparse meal‐and‐insulin events into continuous time series that smoothly rise, peak, and decay over time. This continuous reconstruction is exactly what allows the Data Stratification step to stack those features onto both the low- and high-frequency glucose modes without temporal gaps or misalignment. In other words, SSR makes the event‐based signals compatible with the decomposed glucose signals, greatly enhancing the effectiveness of the stratification in integrating all four modalities into the two forecasting pipelines."
    },
    {
        "question": "How does Knowledge Distillation affect the Transformer component's compactness in the Forecasting Module for high-frequency signals?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Therefore, we extended Knowledge Distillation (KD) to compact the large Transformer (Teacher model) to the small Transformer (Student model) to achieve more accurate forecasting of blood glucose levels for high-frequency features.",
            "Also, the high-frequency model of the proposed GlucoNet offers multiple implementation configurations. It can be deployed with or without knowledge distillation (KD) and implemented with the large Transformer (teacher Transformer) or the small transformer (student Transformer). These options provide flexibility in adapting GlucoNet to various accuracy requirements and computational constraints."
        ],
        "final_answer": "By applying Knowledge Distillation, the original large Transformer (teacher) is converted into a much smaller student Transformer. This distilled student model has fewer parameters (e.g. reduced input dimensions, fewer attention heads and feed-forward units) while retaining the teacher’s predictive performance, thus making the high-frequency forecasting Transformer more compact and efficient.",
        "relevant_elements": [
            "Knowledge Distillation",
            "Transformer",
            "Forecasting Module"
        ],
        "id": 672,
        "masked_question": "How does [mask1] affect the Transformer component's compactness in the Forecasting Module for high-frequency signals?",
        "masked_number": 1,
        "masked_elements": [
            "Knowledge Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10703v1_figure_2.png",
        "paperid": "2411.10703v1",
        "paper_path": "./papers/2411.10703v1.json",
        "figure_id": "2411.10703v1_figure_2.png",
        "caption": "Figure 2. An Overview of GlucoNet includes sensing to measure variables, Sparse signal construction to extract the effective variables, Feature decomposition using the VMD method, Data stratification to combine various variables, and a forecasting module using LSTM and Knowledge Distillation Framework.",
        "qtype": "Experimental_Results",
        "response": "First, note that the little red‐outlined box sitting between the “Teacher” and “Student” sub–blocks in the high‐frequency branch of the Forecasting Module is in fact the Knowledge Distillation (KD) step.  In plain English, what we’ve done there is take the big, heavy Transformer (the “teacher”) and teach a much smaller Transformer (the “student”) to mimic its behavior.  \n\nChain of thought:  \n1.  The high‐frequency signals are notoriously jagged, so the paper authors chose a Transformer to handle them.  \n2.  Out of the box that Transformer is “large” – many heads, large feed-forward layers, etc.  That makes it unwieldy for a real‐time CGM pipeline.  \n3.  They therefore wrap a KD loss around it: at training time the student Transformer tries to match both the true glucose labels and the softened output distribution of the teacher (via the temperature‐scaled softmax).  \n4.  Because the student only needs one encoder layer, fewer heads and smaller feed-forward size, it ends up much more compact – far fewer parameters – yet, thanks to the teacher’s guidance, it still hits nearly the same forecasting accuracy on the high‐frequency component.  \n\nAnswer:  \nKnowledge Distillation is exactly the mechanism that shrinks (“compacts”) the original large Transformer into a much smaller one.  By forcing the student model to learn from the teacher’s softened predictions, it can drop layers, heads and hidden‐unit counts, yielding a lightweight Transformer that retains almost the same forecasting power on high‐frequency glucose swings."
    },
    {
        "question": "What limitations does VMD-based feature decomposition impose on computational efficiency for real-time forecasting?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "Despite the advancements, these approaches face key limitations: VMD models struggle with computational demands, hybrid neural networks require high resource use, and enhanced methods depend on extensive preprocessing."
        ],
        "final_answer": "VMD-based feature decomposition is computationally intensive, requiring significant processing resources that can impede real-time forecasting on resource-constrained devices.",
        "relevant_elements": [
            "Feature Decomposition",
            "Forecasting"
        ],
        "id": 674,
        "masked_question": "What limitations does VMD-based feature decomposition impose on computational efficiency for real-time [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Forecasting"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.10703v1_figure_2.png",
        "paperid": "2411.10703v1",
        "paper_path": "./papers/2411.10703v1.json",
        "figure_id": "2411.10703v1_figure_2.png",
        "caption": "Figure 2. An Overview of GlucoNet includes sensing to measure variables, Sparse signal construction to extract the effective variables, Feature decomposition using the VMD method, Data stratification to combine various variables, and a forecasting module using LSTM and Knowledge Distillation Framework.",
        "qtype": "Others",
        "response": "Chain‐of‐thought  \n1. In the figure, the red box highlights the final “Forecasting” module (an LSTM with knowledge‐distillation teacher–student) that sits downstream of the VMD‐based decomposition block.  \n2. The question asks what computational burden the VMD step imposes on real‐time “[mask1]” – clearly referring to “forecasting.”  \n3. From the text we know that VMD is an iterative variational optimization that splits the glucose signal into K modes (IMFs).  \n4. Each decomposition requires multiple inner‐loop updates (mode‐center and mode‐shape estimates), whose cost grows with signal length, number of modes, and desired convergence accuracy.  \n5. This per‐window, per‐mode optimization therefore adds substantial latency and CPU/memory load before the LSTM can even run.  \n6. On resource‐limited or embedded platforms (e.g. wearables, insulin pumps), that extra VMD overhead can become the bottleneck, making truly real‐time glucose forecasting infeasible without significant simplification or hardware acceleration.  \n\nAnswer  \nVMD-based feature decomposition requires an expensive, iterative optimization for every incoming window and every mode (IMF), whose runtime grows with signal length, number of modes, and convergence tolerance. In practice this adds significant latency and CPU/memory overhead to the pipeline—often enough to prevent truly real-time forecasting on resource-constrained devices (e.g. wearables or closed-loop insulin pumps) unless additional approximation or hardware acceleration is introduced."
    },
    {
        "question": "What alternative approaches could mitigate hallucinations under distribution shifts during deployment & inference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Distribution shifts",
            "Deployment & inference"
        ],
        "id": 675,
        "masked_question": "What alternative approaches could mitigate hallucinations under [mask1] during deployment & inference?",
        "masked_number": 1,
        "masked_elements": [
            "Distribution shifts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15060v1_figure_2.png",
        "paperid": "2411.15060v1",
        "paper_path": "./papers/2411.15060v1.json",
        "figure_id": "2411.15060v1_figure_2.png",
        "caption": "Figure 2: Hallucination causes. We confirm each factor by ablating specific components (bottom row) in the VS pipeline and observing the resultant drop in MS-SSIM. The adopted experiment is from §5: VS of HE (target) from 4 SRS bands (source), trained by Pix2PixHD [30] and evaluated over an ID test set. OOD and adversarial example details in §5.3. Similar results were seen with PSNR and LPIPS.",
        "qtype": "Others",
        "response": "Here’s how I reasoned through the question:\n\n1. First, I needed to know what “[mask1]” refers to.  In Fig. 2 (and again highlighted with a red box in the Deployment & Inference row of Fig. 1), “[mask1]” is the “Distribution shifts” bar—i.e. applying the virtual‐staining model on out‐of‐distribution (OOD) inputs.\n\n2. The question then asks: “What alternative approaches could mitigate hallucinations under distribution shifts during deployment and inference?”\n\n3. From the text we know that hallucinations under distribution shift arise because the VS model was trained under ERM on in-distribution data only, and so it often fails when confronted with new scanners, staining protocols, tissue types, acquisition artifacts, etc.  NHP can detect many of these, but the authors note its performance also degrades under strong corruptions or adversarial attacks.\n\n4. Thus, to “mitigate hallucinations” in this setting (rather than merely detect them), one would want to make the VS model itself more robust to domain shifts, or at least equip the pipeline with mechanisms to (a) anticipate those shifts and (b) either adapt or reject them before a bad stain is shown to a pathologist.\n\n5. Below is a non‐exhaustive list of candidate approaches drawn from the broader literature on domain robustness, adversarial/OOD defense, and image‐to‐image translation.  Any of these could be layered on top of (or in place of) purely ERM‐trained Pix2Pix/CycleGAN models to reduce hallucination rates under OOD at deployment:\n\n • Domain adversarial training (e.g. DANN-style losses) to encourage the generator’s features to be invariant across multiple institutions, scanners or staining protocols.  \n • Domain‐generalization via augmentation/style-randomization: during training, feed the network extensive “synthetic corruptions” (JPEG noise, blur, brightness/contrast shifts, channel misalignments, etc.) so that it learns to ignore them.  \n • Test-time adaptation or normalization‐statistics re-estimation: at inference time, adapt batch-norm (or affine) parameters on the first few OOD patches so the generator “re-centers” to the new domain before full deployment.  \n • Adversarial training (PGD or randomized smoothing) on the I2I loss to harden the generator against small worst‐case perturbations that can trigger hallucinations.  \n • Uncertainty‐aware or Bayesian image‐to‐image networks (e.g. MC-Dropout, deep ensembles, evidential nets) to downweight or abstain from generation in regions of high epistemic uncertainty.  \n • Ensemble of differently initialized/styled VS models, with a downstream consistency check (e.g. only accept patches on which the ensemble agrees above some threshold).  \n • Self‐supervised OOD exposure during calibration: extend the NHP self-tuning bank by including synthetically corrupted or style-swapped examples, so that the KNN detector and/or generator see a wider envelope of possible shifts.  \n\nAny combination of the above can shrink the regions of “Rashomon ambiguity” under distribution shift and thus reduce the frequency or severity of hallucinations when the model is deployed in a new lab, on new scanners, or with unseen tissue/pathology variants."
    },
    {
        "question": "What is the rationale behind reducing SRS bands to assess insufficient content's effect on hallucination?",
        "relevant_section_ids": [
            "3.2",
            "figure_2"
        ],
        "relevant_context": [
            "This includes randomness in the generating process (e.g., measurement noise) or insufficient source domain content (e.g., low resolution). Such intrinsic ill-posedness leads to one-to-many translations for φ*, where plausible translations may not match true observations, causing hallucinations.",
            "Insufficient content (Reduce 4 bands to 2)"
        ],
        "final_answer": "By halving the number of SRS spectral bands from 4 to 2, the experiment artificially removes source‐domain information. This simulates an “insufficient content” scenario—making the translation problem more ill‐posed and thus more prone to hallucinations—so that the effect of lacking input content on hallucination can be directly measured.",
        "relevant_elements": [
            "Insufficient content",
            "SRS bands"
        ],
        "id": 677,
        "masked_question": "What is the rationale behind reducing [mask1] to assess insufficient content's effect on hallucination?",
        "masked_number": 1,
        "masked_elements": [
            "SRS bands"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15060v1_figure_2.png",
        "paperid": "2411.15060v1",
        "paper_path": "./papers/2411.15060v1.json",
        "figure_id": "2411.15060v1_figure_2.png",
        "caption": "Figure 2: Hallucination causes. We confirm each factor by ablating specific components (bottom row) in the VS pipeline and observing the resultant drop in MS-SSIM. The adopted experiment is from §5: VS of HE (target) from 4 SRS bands (source), trained by Pix2PixHD [30] and evaluated over an ID test set. OOD and adversarial example details in §5.3. Similar results were seen with PSNR and LPIPS.",
        "qtype": "Design_Rationale",
        "response": "We want to isolate the “insufficient‐content” cause of hallucinations, which in our SRS→HE setup means “not giving the network enough spectral information.”  In the standard pipeline we feed four SRS bands into the Pix2PixHD model; by ablating two of those bands (i.e. dropping from 4→2) we create a controlled “low‐content” scenario.  Any drop in MS‐SSIM that results can then be attributed purely to the fact that the network has been starved of input content, forcing it to hallucinate missing details."
    },
    {
        "question": "What motivates applying adversarial examples to probe attack-induced hallucination in VS inference?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In addition, cybersecurity is highly relevant for digital pathology [54]. Unfortunately, I2IT models lack adversarial robustness, with research showing that injecting imperceptible noise disrupts malicious applications like deepfake [55, 56] and watermark removers [57]. This hints that VS models too can be attacked with hallucination intent."
        ],
        "final_answer": "The motivation comes from cybersecurity concerns: I2IT (virtual staining) models are known to lack adversarial robustness, and prior work has shown that imperceptible noise can be used to disrupt related applications (e.g., deepfakes, watermark removers). This suggests that VS models could similarly be attacked to induce hallucinations, motivating the use of adversarial examples to probe such vulnerabilities.",
        "relevant_elements": [
            "Attacks",
            "adversarial examples"
        ],
        "id": 678,
        "masked_question": "What motivates applying [mask1] to probe attack-induced hallucination in VS inference?",
        "masked_number": 1,
        "masked_elements": [
            "adversarial examples"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15060v1_figure_2.png",
        "paperid": "2411.15060v1",
        "paper_path": "./papers/2411.15060v1.json",
        "figure_id": "2411.15060v1_figure_2.png",
        "caption": "Figure 2: Hallucination causes. We confirm each factor by ablating specific components (bottom row) in the VS pipeline and observing the resultant drop in MS-SSIM. The adopted experiment is from §5: VS of HE (target) from 4 SRS bands (source), trained by Pix2PixHD [30] and evaluated over an ID test set. OOD and adversarial example details in §5.3. Similar results were seen with PSNR and LPIPS.",
        "qtype": "Design_Rationale",
        "response": "Chain of thought:\n\n1. In Fig. 2 (bottom row), the red‐boxed experiment is labeled “Attacks (Apply over adv. ex.)” – i.e. they inject adversarial perturbations at inference.  \n2. The text (§3.1) notes that I2IT (and thus VS) models “lack adversarial robustness,” mirroring vulnerabilities seen in deep‐fake and watermark‐removal work.  \n3. Because digital pathology is a security‐critical domain, even imperceptible noise could be used maliciously to force a VS model to “hallucinate” pathological features.  \n4. Thus, to capture these worst‐case, security‐driven failure modes, the authors apply adversarial attacks at test time and measure the drop in MS-SSIM.  \n\nAnswer:\nThey apply adversarial attacks because VS models are known to be vulnerable to imperceptible perturbations, and probing hallucination under such attacks reveals the worst-case, security-critical failure modes of the inference process."
    },
    {
        "question": "What is the motivation behind complementing the global image-text objective with a fine-grained region-aware objective?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "However, recent works have suggested that such global image-level strategies (i) discover spurious correlations that align poorly with human-interpretable attributes and (ii) may not effectively enable models to ignore spurious correlations during training. (Section 1)",
            "Our key insight is that utilizing region-level information during VLM training can enable models to focus on relevant image-text relationships and ignore spurious correlations. (Section 4.1)"
        ],
        "final_answer": "Global image–text objectives alone often discover spurious correlations that do not align with human‐interpretable features and fail to teach the model to ignore those spurious signals. By adding a fine‐grained region‐aware objective, the model is explicitly guided to pay attention to the correct local regions (and to push away spurious regions), thereby reinforcing the true image–text relationships and improving robustness.",
        "relevant_elements": [
            "global image-text objective",
            "region-aware objective"
        ],
        "id": 680,
        "masked_question": "What is the motivation behind complementing the [mask1] with a fine-grained [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "global image-text objective",
            "region-aware objective"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04097v1_figure_1.png",
        "paperid": "2411.04097v1",
        "paper_path": "./papers/2411.04097v1.json",
        "figure_id": "2411.04097v1_figure_1.png",
        "caption": "Figure 1: Region-aware Vision-Language learning (RaVL). RaVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed “Global Image–Text Objective” is simply the standard CLIP‐style loss over whole images and their captions.  By itself it will happily latch onto any easy shortcut in the image–text pairing (in our running example it will learn to associate “butterfly” with the pink flower, because almost every “butterfly” image in the fine‐tuning set has a flower in it).  \n\nWe therefore **complement the global image–text loss with a fine‐grained, region‐aware objective** (the blue box) so that during fine‐tuning the model is explicitly  \n  1. **Encouraged** to push up the similarity between the _correct_ region (the actual butterfly) and the text label “butterfly” (green arrow), and  \n  2. **Penalized** for associating the _spurious_ region (the pink flower) with the text label “butterfly” (red arrow).  \n\nIn short, the motivation is that a purely global image–text loss (mask₁) cannot by itself disambiguate which parts of the image really correspond to the textual attribute, so we add a fine‐grained region‐aware loss (mask₂) to force the model to focus on the semantically correct regions and ignore the spurious ones."
    },
    {
        "question": "How do the vision and text encoders synchronize embedding spaces to support region-level zero-shot classification?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "RaVL first utilizes the zero-shot classification dataset U to identify candidate image features. To this end, we use the fine-tuned VLM M to extract an image embedding for each image x_i in U and a text embedding for each class c_j. Zero-shot classification is performed using the computed embeddings; this results in a softmax-normalized image score distribution vector s^I.",
            "We then apply RoIAlign [16,63] to the image encoder of M to extract embeddings for each region. Zero-shot classification is performed using the computed region embeddings, resulting in a softmax-normalized region score distribution matrix S^R."
        ],
        "final_answer": "The vision and text encoders are trained to produce representations in a shared embedding space. At evaluation time, region‐level embeddings are extracted via RoIAlign on the vision encoder, and class labels are encoded via the text encoder; region‐level zero-shot classification is then carried out by computing cosine similarities between these region embeddings and text embeddings and applying a softmax to yield region score distributions.",
        "relevant_elements": [
            "Vision Encoder",
            "Text Encoder"
        ],
        "id": 681,
        "masked_question": "How do the [mask1] and text encoders synchronize embedding spaces to support region-level zero-shot classification?",
        "masked_number": 1,
        "masked_elements": [
            "Vision Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04097v1_figure_1.png",
        "paperid": "2411.04097v1",
        "paper_path": "./papers/2411.04097v1.json",
        "figure_id": "2411.04097v1_figure_1.png",
        "caption": "Figure 1: Region-aware Vision-Language learning (RaVL). RaVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in the figure is simply the vision half of a contrastively trained vision–language model (think CLIP, ALIGN, etc.).  At training time both that vision encoder and its partner text encoder are driven by the same image–text (contrastive) objective, so that visual inputs and their matching captions end up in one shared d-dimensional space.  At test time we exploit exactly that:\n\n  1.  We run the image through the vision encoder and, via RoIAlign, pull out a d-dimensional vector for every candidate region.\n  2.  We run our class prompts (e.g. “butterfly”, “flower”, …) through the text encoder and get one d-dimensional vector per label.\n  3.  Because both the region vectors and the text vectors live in the same embedding space, we can compute cosine similarities (or dot–products), plug them into a softmax, and voilà—zero-shot classification at the level of individual regions.\n\nIn short: the red-boxed vision encoder and the text encoder have been co-trained to output embeddings in the same space, so that you can directly score any region against any text label without further supervision."
    },
    {
        "question": "How does the fine-grained region-aware objective complement the global image-text objective during mitigation?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "We now introduce a novel region-aware contrastive loss function for training VLM M.",
            "For image x, the first loss component L_{r–txt} encourages high embedding similarity between non-spurious regions R^+ and assigned class label y when compared to other class labels. … The term L_{neg} is a penalty that enforces embedding-level dissimilarity between spurious regions and correlated class labels.",
            "The second loss component L_{r–r} encourages high embedding similarity between non-spurious regions R^+ and assigned class label y when compared to other regions.",
            "The final loss is expressed as L = L_{reg} + λ L_{orig}. Here, λ is a hyperparameter and L_{orig} takes the form of the original loss function used for training M; in our experiments, L_{orig} is the CLIP objective."
        ],
        "final_answer": "During mitigation, RaVL combines a global image–text contrastive loss (the CLIP objective) with a fine-grained region-aware loss. The region-aware loss adds two contrastive terms over image subregions—one that pulls non-spurious regions close to the correct class text and another that discriminates them from other regions—while simultaneously pushing spurious regions away from their (incorrectly) correlated class labels. By weighting and summing this region-level loss with the standard global image-text loss (via the hyperparameter λ), RaVL preserves overall vision–language alignment at the image level while explicitly steering the model’s attention toward relevant local features and away from spurious ones.",
        "relevant_elements": [
            "Fine-Grained Region-Aware Objective",
            "Global Image-Text Objective"
        ],
        "id": 682,
        "masked_question": "How does the [mask1] complement the global image-text objective during mitigation?",
        "masked_number": 1,
        "masked_elements": [
            "Fine-Grained Region-Aware Objective"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04097v1_figure_1.png",
        "paperid": "2411.04097v1",
        "paper_path": "./papers/2411.04097v1.json",
        "figure_id": "2411.04097v1_figure_1.png",
        "caption": "Figure 1: Region-aware Vision-Language learning (RaVL). RaVL takes a fine-grained perspective on VLM robustness by discovering and mitigating spurious correlations using local image features.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed “Fine-Grained Region-Aware Objective” (the [mask1]) complements the global image–text objective during Stage 2:\n\n1. Global Objective Alone  \n   • Like standard CLIP, it only aligns the whole‐image embedding to the text embedding.  \n   • If there is a spurious background feature (e.g. “flower”) that co-occurs with the class (“butterfly”) in the fine-tuning data, the global loss will happily reinforce that spurious association.\n\n2. What the Region-Aware Objective Adds  \n   • It breaks each image into regions and uses the discovered “spurious” versus “non-spurious” masks from Stage 1.  \n   • For **non-spurious regions** (the butterfly itself) it **maximizes** their similarity to the correct class text (“butterfly”).  \n   • For **spurious regions** (the flower background) it **minimizes** their similarity to that same text.\n\n3. Why This Complements the Global Loss  \n   • The global loss keeps the model aligned on “image→text” at a coarse level, ensuring the network still recognizes the class overall.  \n   • The region-aware loss then **punishes** any shortcut the model tries to take by relying on flower patches, and **rewards** focusing on the actual object region.  \n   • Together, they force the network to both stay globally grounded in the text and ignore local spurious cues.\n\nIn short, [mask1] is a region-level contrastive penalty that sits on top of the usual whole-image CLIP loss, steering the model away from spurious background correlations while preserving correct global alignment."
    },
    {
        "question": "How do the visual encoder and text encoder outputs integrate to compute individual concept similarity scores?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To estimate the presence of clinical concepts from an input image, we utilize a pretrained VLM and calculate the cosine similarity between the image features and a set of clinical concepts in the concept set (see Section 3.1).",
            "Following previous work [17], we adopt a pretrained VLM and determine the presence of a dermoscopic concept c in the input image by assessing the similarity between the image feature embedding, v, and the feature embedding of each concept t. Formally, the similarity scores are given by: s_i = sim(v, t_i) where sim is a similarity metric (e.g., cosine similarity), f_v is the visual encoder, and f_t is the text encoder."
        ],
        "final_answer": "For each concept, the image is encoded by the visual encoder into an embedding v, the concept name is encoded by the text encoder into an embedding t_i, and their pairwise similarity (e.g., cosine similarity) sim(v, t_i) yields the concept’s score.",
        "relevant_elements": [
            "Visual Encoder",
            "Text Encoder"
        ],
        "id": 683,
        "masked_question": "How do the [mask1] and [mask2] outputs integrate to compute individual concept similarity scores?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Encoder",
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05609v1_figure_1.png",
        "paperid": "2411.05609v1",
        "paper_path": "./papers/2411.05609v1.json",
        "figure_id": "2411.05609v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed framework. The linear classifier layer (left) is replaced by a Large Language Model (LLM) (right), which grounds its responses on clinical concepts predicted by a pretrained vision-language model (VLM). This approach is training-free and not restricted by predefined labels, allowing the LLM to generate diverse diagnostic possibilities for different diseases.",
        "qtype": "Implementation_Details",
        "response": "Let’s unpack what each masked region is doing and how they get turned into a similarity score.\n\n1.  [mask1] (red box) is the output of the **visual encoder** – call it v (e.g. v₁ in the diagram).  It is a high‐dimensional feature vector that represents the input image.  \n2.  [mask2] (blue box) is the output of the **text encoder** for each clinical concept – call them t₁, t₂, …, tₙ.  Each tᵢ is the embedding of the i-th concept’s text description.  \n\nTo decide how much the image “matches” each concept, you simply take **v** and each **tᵢ** and feed them into a similarity function (in practice cosine similarity).  Concretely, for concept i you compute\n\n sᵢ  =  cosine_similarity( v ,  tᵢ )  =  (v · tᵢ) / (‖v‖‖tᵢ‖)  \n\nThose sᵢ scores are your per-concept similarity measures."
    },
    {
        "question": "How does the prompt integrate predicted concept phrases into the LLM input to enable flexible disease outputs?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Using the concept scores generated in the first stage (Equation 1), we binarize them using a threshold and map them to their respective concept names.",
            "These concepts are then incorporated into the designed prompt. An example of this prompt is provided on the right side of Figure 1. This method ensures that the diagnosis is based on the dermoscopic concepts, improving the interpretability and transparency of the model’s output, and eliminates the need for training a linear layer, allowing for more flexible and varied diagnostic output formats."
        ],
        "final_answer": "After predicting and thresholding the concept scores, the model maps each positive concept score to its corresponding phrase and slots those phrases directly into a pre-designed LLM prompt (see Figure 1). By embedding the list of concept names into the \"Question\" portion of the prompt, the LLM is asked to produce a diagnosis grounded on those concepts—eliminating any fixed linear classifier and enabling flexible, varied disease outputs.",
        "relevant_elements": [
            "Prompt",
            "LLM"
        ],
        "id": 684,
        "masked_question": "How does the [mask1] integrate predicted concept phrases into the [mask2] input to enable flexible disease outputs?",
        "masked_number": 2,
        "masked_elements": [
            "Prompt",
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05609v1_figure_1.png",
        "paperid": "2411.05609v1",
        "paper_path": "./papers/2411.05609v1.json",
        "figure_id": "2411.05609v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed framework. The linear classifier layer (left) is replaced by a Large Language Model (LLM) (right), which grounds its responses on clinical concepts predicted by a pretrained vision-language model (VLM). This approach is training-free and not restricted by predefined labels, allowing the LLM to generate diverse diagnostic possibilities for different diseases.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ is nothing more than the natural‐language prompt into which we literally paste the list of predicted dermoscopic concepts, and Mask₂ is the LLM that consumes that prompt.  In practice we take the VLM’s top‐scoring concepts, turn them into human-readable phrases, slot them into the “Question: … concepts: [ … ]” field of our prompt template (that’s the red box), and then hand that entire string straight to the LLM (the blue box).  Because the LLM sees the concepts in plain text, it can freely map them to any disease label you ask for—no linear layer or fixed label set required."
    },
    {
        "question": "How do Visual Encoder and Text Encoder interactions compare to traditional CBM bottleneck for concept mapping?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "Given the task of predicting a target disease y from input x, let D represent a batch of training samples, where c is a vector of m clinical concepts. CBMs first map the input x into a set of interpretable concepts c (the “bottleneck”) by learning a function f: X → C, and use these concepts to predict the target y through g: C → Y. As a result, the final prediction ŷ is entirely based on the predicted concepts c.",
            "To estimate the presence of clinical concepts from an input image, we utilize a pretrained VLM and calculate the cosine similarity between the image features and a set of clinical concepts in the concept set C (see Section 3.1).",
            "Following previous work [17], we adopt a pretrained VLM and determine the presence of a dermoscopic concept cᵢ in the input image by assessing the similarity between the image feature embedding, v = E_v(x), and the feature embedding of each concept tᵢ = E_t(cᵢ). Formally, the similarity scores are given by sᵢ = sim(v, tᵢ), where E_v is a visual encoder and E_t is a text encoder."
        ],
        "final_answer": "Instead of learning an explicit bottleneck mapping f from images to concepts as in traditional CBMs, the proposed method uses a fixed pretrained visual encoder (E_v) and text encoder (E_t) to compute cosine similarity scores between image embeddings and concept text embeddings. This interaction replaces the learned bottleneck with a zero-shot similarity comparison for concept detection, removing the need to train a separate concept prediction layer.",
        "relevant_elements": [
            "Visual Encoder",
            "Text Encoder"
        ],
        "id": 685,
        "masked_question": "How do [mask1] and [mask2] interactions compare to traditional CBM bottleneck for concept mapping?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Encoder",
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05609v1_figure_1.png",
        "paperid": "2411.05609v1",
        "paper_path": "./papers/2411.05609v1.json",
        "figure_id": "2411.05609v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed framework. The linear classifier layer (left) is replaced by a Large Language Model (LLM) (right), which grounds its responses on clinical concepts predicted by a pretrained vision-language model (VLM). This approach is training-free and not restricted by predefined labels, allowing the LLM to generate diverse diagnostic possibilities for different diseases.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In the figure, the red box around “Visual Encoder” is our [mask1], and the blue box around “Text Encoder” is our [mask2].  \n2. In a standard Concept Bottleneck Model (CBM), you learn a dedicated mapping f (x)→c from images x to a fixed set of concepts c by training a network (often a small multi-layer perceptron or CNN head) on concept labels. That mapping is the “bottleneck,” and it must be trained end-to-end with concept supervision.  \n3. In contrast, this paper’s method replaces that learned bottleneck with a frozen vision–language model (VLM) and a frozen text encoder.  \n   – The VLM (our [mask1]) encodes the input image x into a feature vector v.  \n   – The text encoder (our [mask2]) encodes each concept name or description tᵢ into a feature vector τᵢ.  \n   – We then compute a similarity score sim(v,τᵢ) (e.g. cosine similarity) for each concept i. That score directly estimates whether concept i is present—no additional training is needed.  \n4. So whereas traditional CBMs learn a task‐specific, parameterized bottleneck f(x)→c with concept supervision, here the “interaction” between [mask1] and [mask2] is purely a zero-shot similarity lookup in a joint embedding space. This is training‐free, label-agnostic, and can scale to any number of concepts, while the traditional CBM bottleneck is fixed to whatever concepts it was trained on and requires re-training to add or remove concepts.  \n\nAnswer  \nThe red‐boxed visual encoder and blue‐boxed text encoder simply embed images and concept names into the same space and use cosine similarity between them to “predict” concepts on the fly. By contrast, a traditional CBM learns a dedicated neural mapping from images to a fixed concept vector (the bottleneck) through supervised training. The VLM+text‐encoder approach is training-free, zero-shot, and label-unlimited, whereas the classic CBM bottleneck is a learned, task-specific, and label-limited mapping."
    },
    {
        "question": "How does replacing the linear classifier with an LLM affect training demands in disease classification pipelines?",
        "relevant_section_ids": [
            "1",
            "2.2",
            "3.2"
        ],
        "relevant_context": [
            "In this work, we address the limitations of CBMs by proposing a novel two-step approach that provides concept-based explanations and generates disease diagnoses grounded in predicted concepts, all without the need for additional training. … However, unlike CBMs, our approach does not require training to provide the final diagnosis class and can be easily adapted to incorporate new concepts.",
            "Despite these improvements, most models still rely on a linear classifier to predict the final diagnostic label, whether based on concepts or visual features. Our approach overcomes this by prompting an LLM to directly predict the diagnosis using a tailored prompt that incorporates the concepts extracted by a pretrained VLM. This eliminates the fixed label constraint, improving scalability and removing the need for retraining when new diagnostic categories or concepts are introduced.",
            "This method ensures that the diagnosis is based on the dermoscopic concepts, improving the interpretability and transparency of the model’s output, and eliminates the need for training a linear layer, allowing for more flexible and varied diagnostic output formats."
        ],
        "final_answer": "By replacing the linear classifier with an LLM, the pipeline no longer requires training or retraining of a downstream classification layer, making the disease classification step effectively training-free and easily extensible to new diagnostic categories.",
        "relevant_elements": [
            "linear classifier",
            "LLM"
        ],
        "id": 686,
        "masked_question": "How does replacing the [mask1] with an [mask2] affect training demands in disease classification pipelines?",
        "masked_number": 2,
        "masked_elements": [
            "linear classifier",
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.05609v1_figure_1.png",
        "paperid": "2411.05609v1",
        "paper_path": "./papers/2411.05609v1.json",
        "figure_id": "2411.05609v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed framework. The linear classifier layer (left) is replaced by a Large Language Model (LLM) (right), which grounds its responses on clinical concepts predicted by a pretrained vision-language model (VLM). This approach is training-free and not restricted by predefined labels, allowing the LLM to generate diverse diagnostic possibilities for different diseases.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought  \n1. In the original (left-hand) pipeline the red-boxed module is a linear layer that takes concept predictions and must be trained (and is limited to whatever disease labels it was exposed to during that training).  \n2. In the proposed (right-hand) pipeline the blue-boxed module is an off-the-shelf LLM that is simply prompted (few-shot or zero-shot) to map concepts to a disease label, rather than being trained.  \n3. Because the LLM uses in-context learning on the predicted concepts, there is no need to update any weights for the disease–classification step.  \n\nAnswer  \nBy swapping out the red-boxed linear classifier for the blue-boxed LLM, you eliminate the need to train a bespoke classification head altogether—making the disease label prediction training-free."
    },
    {
        "question": "How does RTFA's token clustering refine attention compared to vanilla self-attention token selection?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Due to the intrinsic similarity between the camouflaged objects and background surroundings, directly using the self-attention mechanism to establish long-range dependency will inevitably introduce irrelevant interference by the background distractions, resulting in inferior segmentation outputs for camouflaged object discrimination.",
            "To address this issue, we propose a region-aware token focusing attention (RTFA) module, allowing the model to excavate the potentially distinguishable tokens using a dynamic token clustering strategy.",
            "Then we utilize a dynamic token clustering method based on DPC-KNN [58] to discard the token redundancy.",
            "The cluster centers with the top k scores are selected to construct the discriminative clustered token vc, which is further concatenated with the key-value pairs for token enhancement.",
            "Finally, we apply multi-head self-attention within those enhanced tokens."
        ],
        "final_answer": "Unlike vanilla self-attention, which attends over all tokens (and can be distracted by background regions), RTFA first computes a region-aware affinity matrix and applies a dynamic clustering (via DPC-KNN) to score and select only the most informative tokens. It discards redundant or irrelevant tokens, concatenates the selected cluster-center tokens with the key/value pairs, and then performs self-attention on this refined subset—thereby focusing the attention on discriminative regions and suppressing background interference.",
        "relevant_elements": [
            "RTFA",
            "token clustering",
            "self-attention"
        ],
        "id": 687,
        "masked_question": "How does [mask1]'s token clustering refine attention compared to vanilla self-attention token selection?",
        "masked_number": 1,
        "masked_elements": [
            "RTFA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.15020v2_figure_2.png",
        "paperid": "2408.15020v2",
        "paper_path": "./papers/2408.15020v2.json",
        "figure_id": "2408.15020v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of the proposed HGINet. It mainly consists of a transformer backbone with multiple RTFA blocks, a hierarchical graph interaction transformer (HGIT), and a decoder network with confidence aggregated feature fusion (CAFF) modules. (a) illustrates our RTFA, i.e., region-aware token focusing module, which consists of a pooling and dynamic token clustering strategy to excavate the most distinguishable tokens. (b) demonstrates our graph projection and reprojection strategy in latent space.",
        "qtype": "Literature_Background",
        "response": "Here is the step-by-step reasoning tying the red-boxed module back to the text and then contrasting it with vanilla self-attention:\n\n1.   Identify the red box as RTFA (Region-aware Token Focusing Attention).  \n     – In the figure it shows:  \n       •  A pooling of the Q/K tokens into coarse “pooled features.”  \n       •  Construction of an affinity matrix A between those pooled tokens.  \n       •  A dynamic clustering step (DPC-KNN) that computes for each token a “local density” ρ and a distance δ to higher-density tokens.  \n       •  A score γ = ρ·δ for every token, and selection of the top-k tokens (the cluster centers) to form A′.  \n       •  Those top-k “cluster center” tokens are then concatenated back with the original K/V and fed into a standard multi-head self-attention.  \n\n2.   What vanilla self-attention does instead.  \n     – You linearly project the entire H×W grid into Q, K, V.  \n     – Every query attends to all H·W key/value pairs (or even if you do top-k you simply take the biggest dot-product responses, with no notion of “regional clustering” or density).  \n\n3.   The key difference:  \n     – RTFA’s clustering step explicitly groups semantically similar tokens in the affinity space, computes a density-peak score, and keeps only the most “informative” tokens (cluster centers).  \n     – By discarding low-score (redundant or background) tokens before even doing Q·Kᵀ, you force the attention to focus on a compact set of representative, discriminative regions.  \n     – Vanilla self-attention, by contrast, either attends everywhere (incurring background noise) or simply picks the top-k dot-product keys without any region-aware, density-based pruning.  \n\nAnswer:  \nRTFA’s “token clustering” first builds an affinity graph among pooled tokens, uses a density-peak (DPC-KNN) criterion to pick only the top-k cluster-center tokens, and then runs self-attention over that reduced set of highly informative tokens.  Vanilla self-attention, by comparison, attends over all spatial tokens (or naively prunes by raw dot-product scores) and has no mechanism for grouping semantically similar patches into region-aware representatives."
    },
    {
        "question": "How does HGIT's bi-directional graph interaction differ from classic non-local attention message passing?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "After obtaining graph representations via latent space graph projection, we apply a simple yet effective interaction approach to create the local alignment and communications between the graphs in hierarchical transformer blocks. Specifically, for the graph nodes \\(\\widetilde{v}_i\\) in i-th stage and \\(\\widetilde{v}_j\\) in j-th stage, we use the non-local operation [59] with softmax to perform bi-directional interaction:",
            "The functions \\(\\phi,\\psi,\\theta,\\omega\\) are learnable transformations on the graph nodes. \\(S_{i\\to j}\\) and \\(S_{j\\to i}\\) can be regarded as the alignment matrices measuring the correlation between the nodes in dual graphs, which hint the complementary visual semantics corresponding to the hierarchical feature maps.",
            "Meanwhile, we concatenate the graph nodes and squeeze the feature channel to combine both graph information. Then we perform graph interaction by multiplying \\(S_{i\\to j}\\) with one graph’s nodes and \\(S_{j\\to i}\\) with the other:",
            "By performing such interaction, the latent graph nodes \\(\\hat{V}_i\\) and \\(\\hat{V}_j\\) are simultaneously enhanced, leading to more powerful visual semantic mining of the camouflaged objects."
        ],
        "final_answer": "Classic non-local attention performs self-attention message passing within a single set of nodes (or one feature map), aggregating information from all other locations back into itself. In contrast, HGIT’s bi-directional graph interaction first constructs two separate graph representations from adjacent transformer stages, then uses two cross-graph non-local operations (with learned softmax alignment matrices \\(S_{i→j}\\) and \\(S_{j→i}\\)) to exchange messages in both directions between these two graphs. In other words, instead of only attending within one graph, HGIT aligns and propagates complementary semantics across hierarchical graphs by passing messages both ways between them.",
        "relevant_elements": [
            "HGIT",
            "graph interaction",
            "non-local attention"
        ],
        "id": 688,
        "masked_question": "How does [mask1]'s bi-directional [mask2] differ from classic non-local attention message passing?",
        "masked_number": 2,
        "masked_elements": [
            "HGIT",
            "graph interaction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.15020v2_figure_2.png",
        "paperid": "2408.15020v2",
        "paper_path": "./papers/2408.15020v2.json",
        "figure_id": "2408.15020v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of the proposed HGINet. It mainly consists of a transformer backbone with multiple RTFA blocks, a hierarchical graph interaction transformer (HGIT), and a decoder network with confidence aggregated feature fusion (CAFF) modules. (a) illustrates our RTFA, i.e., region-aware token focusing module, which consists of a pooling and dynamic token clustering strategy to excavate the most distinguishable tokens. (b) demonstrates our graph projection and reprojection strategy in latent space.",
        "qtype": "Literature_Background",
        "response": "Here is the short of it:\n\n• Mask 1 (the red‐boxed module) is the HGIT, i.e. the “Hierarchical Graph Interaction Transformer.”  \n• Mask 2 (the blue box inside the projection/reprojection inset) is the “Graph Interaction” step.\n\nIn a vanilla non-local block you take one feature map X, form Q,K,V = WₓX, compute a single attention A = softmax(QKᵀ), and then do V′ = AV to pass messages within that one set of tokens.\n\nThe HGIT “bi-directional graph interaction,” by contrast, works on *two* different graphs (say G₂ and G₃ coming from two successive RTFA stages).  Concretely:\n\n 1. You project each feature map into its own latent graph node set V₂ and V₃.  \n 2. You form *two* cross-graph affinities  \n     A₂→₃ = softmax( f(V₃)·g(V₂)ᵀ )  \n     A₃→₂ = softmax( f(V₂)·g(V₃)ᵀ )  \n 3. You use A₂→₃ to send messages from V₂→V₃ *and* A₃→₂ to send messages from V₃→V₂—so information truly flows in *both* directions between the two hierarchies.  \n 4. Finally you concatenate the updated V₂ and V₃, wrap them with a graph-aware transformer (using the Laplacian as a positional bias), and then reproject back to the pixel grid.\n\nBy contrast, a classic non-local is *only* doing one self-attention pass within the *same* token set.  HGIT’s “graph interaction” explicitly couples *two* graphs and pushes messages both ways, plus it injects graph Laplacian positional priors and slots the result into a full transformer block, rather than just a single 1×1-conv re–projection."
    },
    {
        "question": "How does dynamic token clustering within RTFA suppress irrelevant tokens effectively?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Then we utilize a dynamic token clustering method based on DPC-KNN [58] to discard the token redundancy.",
            "For each token, a distance indicator δ_i is utilized to measure the closeness between this token and the surrounding tokens as follows: ... We calculate the score of each token by γ_i = δ_i × ρ_i. The higher scores indicate that the token i presents informative visual semantics for camouflaged object discrimination. The cluster centers with the top m scores are selected to construct the discriminative clustered token C, which is further concatenated with the key-value pairs for token enhancement:"
        ],
        "final_answer": "Within RTFA, dynamic token clustering first builds a region-aware affinity matrix and then measures for each token its local density (ρ_i) and its distance to higher-density neighbors (δ_i). By scoring tokens as γ_i = δ_i × ρ_i and selecting only the top-scoring tokens as cluster centers—discarding the rest—the module effectively suppresses redundant or irrelevant tokens (e.g., background distractions) and preserves only the most informative tokens for downstream attention.",
        "relevant_elements": [
            "RTFA",
            "dynamic token clustering"
        ],
        "id": 689,
        "masked_question": "How does [mask1] within [mask2] suppress irrelevant tokens effectively?",
        "masked_number": 2,
        "masked_elements": [
            "dynamic token clustering",
            "RTFA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.15020v2_figure_2.png",
        "paperid": "2408.15020v2",
        "paper_path": "./papers/2408.15020v2.json",
        "figure_id": "2408.15020v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of the proposed HGINet. It mainly consists of a transformer backbone with multiple RTFA blocks, a hierarchical graph interaction transformer (HGIT), and a decoder network with confidence aggregated feature fusion (CAFF) modules. (a) illustrates our RTFA, i.e., region-aware token focusing module, which consists of a pooling and dynamic token clustering strategy to excavate the most distinguishable tokens. (b) demonstrates our graph projection and reprojection strategy in latent space.",
        "qtype": "Experimental_Results",
        "response": "Here is the step-by-step reasoning:\n\n1.  Identify [mask2]: it is the blue-boxed module, namely the RTFA (Region-aware Token Focusing Attention) block.  \n2.  Zoom into what RTFA does (text §III-B and Fig. 2(a)):  \n    –   You start with a set of N tokens from the feature map Fᵢ.  \n    –   You compute a “region-aware” affinity matrix A by average-pooling Q and K into local groups and then taking Q·Kᵀ.  \n3.  The red-boxed [mask1] inside RTFA is the “dynamic token clustering” step (based on DPC-KNN):  \n    a.  For each token i you compute:  \n       •  ρᵢ, its local density, by summing affinities to its k nearest neighbours in A.  \n       •  δᵢ, the distance to the nearest token of higher density.  \n       •  A score γᵢ = ρᵢ·δᵢ so that tokens which are both in a dense region and far from higher-density tokens score highly.  \n    b.  You then pick the top-K tokens by γᵢ as cluster centers and throw away the rest.  \n4.  Why does that suppress irrelevant tokens?  \n    –   Low-density tokens (likely background or redundant) have small ρ or lie too close to another, so γᵢ is low and they are dropped.  \n    –   Only the K “most informative” cluster centers survive, which guarantees the subsequent self-attention only sees tokens that (a) have strong semantic ties to neighbours and (b) are not mere duplicates of an already selected token.  \n5.  Those K clustered tokens are then concatenated with the original K–V pairs and fed into multi-head self-attention.  Because all the low-score, irrelevant tokens were pruned, the transformer “focuses” on the remaining discriminative tokens.\n\nAnswer (concise):\n\nWithin the RTFA block, the dynamic token clustering (the red-boxed step) computes for each token a local-density ρᵢ (via its k-NN affinities) and a distance δᵢ to the nearest higher-density token, multiplies them into a score γᵢ=ρᵢ·δᵢ, and then only keeps the top-K scored tokens as cluster centers.  All other tokens—those with low density or that lie too close to a richer neighbor—are discarded, so the self-attention that follows operates only on these few, highly informative tokens, effectively suppressing irrelevant ones."
    },
    {
        "question": "How does Data Selection balance instruction sample confidence and diversity during finetuning efficiency?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "We design a sampling method to select representative samples to reduce instruction data. The main idea is to opt for high-confidence samples indicated by f, thereby empowering T to acquire intrinsic semantic knowledge of G efficiently.",
            "Specifically, we assess the confidence of s from both global and local perspectives. The global confidence α_g is computed as α_g = 1 – rank(t)/|E|, where rank(t) is the position of the true tail among all entities. The local confidence α_l is computed as the model score f(q, t) of that fact. The final confidence α = λ·α_g + (1–λ)·α_l. Subsequently, we introduce a threshold β and keep the samples with confidence greater than β as the final instruction data."
        ],
        "final_answer": "Data Selection applies a truncated-sampling strategy in which each candidate instruction is scored by a combined confidence metric—α = λ·(1 − rank/|E|) + (1−λ)·f(q,t)—so that both its global standing in the embedding model’s ranking and its local score are taken into account. Only those samples whose combined confidence exceeds a preset threshold β are retained. In this way, the framework dramatically shrinks the training set to high-quality (high-confidence), yet still semantically varied, examples, achieving efficient finetuning without sacrificing diversity.",
        "relevant_elements": [
            "Data Selection"
        ],
        "id": 691,
        "masked_question": "How does [mask1] balance instruction sample confidence and diversity during finetuning efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Data Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.16127v1_figure_1.png",
        "paperid": "2407.16127v1",
        "paper_path": "./papers/2407.16127v1.json",
        "figure_id": "2407.16127v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed DIFT framework.",
        "qtype": "Experimental_Results",
        "response": "Let me walk through what that red-boxed “Data Selection” module is doing:\n\n1.  For every candidate training triple  it first computes two confidence scores:  \n    •  A global confidence,  = 1/rankₜ, which measures how high our tail-predictor ranked the true tail among all  entities.  \n    •  A local confidence,  = scoreₜ, which is the raw embedding‐model score that the predictor assigns to the true tail.  \n\n2.  It then fuses them into a single confidence  \n      c = λ·μ + (1 – λ)·ℓ  \n    where λ is a tunable hyperparameter balancing “global” versus “local” trust.  \n\n3.  Finally it throws away any example whose combined confidence c ≤ β (another hyperparameter).  \n\nBy adjusting λ and the cutoff β, this “truncated sampling” step  1) keeps only very high‐confidence facts (so the LLM isn’t wasting time on noisy or ambiguous triples) yet  2) still mixes in examples that might not be top-ranked overall but have strong local scores, preserving a bit of variety.  The result is an instruction set that is both compact (efficient to fine-tune on) and representative enough to teach the LLM the core patterns of the KG."
    },
    {
        "question": "How does Knowledge Adaption utilize embeddings from embedding-based models to improve LLM predictions?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "The facts provided in P are presented in the text format, losing the global structure information of KGs. Therefore, we propose to inject the embeddings learned from E with KG structure into L to further improve its graph reasoning ability.",
            "We align the embeddings from E with the semantic space of L, to get the knowledge representations:",
            "Considering that E scores a fact based on the embeddings of the query Q and the candidate entity t, we inject the knowledge representations of Q and all candidate entities in C into L.",
            "Specifically, we place a “[QUERY]” after the missing entity in P and an “[ENTITY]” after each entity name in C."
        ],
        "final_answer": "Knowledge Adaption first takes the query and candidate-entity embeddings produced by the pre-trained KG embedding model and passes them through a trainable linear projection (with SwiGLU activation) to align them with the LLM’s semantic space. It then inserts these projected embeddings—called \"knowledge representations\"—directly into the LLM’s input stream at special placeholder tokens ([QUERY] following the missing head/tail in the prompt, and [ENTITY] following each candidate name). By doing so, the LLM receives both the natural-language prompt and structured KG information, which enhances its graph reasoning capability and leads to more accurate entity predictions.",
        "relevant_elements": [
            "Knowledge Adaption",
            "Embedding-based models"
        ],
        "id": 692,
        "masked_question": "How does [mask1] utilize embeddings from [mask2] to improve LLM predictions?",
        "masked_number": 2,
        "masked_elements": [
            "Knowledge Adaption",
            "Embedding-based models"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.16127v1_figure_1.png",
        "paperid": "2407.16127v1",
        "paper_path": "./papers/2407.16127v1.json",
        "figure_id": "2407.16127v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed DIFT framework.",
        "qtype": "Experimental_Results",
        "response": "Here is a concise, step-by-step explanation of how the red‐boxed “Knowledge Adaption” module uses the blue‐boxed embeddings to boost the LLM’s predictions:\n\n1.  Embedding‐based model (blue box)  \n    – This is any off-the-shelf KG embedding method (e.g. TransE, SimKGC, CoLE) that, given a partial triple \\((h,r,?)\\), can score and rank all candidate tail entities and also produce continuous vector representations for the query \\((h,r)\\) and for each candidate entity \\(t\\).\n\n2.  Extraction of query and candidate embeddings  \n    – At training time, for each prompt we pull out two sets of vectors from the pretrained embedding model:  \n       • \\(\\mathbf{e}_{\\text{query}}\\), the embedding of the incomplete fact \\((h,r)\\)  \n       • \\(\\{\\mathbf{e}_{t_i}\\}\\), the embeddings of the top-\\(k\\) candidate entities  \n\n3.  Alignment into the LLM’s space (the heart of Knowledge Adaption, red box)  \n    – We learn a small projection head with parameters \\(W_q, W_e, b_q, b_e\\) (plus a SwiGLU nonlinearity) that maps each embedding into the LLaMA hidden dimension:  \n      \\[\n        \\mathbf{k}_{\\text{query}}\n        = \\text{SwiGLU}\\bigl(W_q\\,\\mathbf{e}_{\\text{query}} + b_q\\bigr)\n        \\quad,\\quad\n        \\mathbf{k}_{t_i}\n        = \\text{SwiGLU}\\bigl(W_e\\,\\mathbf{e}_{t_i} + b_e\\bigr)\\,.\n      \\]\n    – These \\(\\mathbf{k}_{\\text{query}}\\) and \\(\\{\\mathbf{k}_{t_i}\\}\\) are the “knowledge representations.”\n\n4.  Injection into the prompt  \n    – In the textual prompt we insert two special tokens:  \n       • `[QUERY]` immediately after the missing slot in the triplet  \n       • one `[ENTITY]` after each of the \\(k\\) candidate names  \n    – During finetuning, the hidden states at those special tokens are replaced (or fused) with the corresponding \\(\\mathbf{k}_{\\text{query}}\\) and \\(\\{\\mathbf{k}_{t_i}\\}\\).  \n\n5.  Effect on LLM predictions  \n    – By grafting in these aligned KG embeddings, the LLM now has direct access to global graph structure and learned link‐prediction scores in its hidden layers.  \n    – This reduces grounding errors (it will pick only from the \\(k\\) supplied candidates) and greatly improves its accuracy at choosing the correct tail entity.\n\nIn short, the red-boxed “Knowledge Adaption” module takes the query-and-entity embeddings produced by the blue-boxed KG embedding model, projects them into the LLM’s latent space via small trainable layers, and then injects them at designated slots in the prompt so that the finetuned LLM has both the textual and structural knowledge it needs to pick the right answer."
    },
    {
        "question": "What challenges arise in distillation loss when aligning LLM-based and online query tower embeddings?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the first stage, our model measures distance using cosine similarity, which effectively captures the directional alignment of embeddings but overlooks the importance of magnitude.",
            "To address this limitation, we also use MSE (Wang and Hong, 2023) to align the representations of the teacher and student models more comprehensively, considering both direction and magnitude."
        ],
        "final_answer": "When distilling from the LLM-based query tower to the online query tower, using only a cosine‐similarity loss aligns embedding directions but ignores their magnitudes. This necessitates adding an MSE term to the distillation loss so that both direction and magnitude of the embeddings are matched.",
        "relevant_elements": [
            "Distillation Loss",
            "LLM-based Query Tower",
            "Online Query Tower"
        ],
        "id": 696,
        "masked_question": "What challenges arise in [mask1] when aligning LLM-based and online query tower embeddings?",
        "masked_number": 1,
        "masked_elements": [
            "Distillation Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15766v1_figure_2.png",
        "paperid": "2411.15766v1",
        "paper_path": "./papers/2411.15766v1.json",
        "figure_id": "2411.15766v1_figure_2.png",
        "caption": "Figure 2. The framework of ScalingNote. The first stage is fully scaling the dual-tower using scaled training data, which learns through cross-device contrastive learning and hard negative mining.\nThe second stage is query-based knowledge distillation (QKD), which transfers the scaled query knowledge from the LLM-based query tower to the faster online query tower.",
        "qtype": "Others",
        "response": "In that red‐boxed “Distillation Loss” block the authors point out that if you only line up the LLM‐tower and the small (online) tower by cosine similarity you will get perfect directional agreement but completely lose any sense of how big or small the two vectors actually are.  In other words, cosine similarity alone ignores the embeddings’ norms (magnitudes), so you need to add an MSE term to the loss to force the student to match both direction and length of the teacher’s query vectors—and even then you must pick the right weighting (λ) between the two losses."
    },
    {
        "question": "What rationale drives integrating cross-device contrastive learning with MRL alongside Hard Negative Mining with MRL?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To facilitate the future use of our representations by downstream applications with varying resource constraints, we adopt efficient MRL (Kusupati et al., 2022). We set the target vector dimension as d_max, and the least dimension as d_min. Then, we obtain the candidate available vector dimensions {d_min, ..., d_max}. These embeddings are transformed into lower dimensions to reduce memory cost. We can select any dimension d_i and truncate these embeddings to form shorter representations, such as e_i.",
            "Upon completion of encoding by each GPU device, we utilize cross-device contrastive learning to expand the number of in-batch negatives. Through the all-gather operation, each device obtains all query and document representations from other devices. We denote the batch of all-gathered query representations as Q̃, and the batch of all-gathered document representations of three types as D̃_title, D̃_topic, and D̃_content. For any i, we conduct contrastive learning for queries and three types of document embeddings, aggregating the losses across different embedding dimensions.",
            "Hard negatives are crucial for model performance, especially for top-position ranking (Xiong et al., [n. d.]; Zhan et al., 2021). Therefore, we mine the hard negatives using margin loss. We compute a margin-based loss for each dimension of the document embeddings and then aggregate these losses across all dimensions to form the final hard-negative loss."
        ],
        "final_answer": "The model combines cross-device contrastive learning with MRL and hard-negative mining with MRL so that it can both (1) leverage a large, diverse pool of in-batch negatives—gathered across multiple GPUs—to improve overall discriminative power, and (2) focus specifically on the most challenging (hard) negatives to boost top‐rank retrieval performance. At the same time, MRL (multi-resolution learning) lets the system flexibly truncate or expand embedding dimensions for different downstream resource constraints, enabling memory-efficient yet high-quality representations throughout both contrastive and hard-negative training.",
        "relevant_elements": [
            "Cross-device Contrastive Learning with MRL",
            "Hard Negative Mining with MRL"
        ],
        "id": 697,
        "masked_question": "What rationale drives integrating [mask1] alongside [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Cross-device Contrastive Learning with MRL",
            "Hard Negative Mining with MRL"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15766v1_figure_2.png",
        "paperid": "2411.15766v1",
        "paper_path": "./papers/2411.15766v1.json",
        "figure_id": "2411.15766v1_figure_2.png",
        "caption": "Figure 2. The framework of ScalingNote. The first stage is fully scaling the dual-tower using scaled training data, which learns through cross-device contrastive learning and hard negative mining.\nThe second stage is query-based knowledge distillation (QKD), which transfers the scaled query knowledge from the LLM-based query tower to the faster online query tower.",
        "qtype": "Design_Rationale",
        "response": "First, let’s unpack what the two masked components actually are:\n\n  – mask1 (red box) is the “Cross‐device Contrastive Learning with MRL” module.  \n  – mask2 (blue box) is the “Hard Negative Mining with MRL” module.  \n\nWhy do we use both, rather than only one?  \n\n1. Contrastive learning (red box) by itself treats every other document in the (expanded) batch as a negative.  By doing an all-gather across multiple GPUs we massively increase the pool of “easy” negatives, which forces the model to learn broadly discriminative features across many examples and – thanks to MRL – across several embedding resolutions at once.  \n\n2. Hard negative mining (blue box) then takes the *most confusing* negatives (those the current model still ranks too highly) and applies a margin‐based loss to push those very specific negatives down.  This sharpens the decision boundary where it matters most for top‐k ranking.  \n\nTogether they are complementary:  \n  - Cross-device contrastive gives you *scale* (lots of varied negatives at multiple resolutions),  \n  - Hard mining gives you *precision* (focus on the toughest, most informative negatives).  \n\nBy combining them in a single training objective (and doing both losses in the multi–resolution embedding space), ScalingNote gets both broad generalization and fine‐grained discriminative power out of its huge LLM-based towers."
    },
    {
        "question": "What challenges motivate transferring knowledge from LLM-based Query Tower to Online Query Tower via Query-based Knowledge Distillation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the first stage, we simultaneously optimize the query tower and the document tower. However, the LLM-based query tower significantly impacts online query latency.",
            "Therefore, it is necessary to reduce the online inference time by minimizing the model size.",
            "Compared to documents, queries are shorter and contain less information. This makes knowledge transfer based on queries easier and more efficient."
        ],
        "final_answer": "Because the full LLM-based query tower is too large and slow for real-time use—leading to high online latency—and queries themselves are short (making them good candidates for lightweight distillation), the authors transfer knowledge via Query-based Knowledge Distillation to produce a much smaller, faster online query encoder.",
        "relevant_elements": [
            "LLM-based Query Tower",
            "Online Query Tower",
            "Query-based Knowledge Distillation"
        ],
        "id": 698,
        "masked_question": "What challenges motivate transferring knowledge from [mask1] to [mask2] via Query-based Knowledge Distillation?",
        "masked_number": 2,
        "masked_elements": [
            "LLM-based Query Tower",
            "Online Query Tower"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15766v1_figure_2.png",
        "paperid": "2411.15766v1",
        "paper_path": "./papers/2411.15766v1.json",
        "figure_id": "2411.15766v1_figure_2.png",
        "caption": "Figure 2. The framework of ScalingNote. The first stage is fully scaling the dual-tower using scaled training data, which learns through cross-device contrastive learning and hard negative mining.\nThe second stage is query-based knowledge distillation (QKD), which transfers the scaled query knowledge from the LLM-based query tower to the faster online query tower.",
        "qtype": "Design_Rationale",
        "response": "Mask1 denotes the large LLM-based query tower (the red‐boxed “teacher”), and Mask2 denotes the lightweight, BERT-style online query tower (the blue‐boxed “student”).  The key challenges driving the knowledge transfer are:\n\n1.  Online latency: the full LLM query tower is far too slow for real‐time serving.  \n2.  Model size and cost: the heavyweight LLM incurs excessive compute and memory overhead at query time.  \n3.  Maintaining high precision: we want to preserve the rich representations learned by the large model without paying its inference cost.  \n\nBy distilling query embeddings from the LLM into a much smaller, faster student model, we achieve near–teacher accuracy at a fraction of the latency and resource usage."
    },
    {
        "question": "What rationale supports splitting long-horizon tasks into subgoal segments for stable dynamical policy learning?",
        "relevant_section_ids": [
            "1",
            "2.4",
            "3.1"
        ],
        "relevant_context": [
            "However, these methods are designed to learn from a single primitive. Even the most expressive dynamical policies struggle to learn long-horizon tasks [15, 17], as ensuring global stability becomes increasingly difficult over extended time horizons.",
            "The prior work on data-driven methods for learning stable policies (such as the one in Sec. II-B) was designed for solving a single task. In our work, we will adapt the same network architecture for each subtask of a long-horizon problem.",
            "Our first step is to identify key states in the trajectory where major stages of the overall task take place, thereby breaking down complex trajectories into more manageable segments for learning.",
            "By defining these actions as subgoals, we can divide the demonstrations into sub-demos, where each segment can be easily described by a single dynamical policy.",
            "Our insight is that the most important requirement of manipulation tasks lies in achieving the subgoal, while precise imitation may not be essential."
        ],
        "final_answer": "Long-horizon tasks are difficult to learn as a single globally stable dynamical policy, since ensuring stability over long horizons leads to compounding errors and theoretical challenges. By splitting demonstrations into segments at meaningful subgoals (e.g., gripper open/close), each segment becomes a simpler motion that a single stable dynamical system can learn with rigorous stability guarantees. This segmentation reduces uncertainty, limits compounding errors, and focuses each policy on reaching just one subgoal, making reliable one-shot learning feasible.",
        "relevant_elements": [
            "Subgoal Identification",
            "Learning Dynamical Systems"
        ],
        "id": 699,
        "masked_question": "What rationale supports splitting long-horizon tasks into [mask1] segments for stable [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Subgoal Identification",
            "Learning Dynamical Systems"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.01033v2_figure_1.png",
        "paperid": "2410.01033v2",
        "paper_path": "./papers/2410.01033v2.json",
        "figure_id": "2410.01033v2_figure_1.png",
        "caption": "Figure 1: Overview of our approach: Long-horizon demonstrations (1) are first segmented into subgoals (2). Low-level stable dynamical policies are then learned to robustly reach each subgoal, even in the presence of perturbations (3). Finally, a high-level policy orchestrates a cascade of these stable policies for each segment, replicating the long-horizon expert demonstrations (4).",
        "qtype": "Design_Rationale",
        "response": "Here is the chain of reasoning:\n\n1.  Why not learn one giant policy over the entire long‐horizon demonstration?  \n    – As the horizon grows, compounding errors and uncertainty make it almost impossible to guarantee global stability (i.e. that every perturbed trajectory will still converge to the final goal).  \n\n2.  What do we do instead?  \n    – We first break the demonstration into smaller pieces—namely, “subgoal” segments (the red‐boxed step in Fig. 1).  \n    – Then for each little segment we train its own Stable Neural Dynamical System (the blue‐boxed step) so that it’s guaranteed (via a Lyapunov certificate) to drive the state to that segment’s subgoal.  \n\n3.  What does this achieve?  \n    – Each segment’s controller is simple enough that we can prove it is globally stable.  \n    – Cascading these tiny, provably‐stable controllers yields an overall long‐horizon policy that remains robust to disturbances and avoids the blow-up of error.  \n\nAnswer to the masked question:\n\nBy chopping a very long demonstration into subgoal segments, we can train a separate, globally‐stable dynamical system on each piece—something that would be intractable if we tried to learn one monolithic dynamical policy over the full horizon."
    },
    {
        "question": "What benefits emerge from cascading stable dynamical systems under a high-level policy for task reproduction?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The high-level policy outlined in Eq. 6 is globally stable at the last subgoal.",
            "This architecture ensures resilience against noise and external perturbations.",
            "This allows the system to quickly return to the original path and avoid collisions in cluttered environments."
        ],
        "final_answer": "By cascading stable dynamical systems under a high-level policy, the overall controller becomes globally stable at the final subgoal, is robust to noise and external disturbances, can rapidly recover the intended trajectory, and safely avoids collisions in cluttered settings.",
        "relevant_elements": [
            "Learning Dynamical Systems",
            "Stable Task Reproduction"
        ],
        "id": 700,
        "masked_question": "What benefits emerge from cascading [mask1] under a high-level policy for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Learning Dynamical Systems",
            "Stable Task Reproduction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.01033v2_figure_1.png",
        "paperid": "2410.01033v2",
        "paper_path": "./papers/2410.01033v2.json",
        "figure_id": "2410.01033v2_figure_1.png",
        "caption": "Figure 1: Overview of our approach: Long-horizon demonstrations (1) are first segmented into subgoals (2). Low-level stable dynamical policies are then learned to robustly reach each subgoal, even in the presence of perturbations (3). Finally, a high-level policy orchestrates a cascade of these stable policies for each segment, replicating the long-horizon expert demonstrations (4).",
        "qtype": "Design_Rationale",
        "response": "First, let’s pin down what the two masked phrases refer to:\n\n  • Mask₁ (red box) is the bundle of low-level, stable dynamical policies (our SNDS modules), each of which is trained to drive the robot to a single subgoal.  \n  • Mask₂ (blue box) is the overall “Stable Task Reproduction” stage, where a high-level controller sequences those SNDS modules to replay a long-horizon demonstration.\n\nWhat happens when we cascade those stable dynamical systems under a simple high-level supervisor?  \n\n  1.  **Global stability**  \n     – Each SNDS policy is by construction a Lyapunov-stable dynamical system that converges to its own subgoal.  \n     – When you switch or cascade them in series, you inherit a global stability guarantee: the whole chain of sub-policies will still drive the robot to the final goal.\n\n  2.  **Robustness to disturbances and noise**  \n     – If any segment is perturbed (sensor noise, external pushes, model error), its local SNDS will “pull” the state back toward the current subgoal.  \n     – The high-level switch simply hands off to the next SNDS once the subgoal is reached, so the system as a whole is remarkably resilient.\n\n  3.  **Quick recovery and collision safety**  \n     – Because each segment is stable, the robot never “wanders off” permanently; it always returns to the intended trajectory.  \n     – In cluttered or dynamic environments this rapid self‐correction reduces collision risk.\n\n  4.  **Modularity and one-shot generalization**  \n     – You can learn each SNDS from a single segment of demonstration and then reuse or reorder them under the high-level policy for new but similar tasks.  \n\nIn short, by cascading low‐level stable dynamical systems under a high‐level policy, you get an end‐to‐end controller that is provably stable, disturbance-rejective, quick to recover from mistakes, and able to faithfully reproduce long-horizon demonstrations in one shot."
    },
    {
        "question": "How does Stable Task Reproduction orchestrate multiple dynamical policies during segment transitions?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Having learned a unique dynamical system for each segment, what remains is to define a high-level controller C to imitate the task by returning desired velocities at each state during execution.",
            "The high-level controller C takes as input the set of subgoals (from Sec. III-A) and learned dynamical systems (from Sec. III-B).",
            "At each time step, the high-level controller evaluates the current state x_t and determines which subgoal should be the target and whether the current subgoal was achieved, based on a distance threshold ε. Then, the high-level controller applies policy π_i and executes the predicted velocity v_t during the execution of segment i of the trajectory."
        ],
        "final_answer": "Stable Task Reproduction uses a high-level controller that, at each time step, checks the robot’s current state against segment-specific subgoals with a distance threshold ε. When the controller judges that the current subgoal is reached, it switches to the next segment’s learned dynamical policy π_i and uses that policy to generate the desired velocity for the new segment.",
        "relevant_elements": [
            "Learning Dynamical Systems",
            "Stable Task Reproduction"
        ],
        "id": 701,
        "masked_question": "How does [mask1] orchestrate multiple dynamical policies during segment transitions?",
        "masked_number": 1,
        "masked_elements": [
            "Stable Task Reproduction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.01033v2_figure_1.png",
        "paperid": "2410.01033v2",
        "paper_path": "./papers/2410.01033v2.json",
        "figure_id": "2410.01033v2_figure_1.png",
        "caption": "Figure 1: Overview of our approach: Long-horizon demonstrations (1) are first segmented into subgoals (2). Low-level stable dynamical policies are then learned to robustly reach each subgoal, even in the presence of perturbations (3). Finally, a high-level policy orchestrates a cascade of these stable policies for each segment, replicating the long-horizon expert demonstrations (4).",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed module—the high-level controller in Step 4—manages and stitches together all of the segment-wise dynamical systems on-the-fly:\n\n1. At each control step it measures the robot’s current state x.  \n2. It keeps track of the ordered list of subgoals {g₁, g₂, …, g_N} and their corresponding learned SNDS policies {f₁, f₂, …, f_N}.  \n3. It computes the distance ‖x–g_k‖ to the active subgoal g_k.  \n4. As long as ‖x–g_k‖ remains above a small threshold ε, it routes the velocity command to the current policy f_k(x).  \n5. The moment ‖x–g_k‖ ≤ ε, it flags “subgoal achieved” and increments k → k+1, switching over to the next dynamical policy f_{k+1}.  \n6. Because each f_k is itself globally stable toward its g_k, the overall cascade remains stable all the way through to the final goal.\n\nIn this way the high-level controller “orchestrates” a smooth hand-off between segment policies by simply monitoring proximity to each subgoal and toggling which SNDS is driving the robot."
    },
    {
        "question": "How does Subgoal Identification define termination criteria for each segment before Learning Dynamical Systems training?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We opt for a straightforward method, defining a subgoal as the activation of the gripper.",
            "We perform a forward pass in the trajectory to find indices {i_k} such that i_k denotes the K occasions where the gripper opens or closes.",
            "Based on the selected indices, we divided the trajectory X into K segments, and define the subgoal for each segment as g_k."
        ],
        "final_answer": "Subgoal Identification segments the demonstration by detecting gripper activations (open or close). Each segment terminates at the frame where the gripper changes state, and that frame’s robot state is taken as the subgoal for the segment.",
        "relevant_elements": [
            "Subgoal Identification",
            "Learning Dynamical Systems"
        ],
        "id": 702,
        "masked_question": "How does [mask1] define termination criteria for each segment before [mask2] training?",
        "masked_number": 2,
        "masked_elements": [
            "Subgoal Identification",
            "Learning Dynamical Systems"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.01033v2_figure_1.png",
        "paperid": "2410.01033v2",
        "paper_path": "./papers/2410.01033v2.json",
        "figure_id": "2410.01033v2_figure_1.png",
        "caption": "Figure 1: Overview of our approach: Long-horizon demonstrations (1) are first segmented into subgoals (2). Low-level stable dynamical policies are then learned to robustly reach each subgoal, even in the presence of perturbations (3). Finally, a high-level policy orchestrates a cascade of these stable policies for each segment, replicating the long-horizon expert demonstrations (4).",
        "qtype": "Implementation_Details",
        "response": "Let’s first decode what the two masks stand for:\n\n 1. The red‐boxed panel is “Subgoal Identification” – the procedure that chops the raw demonstration into shorter segments by looking for gripper open/close events.  \n 2. The blue‐boxed panel is the “Learning Dynamical Systems” stage – where we take each of those sub‐demonstrations and fit a stable dynamical policy (SNDS) to it.  \n\n  \nQuestion restated:  \n“How does Subgoal Identification define termination criteria for each segment before Learning Dynamical Systems training?”  \n\nChain‐of‐thought:  \n1. In Sec. III-A (“Subgoal Identification”) we process the one long, un­trimmed demo and seek the points at which the gripper changes state (either goes from open→closed or closed→open).  \n2. We do this by making a simple forward pass through the trajectory and recording all time‐indices t₁,…,t_K where the gripper activates or releases.  \n3. Those indices {t_k} become our cut‐points: each segment runs from t_{k-1} to t_k, and we stop (terminate) one segment exactly when the gripper toggles.  \n4. Only after having those segments do we move on to stage “Learning Dynamical Systems” (the blue box) and train a separate SNDS policy on the data in each segment, using the final gripper‐toggle frame as that segment’s subgoal.  \n\nAnswer:  \nSubgoal Identification simply treats every gripper open/close event as the end of one segment (and the start of the next).  Those toggle‐indices are the termination criteria for each sub‐demo, and only then do we pass each segment off to the Learning Dynamical Systems stage for policy fitting."
    },
    {
        "question": "How does the exit criterion monitor action prediction consistency across sequential MLLM exits?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In our case, where the goal is action prediction and SoftMax output is not readily available, we adopt a different approach by leveraging the consistency of action predictions from adjacent intermediate features as our criterion.",
            "The underlying intuition is that if the action predictions from two differently sized MLLMs remain consistent, it suggests that the computational model may have reached saturation, and further processing is unlikely to yield any further improvements.",
            "For a given timestep t, we identify the smallest l within the range 1 to L that satisfies the following action consistency condition as termination exit: ||a_t^{l} – a_t^{l–1}||₂ ≤ ε, where we disregard the hidden state outputs of the LSTM and focus solely on comparing the L₂ norm of the difference in predicted actions against a predefined threshold ε."
        ],
        "final_answer": "The exit criterion computes the L₂ norm of the difference between the actions predicted at two consecutive exits. As soon as this norm falls below a preset threshold ε—indicating that the predictions have become sufficiently consistent—it triggers an early exit and stops further MLLM layers.",
        "relevant_elements": [
            "Exit Criterion",
            "MLLM"
        ],
        "id": 703,
        "masked_question": "How does the [mask1] monitor action prediction consistency across sequential [mask2] exits?",
        "masked_number": 2,
        "masked_elements": [
            "Exit Criterion",
            "MLLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02359v1_figure_1.png",
        "paperid": "2411.02359v1",
        "paper_path": "./papers/2411.02359v1.json",
        "figure_id": "2411.02359v1_figure_1.png",
        "caption": "Figure 1: \nLeft: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLM based on an exit criterion c𝑐citalic_c, which accounts for the current situation (including task instruction l𝑙litalic_l and observation otsubscript𝑜𝑡o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) and predefined computational and GPU memory budgets.\nThe language instruction and gripper camera image, not shown in this figure, are also inputs to the MLLM.\nAn action is then obtained using the intermediate feature x~tc⁢(t)subscriptsuperscript~𝑥𝑐𝑡𝑡\\tilde{x}^{c(t)}_{t}over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_c ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and historical information.\nRight: Training of DeeR.We randomly sample features from all exits during training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover, we employ several auxiliary action heads (AuxH) to better optimize the MLLM.",
        "qtype": "Implementation_Details",
        "response": "The thing in the red box is the “Exit Criterion,” and the thing in the blue box is the MLLM (i.e. the network with multiple exits).  At inference time DeeR runs the input through increasing numbers of MLLM layers, producing an action prediction at each exit.  The Exit Criterion then looks at two consecutive action outputs (say from exit k–1 and exit k) and computes the L₂‐norm of their difference.  As soon as that difference falls below a preset threshold ε, the criterion deems the two predictions “consistent” and stops all further MLLM processing, returning the action from the later exit."
    },
    {
        "question": "How do sampling strategies determine AuxH inputs across multiple exit features during training?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "As depicted by the “winding” curves on the right side of Figure 1, our approach involves sampling an exit index from 1 to N at each timestep. We implement two types of sampling strategies.",
            "The first strategy, denoted as s^u, is to uniformly sample an exit index from 1 to N at each step. This ensures that features from all possible exits are effectively captured in the action head during training. It simulates scenarios where the action head might encounter features from all exits within a given time window, thus accommodating an arbitrary inference pattern and reducing the training–inference discrepancy.",
            "Moreover, we observe that in practice, the dynamic model often terminates at the same exit for multiple consecutive timesteps, as the neighboring observations tend to be quite similar. The model then switches to another exit for a sequence of subsequent timesteps. To better emulate this pattern during training, we adopt a second sampling strategy denoted as s^c. Specifically, we split the time window H into two consecutive segments H1 and H2, with H1 chosen randomly. In each segment, a single uniformly sampled index is assigned and shared across all timesteps."
        ],
        "final_answer": "During training, the model uses two sampling strategies to decide which intermediate-exit features are fed into each auxiliary head (AuxH). Under the uniform strategy (s^u), at every timestep an exit index is drawn uniformly from 1 to N, so AuxH sees features from all exits over time. Under the chunked strategy (s^c), the history window is split into two random segments and a single exit index is sampled per segment and reused for all timesteps in that segment, mirroring the temporal clustering of exit decisions seen at inference.",
        "relevant_elements": [
            "Sampling strategy",
            "AuxH"
        ],
        "id": 704,
        "masked_question": "How do sampling strategies determine [mask1] inputs across multiple exit features during training?",
        "masked_number": 1,
        "masked_elements": [
            "AuxH"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02359v1_figure_1.png",
        "paperid": "2411.02359v1",
        "paper_path": "./papers/2411.02359v1.json",
        "figure_id": "2411.02359v1_figure_1.png",
        "caption": "Figure 1: \nLeft: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLM based on an exit criterion c𝑐citalic_c, which accounts for the current situation (including task instruction l𝑙litalic_l and observation otsubscript𝑜𝑡o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) and predefined computational and GPU memory budgets.\nThe language instruction and gripper camera image, not shown in this figure, are also inputs to the MLLM.\nAn action is then obtained using the intermediate feature x~tc⁢(t)subscriptsuperscript~𝑥𝑐𝑡𝑡\\tilde{x}^{c(t)}_{t}over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_c ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and historical information.\nRight: Training of DeeR.We randomly sample features from all exits during training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover, we employ several auxiliary action heads (AuxH) to better optimize the MLLM.",
        "qtype": "Implementation_Details",
        "response": "During training we need to expose the Aux-heads to features coming from all of the different “exit” depths, rather than always feeding them the final layer.  We do that by randomly choosing which of the N available intermediate features to hand to the AuxH at each time step (or block of time steps).  Concretely we have two sampling schemes:\n\n  1. Uniform sampling (Sᵤ): at every time step t we draw k∼Uniform({1,…,N}) and feed x̃ₜᵏ into its corresponding auxiliary head.  \n  2. Segmented sampling (Sₛ): we split the t–window into two random segments, pick one exit index for each segment, and feed that same exit’s feature into the AuxH for all t in the segment.\n\nIn this way the “AuxH input” is chosen by one of these sampling strategies, forcing the model during training to learn from features at every possible early‐exit depth."
    },
    {
        "question": "How does the exit criterion leverage action head outputs to decide early-termination?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In our case, where the goal is action prediction and SoftMax output is not readily available, we adopt a different approach by leveraging the consistency of action predictions from adjacent intermediate features as our criterion.",
            "The underlying intuition is that if the action predictions from two differently sized MLLMs remain consistent, it suggests that the computational model may have reached saturation, and further processing is unlikely to yield any further improvements.",
            "For a given timestep t, we identify the smallest l within the range [1, N] that satisfies the following action consistency condition as termination exit: ‖a_t^l − a_t^{l−1}‖₂ ≤ τ.",
            "We disregard the hidden state outputs of the LSTM and focus solely on comparing the L2 norm of the difference in predicted actions against a predefined threshold τ."
        ],
        "final_answer": "The exit criterion computes the actions predicted by the action head at each intermediate exit and measures the L₂ distance between consecutive predictions. As soon as the difference between the predictions from two successive exits falls below a predefined threshold τ, the model stops processing further layers and exits early.",
        "relevant_elements": [
            "exit criterion",
            "action head"
        ],
        "id": 705,
        "masked_question": "How does the [mask1] leverage [mask2] outputs to decide early-termination?",
        "masked_number": 2,
        "masked_elements": [
            "exit criterion",
            "action head"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02359v1_figure_1.png",
        "paperid": "2411.02359v1",
        "paper_path": "./papers/2411.02359v1.json",
        "figure_id": "2411.02359v1_figure_1.png",
        "caption": "Figure 1: \nLeft: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLM based on an exit criterion c𝑐citalic_c, which accounts for the current situation (including task instruction l𝑙litalic_l and observation otsubscript𝑜𝑡o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) and predefined computational and GPU memory budgets.\nThe language instruction and gripper camera image, not shown in this figure, are also inputs to the MLLM.\nAn action is then obtained using the intermediate feature x~tc⁢(t)subscriptsuperscript~𝑥𝑐𝑡𝑡\\tilde{x}^{c(t)}_{t}over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_c ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and historical information.\nRight: Training of DeeR.We randomly sample features from all exits during training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover, we employ several auxiliary action heads (AuxH) to better optimize the MLLM.",
        "qtype": "Literature_Background",
        "response": "The little “Exit Criterion” in the red box simply sits on top of the blue‐boxed action head and says:\n\n  1.  At each early‐exit point  you pull off the current hidden feature  and run it through the action head to get a tentative action  (just as you would at the very end).  \n  2.  You compare that  to the last action you predicted at the previous exit.  \n  3.  If the ℓ₂ distance between the two action‐head outputs is below a fixed threshold ε, you declare “converged,” fire off , and stop.  \n\nIn other words, the exit criterion “listens” to the successive action‐head outputs and as soon as they stop changing (to within ε) it terminates early."
    },
    {
        "question": "How does the sampling strategy complement auxiliary action heads to emulate inference dynamics?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To reduce the aforementioned discrepancy, we propose a simple yet effective random sampling strategy during training. As depicted by the “winding” curves on the right side of Figure 1, our approach involves sampling an exit index from 1 to N at each timestep. We implement two types of sampling strategies. The first strategy, denoted as T, is to uniformly sample an exit index from 1 to N at each step. This ensures that features from all possible exits are effectively captured in the action head during training. It simulates scenarios where the action head might encounter features from all exits within a given time window, thus accommodating an arbitrary inference pattern and reducing the training–inference discrepancy.",
            "To ensure that each activated size of the MLLM in our framework produces features suitable for predicting actions, we introduce auxiliary losses. Specifically, we attach N auxiliary action heads (denoted as UAH in Figure 1) at the exits. The i-th auxiliary head processes temporal features from the i-th exit and predicts the action a_i^t. We jointly train the auxiliary heads and the MLLM using the loss function: ... These auxiliary heads are employed only during training and are not used for inference."
        ],
        "final_answer": "During training, a random sampling strategy picks exit depths (either independently at each timestep or in consecutive blocks) to mirror the variable early-exit behavior at inference time. Auxiliary action heads are attached at each of these exits and trained to predict actions from their respective intermediate features. Together, the sampling strategy exposes the action heads to features from all possible exits in realistic temporal patterns, while the auxiliary heads supply exit-specific supervision, thereby emulating the inference dynamics of the early-exit MLLM.",
        "relevant_elements": [
            "sampling strategy",
            "auxiliary action heads"
        ],
        "id": 706,
        "masked_question": "How does the [mask1] complement auxiliary action heads to emulate inference dynamics?",
        "masked_number": 1,
        "masked_elements": [
            "sampling strategy"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.02359v1_figure_1.png",
        "paperid": "2411.02359v1",
        "paper_path": "./papers/2411.02359v1.json",
        "figure_id": "2411.02359v1_figure_1.png",
        "caption": "Figure 1: \nLeft: Dynamic inference of DeeR. For inference, we adaptively activate an appropriate size of MLLM based on an exit criterion c𝑐citalic_c, which accounts for the current situation (including task instruction l𝑙litalic_l and observation otsubscript𝑜𝑡o_{t}italic_o start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT) and predefined computational and GPU memory budgets.\nThe language instruction and gripper camera image, not shown in this figure, are also inputs to the MLLM.\nAn action is then obtained using the intermediate feature x~tc⁢(t)subscriptsuperscript~𝑥𝑐𝑡𝑡\\tilde{x}^{c(t)}_{t}over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_c ( italic_t ) end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT and historical information.\nRight: Training of DeeR.We randomly sample features from all exits during training. This strategy helps minimize the discrepancy between training and dynamic inference. Moreover, we employ several auxiliary action heads (AuxH) to better optimize the MLLM.",
        "qtype": "Literature_Background",
        "response": "Here’s the key idea in plain English:\n\n1.  What the red‐boxed block is  \n    In the right half of Figure 1, the red box simply labels the “sampling strategy” used during training.  \n\n2.  Why we need it  \n    At inference time DeeR will *dynamically* choose one of the intermediate exits (i.e. one of the k different feature depths) on the fly, depending on how hard the current frame looks.  During training, however, you do *not* yet have a criterion to decide which exit you would have used.  If you trained only on the final exit, or only on a fixed exit, your model would never learn to make good predictions from the earlier exits.  \n\n3.  How it works together with the auxiliary heads  \n    •  DeeR attaches one small auxiliary action head to *each* possible exit.  Each of those “AuxH” heads tries to predict the robot’s action from its own exit’s features.  That gives us k extra losses, one per exit.  \n    •  At every training step, we run the full network up through *all* k exits, collect all k hidden‐state sequences, and then *randomly sample* an exit index c(t) according to one of two simple schemes:  \n       – uniform sampling over {1,…,k} at each timestep, or  \n       – a two-segment scheme that picks one exit for the first half of the window and possibly a different one for the second half.  \n    •  We feed the sampled‐exit features into the main (LSTM‐based) action head to make a “real” action prediction for that step, and we simultaneously feed *every* exit’s features into its corresponding auxiliary head.  \n\n4.  Why this emulates inference  \n    By randomly choosing exits during training and forcing *every* auxiliary head to solve the same prediction problem, the network sees exactly the kind of pattern it will encounter at test time (sometimes shallow exits, sometimes deeper ones, sometimes staying at the same exit for several frames).  This “sampling + auxiliary heads” combo closes the gap between:\n     - the *fixed* computation graph you use in training, and  \n     - the *adaptive*, early-exiting graph you actually run at inference.\n\nIn short, the red-boxed “sampling strategy” ensures that during training you learn to predict well from *all* exit depths in the *same stochastic pattern* that the real early-exit controller will produce, and the auxiliary heads turn each of those depths into a concrete loss signal."
    },
    {
        "question": "How does three-stage training integrate realistic dataset and synthetic dataset to tune spatial and temporal layers?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Considering both the two-style paired datasets and the long temporal context requirement, we design a three-stage training strategy to harvest the variety of video content, the precise depth details, as well as the support for long and variable sequences.",
            "We first train it on our large realistic dataset to adapt the model to the video-to-depth generation task. The sequence length in this stage is randomly sampled from  frames, such that the model can learn to generate depth sequences with variable lengths.",
            "In the second stage, we only fine-tune the temporal layers of the model still on our large realistic dataset, with the sequence length randomly sampled from  frames. The reason why we only fine-tune the temporal layers is that the temporal layers are more sensitive to the sequence length while the spatial layers are already adapted to the video-to-depth generation task in the first stage, and doing so significantly reduces memory consumption compared to fine-tuning the full model. The long temporal context in this stage enables the model to precisely arrange the entire depth distributions for long and variable sequences.",
            "In the third stage, we fine-tune the spatial layers of the model on our small synthetic dataset, with a fixed sequence length of 45 frames since the model has already learned to generate depth sequences with variable lengths in the first two stages and tuning the spatial layers would not affect the temporal context. As the depth annotations in the synthetic dataset are more accurate and fine-grained, the model can learn more precise depth details in this stage."
        ],
        "final_answer": "The three-stage training first uses the large realistic dataset to adapt all layers of the pre‐trained video diffusion model to the depth‐generation task with variable short sequences. Then, still on the realistic data but with longer sequences, only the temporal U‐Net layers are fine‐tuned to learn long‐range consistency. Finally, using the small but highly accurate synthetic dataset, only the spatial layers of the U‐Net are fine‐tuned (with a fixed length) to inject fine‐grained depth detail without disturbing the learned temporal context.",
        "relevant_elements": [
            "three-stage training",
            "realistic dataset",
            "synthetic dataset"
        ],
        "id": 707,
        "masked_question": "How does [mask1] integrate realistic dataset and synthetic dataset to tune spatial and temporal layers?",
        "masked_number": 1,
        "masked_elements": [
            "three-stage training"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.02095v1_figure_2.png",
        "paperid": "2409.02095v1",
        "paper_path": "./papers/2409.02095v1.json",
        "figure_id": "2409.02095v1_figure_2.png",
        "caption": "Figure 2: \nOverview of our DepthCrafter. It is a conditional diffusion model that models the distribution p (𝐝|𝐯)𝑝conditional𝐝𝐯p(\\mathbf{d}\\,|\\,\\mathbf{v})italic_p ( bold_d | bold_v ) over the depth sequence 𝐝𝐝\\mathbf{d}bold_d conditioned on the input video 𝐯𝐯\\mathbf{v}bold_v.\nWe train the model in three stages, where the spatial or temporal layers of the diffusion model are progressively learned on our compiled realistic or synthetic datasets with variable lengths T𝑇Titalic_T.\nDuring inference, given an open-world video, it can generate temporally consistent long depth sequences with fine-grained details for the entire video from initialized Gaussian noise, without requiring any supplementary information, such as camera poses or optical flow.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed diagram is simply our three-stage curriculum for “where” and “when” to expose the model to the big, messy real data versus the small, pristine synthetic data, and it literally alternates which U-Net blocks are being trained.  Concretely:\n\n1. Stage 1 (Realistic; T∈[1,25])  \n   – Dataset: large-scale, diverse “realistic” video–depth pairs  \n   – Layers trainable: both spatial AND temporal U-Net blocks  \n   – Goal: warm up the entire network on real world content and learn the basic video-to-depth mapping  \n\n2. Stage 2 (Realistic; T∈[1,110])  \n   – Dataset: still the same realistic set  \n   – Layers trainable: ONLY the temporal blocks (spatial blocks frozen)  \n   – Goal: learn to arrange and maintain depth distributions over long, variable-length clips without blowing up memory  \n\n3. Stage 3 (Synthetic; T=45)  \n   – Dataset: small, high-fidelity synthetic video–depth pairs  \n   – Layers trainable: ONLY the spatial blocks (temporal blocks frozen)  \n   – Goal: “sharpen” the per‐frame spatial detail using perfectly accurate synthetic depth  \n\nBy freezing whichever part you’re not focusing on and switching from real→real→synthetic, you get both broad coverage of real-world scenes (and long-range temporal consistency) and the razor-sharp spatial precision that only a synthetic ground truth can provide."
    },
    {
        "question": "How does VAE enc./dec. latent transformation impact Diffusion U-Net denoising relative to standard latent diffusion methods?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To generate high-resolution depth sequences without sacrificing computational efficiency, we adopt the framework of Latent Diffusion Models (LDMs) that perform in a low-dimensional latent space, rather than the original data space.",
            "The transformation between the latent and data spaces is achieved by a Variational Autoencoder (VAE), which was originally designed for encoding and decoding video frames in SVD [3].",
            "Fortunately, we found it can be directly used for depth sequences with only a negligible reconstruction error, which is similar to the observation in Marigold [32] for image depth estimation.",
            "For the depth sequence, we replicate it three times to meet the 3-channel input format of the encoder in VAE and average the three channels of the decoder output to obtain the final latent of the depth sequence."
        ],
        "final_answer": "DepthCrafter follows the standard Latent Diffusion Model design by running all diffusion steps in a VAE’s low-dimensional latent space rather than in pixel space.  Crucially, the off-the-shelf SVD VAE used to compress RGB videos works out of the box on depth sequences with only negligible reconstruction error.  As a result, the Diffusion U-Net denoiser sees very similar latent statistics to a conventional RGB latent model and can be used without any architectural changes.  The only adaptation is to replicate single-channel depth maps into three channels before encoding (and then average the decoder’s three-channel output back into one).  This latent transform thus preserves high-resolution detail and lets the U-Net operate exactly as in a standard LDM.",
        "relevant_elements": [
            "VAE enc./dec.",
            "Diffusion U-Net"
        ],
        "id": 708,
        "masked_question": "How does [mask1] latent transformation impact [mask2] denoising relative to standard latent diffusion methods?",
        "masked_number": 2,
        "masked_elements": [
            "VAE enc./dec.",
            "Diffusion U-Net"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.02095v1_figure_2.png",
        "paperid": "2409.02095v1",
        "paper_path": "./papers/2409.02095v1.json",
        "figure_id": "2409.02095v1_figure_2.png",
        "caption": "Figure 2: \nOverview of our DepthCrafter. It is a conditional diffusion model that models the distribution p (𝐝|𝐯)𝑝conditional𝐝𝐯p(\\mathbf{d}\\,|\\,\\mathbf{v})italic_p ( bold_d | bold_v ) over the depth sequence 𝐝𝐝\\mathbf{d}bold_d conditioned on the input video 𝐯𝐯\\mathbf{v}bold_v.\nWe train the model in three stages, where the spatial or temporal layers of the diffusion model are progressively learned on our compiled realistic or synthetic datasets with variable lengths T𝑇Titalic_T.\nDuring inference, given an open-world video, it can generate temporally consistent long depth sequences with fine-grained details for the entire video from initialized Gaussian noise, without requiring any supplementary information, such as camera poses or optical flow.",
        "qtype": "Literature_Background",
        "response": "In our figure the little red‐box is simply the VAE encoder/decoder (mask₁) that takes a full-resolution depth map (replicated into three channels) and squashes it into a small latent 𝑧ᵈ.  The big blue box is the U-Net diffusion head (mask₂) that learns to denoise those latents over time.  In other words, instead of running the diffusion directly in pixel space we first “wrap’’ the depth sequence through a frozen (or lightly fine-tuned) video VAE.  That VAE does two things:\n\n 1. It collapses the H×W×1 (well, H×W×3 after replication) depth frames into a much smaller spatial latent (typically ¼ or 1/8 the resolution and far fewer channels), so the U-Net never has to touch huge H×W maps.  \n 2. Because we share a single encoder/decoder (and its learned scale/shift) across all frames, the latent distributions of consecutive frames stay on the same “affine‐invariant’’ footing, which directly helps the U-Net produce temporally consistent outputs.\n\nContrast that with a vanilla latent diffusion pipeline, where you might train a VAE from scratch on RGB images (or depth maps) and then diffuse in that latent space but with a per-image affine normalization—there the U-Net never sees a guaranteed consistent scale/shift across time, and it’s still operating on higher-dimensional latents.  \n\nBy reusing the pre-trained video VAE from SVD (mask₁) and only lightly adapting it to depth, we force all of the heavy spatial detail into the VAE’s fixed decoder.  The U-Net (mask₂) can then focus its capacity on learning the conditional noise schedule and temporal smoothing over a very compact, stable latent.  The end result is:\n\n • A ≈8× reduction in spatial resolution (and a 5×–10× smaller channel dimension) so each diffusion step is much cheaper.  \n • A shared affine normalization across frames, which gives the U-Net a stationary target distribution and makes it far easier to learn long-range temporal consistency.  \n • The ability to start from the SVD pre-trained weights almost “out of the box,” rather than re-training a depth-specific VAE + U-Net from scratch."
    },
    {
        "question": "How does frame-wise concatenation of video latents to the diffusion U-Net inputs affect temporal consistency?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As shown in Fig. 2, given the encoded latent of depth sequence z₀^(d) and video frames z^(v) from Eq. (4), we concatenate the video latent to the input noisy depth latent frame-wisely, rather than only the first frame, to condition the denoiser for generating the depth sequence.",
            "Compared to the original conditioning mechanism, our adapted conditioning provides more comprehensive information from the video frames to the denoiser, which significantly improves the alignment between the generated depth sequences and the video content, as well as the temporal consistency."
        ],
        "final_answer": "Frame-wise concatenation of video latents to the diffusion U-Net inputs provides more comprehensive conditioning information from each frame, which significantly improves the temporal consistency of the generated depth sequences.",
        "relevant_elements": [
            "frame-wise concatenation",
            "Diffusion U-Net"
        ],
        "id": 709,
        "masked_question": "How does frame-wise concatenation of video latents to the [mask1] inputs affect temporal consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Diffusion U-Net"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.02095v1_figure_2.png",
        "paperid": "2409.02095v1",
        "paper_path": "./papers/2409.02095v1.json",
        "figure_id": "2409.02095v1_figure_2.png",
        "caption": "Figure 2: \nOverview of our DepthCrafter. It is a conditional diffusion model that models the distribution p (𝐝|𝐯)𝑝conditional𝐝𝐯p(\\mathbf{d}\\,|\\,\\mathbf{v})italic_p ( bold_d | bold_v ) over the depth sequence 𝐝𝐝\\mathbf{d}bold_d conditioned on the input video 𝐯𝐯\\mathbf{v}bold_v.\nWe train the model in three stages, where the spatial or temporal layers of the diffusion model are progressively learned on our compiled realistic or synthetic datasets with variable lengths T𝑇Titalic_T.\nDuring inference, given an open-world video, it can generate temporally consistent long depth sequences with fine-grained details for the entire video from initialized Gaussian noise, without requiring any supplementary information, such as camera poses or optical flow.",
        "qtype": "Experimental_Results",
        "response": "Here “​[mask1]​” is the noisy depth‐latent input to the U-Net.  By concatenating each video frame’s latent side-by-side with its corresponding noisy depth latent – instead of only using the first frame’s latent – the denoiser sees the actual video content at every time step.  This per-frame conditioning ties the depth prediction back to the true video features continuously, which dramatically reduces frame-to-frame jitter and enforces a smooth, temporally consistent depth sequence."
    },
    {
        "question": "How does fine-tuning only temporal layers in stage two facilitate variable-length sequence learning?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the second stage, we only fine-tune the temporal layers of the model still on our large realistic dataset, with the sequence length randomly sampled from frames.",
            "The reason why we only fine-tune the temporal layers is that the temporal layers are more sensitive to the sequence length while the spatial layers are already adapted to the video-to-depth generation task in the first stage, and doing so significantly reduces memory consumption compared to fine-tuning the full model.",
            "The long temporal context in this stage enables the model to precisely arrange the entire depth distributions for long and variable sequences."
        ],
        "final_answer": "By freezing the spatial layers and fine-tuning only the temporal layers on randomly sampled (and progressively longer) video clips, the model focuses its capacity on learning how to model inter-frame dependencies over varying lengths. Because the temporal layers are the parts of the network most sensitive to sequence length, this targeted fine-tuning both reduces memory consumption (compared to full fine-tuning) and lets the model adapt to long, variable-length sequences, ensuring that depth distributions are arranged consistently across any length of video.",
        "relevant_elements": [
            "three-stage training",
            "temporal layers"
        ],
        "id": 710,
        "masked_question": "How does fine-tuning only [mask1] in stage two facilitate variable-length sequence learning?",
        "masked_number": 1,
        "masked_elements": [
            "temporal layers"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.02095v1_figure_2.png",
        "paperid": "2409.02095v1",
        "paper_path": "./papers/2409.02095v1.json",
        "figure_id": "2409.02095v1_figure_2.png",
        "caption": "Figure 2: \nOverview of our DepthCrafter. It is a conditional diffusion model that models the distribution p (𝐝|𝐯)𝑝conditional𝐝𝐯p(\\mathbf{d}\\,|\\,\\mathbf{v})italic_p ( bold_d | bold_v ) over the depth sequence 𝐝𝐝\\mathbf{d}bold_d conditioned on the input video 𝐯𝐯\\mathbf{v}bold_v.\nWe train the model in three stages, where the spatial or temporal layers of the diffusion model are progressively learned on our compiled realistic or synthetic datasets with variable lengths T𝑇Titalic_T.\nDuring inference, given an open-world video, it can generate temporally consistent long depth sequences with fine-grained details for the entire video from initialized Gaussian noise, without requiring any supplementary information, such as camera poses or optical flow.",
        "qtype": "Experimental_Results",
        "response": "In Stage 2 we freeze everything except the U-Net’s temporal modules (the green blocks in the diagram).  Because those are the only layers being updated, you can afford to train on much longer and variable-length clips (T up to 110) without running out of GPU memory, and—all the while—the network’s entire capacity for modeling cross-frame (temporal) dependencies is focused on learning how to arrange depth over an arbitrary number of frames.  In short, by tuning only the temporal layers (and keeping the spatial ones fixed) you (a) drastically cut down on memory footprint and (b) force the model to specialize its temporal blocks to handle sequences of arbitrary length."
    },
    {
        "question": "How does the BERT-based data labeling module interact with GPT-4 filtering to refine the sample dataset for evaluation?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "We finally collected more than 6,000 entries, and we fine-tune a bert-base-uncased [21] as text classifier to label new data.",
            "Then we built a filter based on prompting gpt-4-turbo [5], to remove the harmless ones, resulting in 1,525 entries (See the Appendix 9.2 for the classifier and filter details), ensuring both the consistency and the relevance of the enhanced dataset for comprehensive evaluation."
        ],
        "final_answer": "The pipeline first fine-tunes a BERT-base-uncased classifier to automatically label the raw pool of over 6,000 collected queries. That labeled output is then passed through a GPT-4-turbo prompt-based filter which removes any queries judged to be harmless, producing a final curated set of 1,525 malicious queries for the evaluation.",
        "relevant_elements": [
            "Data Labeling",
            "Filtering"
        ],
        "id": 711,
        "masked_question": "How does the [mask1]-based data labeling module interact with GPT-4 [mask2] to refine the sample dataset for evaluation?",
        "masked_number": 2,
        "masked_elements": [
            "Data Labeling",
            "Filtering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.09326v1_figure_2.png",
        "paperid": "2408.09326v1",
        "paper_path": "./papers/2408.09326v1.json",
        "figure_id": "2408.09326v1_figure_2.png",
        "caption": "Figure 2: Workflow of the evaluation framework",
        "qtype": "Experimental_Results",
        "response": "The red-boxed module is nothing mystical – it’s simply a fine-tuned bert-base-uncased classifier (“BERT-based”) that we train on our curated jailbreak queries to automatically tag every new example with its harm category.  Once that BERT model has sifted through the roughly 6 000 candidates and labeled them, we hand its output to the blue-boxed GPT-4 “filter” step.  In practice this means we prompt GPT-4-Turbo on each BERT-labeled query and ask it to identify and drop anything that looks benign or irrelevant.  Any example that survives GPT-4’s sanity check becomes part of the final 1 525-entry evaluation set.  So, in short, BERT does the bulk labeling, GPT-4 does a second­stage filter, and together they whittle the pool down to a clean, tightly focused sample."
    },
    {
        "question": "How does the normalization procedure reconcile metrics like ASR and Token Length during the aggregation into a unified reliability score?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "For metrics that improve with minimization (e.g., ASR, Toxicity Score, Grammatical Errors, and Fluency), a higher value indicates decreased reliability. Conversely, for the metric that benefits from maximization (i.e., Token Length), a higher value implies enhanced reliability.",
            "We normalize each metric to a range between 0 and 1, whereby a higher value consistently denotes increased reliability.",
            "For metrics to be minimized (M−), such as ASR, Toxicity Score, Grammatical Errors, Fluency, we define a normalization function f′(x) = (max – x) / (max – min).",
            "For the metric to be maximized (M+), such as Token Length, we define a normalization function f+(x) = (x – min) / (max – min).",
            "To derive a reliability score for each model, we amalgamate all the normalized values ... R = ∑ (1/n) W_i f_i(M_i), where each f_i is the appropriate normalization function and W_i the user‐assigned weight."
        ],
        "final_answer": "The procedure first maps every metric onto a common [0,1] scale so that larger normalized values always imply higher model reliability. Metrics to be minimized (like ASR) use f′(x) = (max − x)/(max − min), while metrics to be maximized (like Token Length) use f+(x) = (x − min)/(max − min). These normalized scores are then combined (via a weighted average) into a single reliability score.",
        "relevant_elements": [
            "Normalization",
            "Aggregation"
        ],
        "id": 712,
        "masked_question": "How does the [mask1] procedure reconcile metrics like ASR and Token Length during the [mask2] into a unified reliability score?",
        "masked_number": 2,
        "masked_elements": [
            "Normalization",
            "Aggregation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.09326v1_figure_2.png",
        "paperid": "2408.09326v1",
        "paper_path": "./papers/2408.09326v1.json",
        "figure_id": "2408.09326v1_figure_2.png",
        "caption": "Figure 2: Workflow of the evaluation framework",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the “Normalization” step (the red-boxed curves), and Mask2 is the “Aggregation” step (the blue-boxed [0,1] bar).  Here is how they work together to turn ASR, Token Length, etc., into one reliability number:\n\nChain of thought  \n1. We have five heterogeneous metrics – e.g. Attack Success Rate (ASR), Toxicity, Fluency (perplexity), Grammatical Errors, and Token Length.  Some are “better when smaller” (ASR, Toxicity, Errors, Fluency), one is “better when larger” (Token Length).  \n2. In the red‐boxed Normalization stage, each raw metric uᵢ is linearly mapped into [0,1].  \n   • For metrics to be minimized (m ∈ {ASR, Toxicity, Errors, Fluency}):  \n     norm(u) = 1 – (u – uₘᵢₙ)/(uₘₐₓ – uₘᵢₙ)  \n   • For the metric to be maximized (Token Length):  \n     norm(u) = (u – uₘᵢₙ)/(uₘₐₓ – uₘᵢₙ)  \n   After this, higher = more reliable for every metric.  \n3. In the blue-boxed Aggregation stage, the user chooses a weight wᵢ for each normalized metric.  The final reliability score R is simply the weighted average:  \n   R = Σᵢ wᵢ · norm(uᵢ)  \n   (often Σᵢ wᵢ = 1).  \n\nThus Normalization reconciles all metrics onto the same 0–1 scale, and Aggregation combines them by user-defined weights into a single reliability score."
    },
    {
        "question": "What ethical risks emerge from automated filtering during dataset construction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Dataset Construction",
            "Filtering"
        ],
        "id": 713,
        "masked_question": "What ethical risks emerge from automated [mask1] during dataset construction?",
        "masked_number": 1,
        "masked_elements": [
            "Filtering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.09326v1_figure_2.png",
        "paperid": "2408.09326v1",
        "paper_path": "./papers/2408.09326v1.json",
        "figure_id": "2408.09326v1_figure_2.png",
        "caption": "Figure 2: Workflow of the evaluation framework",
        "qtype": "Others",
        "response": "First, let’s pin down what “[mask1]” refers to.  If you look at the left‐most panel of Fig. 2 (“Dataset Construction”), you’ll see three stages: (1) data collection (from GitHub, forums, the web, etc.), (2) data labeling (with a fine-tuned BERT), and then (3) a red–boxed “Filtering” step performed by GPT-4 (“Prompt Engineering → GPT-4 → Sample Dataset”).  In other words, “[mask1]” is the automated GPT-4-based filtering of the raw query pool.\n\nNow: what ethical risks do we introduce when we lean on an opaque, automated filter like GPT-4 to cull “harmless” versus “harmful” examples?\n\n1.  **Bias amplification**  \n    •  GPT-4 has its own training-data quirks and policy layers.  If it overflags certain dialects, minority viewpoints, or culturally specific phrasing as “harmful,” we systematically remove them.  \n    •  Conversely, it may under-flag other truly harmful content if it resembles benign examples it saw during pretraining.  \n\n2.  **Lack of transparency and auditability**  \n    •  We cannot easily inspect GPT-4’s internal decision boundary.  When it says “this looks harmless” or “this looks harmful,” there’s no clear, inspectable rule set.  \n    •  That makes it hard to contest or correct misclassifications—even if we spot large swaths of legitimate queries being thrown out.  \n\n3.  **Over- or under-censorship**  \n    •  Over-censorship: legitimate edge cases, satirical or educational examples, and even minority questions about sensitive topics get tossed.  That skews the dataset and silences important “hard” questions that our later jailbreak tests should cover.  \n    •  Under-censorship: the filter might miss certain cleverly phrased harmful queries, leaving dangerous examples in the so-called “safe” set.  \n\n4.  **Propagation of hidden value judgments**  \n    •  The GPT-4 filter reflects OpenAI’s policy decisions—what counts as “allowed” or “disallowed” content.  By outsourcing filtering to a proprietary model, we import those judgments wholesale, without giving dataset users much say.  \n\n5.  **Privacy and data-leakage concerns**  \n    •  Sending potentially private or sensitive queries to a third-party API (GPT-4) can violate data-privacy agreements or leak personally identifiable information.  \n\n6.  **Erosion of reproducibility**  \n    •  If tomorrow someone else reruns the GPT-4 filter, they may get a different “1,525 entry” dataset—GPT-4 itself changes over time.  That harms the scientific principle of replicability.  \n\nIn short, automated GPT-4 filtering can introduce unjustified bias, opaque judgments, censorship artifacts, and privacy risks into what is meant to be an impartial test‐bed of “harmful” vs. “harmless” prompts."
    },
    {
        "question": "What limitations arise from normalizing metrics during aggregation of model reliability?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Normalization",
            "Aggregation"
        ],
        "id": 714,
        "masked_question": "What limitations arise from [mask1] metrics during [mask2] of model reliability?",
        "masked_number": 2,
        "masked_elements": [
            "Normalization",
            "Aggregation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.09326v1_figure_2.png",
        "paperid": "2408.09326v1",
        "paper_path": "./papers/2408.09326v1.json",
        "figure_id": "2408.09326v1_figure_2.png",
        "caption": "Figure 2: Workflow of the evaluation framework",
        "qtype": "Others",
        "response": "First, let’s pin down what the two masked terms refer to in the figure:\n\n  – Mask 1 (red box) is the “Normalization” step, in which each raw metric (ASR, toxicity score, fluency/perplexity, token-length, grammar-error count) is rescaled.  \n  – Mask 2 (blue box) is the fact that after normalization every metric is forced into the [0, 1] interval.\n\nRewriting the question with those filled in:\n\n  “What limitations arise from the raw metrics during the normalization of model reliability?”  \n  or equivalently  \n  “What limitations arise from the normalization step when mapping metrics into [0, 1] for computing model reliability?”\n\nChain of thought:\n\n  1.  Our five “raw” metrics—attack success rate, toxicity probability, perplexity, token length, and number of grammar errors—each live on very different scales, have different units, even opposite “directions” (some should be minimized, some maximized).  \n  2.  To aggregate them into a single “reliability” score, the framework uses min–max normalization to push every metric into the [0, 1] range.  \n  3.  But min–max normalization (or any simple linear rescaling) brings several well-known pitfalls:  \n     •  **Outlier sensitivity.** A single extreme value (say one model with extremely high toxicity) will stretch the normalization so that all other scores get compressed into a tiny corner of [0, 1].  \n     •  **Bound selection.** You have to choose what you call “min” and “max” for each metric. If tomorrow you add a new model whose ASR is higher than anything before, you have to renormalize everything.  \n     •  **Loss of shape.** A highly skewed raw distribution (e.g. most models cluster at low perplexity but one is very bad) will be “flattened” by a linear transform—differences in the bulk of the distribution become invisible.  \n     •  **Imposes linear trade‐offs.** By squashing each metric into [0, 1], you assume that moving from 0.2→0.4 in one metric is “worth” exactly the same as 0.2→0.4 in another. Realistically, users may feel that a small gain in toxicity avoidance is far more important than a similar gain in grammar mistakes.  \n     •  **Mixing minimization and maximization.** Since some metrics get better when they get smaller (ASR, toxicity, errors, perplexity) and some when they get larger (token length), you must invert some of them before min–maxing.  Any slip in flipping the direction will corrupt the final score.  \n\nAll of these issues mean that the final “reliability” number is extremely sensitive to how you normalize—both the numeric bounds you pick and the assumption of linearity.  In practice it can make the composite score unstable (adding a new model changes everybody’s score), hard to interpret, and liable to mask important performance differences under jailbreak attack."
    },
    {
        "question": "What limitations could arise when relying solely on RPSN for chassis position sampling in novel environments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "RPSN",
            "chassis position sampling"
        ],
        "id": 715,
        "masked_question": "What limitations could arise when relying solely on [mask1] for [mask2] in novel environments?",
        "masked_number": 2,
        "masked_elements": [
            "RPSN",
            "chassis position sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.06590v1_figure_1.png",
        "paperid": "2407.06590v1",
        "paper_path": "./papers/2407.06590v1.json",
        "figure_id": "2407.06590v1_figure_1.png",
        "caption": "Figure 1: System architecture of our BEAM-1. According to different functions, it can be categorized into (a) body composition-Hardware level, (b) intuition-guided motion sampling algorithm-Motion level, (c) predicate and primitive-based high precision control-Execution level, and (d) LLM-heuristic tree-searching task planning-Task level.",
        "qtype": "Others",
        "response": "First, let’s identify the two masked items in the Motion‐level panel (Fig 1 (b)):\n\n1.  Mask₁ (red box) is the “Direct generation – no simulation required!” step in the RPSN pipeline.  \n2.  Mask₂ (blue text) is the overall module: the “Intuitive Position Speculation Network” (RPSN).\n\nNow: what could go wrong if you drop the simulation/verification and lean entirely on the RPSN’s direct‐generation output when you move into novel, unseen environments?\n\nChain of thought:\n\n1.  RPSN is trained to spit out chassis poses that satisfy the robot’s kinematic equations.  \n2.  By design it skips any downstream collision‐checking or environmental simulation.  \n3.  In practice real workspaces are full of:\n    - unexpected obstacles (e.g. cables, tools, debris),  \n    - uneven floors or sloped surfaces,  \n    - dynamic actors (people or moving trolleys),  \n    - sensor noise or drift that wasn’t in the training set.  \n4.  A purely feed‐forward network has no built-in mechanism to  \n    - detect collisions,  \n    - respect dynamic feasibility (braking distances, accelerations),  \n    - cope with environment changes on the fly.  \n5.  As a result, in novel settings the “direct generation” may\n    - propose chassis targets that look kinematically valid but actually intersect obstacles,  \n    - steer the robot into singular arm configurations or too-tight corridors,  \n    - violate safety or stability constraints that were not encoded.  \n6.  Without a simulation or verification step, there is no firewall to catch or correct these bad proposals before motion is executed.\n\nAnswer:\n\nIf you rely solely on the RPSN’s “direct‐generation, no simulation” output for the Intuitive Position Speculation Network in new environments, you lose all of the collision checks, dynamic feasibility tests, and environment‐specific constraint handling that a simulator or planner would enforce. In practice this can lead to:\n•  Unsafe chassis targets that collide with obstacles or walls.  \n•  Requests for arm configurations that are at or beyond joint limits or singularities.  \n•  Poor generalization to new terrain, debris, or layout variations not seen during training.  \n•  A lack of real‐time feedback to catch drift or sensor errors, resulting in failed or dangerous maneuvers."
    },
    {
        "question": "What is the rationale behind integrating neural predicates with action primitives for high-precision control?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Most current robotic systems heavily rely on high-precision sensors to perceive their environment. These systems execute predefined programs to perform corresponding robot operations under specific conditions, such as distance parameters. However, this approach fails to address the uncertainty during the battery disassembly process in highly dynamic environments. Different batteries, various bolts, and diverse disassembly scenarios cannot be universally dismantled using a standardized predefined method.",
            "To achieve a more intelligent system, we introduce neural predicates to help BEAM-1 for environment state recognition based on the NeuralSymbolic AI. Each neural predicate can be regarded as a neural network, which maps the multi-sensor perception information of the environment to the quasi-symbolic space to complete the characterization of the state.",
            "Having accomplished the precise perception of the environment state, we realize high-precision control based on action primitives at the execution level. We subdivided the disassembly process and defined 12 action primitives such as Approach, Mate, Push, Insert, and so on. Each primitive is defined by PDDL with execution pre-requirements and execution target effects in symbol space, which will be used for searching during task planning.",
            "The definition of primitives ensures that BEAM-1 can autonomously plan appropriate action sequences in dynamic and complex environments to cope with various environmental states and accomplish various tasks.",
            "The accuracy of current popular control methods failed to meet the millimeter-level requirements in the disassembly environment [15, 28], and this study uses manually implemented primitives to achieve high-precision accurate control while adding a layer of detection and verification at the primitive level."
        ],
        "final_answer": "Because battery disassembly involves unpredictable and unstructured scenarios—different battery types, bolt shapes, corrosion states, and environmental conditions—a purely predefined, sensor-threshold based approach cannot reliably locate and manipulate each bolt to millimeter accuracy. By first using neural predicates (neural networks mapping multi-sensor data into symbolic state representations), BEAM-1 gains a robust understanding of the current environment. It then executes tightly defined action primitives (each with symbolic preconditions, precise motion logic, and verification checks) to achieve the required high-precision control. The integration ensures that perception and execution remain tightly coupled and calibrated, enabling both flexible decision-making in novel situations and millimeter-level accuracy in the disassembly process.",
        "relevant_elements": [
            "neural predicates",
            "action primitives"
        ],
        "id": 717,
        "masked_question": "What is the rationale behind integrating [mask1] with action primitives for high-precision control?",
        "masked_number": 1,
        "masked_elements": [
            "neural predicates"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.06590v1_figure_1.png",
        "paperid": "2407.06590v1",
        "paper_path": "./papers/2407.06590v1.json",
        "figure_id": "2407.06590v1_figure_1.png",
        "caption": "Figure 1: System architecture of our BEAM-1. According to different functions, it can be categorized into (a) body composition-Hardware level, (b) intuition-guided motion sampling algorithm-Motion level, (c) predicate and primitive-based high precision control-Execution level, and (d) LLM-heuristic tree-searching task planning-Task level.",
        "qtype": "Design_Rationale",
        "response": "Step 1: Locate the red‐boxed area in the figure.  \n• In Fig. 1(c) the red box surrounds the small “Neural predicate” table listing entries like target_aim(), target_clear(), target_match(), etc.  \n\nStep 2: Find where this shows up in the text.  \n• Section III-B (“High Precision Control Based on Neural Predicates and Action Primitives”) describes exactly this list as “neural predicates” that take multi‐sensor (RGB-D + force/torque) inputs and map them into a quasi-symbolic state space.  \n• Immediately after defining those predicates, the text explains that 12 action primitives (Approach, Mate, Push, Insert, …) are each specified in PDDL by symbolic preconditions and effects, and that these PDDL specifications are driven by the outputs of the neural predicates.  \n\nStep 3: Understand why they marry neural predicates and action primitives.  \n• Raw depth, RGB and force/torque data are too noisy and unstructured for reliable, millimeter-level control in a highly variable battery-disassembly task.  \n• By first converting those raw signals into a small set of high-level, tunable predicates (e.g. “is the target aligned?”, “is the bolt matched?”, “is there rust?”), the system can then choose and verify each primitive based on crisp pre- and post-conditions in symbolic form.  \n• This structure lets BEAM-1 plan and execute each motion primitive adaptively in the face of unpredictable variations—and embed a second layer of “primitive-level” sensing and verification—to guarantee sub-millimeter accuracy and 100 % success in mating and disassembling bolts.  \n\nFinal answer:  \nThe red‐boxed module is the bank of neural predicates.  They are integrated with the action primitives so that continuous, noisy sensor streams (RGB-D plus force/torque) can be lifted into a small set of symbolic state descriptors that serve as precise preconditions and effects in each PDDL‐defined primitive.  This coupling of learned predicates with hand-coded motion primitives closes the loop at every step—driving adaptive primitive selection, real-time verification, and millimeter-level control accuracy."
    },
    {
        "question": "What is the rationale behind combining breadth-first tree search with LLM-heuristic tree search for efficient task planning?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "At the task level, BEAM-1 generates the optimal sequence of action primitives from the current state to the goal state in the symbol space using the pre-requirements of the primitives and the current state given by the neural predicates, by using BFS tree search (Figure1(d)).",
            "In scenarios with complex tasks, the problem of exploding search space arises during the search for feasible solutions, leading to excessively long computation times.",
            "To tackle this issue, we introduce the LLM heuristic search, further enhancing the efficiency of task planning in unstructured environments."
        ],
        "final_answer": "Breadth-first tree search provides a systematic, complete way to explore all action-primitive sequences, but it can explode combinatorially in complex disassembly tasks. By integrating an LLM-based heuristic search—where the LLM predicts the most promising next primitives, checks their feasibility, and evaluates their likely effects—the combined approach prunes infeasible or low-value branches and steers the BFS toward high-value paths, greatly speeding up planning without sacrificing completeness.",
        "relevant_elements": [
            "breadth-first tree search",
            "LLM-heuristic tree search"
        ],
        "id": 718,
        "masked_question": "What is the rationale behind combining [mask1] with [mask2] for efficient task planning?",
        "masked_number": 2,
        "masked_elements": [
            "breadth-first tree search",
            "LLM-heuristic tree search"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.06590v1_figure_1.png",
        "paperid": "2407.06590v1",
        "paper_path": "./papers/2407.06590v1.json",
        "figure_id": "2407.06590v1_figure_1.png",
        "caption": "Figure 1: System architecture of our BEAM-1. According to different functions, it can be categorized into (a) body composition-Hardware level, (b) intuition-guided motion sampling algorithm-Motion level, (c) predicate and primitive-based high precision control-Execution level, and (d) LLM-heuristic tree-searching task planning-Task level.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning step‐by‐step:\n\n1.  Identify the two modules in question.  \n    –  [mask1] (red box) is the *symbolic, primitive‐and‐predicate based breadth‐first search* over PDDL primitives (the classic tree‐search that guarantees completeness but blows up combinatorially).  \n    –  [mask2] (blue box) is the *LLM‐heuristic tree‐search*, which wraps three LLM “subengines” (thought‐generation, feasibility checking, effect prediction) plus a real‐time verification loop.\n\n2.  Understand their strengths and weaknesses.\n    –  Pure BFS over all primitives (mask1) will eventually find a sequence, but in an unstructured disassembly domain its branching factor is enormous, so it is too slow.  \n    –  A naïve LLM approach can suggest likely next steps but has no hard guarantee of feasibility or of reaching the goal.\n\n3.  See how they complement each other.\n    –  The LLM layers guide the search—asking “Which primitive is most promising?”; “Is its precondition satisfied?”; “Does it move us closer to the goal?”—so that the search graph is pruned aggressively.  \n    –  Behind the scenes you still retain the PDDL/BFS apparatus to do the bookkeeping of symbolic states and to verify (in real time) that you haven’t strayed off into an impossible branch.\n\n4.  Hence the *rationale* for putting them together is:\n    –  You keep the *completeness and rigor* of a predicate‐and‐primitive BFS, and you add *LLM‐driven heuristics* to dramatically cut down the number of branches you actually explore.  \n    –  The result is an efficient, real‐time capable task planner that can handle the huge combinatorial space of battery-disassembly actions without losing the strong correctness guarantees of symbolic search."
    },
    {
        "question": "What motivates freezing the image encoder weight instead of jointly fine-tuning it with the transformer?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "image encoder",
            "transformer"
        ],
        "id": 719,
        "masked_question": "What motivates freezing the [mask1] weight instead of jointly fine-tuning it with the transformer?",
        "masked_number": 1,
        "masked_elements": [
            "image encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17820v1_figure_2.png",
        "paperid": "2411.17820v1",
        "paper_path": "./papers/2411.17820v1.json",
        "figure_id": "2411.17820v1_figure_2.png",
        "caption": "Figure 2: Overall Illustration of CityWalker. Our training pipeline starts with internet-sourced videos, using visual odometry to obtain relative poses between frames. At each time step, the model receives past observations, past trajectory, and target location as input. They are encoded via a frozen image encoder and a trainable coordinate encoder. A transformer processes these inputs to generate future tokens. An action head and an arrival head decode these tokens into action and arrival status predictions. During training, future frame tokens from future frames guide the transformer to hallucinate future tokens.",
        "qtype": "Design_Rationale",
        "response": "Image‐text alignment  \n• In Figure 2 the red‐boxed module is the “Image Encoder” that sits upstream of the transformer.  \n• In the text (§3.1, “Pipeline Architecture”) this is identified as a pre‐trained DINOv2 backbone whose weights are held fixed during CityWalker training.\n\nChain of thought  \n1. The encoder was pre-trained on massive, diverse vision data (DINOv2) and produces strong, general image features out of the box.  \n2. Our urban-navigation fine-tuning set (even when drawing on thousands of hours of web video) and the small amount of expert teleop trajectories are still a drop in the bucket compared to what the backbone saw during pretraining.  \n3. If we were to jointly update the image‐encoder weights with the transformer, two things could happen:  \n   a. We might overfit those representations to our narrow domain (walking/driving videos + quadruped data), losing generality.  \n   b. Training would become much more expensive (GPU memory, time), since backpropagating through a large vision backbone is costly.  \n4. By freezing the DINOv2 encoder we  \n   • preserve its broad, robust visual priors,  \n   • avoid catastrophic forgetting or overfitting to our relatively small “expert” dataset, and  \n   • reduce computation and memory footprint so we can scale to thousands of hours of video.  \n\nAnswer  \nWe freeze the image‐encoder weights because they come from a large, generalist DINOv2 model whose representations we want to preserve. Freezing prevents catastrophic forgetting or overfitting during fine-tuning, reduces GPU/memory cost, and yields more stable, transferable image features than if we jointly trained the backbone together with the transformer."
    },
    {
        "question": "How does the coordinate MLP normalize and encode relative poses with target coordinates into the input coordinate token?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Past Traj.",
            "Target Coord.",
            "MLP"
        ],
        "id": 721,
        "masked_question": "How does the coordinate [mask1] normalize and encode relative poses with target coordinates into the input coordinate token?",
        "masked_number": 1,
        "masked_elements": [
            "MLP"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17820v1_figure_2.png",
        "paperid": "2411.17820v1",
        "paper_path": "./papers/2411.17820v1.json",
        "figure_id": "2411.17820v1_figure_2.png",
        "caption": "Figure 2: Overall Illustration of CityWalker. Our training pipeline starts with internet-sourced videos, using visual odometry to obtain relative poses between frames. At each time step, the model receives past observations, past trajectory, and target location as input. They are encoded via a frozen image encoder and a trainable coordinate encoder. A transformer processes these inputs to generate future tokens. An action head and an arrival head decode these tokens into action and arrival status predictions. During training, future frame tokens from future frames guide the transformer to hallucinate future tokens.",
        "qtype": "Implementation_Details",
        "response": "Here is how the little red‐boxed module (“[mask1]”) works, step by step:\n\n1.   What goes in  \n     •  A short sequence of 2D displacements Δpₜ = (xₜ – xₜ₋₁, yₜ – yₜ₋₁) for the past m time-steps, plus the 2D vector from the current pose to the next waypoint.  \n     •  All of these raw (x,y) offsets come straight out of the VO pipeline, so they inherit its arbitrary scale.  \n\n2.   Scale normalization  \n     •  We divide each Δp (and the final “to‐goal” vector) by the average step length within that clip.  \n     •  That one scalar is computed once per video segment.  This removes the scale ambiguity and makes walking and driving steps comparable.  \n\n3.   MLP coordinate encoder  \n     •  The now-normalized (x,y) vectors are concatenated into a single flat vector.  \n     •  This vector is passed through a small trainable MLP (the red box)  \n       –  a couple of linear layers with non-linearities  \n       –  projecting it up to the same dimensionality as the image tokens.  \n\n4.   Output  \n     •  The MLP spits out one “coordinate token” that carries both your recent motion history and the relative goal location, on the same footing as the frozen image‐feature tokens, ready to be fed into the transformer."
    },
    {
        "question": "How does RCA leverage the importance sampling ratio between current and previous πθ during optimization?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "Inspired by TRPO (Schulman et al., 2015  ###reference_b35###), we employ importance sampling, which enables the network to incorporate historical data into its updates, thereby enhancing the overall efficiency of the optimization process.",
            "We reformulate Eq.7 ###reference_### to a loss function as follows:\n\n    L(θ) = – E_{a ∼ πθ_old}[r(a) · (πθ(a) / πθ_old(a))]\n\nwhere πθ_old is the policy of the previous step.",
            "Specifically, we define the ratio of new policy and old policy as ratio = πθ(a) / πθ_old(a). When the new policy is equal to the old policy, ratio is equal to 1. To limit the magnitude of updates to the new policy, we set a margin ε, ensuring that ratio falls within the range of [1–ε, 1+ε]."
        ],
        "final_answer": "RCA incorporates an importance sampling correction by weighting each sampled reward with the ratio πθ(a)/πθ_old(a) between the current and previous policies. It then clips this ratio to lie within [1–ε, 1+ε] to prevent overly large updates and thus stabilizes the optimization of the initial noise distribution.",
        "relevant_elements": [
            "RCA",
            "πθ"
        ],
        "id": 723,
        "masked_question": "How does [mask1] leverage the importance sampling ratio between current and previous πθ during optimization?",
        "masked_number": 1,
        "masked_elements": [
            "RCA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19453v1_figure_2.png",
        "paperid": "2407.19453v1",
        "paper_path": "./papers/2407.19453v1.json",
        "figure_id": "2407.19453v1_figure_2.png",
        "caption": "Figure 2. The optimization iteration of our FIND. Firstly, we sample 𝐳T∼πθsimilar-tosubscript𝐳𝑇subscript𝜋𝜃\\mathbf{z}_{T}\\sim\\pi_{\\theta}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, then generate an image through a T-step denoising process. Next, we optimize the reward prediction network g𝑔gitalic_g by ℒg∗subscriptsuperscriptℒ𝑔\\mathcal{L}^{*}_{g}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT. Subsequently, we update the initial distribution πθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT using the policy gradient by ℒp∗subscriptsuperscriptℒ𝑝\\mathcal{L}^{*}_{p}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT.",
        "qtype": "Implementation_Details",
        "response": "[mask1] is the “Ratio Clipping Algorithm” (RCA).  In practice RCA takes the usual REINFORCE‐style gradient  \n   \n ∇θ Eπθ[r] ≃ Eπθ_old [ (πθ(z)/πθ_old(z))·r ∇θ log πθ(z) ]  \n   \nand then “clips’’ the importance‐sampling weight  \n   \n ρ(z) = πθ(z) / πθ_old(z)  \n   \nto lie in [1–ε, 1+ε] (or [1–β, 1+β] in the paper’s notation).  By doing so it (a) reuses past samples via the IS ratio ρ(z) and (b) prevents any single update from drifting too far away from the previous policy, greatly improving stability."
    },
    {
        "question": "How does DRCM train its MLP to predict expected reward for πθ using L_g* calibration?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "We propose a Dynamic Reward Calibration Module (DRCM) to predict the expected reward \\bar{r} of the current initial distribution by a simple 3-layer MLP network g defined as \\bar{r} = g(z_T; φ).",
            "The loss function of g is formulated as: L_g = E[(g(z_T) - r)^2].",
            "Considering the K times multiple optimization steps involved in the entire process, we optimize g using the rewards r corresponding to the sampled z_T at each optimization step, as well as the initial distribution, as shown in the left part of Fig.3.",
            "We reformulate the loss function of g as follows: L_g = \\frac{1}{N} \\sum_{i=1}^N (g(z_T^i) - r^i)^2. Considering the efficiency of optimization, here N is set to 1."
        ],
        "final_answer": "DRCM trains its 3-layer MLP g by minimizing a squared-error calibration loss L_g between g’s prediction and the actual reward observed for each sampled initial noise z_T.  Since no ground-truth expected reward is available, g is updated online at each optimization step using the current sample’s reward as the target: L_g = (1/N) Σ_i (g(z_T^i) – r^i)^2 (with N=1 for efficiency).  Over iterations, g learns to approximate the true expected reward of πθ.",
        "relevant_elements": [
            "DRCM",
            "L_g*",
            "πθ"
        ],
        "id": 724,
        "masked_question": "How does [mask1] train its MLP to predict expected reward for [mask2] using L_g* calibration?",
        "masked_number": 2,
        "masked_elements": [
            "DRCM",
            "πθ"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.19453v1_figure_2.png",
        "paperid": "2407.19453v1",
        "paper_path": "./papers/2407.19453v1.json",
        "figure_id": "2407.19453v1_figure_2.png",
        "caption": "Figure 2. The optimization iteration of our FIND. Firstly, we sample 𝐳T∼πθsimilar-tosubscript𝐳𝑇subscript𝜋𝜃\\mathbf{z}_{T}\\sim\\pi_{\\theta}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, then generate an image through a T-step denoising process. Next, we optimize the reward prediction network g𝑔gitalic_g by ℒg∗subscriptsuperscriptℒ𝑔\\mathcal{L}^{*}_{g}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT. Subsequently, we update the initial distribution πθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT using the policy gradient by ℒp∗subscriptsuperscriptℒ𝑝\\mathcal{L}^{*}_{p}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step story:\n\n1.  What is [mask1]?  \n    “The thing in the red box” is the Dynamic Reward Calibration Module (DRCM), i.e. the small 3-layer MLP we call \\(g\\).\n\n2.  What is [mask2]?  \n    “The thing in the blue box” is our policy network \\(\\pi_{\\theta}\\), which samples initial noise \\(z_T\\).\n\n3.  What does DRCM (the MLP \\(g\\)) need to learn?  \n    It needs to predict the expected reward under the current policy \\(\\pi_{\\theta}\\), i.e.  \n    \\[\n       E_{z_T\\sim\\pi_{\\theta}}[\\,r\\,].\n    \\]\n\n4.  How do we get training data for \\(g\\)?  \n    – We sample one noise vector \\(z_T\\sim\\pi_{\\theta}\\).  \n    – We run the frozen diffusion model (U-Net) on \\(z_T\\) to produce an image, feed it through the frozen reward model \\(f_r\\) to get a scalar reward \\(r\\).  \n\n5.  What is the loss \\(\\mathcal{L}_g^*\\)?  \n    We simply regress \\(g\\) on this one sample by mean‐squared error:  \n    \\[\n      \\mathcal{L}_g^*\n      \\;=\\;\n      \\bigl(g(z_T)\\;-\\;r\\bigr)^{2}\n      \\quad\\bigl(\\text{or averaged over a small batch of such }(z_T,r)\\bigr).\n    \\]\n\n6.  How does this train \\(g\\) to predict \\(E[r]\\)?  \n    By repeatedly sampling \\(z_T\\sim\\pi_{\\theta}\\), computing the true reward \\(r\\), and taking gradient steps on  \n    \\(\\mathcal{L}_g^*=(g(z_T)-r)^2,\\)  \n    \\(g\\) learns to approximate the mean reward of \\(\\pi_{\\theta}\\).\n\nIn short:\n\nDRCM (the MLP \\(g\\)) is trained via the calibration loss  \n\\[\n  \\mathcal{L}_g^* \\;=\\; (\\,g(z_T)\\;-\\;r\\,)^2\n\\]\nwhere each \\(r\\) is the scalar reward obtained by sampling \\(z_T\\) from the policy \\(\\pi_\\theta\\). Over many such updates, \\(g\\) converges to predict the expected reward of \\(\\pi_\\theta\\)."
    },
    {
        "question": "How does g’s calibrated baseline reflect classical reward baseline subtraction in policy gradient?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "We propose a Dynamic Reward Calibration Module (DRCM) to predict r̄ of z_T by a simple 3-layer MLP network g_φ defined as g_φ: z_T → ℝ.",
            "We define the optimized reward for our current sample as r* = r – g_φ(z_T), as the difference between the reward obtained from sampling and the reward predicted by the network."
        ],
        "final_answer": "The network g_φ learns to predict the expected reward (the baseline) for a given initial noise z_T, and this baseline is then subtracted from the actual reward r to form r* = r – g_φ(z_T). This exactly mirrors the classical policy‐gradient practice of subtracting a learned reward baseline from sampled rewards to compute an advantage signal and reduce gradient variance.",
        "relevant_elements": [
            "g",
            "reward"
        ],
        "id": 725,
        "masked_question": "How does [mask1]’s calibrated baseline reflect classical [mask2] baseline subtraction in policy gradient?",
        "masked_number": 2,
        "masked_elements": [
            "g",
            "reward"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.19453v1_figure_2.png",
        "paperid": "2407.19453v1",
        "paper_path": "./papers/2407.19453v1.json",
        "figure_id": "2407.19453v1_figure_2.png",
        "caption": "Figure 2. The optimization iteration of our FIND. Firstly, we sample 𝐳T∼πθsimilar-tosubscript𝐳𝑇subscript𝜋𝜃\\mathbf{z}_{T}\\sim\\pi_{\\theta}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT, then generate an image through a T-step denoising process. Next, we optimize the reward prediction network g𝑔gitalic_g by ℒg∗subscriptsuperscriptℒ𝑔\\mathcal{L}^{*}_{g}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT. Subsequently, we update the initial distribution πθsubscript𝜋𝜃\\pi_{\\theta}italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT using the policy gradient by ℒp∗subscriptsuperscriptℒ𝑝\\mathcal{L}^{*}_{p}caligraphic_L start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT.",
        "qtype": "Literature_Background",
        "response": "Here’s the intuition step by step:\n\n1.  In REINFORCE (and most policy‐gradient methods) you form an *advantage* by subtracting a baseline \\(b(s)\\) from the raw return \\(r\\) in order to reduce variance:\n   \n   $$A(s,a)\\;=\\;r\\;-\\;b(s)\\,.$$\n\n2.  In our FIND framework (see the blue‐boxed “Reward” module), the trainable network **g** (that red‐boxed module) is exactly playing the role of that baseline.  Concretely,  \n   \n   – g predicts an expected reward \\(\\bar r\\) for the current initial‐noise distribution.  \n   – We then *calibrate* the actual reward by computing  \n     $$r^* \\;=\\;r \\;-\\;\\bar r\\,. $$\n\n3.  This calibrated reward \\(r^*\\) is the policy‐gradient “advantage,” just as in classical REINFORCE you would use \\(r - b(s)\\).  \n\nSo, **g**’s learned predicted reward \\(\\bar r\\) acts *just like* the classical value‐function or baseline subtraction in policy gradient: subtracting that baseline from the raw reward yields the low‐variance training signal for \\(\\pi_\\theta\\)."
    },
    {
        "question": "How does the RNN/CNN/GNN/Transformer encoder module build on SL-based next-item prediction frameworks?",
        "relevant_section_ids": [
            "3.1.2",
            "3.1.3"
        ],
        "relevant_context": [
            "With embedding vectors of items until timestep t, supervised learning methods [3,4,6,5,7,32] dwell in constructing an effective encoder f in order to model the user preference as another dense vector h_t, i.e.,\nGenerally, we do not expect any dimensional alignment between user preference and item embedding spaces here.",
            "In supervised learning methods, the continuous version is preferred where the major idea is to obtain an effective projection mapping g from H to R^d. With this g at hand, h_t is converted into h_t' in order to match m s. Then, decided jointly by h_t' and the item embedding matrix E, the ranking order is up to the utility scores where the concrete form is often chosen as the inner product. Consequently, the final top item for recommendation is\nwhere e_j denotes the j-th row of E, which is actually e_j."
        ],
        "final_answer": "In SL-based next-item prediction, an encoder module (RNN/CNN/GNN/Transformer) f is trained to consume the sequence of past item embeddings and produce a single dense user–preference vector h_t. A lightweight projection g then maps h_t into the same latent space as the item embeddings. Finally, the model ranks all candidate items by taking inner products between the projected user vector and each item embedding to predict the next item.",
        "relevant_elements": [
            "RNN/CNN/GNN/Transformer",
            "user preference"
        ],
        "id": 727,
        "masked_question": "How does the [mask1] encoder module build on SL-based next-item prediction frameworks?",
        "masked_number": 1,
        "masked_elements": [
            "RNN/CNN/GNN/Transformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.08047v1_figure_1.png",
        "paperid": "2408.08047v1",
        "paper_path": "./papers/2408.08047v1.json",
        "figure_id": "2408.08047v1_figure_1.png",
        "caption": "Figure 1: Two implementation manners for sequential recommendation. The discrete version is presented in the dashed box on the left-hand side whereas the continuous version is illustrated in the dashed box on the right-hand side.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed “[mask1] encoder” is nothing exotic at all but simply the standard SL–based sequence encoder (i.e. an RNN, CNN, GNN or Transformer) that you find in any supervised next-item prediction model.  Concretely, it takes your embedded item–feedback pairs up to time t, runs them through a shallow or deep sequence net, and spits out the dense “preference” vector pₜ exactly as in Equ. (2) of SL methods.  We then plug that very same encoder into our RL actor–critic pipeline so that we retain all of the continuous‐space, low-complexity advantages of supervised next-item recommenders."
    },
    {
        "question": "How does projection mapping transform inferred preferences into utility scores for discrete recommendation?",
        "relevant_section_ids": [
            "3.1.3"
        ],
        "relevant_context": [
            "In supervised learning methods, the continuous version is preferred where the major idea is to obtain an effective projection mapping P from P to E. With this P at hand, \\tilde{p} is converted into u in order to match e's.",
            "Then, decided jointly by \\tilde{p} and the item embedding matrix E, the ranking order is up to the utility scores where the concrete form is often chosen as the inner product [51]. Consequently, the final top item for recommendation is argmax_i u^T e_i, where e_i denotes the i-th row of E."
        ],
        "final_answer": "Projection mapping P is applied to the inferred preference vector p̃ to produce a projected vector u in the same space as item embeddings, and then each item’s utility score is computed as the inner product between u and the item’s embedding e_i; items are ranked by these utility scores.",
        "relevant_elements": [
            "projection mapping",
            "preference inferring"
        ],
        "id": 729,
        "masked_question": "How does [mask1] transform inferred preferences into utility scores for discrete recommendation?",
        "masked_number": 1,
        "masked_elements": [
            "projection mapping"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.08047v1_figure_1.png",
        "paperid": "2408.08047v1",
        "paper_path": "./papers/2408.08047v1.json",
        "figure_id": "2408.08047v1_figure_1.png",
        "caption": "Figure 1: Two implementation manners for sequential recommendation. The discrete version is presented in the dashed box on the left-hand side whereas the continuous version is illustrated in the dashed box on the right-hand side.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. In the discrete (RL‐style) branch the goal is no longer to project the user embedding into the same space as the item embeddings and take inner products, but rather to directly produce one score (or probability) for each item.  \n2. Visually, the red‐boxed module sits on top of the inferred preference vector and spits out, in one shot, a list of grey bars—one per candidate item—which are precisely those utility scores.  \n3. In other words, it is a small neural “head” (usually a fully‐connected MLP) that takes the d–dimensional preference embedding as input and outputs an |I|–dimensional vector of scalars. Each scalar is interpreted as the utility (or Q-value) of recommending that particular item next.  \n4. Once you have that |I|-vector of utilities, you can apply an arg max (or softmax) to choose or sample the next item to recommend.  \n\nAnswer  \nThe red‐boxed block is simply a neural scoring head that maps the d-dimensional inferred preference vector to an |I|-dimensional utility vector—one score for each discrete item. Concretely, it feeds the user preference through a small fully-connected network that outputs a scalar utility/Q-value for every candidate item, which are then ranked (or turned into probabilities) to make the next‐item recommendation."
    },
    {
        "question": "How does Query-centered Expanding Ripple complement Webpage-centered Shrinking Ripple in graph construction?",
        "relevant_section_ids": [
            "2",
            "2.1"
        ],
        "relevant_context": [
            "Specifically, MPGraf first conducts high-quality pseudo-label links for each unlabeled query-webpage pair by annotating all unlabeled pairs with pseudo-ranking scores, and then assigns every query webpages with high-ranking scores and also webpages with low scores to conduct Query-centered Expanding Ripple from training data. Next, MPGraf links every webpage to irrelevant queries with poor relevance scores to conduct Webpage-centered Shrinking Ripple.",
            "Query-centered Expanding Ripple. Given the set of queries and the set of webpages, MPGraf first obtains each possible query-webpage pair ... MPGraf further takes a self-tuning approach ... to propagate labels from annotated query-webpage pairs to unlabeled ones.",
            "Webpage-centered Shrinking Ripple. Though Query-centered Expanding Ripple algorithm could generate ranking scores for every query-webpage pair in training data, it is still difficult to construct webpage-centered graphs using predicted scores at full-scale. ... To conduct webpage-centered graphs for a webpage, MPGraf leverages a Webpage-centered Shrinking Ripple approach. Given a webpage, MPGraf retrieves all query-webpage pairs and builds a webpage-centered graph for every query-webpage with relevance scores higher than 1-fair ... MPGraf randomly picks up a query that does not connect to the webpage as the irrelevant query ..."
        ],
        "final_answer": "Query-centered Expanding Ripple first uses high- and low-scoring pseudo-labels to grow a query-centric subgraph around each query (adding both relevant and irrelevant webpages). Since webpages typically connect to only a few queries, Webpage-centered Shrinking Ripple then complements this by focusing on each webpage in turn, randomly sampling queries that were not linked (i.e., poor relevance) to build balanced webpage-centric graphs. Together, they ensure that both query-side expansions and webpage-side negative samples are included in the final graph.",
        "relevant_elements": [
            "Query-centered Expanding Ripple",
            "Webpage-centered Shrinking Ripple"
        ],
        "id": 731,
        "masked_question": "How does [mask1] complement [mask2] in graph construction?",
        "masked_number": 2,
        "masked_elements": [
            "Query-centered Expanding Ripple",
            "Webpage-centered Shrinking Ripple"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.16590v1_figure_1.png",
        "paperid": "2409.16590v1",
        "paper_path": "./papers/2409.16590v1.json",
        "figure_id": "2409.16590v1_figure_1.png",
        "caption": "Figure 1: The framework of the proposed MPGraf.",
        "qtype": "Experimental_Results",
        "response": "Here, the red‐boxed “Query-centered Expanding Ripple” (mask1) and the blue-boxed “Webpage-centered Shrinking Ripple” (mask2) are two sides of the same graph‐building coin:\n\n1. Query-centered Expanding Ripple (mask1) starts from each query and “ripples out” to collect webpages that score highly (and even those that score poorly) under that query.  It therefore builds up positive (and some negative) query→page links around every query node.\n\n2. Webpage-centered Shrinking Ripple (mask2) does the converse: it starts from each webpage and “shrinks in” by deliberately sampling an irrelevant query (a low-score link) alongside a relevant one.  This injects explicit negative edges around the page.\n\nTogether they yield a balanced query–webpage graph: the first module densely populates each query’s neighborhood with its strongest and weakest page links, and the second module ensures every page also carries strong negative signals by linking it to irrelevant queries."
    },
    {
        "question": "How does Parallelizing Graphformer integrate GNN and Transformer representations before regression?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Given the learned representation z_g and z_t, MPGraf concatenates two items as z and performs a linear projection to transform z into a low-dimensional vector space as z'.",
            "Given the learned generalizable representation z' (or z), MPGraf adopts an MLP-based regressor to compute the ranking score."
        ],
        "final_answer": "Parallelizing Graphformer takes the GNN output representation and the Transformer output representation, concatenates them into one vector, applies a linear projection to reduce it to a low-dimensional space, and then feeds this combined vector into an MLP regressor to produce the final ranking score.",
        "relevant_elements": [
            "Parallelizing Graphformer",
            "GNN module",
            "Transformer module"
        ],
        "id": 732,
        "masked_question": "How does [mask1] integrate GNN and Transformer representations before regression?",
        "masked_number": 1,
        "masked_elements": [
            "Parallelizing Graphformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.16590v1_figure_1.png",
        "paperid": "2409.16590v1",
        "paper_path": "./papers/2409.16590v1.json",
        "figure_id": "2409.16590v1_figure_1.png",
        "caption": "Figure 1: The framework of the proposed MPGraf.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is the Parallelizing Graphformer.  In that design, we run the GNN and the Transformer side-by-side, produce two embeddings (one from the GNN, one from the Transformer), then simply concatenate those two vectors and pass the result through a single linear (projection) layer to get the final pair representation that is fed into the MLP regressor."
    },
    {
        "question": "What biases could arise from Webpage-centered Shrinking Ripple when generating negative samples for relevance graphs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Webpage-centered Shrinking Ripple"
        ],
        "id": 733,
        "masked_question": "What biases could arise from [mask1] when generating negative samples for relevance graphs?",
        "masked_number": 1,
        "masked_elements": [
            "Webpage-centered Shrinking Ripple"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.16590v1_figure_1.png",
        "paperid": "2409.16590v1",
        "paper_path": "./papers/2409.16590v1.json",
        "figure_id": "2409.16590v1_figure_1.png",
        "caption": "Figure 1: The framework of the proposed MPGraf.",
        "qtype": "Others",
        "response": "The red‐boxed “Webpage‐centered Shrinking Ripple” is nothing more exotic than the procedure where, for each target page, MPGraf simply picks a random query that it has never been seen under and calls it a “negative” example.  Stepping through what that means, you can see at least three ways in which this can skew your training data:\n\n 1.  Easy‐negative bias: by drawing uniformly from the huge set of all queries that have never been paired with that page, you almost always pick queries that are utterly unrelated (e.g. “how to tie a tie” vs. a page about “quantum field theory”).  The model never learns to distinguish subtle “near misses,” because it is almost never shown them.\n\n 2.  False negatives (selection bias): many legitimate but unobserved relevant query–page pairs simply aren’t in your click logs or annotations, so you may be labelling actually relevant documents as “irrelevant.”  That mislabelling distorts the decision boundary.\n\n 3.  Popularity bias: popular pages have been seen under many queries, so their remaining “unused” queries are by construction more obscure—again producing very easy negatives—whereas long‐tail pages may only get a handful of negatives.  Your negative pool ends up looking quite different across head vs. tail pages.\n\nIn short, random “does‐not‐connect” sampling induces an overabundance of trivial negatives, an under‐representation of hard negatives, and mis-labelling of missing but genuine relevance."
    },
    {
        "question": "What trade-offs exist between Parallelizing Graphformer and Stacking Graphformer regarding computational efficiency versus representation richness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Parallelizing Graphformer",
            "Stacking Graphformer"
        ],
        "id": 734,
        "masked_question": "What trade-offs exist between [mask1] and [mask2] regarding computational efficiency versus representation richness?",
        "masked_number": 2,
        "masked_elements": [
            "Parallelizing Graphformer",
            "Stacking Graphformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.16590v1_figure_1.png",
        "paperid": "2409.16590v1",
        "paper_path": "./papers/2409.16590v1.json",
        "figure_id": "2409.16590v1_figure_1.png",
        "caption": "Figure 1: The framework of the proposed MPGraf.",
        "qtype": "Others",
        "response": "In our figure, the red‐boxed module is the “Parallelizing Graphformer,” and the blue‐boxed one is the “Stacking Graphformer.”  Broadly speaking, the trade‐off between the two looks like this:\n\n1. Parallelizing Graphformer (red box)  \n   – Computational Efficiency: high  \n     • The GNN and Transformer blocks run side-by-side on the same inputs, so you can literally parallelize the two feature‐extractors and fuse their outputs in one shot.  \n     • Less sequential dependency means lower latency and better use of multi-core/GPU parallelism.  \n   – Representation Richness: moderate  \n     • You get both graph‐structural and contextual features, but they never “talk to each other” in a deeply interleaved way.  \n     • The fusion is a simple concatenation (or linear projection), so you sacrifice some of the fine‐grained, higher‐order interactions between the two modalities.\n\n2. Stacking Graphformer (blue box)  \n   – Computational Efficiency: lower  \n     • You must first run the GNN (or Transformer) to completion, then feed its outputs into the other module.  \n     • This strictly sequential pipeline doubles up on compute and memory (especially if you keep the full intermediate activations for back-prop).  \n   – Representation Richness: high  \n     • Each module has the opportunity to refine and condition on the other’s output, layer by layer.  \n     • This interleaving lets you capture deep, hierarchical relationships (e.g. graph topology influencing self-attention, and vice versa) that a single-stage fusion can’t match.\n\nIn short, if you care most about throughput and parallel hardware utilization, the Parallelizing Graphformer wins. If you need the richest joint graph + text representations at the expense of extra compute and latency, the Stacking Graphformer is the better choice."
    },
    {
        "question": "What limitations might embedding and rounding process introduce into Diffusion SR including discrete item z?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "As illustrated in Figure 1 (a), the traditional methods overlook a critical step: mapping the reversed target item representation into the discrete item index space. These methods often determine the recommended item by calculating the similarity (e.g., inner product) between the reversed target item representation and candidate item embeddings, selecting the item with the highest similarity score. While this method works for item ranking, it does not align with the core principles of diffusion models. Diffusion models are inherently probabilistic and continuous, but this step—based on a direct inner product comparison—introduces a deterministic, discrete decision process that breaks the continuous generative flow of diffusion. This inconsistency creates a gap between the optimization directions of the ranking loss for the recommendation task and the denoising loss for the diffusion model, resulting in suboptimal recommendation performance.",
            "As for the reverse process, we define the predicted distribution of \\(\\tilde z_0\\) as: \\(p_\\phi(\\tilde z_0\\mid e_0)\\propto\\exp\\bigl(\\cos(e_0, E_j)\\bigr)\\). However, the transition distribution \\(p_\\phi(\\tilde z_0\\mid e_0)\\) lacks a direct analytical formula."
        ],
        "final_answer": "Embedding the discrete item into a continuous space and then rounding (or selecting) back into a discrete index can introduce two key limitations:  (1) it breaks the purely probabilistic, continuous nature of the diffusion process by inserting a hard, deterministic decision, creating a mismatch between the diffusion’s denoising objective and the ranking loss used for recommendation, and (2) the reverse mapping from continuous embeddings back to discrete items (i.e., the rounding or categorical distribution) has no closed‐form expression, forcing practitioners to resort to ad‐hoc approximations that can further degrade performance.",
        "relevant_elements": [
            "Embedding and rounding process",
            "Diffusion SR including discrete item z"
        ],
        "id": 735,
        "masked_question": "What limitations might [mask1] introduce into Diffusion SR including discrete item z?",
        "masked_number": 1,
        "masked_elements": [
            "Embedding and rounding process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.21967v1_figure_1.png",
        "paperid": "2410.21967v1",
        "paper_path": "./papers/2410.21967v1.json",
        "figure_id": "2410.21967v1_figure_1.png",
        "caption": "Figure 1. Figures (a) and (b) illustrate the differences between previous methods and our proposed approach in Markov modeling, while Figures (c) and (d) highlight the contrast between implicit and explicit conditional diffusion models for SR.",
        "qtype": "Others",
        "response": "Here, the thing in the red box is the little “encoder/decoder” that we must now insert into the diffusion chain to go back and forth between a discrete item index z and its continuous embedding e₀ (that is, qₚₕᵢ(e₀|z) in the forward pass and pₚₕᵢ(z|e₀) in the reverse).  While this does allow you to formally close the loop and treat z as part of your Markov chain, it also brings in a number of practical headaches:\n\n 1.  **Intractable or approximate transitions.**  \n    Neither qₚₕᵢ(e₀|z) nor pₚₕᵢ(z|e₀) admits a neat closed‐form in the way Gaussian noise does, so you have to introduce extra approximation (e.g. a surrogate KL, or replace an unknown discrete normalizer with a softmax) just to make the math go through.\n\n 2.  **Extra optimization terms.**  \n    Closing the discrete–continuous gap forces you to add a whole new “domain-transition” term to your ELBO (on top of the usual denoising loss).  That means more hyper-parameters, more weight balancing between ranking and diffusion objectives, and more opportunities for the losses to pull in different directions.\n\n 3.  **Quantization/rounding error.**  \n    At sampling time you still need to pick a single z (usually via an argmax or Gumbel-softmax), which is a non-differentiable, highly non‐linear step.  That breaks the clean, fully continuous flow of a vanilla diffusion model and can inject bias or high variance into gradients.\n\n 4.  **Mismatch between ranking and generative losses.**  \n    Because you now have to optimize pₚₕᵢ(z|e₀) to agree with a nearest‐neighbor or inner‐product retrieval over all candidate embeddings, you can end up with a tug-of-war between the diffusion loss (which wants smooth Gaussians) and the ranking loss (which wants sharp peaks on the “correct” discrete indices).\n\nTaken together, these factors make training more fragile, slow down convergence, and generally limit the recommendation quality you can squeeze out of the diffusion‐based SR model when you insist on explicitly modeling the discrete item→embedding→discrete item loop."
    },
    {
        "question": "How could implicit conditional diffusion SR and explicit conditional diffusion SR be combined to reduce sampling complexity?",
        "relevant_section_ids": [
            "1",
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Based on our sequential recommendation diffusion framework, we introduce the Dual Conditional Diffusion Transformer (DCDT). This model concatenates historical information as an implicit condition while leveraging a cross-attention mechanism to incorporate explicit control signals throughout the diffusion process. By embedding dual conditions into both the forward and reverse diffusion stages, our approach ensures more precise and contextually relevant recommendations. In addition, the dual conditional mechanism leads to improved model performance, allowing the inference process to achieve optimal results with only a few sampling steps. This reduction in sampling steps significantly lowers computational overhead, making our model more efficient and better suited for real-time applications.",
            "To tackle these limitations, we propose the Dual Conditioned Diffusion Transformer (DCDT), a novel approach that integrates both implicit and explicit conditioning strategies. By combining the representation of the user’s global preferences with the direct utilization of detailed historical behaviors, DCDT leverages the strengths of both methods.",
            "A natural idea for acceleration is that, if the approximation at the early is accurate enough with no need for adjustment, DCRec can directly skip several intermediate steps. Here, we define the number of skipping steps as k. For our DCRec, we observe that DCDT is robust enough to fulfill the early approximation, which significantly reduces time overhead."
        ],
        "final_answer": "By building a single diffusion model (DCDT) that uses an implicit condition in the forward process (concatenating noisy history) and an explicit condition in the reverse process (cross-attention to full history), the denoising trajectory is guided more accurately toward the target. This dual conditioning lets the model reach a good approximation in far fewer steps, so it can skip intermediate iterations and thus dramatically reduce sampling complexity.",
        "relevant_elements": [
            "Implicit conditional diffusion SR",
            "Explicit conditional diffusion SR"
        ],
        "id": 736,
        "masked_question": "How could [mask1] and [mask2] be combined to reduce sampling complexity?",
        "masked_number": 2,
        "masked_elements": [
            "Implicit conditional diffusion SR",
            "Explicit conditional diffusion SR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.21967v1_figure_1.png",
        "paperid": "2410.21967v1",
        "paper_path": "./papers/2410.21967v1.json",
        "figure_id": "2410.21967v1_figure_1.png",
        "caption": "Figure 1. Figures (a) and (b) illustrate the differences between previous methods and our proposed approach in Markov modeling, while Figures (c) and (d) highlight the contrast between implicit and explicit conditional diffusion models for SR.",
        "qtype": "Others",
        "response": "Here’s the key idea in a nutshell: the red‐boxed “implicit” conditioner and the blue‐boxed “explicit” conditioner are not two competing schemes, but two complementary ways of letting the diffusion network “see” the user’s history.  If you merge them – i.e. in each denoising step you  \n  1. Inject the *implicit* history–feature (the compact, fused embedding of the past sequence) exactly as in the red box  \n  2. Also attend *explicitly* over the entire history sequence via cross–attention (or CondLN) exactly as in the blue box  \nthen your model suddenly has a *much* stronger, more precise guide for removing noise.  In practice that means the chain of T diffusion steps collapses to something like k≪T steps: the dual‐conditioned denoiser converges so much faster that you can simply “skip” most of the intermediate transitions (see Eqn. 36 in the paper) and still land almost exactly on the right item embedding.  The end result is a full‐fidelity diffusion SR but with only a handful of sampling steps – i.e. a dramatic drop in sampling complexity from O(T) down to O(k)."
    },
    {
        "question": "What motivates embedding and rounding process integration for discrete item z in diffusion chain?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "However, they do not explicitly model the diffusion process for the discrete target item itself (Gap 1). As illustrated in Figure 1 (a), the traditional methods overlook a critical step: mapping the reversed target item representation into the discrete item index space. While this method works for item ranking, it does not align with the core principles of diffusion models. Diffusion models are inherently probabilistic and continuous, but this step—based on a direct inner product comparison—introduces a deterministic, discrete decision process that breaks the continuous generative flow of diffusion. This inconsistency creates a gap between the optimization directions of the ranking loss for the recommendation task and the denoising loss for the diffusion model, resulting in suboptimal recommendation performance.",
            "To address this issue and motivated by diffusion models in text domains (Li et al., 2022), we explicitly extend continuous diffusion models to discrete item domains as shown in Figure 2 (a). As for discrete target item z from the item pool V, the Markov chain in the forward and reverse processes are extending as q(e_T|z) and p(z|e_0). Specifically, to map the discrete variables into continuous domains, we define a learnable embedding function g. As for the reverse process, we define the predicted distribution of z as p_φ(z|e_0), but this rounding step has no closed‐form and must be integrated into the diffusion chain."
        ],
        "final_answer": "Because standard diffusion‐based recommenders break the continuous, probabilistic Markov chain when they deterministically pick a discrete item by nearest-neighbor ranking, the authors are motivated to embed the discrete item z into the continuous diffusion process (via a learnable embedding) and then round back (via a probabilistic softmax over embeddings) so as to restore a fully coherent, end-to-end diffusion model that unifies the denoising and ranking objectives.",
        "relevant_elements": [
            "embedding and rounding process",
            "discrete item z"
        ],
        "id": 737,
        "masked_question": "What motivates [mask1] integration for discrete item z in diffusion chain?",
        "masked_number": 1,
        "masked_elements": [
            "embedding and rounding process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.21967v1_figure_1.png",
        "paperid": "2410.21967v1",
        "paper_path": "./papers/2410.21967v1.json",
        "figure_id": "2410.21967v1_figure_1.png",
        "caption": "Figure 1. Figures (a) and (b) illustrate the differences between previous methods and our proposed approach in Markov modeling, while Figures (c) and (d) highlight the contrast between implicit and explicit conditional diffusion models for SR.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed module in Fig. 1(b) is simply the bi‐directional mapping between the discrete item index z and its continuous embedding e₀ (i.e. qφ(e₀|z) and pφ(z|e₀)).  We introduce that mapping for one reason only: in all prior diffusion-based recommenders you completely ignore the discrete item until the very end, breaking the Markov chain and forcing a hard, inner‐product lookup to pick “the” item.  By explicitly inserting qφ(e₀|z) and pφ(z|e₀) into the chain you restore a proper discrete↔continuous transition.  This keeps the entire recommendation process a single coherent probabilistic diffusion model, rather than a diffusion model plus a disconnected, deterministic ranking step."
    },
    {
        "question": "What motivates integrating hierarchical perturbation with statistical tests to address biased response styles?",
        "relevant_section_ids": [
            "3",
            "4",
            "4.3.1"
        ],
        "relevant_context": [
            "Section 3: \"The variance in the response distributions indicates the presence of bias that can significantly affect alignment (ρ), illustrating that alignment is not a direct or credible metric for assessing the ability of LLMs as NLG evaluators. It is crucial to develop a new metric and measurement for evaluation that is not influenced by the evaluators’ biased response styles, ensuring a more accurate and fair assessment of LLM capabilities.\"",
            "Section 4: \"The fundamental principle of our assessment is that a qualified LLM evaluator should be able to independently identify issues in perturbed data (which contains some quality issues) and assign relatively lower scores compared to the original reference data during two separate evaluations. This approach does not rely on human scores, thus eliminating the influence of human response styles.\"",
            "Section 4.3.1: \"Because the W-Test does not assume any specific distribution for the scores and does not focus on their absolute values, the resulting p-values solely reflect whether the LLMs are able to detect the quality issues and assign lower scores to the perturbed data compared to the original data. Consequently, this testing approach inherently avoids the influence of response styles, instead focusing on the relative quality assessment.\""
        ],
        "final_answer": "Because LLMs exhibit individual response-style biases that distort absolute score alignment, the authors introduce hierarchical perturbations (to create controlled quality differences) plus statistical testing (Wilcoxon Signed-Rank Test focusing on relative score changes) to eliminate the influence of those biased response styles and obtain a fair, content-oriented measure of evaluators’ discernment.",
        "relevant_elements": [
            "Hierarchical Perturbation",
            "Statistical Test",
            "Biased Response Styles"
        ],
        "id": 739,
        "masked_question": "What motivates integrating [mask1] with statistical tests to address biased response styles?",
        "masked_number": 1,
        "masked_elements": [
            "Hierarchical Perturbation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.13704v1_figure_1.png",
        "paperid": "2408.13704v1",
        "paper_path": "./papers/2408.13704v1.json",
        "figure_id": "2408.13704v1_figure_1.png",
        "caption": "Figure 1: Challenges in Assessing LLMs as NLG Evaluators: Biased Response Styles and Multiple Evaluation Metrics. Our DHP Framework employs hierarchical perturbation and statistical tests to address these challenges, offering quantitative discernment scores for effective comparison.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1.  Problem with “raw” alignments  \n    –  Both human annotators and LLM evaluators bring their own response‐style biases (e.g. some always score higher, some always score lower).  \n    –  If you just look at absolute scores or simple correlations against a handful of expert labels, you end up rewarding models whose “taste” happens to match those particular experts, not models that actually understand the content.\n\n2.  Role of hierarchical perturbation (the red‐boxed module)  \n    –  We automatically generate a bunch of systematically degraded versions of each reference (character‐, word-, sentence-level; minor vs. major; rule vs. LLM perturbations).  \n    –  This produces paired data: the original high-quality text and many controlled low-quality variants, each targeting a specific type of error.\n\n3.  Why bring in statistical tests  \n    –  We don’t want our metric to be swamped by whether an LLM is a “harsh grader” or a “lenient grader.”  \n    –  By comparing each original–perturbed pair with a non-parametric test (Wilcoxon Signed-Rank), we ask only: “Does the model reliably give the degraded text a lower score than the pristine text?”  \n    –  The p-value from that test measures confidence in the model’s ability to detect genuine quality drops, completely abstracting away from its absolute scoring scale.\n\n4.  Putting it together  \n    –  Hierarchical perturbation provides the controlled content differences.  \n    –  The statistical test measures consistent, relative score drops.  \n    –  Together, they yield a bias-resistant, content-focused “discernment score,” telling us how well an LLM really recognizes errors rather than how its personal scoring habits line up with a handful of human judges.\n\nAnswer:  \nBecause simple score alignments get distorted by each annotator’s personal scale, we inject a hierarchy of controlled perturbations (the red-boxed module) and then use pairwise statistical tests so that we only judge whether a model consistently rates degraded text lower than original text. This two-step design strips out absolute response‐style bias and focuses squarely on an evaluator’s real ability to detect content problems."
    },
    {
        "question": "How does Hierarchical Perturbation structure minor versus major character, word, and sentence perturbations?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "our approach encompasses three levels of perturbation content: character, word, and sentence levels; two methods of perturbation: rule-based and LLM-based; and two degrees of perturbation: minor and major as illustrated in Figure 3.",
            "First, at the character level, we alter some characters or letters in the given original texts independently. At the word and sentence levels, we degrade the text by processing entire words or sentences, respectively.",
            "Additionally, if the text data is sufficiently long for more perturbation, we implement two degrees of perturbation – minor and major – for each method."
        ],
        "final_answer": "Hierarchical Perturbation works in three content levels (character, word, sentence).  For each level it applies either rule-based or LLM-based methods, and—when the text is long enough—each method is applied in two severity degrees: “minor” perturbations introduce slight changes (e.g. a few characters or words), while “major” perturbations introduce more extensive alterations (e.g. many characters, whole words, or full sentences).",
        "relevant_elements": [
            "Hierarchical Perturbation"
        ],
        "id": 741,
        "masked_question": "How does [mask1] structure minor versus major character, word, and sentence perturbations?",
        "masked_number": 1,
        "masked_elements": [
            "Hierarchical Perturbation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.13704v1_figure_1.png",
        "paperid": "2408.13704v1",
        "paper_path": "./papers/2408.13704v1.json",
        "figure_id": "2408.13704v1_figure_1.png",
        "caption": "Figure 1: Challenges in Assessing LLMs as NLG Evaluators: Biased Response Styles and Multiple Evaluation Metrics. Our DHP Framework employs hierarchical perturbation and statistical tests to address these challenges, offering quantitative discernment scores for effective comparison.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module in the figure is nothing more than our Hierarchical Perturbation pipeline.  In practice it works as follows:\n\n1.  Three Granularity Levels  \n    – Character level: individual letters or code‐points are tweaked.  \n    – Word level: entire tokens are substituted, inserted, deleted or reordered.  \n    – Sentence level: full sentences are paraphrased, shuffled or broken up.  \n\n2.  Two Degrees of Severity  \n    – Minor perturbations: only a handful of characters/words/sentences are changed (e.g. a single typo, one word swap, one slight sentence rephrase).  \n    – Major perturbations: a much larger fraction of the text is corrupted (e.g. many characters mangled, large‐scale word replacements, whole‐sentence rewrites).  \n\n3.  Two Implementation Styles  \n    – Rule-based: applying hand-crafted edit rules (typo tables, simple word lists, sentence-shuffling heuristics).  \n    – LLM-based: prompting a language model to carry out those same edits in a more varied, “natural” way.  \n\nBy combining each of the three levels with both a “light” (minor) and a “heavy” (major) perturbation procedure, we systematically generate a spectrum of degraded versions of every reference.  Competent LLM evaluators should then assign progressively lower scores as the perturbations grow more severe."
    },
    {
        "question": "How does Evaluation + Statistical Test determine significance using p-value thresholds before computing discernment scores?",
        "relevant_section_ids": [
            "4.3.1",
            "4.3.3"
        ],
        "relevant_context": [
            "In our analysis, we adopt a one-sided alternative hypothesis. The resulting p-value indicates the confidence level at which we can reject the null hypothesis—that the original and perturbed score distributions have the same distribution—and accept the alternative hypothesis—that the original scores are greater than the perturbed scores. We consider a difference to be statistically significant if p < 0.05. A lower p-value represents a more significant score difference between the original data and perturbed data.",
            "Here, S_R and S_M are positive values and the higher the better. A value of 1 for S_R and S_M is a threshold corresponding to a p-value of 0.05, indicating statistical significance. If S_R or S_M is less than 1, it means that the LLM evaluators do not assign significantly lower scores to the perturbed data compared to the original data, suggesting a lack of discernment for specific quality issues during the NLG evaluation."
        ],
        "final_answer": "The framework runs a one-sided Wilcoxon Signed-Rank Test on the original versus perturbed scores and treats any test with p < 0.05 as a statistically significant difference. These p-values are then combined (via harmonic mean, optionally weighted by expert votes) and transformed into discernment scores such that a combined p-value of 0.05 maps to a score of 1—scores below that indicate non-significance.",
        "relevant_elements": [
            "Evaluation + Statistical Test",
            "Discernment Score"
        ],
        "id": 742,
        "masked_question": "How does [mask1] determine significance using p-value thresholds before computing discernment scores?",
        "masked_number": 1,
        "masked_elements": [
            "Evaluation + Statistical Test"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.13704v1_figure_1.png",
        "paperid": "2408.13704v1",
        "paper_path": "./papers/2408.13704v1.json",
        "figure_id": "2408.13704v1_figure_1.png",
        "caption": "Figure 1: Challenges in Assessing LLMs as NLG Evaluators: Biased Response Styles and Multiple Evaluation Metrics. Our DHP Framework employs hierarchical perturbation and statistical tests to address these challenges, offering quantitative discernment scores for effective comparison.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module in the figure is nothing more exotic than the Wilcoxon signed‐rank test step of our DHP pipeline.  Concretely, for each metric it takes the paired scores on the original versus the perturbed texts, runs a one‐sided Wilcoxon signed‐rank test, and looks at the resulting p-value.  If that p-value falls below the 0.05 threshold, the difference is deemed “significant.”  Only once you have those per‐metric significance p-values do you go on to combine them (via the harmonic mean, plus any expert weights) and finally turn them into the positive “discernment scores.”"
    },
    {
        "question": "How does the VAE integrate random noise into latent vectors to ensure semantic diversity of synthetic images?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "To generate the synthetic images X, we add some random noise z to the latent vector, where μ and σ are the mean and standard deviation towards the learned representation of the encoder.",
            "The input of our decoder model can be generated by randomly sampling from each latent representation, and the reconstructed output of the decoder D(X) is, where θ is the training parameter of the decoder D."
        ],
        "final_answer": "The VAE injects semantic diversity by sampling noise into its latent codes: the encoder produces a mean μ and standard deviation σ for each dimension, then a random noise vector z is drawn (typically from a standard normal) and combined with μ and σ (i.e. latent = μ + σ·z). This noisy latent vector is then passed through the decoder to yield varied but semantically meaningful synthetic images.",
        "relevant_elements": [
            "VAE",
            "Noise",
            "Latent Vectors"
        ],
        "id": 743,
        "masked_question": "How does the [mask1] integrate random [mask2] into latent vectors to ensure semantic diversity of synthetic images?",
        "masked_number": 2,
        "masked_elements": [
            "VAE",
            "Noise"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.13137v1_figure_2.png",
        "paperid": "2409.13137v1",
        "paper_path": "./papers/2409.13137v1.json",
        "figure_id": "2409.13137v1_figure_2.png",
        "caption": "Fig. 2: Overview of our approach. For a given image, we first use a pre-trained VAE to generate some synthetic images by perturbing the latent vector with random noise. Then, we re-label these synthetic images through a pre-trained CNN into one of two classed by identifying whether their predictions shift.\nFinally, we train a two-class linear model by distilling the soft logits from CNN with these re-labeled synthetic images.\nTherefore, the weights of the trained linear model can mark the location of the important features contributed to its prediction, which could generate a saliency map to interpret the prediction of the image.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ is the VAE generator (the entire red‐boxed module), and Mask₂ is the little strip of “Noise” in the blue box.  What the paper does is exactly the standard VAE re­parameterization trick: during synthesis the encoder produces a mean vector μ and a standard­-deviation vector σ, and then we sample a small random vector ε (Mask₂) from N(0,I) and form the latent code as\n\n    z = μ + σ ⊙ ε.\n\nBecause ε is drawn anew each time, z wanders around μ in all directions allowed by σ, and when you feed these perturbed z’s into the decoder you get different—but still semantically related—images.  Injecting ε in this way guarantees that the synthetic examples cover a neighborhood of the original encoding and thus have semantic diversity."
    },
    {
        "question": "How are the classification loss and distillation loss balanced when training the two-class linear model?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The loss of our proposed re-label distillation to train the student model S can be defined as,  where W denotes the weights of the linear model, α and β are the weight coefficients, and ŷS denotes the prediction of the student model."
        ],
        "final_answer": "When training the two-class linear (student) model, the total loss is formed as a weighted sum of the classification loss and the distillation loss.  Specifically, the classification term is multiplied by a coefficient α and the distillation term by a coefficient β, and the student is trained to minimize α·(classification loss) + β·(distillation loss).",
        "relevant_elements": [
            "Classification Loss",
            "Distillation Loss",
            "Two-class Linear Model"
        ],
        "id": 744,
        "masked_question": "How are the classification loss and distillation loss balanced when training the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Two-class Linear Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.13137v1_figure_2.png",
        "paperid": "2409.13137v1",
        "paper_path": "./papers/2409.13137v1.json",
        "figure_id": "2409.13137v1_figure_2.png",
        "caption": "Fig. 2: Overview of our approach. For a given image, we first use a pre-trained VAE to generate some synthetic images by perturbing the latent vector with random noise. Then, we re-label these synthetic images through a pre-trained CNN into one of two classed by identifying whether their predictions shift.\nFinally, we train a two-class linear model by distilling the soft logits from CNN with these re-labeled synthetic images.\nTherefore, the weights of the trained linear model can mark the location of the important features contributed to its prediction, which could generate a saliency map to interpret the prediction of the image.",
        "qtype": "Implementation_Details",
        "response": "The little red box is around the “Two‐class Linear Model” – i.e. the student we are fitting.  If you look at the loss they give for training that model (just below Eqn. (14) in the text), you’ll see it is literally a weighted sum of\n\n  • the ordinary two‐class (cross‐entropy) classification loss, and  \n  • the distillation (logit‐matching) loss  \n\nThey introduce two scalars, α and β, and form\n\n   L_total(w) = α·L_cls(y′, ŷ_s)  +  β·L_distill(z_t, z_s)\n\nso by tuning α versus β you “balance” how much emphasis goes on matching the hard labels (classification loss) versus matching the teacher’s soft logits (distillation loss)."
    },
    {
        "question": "How does distillation loss guide the two-class linear model to mimic CNN logits for interpretable explanations?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Then, we train a two-class linear model with these re-labeled synthetic images by distilling the soft knowledge of the deep network.",
            "The loss of our proposed re-label distillation to train the student model \\(S\\) can be defined as, where \\(w\\) denotes the weights of the linear model, \\(\\alpha\\) and \\(\\beta\\) are the weight coefficients, and \\(\\hat y=S(x;w)\\) denotes the prediction of the student model.",
            "The trained linear model establishes an interpretable relation between the prediction and the input. The weights \\(w\\) could measure the significance of different pixels contributed to its prediction."
        ],
        "final_answer": "The distillation loss term forces the two-class linear student model to match its output distribution on re-labeled synthetic samples to the pre-trained CNN’s logits. By minimizing this loss, the student’s linear weights are adjusted so that its predictions align with the teacher’s soft outputs around the decision boundary. As a result, the learned weight vector approximates the CNN’s logit responses and can be directly interpreted as a saliency map highlighting the features that drove the CNN’s decision.",
        "relevant_elements": [
            "distillation loss",
            "CNN logits",
            "two-class linear model"
        ],
        "id": 745,
        "masked_question": "How does [mask1] guide the two-class linear model to mimic CNN logits for interpretable explanations?",
        "masked_number": 1,
        "masked_elements": [
            "distillation loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.13137v1_figure_2.png",
        "paperid": "2409.13137v1",
        "paper_path": "./papers/2409.13137v1.json",
        "figure_id": "2409.13137v1_figure_2.png",
        "caption": "Fig. 2: Overview of our approach. For a given image, we first use a pre-trained VAE to generate some synthetic images by perturbing the latent vector with random noise. Then, we re-label these synthetic images through a pre-trained CNN into one of two classed by identifying whether their predictions shift.\nFinally, we train a two-class linear model by distilling the soft logits from CNN with these re-labeled synthetic images.\nTherefore, the weights of the trained linear model can mark the location of the important features contributed to its prediction, which could generate a saliency map to interpret the prediction of the image.",
        "qtype": "Literature_Background",
        "response": "The mystery “red‐boxed” module is nothing exotic at all but simply the distillation loss that sits between the CNN’s output and our little two–class linear model.  In practice what happens is this:\n\n  1.  We generate a batch of synthetic samples around our original image via the VAE.\n  2.  We run those same samples through the frozen, pre-trained CNN (“teacher”) to get its soft‐logit outputs.\n  3.  We also run them through our two–class linear model (“student”) and compute two things in the loss:\n       ·  A standard cross‐entropy against the hard 0/1 re-labels to make sure the student knows which side of the boundary it lives on.\n       ·  The red-boxed distillation term, which is typically a KL‐divergence or L₂ penalty between the student’s pre-sigmoid scores and the teacher’s logits (or softened probabilities).\n\nBy adding that second term we are literally forcing the linear model’s scores to line up with the CNN’s scores on every synthetic point.  In so doing the weight vector of the student is “pulled” into the same local decision boundary as the CNN.  Since the student is just a linear mapping from pixels to logit, its learned weights can then be read off as a saliency map, thus yielding a pixel–wise, interpretable explanation of what the CNN was looking at."
    },
    {
        "question": "How does vector quantization refine reparametrized embeddings differently from VQ-VAE’s discrete codebook approach?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Vector Quantization",
            "Reparametrization"
        ],
        "id": 746,
        "masked_question": "How does [mask1] refine [mask2] embeddings differently from VQ-VAE’s discrete codebook approach?",
        "masked_number": 2,
        "masked_elements": [
            "Vector Quantization",
            "Reparametrization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.13342v1_figure_1.png",
        "paperid": "2410.13342v1",
        "paper_path": "./papers/2410.13342v1.json",
        "figure_id": "2410.13342v1_figure_1.png",
        "caption": "Figure 1: Architecture of DART including encoder, the ML-VAE, VQ, variance adapter, and decoder.",
        "qtype": "Literature_Background",
        "response": "Here’s the crux of what’s happening in DART, cast against the vanilla VQ–VAE story:\n\n1.  In a standard VQ–VAE you have  \n    –  an encoder that spits out a continuous vector y  \n    –  a single, “flat” codebook of K prototype vectors  \n    –  a hard‐assignment yq = nearest_codebook(y) whose only job is to discretize y and impose a bottleneck.  \n\n2.  In DART, by contrast, we do two things differently:  \n    a)  We first go through a full variational reparameterization step –  \n        z = μ + σ ⊙ ε,  ε∼N(0,1)  –  separately for speaker and for accent.  That gives us two *continuous* latents, zs and za, each already disentangled by the ML-VAE grouping priors.  \n    b)  **Then** we feed each of those Gaussians into *its own* VQ module (the red‐boxed blocks).  Each VQ has its own codebook (one for speaker, one for accent), its own commitment loss, its own prototype vectors.  \n\n3.  Why does that make a difference?  \n    –  Because the reparameterization step (blue box) is still in play you retain all the benefits of the VAE — uncertainty modeling, smooth interpolation, hierarchical grouping.  \n    –  The VQ step now *filters* that rich, disentangled latent through a discrete bottleneck *per factor*, snapping zs and za to the nearest speaker or accent code.  \n    –  In vanilla VQ–VAE you lose that factorized structure and you quantize raw encoder outputs; here you quantize *after* you’ve explicitly teased apart speaker from accent, giving you two small, targeted codebooks rather than one big undifferentiated one.  \n\nBottom line: The DART VQ modules don’t just replace your encoder activations with discrete codes. They *refine* a pre-disentangled, reparameterized latent (μ+σ⊙ε) into separate speaker‐ and accent‐specific codewords, preserving the VAE’s probabilistic structure upstream while still reaping the information-bottleneck benefits of vector quantization."
    },
    {
        "question": "How does the Variance Adapter leverage accent embeddings from ML-VAE relative to Fastspeech2’s variance modeling?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Variance Adapter",
            "ML-VAE"
        ],
        "id": 747,
        "masked_question": "How does the [mask1] leverage accent embeddings from ML-VAE relative to Fastspeech2’s variance modeling?",
        "masked_number": 1,
        "masked_elements": [
            "Variance Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.13342v1_figure_1.png",
        "paperid": "2410.13342v1",
        "paper_path": "./papers/2410.13342v1.json",
        "figure_id": "2410.13342v1_figure_1.png",
        "caption": "Figure 1: Architecture of DART including encoder, the ML-VAE, VQ, variance adapter, and decoder.",
        "qtype": "Literature_Background",
        "response": "In our DART setup the red‐boxed “Variance Adapter” is exactly the place where we inject the disentangled accent codes coming out of ML-VAE + VQ, whereas in vanilla FastSpeech2 the variance adaptor only ever sees phoneme‐level features (and perhaps a global style‐token or speaker embedding) and learns duration, pitch and energy in a completely undifferentiated way.  Concretely, during synthesis DART first runs the reference encoder → ML-VAE → VQ to produce a quantized accent vector zaq (and speaker vector zsq).  We then concatenate (or project and add) zaq (and zsq) into the inputs of FastSpeech2’s duration/pitch/energy predictors (the Variance Adapter block).  In this way the Variance Adapter is no longer “just” predicting prosodic variables from text but doing so under the explicit control of an accent embedding, giving us fine‐grained, disentangled accent modulation that vanilla FastSpeech2 does not have."
    },
    {
        "question": "How does vector quantization bottleneck enhance disentanglement in ML-VAE encoding?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The reparametrized speaker $z_s$ and grouped accent $z_a$ representations pass through the VQ layer, acting as a bottleneck (van2017neural), filtering out irrelevant information.",
            "This integration improves accent conversion and preserves key information by effectively disentangling speaker and accent attributes.",
            "The VQ block incorporates an information bottleneck, ensuring effective utilization of codebooks."
        ],
        "final_answer": "By inserting a VQ layer as an information bottleneck on the speaker and accent latents, irrelevant details are filtered out. This forces the model to commit to a limited set of discrete codes, which in turn encourages the separation of speaker and accent factors and preserves only the information critical for each attribute’s representation.",
        "relevant_elements": [
            "Vector Quantization",
            "ML-VAE"
        ],
        "id": 748,
        "masked_question": "How does [mask1] bottleneck enhance disentanglement in ML-VAE encoding?",
        "masked_number": 1,
        "masked_elements": [
            "Vector Quantization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.13342v1_figure_1.png",
        "paperid": "2410.13342v1",
        "paper_path": "./papers/2410.13342v1.json",
        "figure_id": "2410.13342v1_figure_1.png",
        "caption": "Figure 1: Architecture of DART including encoder, the ML-VAE, VQ, variance adapter, and decoder.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the red‐boxed “bottleneck” (the pair of vector-quantization modules) sharpens the separation of speaker and accent factors in the ML-VAE encoder:\n\n1. Discrete information bottleneck  \n   • After the ML-VAE reparametrization produces two continuous vectors zₛ (speaker) and zₐ (accent), each is forced to “snap” to the nearest entry in its own codebook.  \n   • By restricting each latent to choose from a small, fixed set of prototypes, anything that doesn’t fit neatly into a speaker code or an accent code is thrown away.  \n\n2. Factor-specific codebooks  \n   • There are two independent VQ codebooks—one dedicated solely to speaker variation, the other solely to accent variation.  \n   • This architectural split makes it much harder for accent cues to leak into the speaker latent (and vice versa), because each pathway has its own quantizer.  \n\n3. Commitment loss & KL regularization  \n   • A commit­ment loss (‖sg[z]−e‖²) forces the continuous encoder outputs to stay close to their chosen codebook entries, which stabilizes training and prevents “drift.”  \n   • The usual VAE KL term further keeps each latent space compact, discouraging the model from encoding extra factors in either bottleneck.  \n\nResult: by squeezing the continuous latents through two small, discrete “gates,” the VQ bottleneck filters out nuisance variation and compels the network to parcel speaker-specific versus accent-specific information into the two separate latents. This yields much cleaner disentanglement than a purely Gaussian VAE would."
    },
    {
        "question": "How does the variance adapter utilize quantized latent embeddings to adjust prosody before mel decoding?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Variance Adapter",
            "Mel Decoder"
        ],
        "id": 749,
        "masked_question": "How does the [mask1] utilize quantized latent embeddings to adjust prosody before mel decoding?",
        "masked_number": 1,
        "masked_elements": [
            "Variance Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.13342v1_figure_1.png",
        "paperid": "2410.13342v1",
        "paper_path": "./papers/2410.13342v1.json",
        "figure_id": "2410.13342v1_figure_1.png",
        "caption": "Figure 1: Architecture of DART including encoder, the ML-VAE, VQ, variance adapter, and decoder.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module in Figure 1 is exactly the “Variance Adapter” from FastSpeech 2.  Its job in vanilla FastSpeech2 is to take the phoneme‐level encoder outputs and predict (and then add back in) three kinds of prosodic information—duration, pitch, and energy—before passing the result on to the Mel decoder.  \n\nIn DART, we now have two extra streams of information coming out of the ML-VAE + VQ encoder:  \n  •  \\(z_s^q\\), a discretized speaker embedding  \n  •  \\(z_a^q\\), a discretized accent embedding  \n\nThese two quantized vectors are fed directly into the Variance Adapter and used as conditioning for all of its prosody predictors.  Concretely, at each phoneme position the adapter:  \n  1.  Broadcasts (or projects) \\(z_s^q\\) and \\(z_a^q\\) to match the encoder hidden‐state dimension,  \n  2.  Adds or concatenates them to the encoder output,  \n  3.  Feeds that “augmented” representation into the duration, pitch, and energy predictors.  \n\nBecause those predictors now see the discrete speaker/accent codes, they learn to output timing, fundamental‐frequency, and intensity curves that carry the style information encoded in the VQ codebooks.  Once the adapter has produced those prosodic adjustments (duration stretches, F0 contours, energy contours), it applies them back onto the hidden sequence and hands the result to the Mel decoder, which finally generates a spectrogram that reflects the desired prosody of both speaker identity and accent."
    },
    {
        "question": "How do Duration Predictor and Intensity Predictor coordinate to enhance temporal and amplitude accuracy of gestural scores?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Duration Predictor",
            "Intensity Predictor"
        ],
        "id": 750,
        "masked_question": "How do [mask1] and [mask2] coordinate to enhance temporal and amplitude accuracy of gestural scores?",
        "masked_number": 2,
        "masked_elements": [
            "Duration Predictor",
            "Intensity Predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.16221v3_figure_2.png",
        "paperid": "2408.16221v3",
        "paper_path": "./papers/2408.16221v3.json",
        "figure_id": "2408.16221v3_figure_2.png",
        "caption": "Figure 2: SSDM architecture",
        "qtype": "Experimental_Results",
        "response": "chain‐of‐thought (abbreviated):\n\n1. In Fig. 2 the red box sits over the Duration Predictor and the blue box over the Intensity Predictor.  \n2. The Duration Predictor outputs a discrete duration distribution (via Gumbel-softmax) for each gesture patch, fixing its temporal extent.  \n3. The Intensity Predictor emits a positive amplitude weight (via a sigmoid) for that same patch, modulating how strongly the gesture is activated.  \n4. Both outputs enter a local Hann‐window smoothing and are multiplicatively combined into a “duration × intensity” score.  \n5. A sparse mask then selects the top-ranked patches, yielding gestural scores that precisely capture both when (time boundary) and how much (amplitude) each articulatory gesture occurs.  \n\nAnswer:\nmask1 = Duration Predictor  \nmask2 = Intensity Predictor  \nThey coordinate by first predicting per-gesture durations to nail down the timing and then assigning per-gesture intensities to scale the amplitude. These two streams are smoothed together (via a Hann window), combined into a unified score, and sparsely sampled, thereby producing gestural representations with high temporal precision and correct amplitude weighting."
    },
    {
        "question": "How does Connectionist Subsequence Aligner output guide Multimodal Tokenizer to encode dysfluency-aware tokens?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Connectionist Subsequence Aligner",
            "Multimodal Tokenizer"
        ],
        "id": 751,
        "masked_question": "How does [mask1] output guide Multimodal Tokenizer to encode dysfluency-aware tokens?",
        "masked_number": 1,
        "masked_elements": [
            "Connectionist Subsequence Aligner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.16221v3_figure_2.png",
        "paperid": "2408.16221v3",
        "paper_path": "./papers/2408.16221v3.json",
        "figure_id": "2408.16221v3_figure_2.png",
        "caption": "Figure 2: SSDM architecture",
        "qtype": "Experimental_Results",
        "response": "Here is the intuition in three quick steps:\n\n1.  What lives in the red‐box?  \n    Mask1 is the Connectionist Subsequence Aligner (CSAθ), which takes  \n      •  τ, the sequence of gestural (phoneme) embeddings,  \n      •  C, the sequence of text‐token embeddings  \n    and produces a *time‐synchronous subsequence alignment* between the two—i.e. it tells you, for each text token, which phoneme‐frames actually “land” on it (including repeats, deletions, insertions, blocks, etc.).\n\n2.  What does the Multimodal Tokenizer need?  \n    The Multimodal Tokenizer’s job is to turn “plain text + phoneme‐alignment + dysfluency events” into a single stream of tokens that the downstream LLM (LLaMA) can consume.  Concretely it wants, for each text token,  \n      – the *identity* of the token itself,  \n      – *where* (in time) it actually occurred, and  \n      – *what kind* of dysfluency (if any) happened at that point.\n\n3.  How CSA’s output drives tokenization  \n    •  CSA spits out an explicit mapping from text tokens → phoneme‐frame indices + dysfluency flags.  \n    •  The tokenizer reads that mapping and, for each text token, it “fuses”  \n       – the original text embedding,  \n       – a learned embedding for the dysfluency type (stutter, insertion, block, etc.), and  \n       – a learned positional/time embedding derived from the aligned frame indices.  \n    •  The result is a single *multimodal* token whose id captures *all three* pieces of information.  Those enriched tokens—not just “foo,” but “foo + stutter @ 0.60 s,” or “bar + deletion @ 2.92 s”—are what gets fed into the LLM.\n\nIn short, the CSA alignment (mask1) converts raw phoneme streams and reference text into a subsequence that *marks exactly where* and *what kind* of dysfluency occurred.  The Multimodal Tokenizer then uses that alignment to package each text unit together with its timing and dysfluency tag into one fused token, giving the LLM “dysfluency-aware” input."
    },
    {
        "question": "What are the potential scalability limitations of Self-Distillation between Acoustic Encoder and Gestural Scores?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Self-Distillation",
            "Acoustic Encoder",
            "Gestural Scores"
        ],
        "id": 752,
        "masked_question": "What are the potential scalability limitations of [mask1] between Acoustic Encoder and Gestural Scores?",
        "masked_number": 1,
        "masked_elements": [
            "Self-Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.16221v3_figure_2.png",
        "paperid": "2408.16221v3",
        "paper_path": "./papers/2408.16221v3.json",
        "figure_id": "2408.16221v3_figure_2.png",
        "caption": "Figure 2: SSDM architecture",
        "qtype": "Others",
        "response": "Here is a step‐by‐step reasoning of why that little “Self-Distillation” block becomes a scalability choke-point as you pour more and more data through the system:\n\n1.  It doubles up on heavy latent‐space inference.  \n    – Every training example must go through the acoustic encoder → adaptor to produce an “acoustic-conditioned” gestural posterior  *and* through the text encoder + priors to produce a “text-conditioned” gestural posterior .  \n    – You then have to evaluate two KL-divergences (acoustic → gestural and text → gestural) at every time step and back-propagate through *both* pathways.\n\n2.  It adds significant compute and memory overhead.  \n    – Sampling, reparameterizing and distilling two distributions for each patch in the utterance inflates both peak memory (to hold all the intermediate posteriors) and total FLOPs.  \n    – On very large or long utterances, the double forward/backward pass can become the dominant cost.\n\n3.  It depends on high-quality forced alignments and a fixed AAI teacher.  \n    – The acoustic-conditioned posterior is only as good as the pretrained inversion model and the downstream adaptor. As you scale to new speakers, accents or languages, the fixed AAI model may fail to cover the full variability—so simply adding more data no longer moves the needle.  \n    – Likewise, the text-conditioned prior relies on reasonably accurate alignments; when you crank up data volume, alignment errors compound, and the distillation signal becomes noisy.\n\n4.  It requires careful balancing of multiple KL weights.  \n    – Scaling to larger, more diverse corpora generally forces you to retune the loss weights between the acoustic-distill KL and the text-distill KL. Without that, one modality will dominate and the gestural encoder collapses.\n\n5.  You hit diminishing returns.  \n    – Once the gestural encoder has “matched” both teacher posteriors to within a small tolerance, throwing more data at those same distillation objectives leads to almost zero further gain.  \n\nIn short, although self-distillation is crucial for grounding the gestural VAE in both acoustic and textual space, it introduces a fundamentally *non-linear* compute/memory bottleneck and sensitivity to alignment quality.  Beyond a few hundred hours of speech, it tends to saturate and become the main obstacle to scaling up further."
    },
    {
        "question": "How might convergence of the Pareto Selection process be impacted by high-dimensional LLM parameter spaces?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pareto Selection"
        ],
        "id": 754,
        "masked_question": "How might convergence of the [mask1] process be impacted by high-dimensional LLM parameter spaces?",
        "masked_number": 1,
        "masked_elements": [
            "Pareto Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.14500v1_figure_1.png",
        "paperid": "2411.14500v1",
        "paper_path": "./papers/2411.14500v1.json",
        "figure_id": "2411.14500v1_figure_1.png",
        "caption": "Figure 1: Overview of our framework.",
        "qtype": "Others",
        "response": "Here is a step-by-step unpacking of the question and how we arrive at the answer:\n\n1.   Identify [mask1] in the figure.  \n     •  The red box in the top-right of Fig. 1 is labeling the “Pareto Selection” step, i.e. the operation that takes the newly generated and evaluated offspring together with the current population and selects the non-dominated (Pareto‐optimal) models to carry forward.\n\n2.   Recall what “Pareto Selection” does.  \n     •  After every generation of crossover/mutation and multi‐objective evaluation, Pareto Selection identifies those models that are not strictly dominated by any other on the two objectives (error and fairness).  \n     •  In most MOEAs this involves sorting by dominance, computing crowding‐distances (or hypervolume contributions), and then picking the best N individuals.\n\n3.   Consider the impact of very high-dimensional LLM parameter spaces on this step.  \n     a)  Search‐space explosion  \n         –  LLMs have hundreds of millions to billions of parameters.  That makes the space of possible models astronomically large.  \n         –  Even if we only track two objectives, the population is exploring variations in a huge parameter manifold.  \n     b)  Slower convergence  \n         –  Because there are so many ways to perturb weights and so many local Pareto‐frontiers hidden in the high-dimensional landscape, it takes many more generations (and a far larger population) to “fill out” and refine the true Pareto front.  \n         –  The dominance relations themselves become “noisier” as small parameter tweaks can cause tiny objective changes, so sorting and crowding‐distance computations become less discriminating.  \n     c)  Computational burden  \n         –  Every candidate in a generation must be fully evaluated (forward/backward) to yield its accuracy and fairness scores.  Multiply that cost by population size × number of generations and complexity skyrockets.  \n         –  Pareto sorting is itself O(N²) or worse in many implementations, so high N (needed to cover the space) makes even the selection step heavy.  \n     d)  Risk of premature convergence or loss of diversity  \n         –  With so many degrees of freedom, it is easy for the algorithm to overcommit to a small niche of the front, discarding individuals that might explore other trade-off regions.  \n         –  Standard crowding or diversity‐maintenance schemes may not suffice in billions-dimensional weight spaces.\n\n4.   Summarize the effect.  \n     •  In practice, when you raise the dimensionality of the model parameters by orders of magnitude, the Pareto Selection step will converge far more slowly, require much larger populations and generation counts, and incur a huge computational cost per generation.  If unmitigated, you either end up with an extremely noisy/undersampled Pareto front or you must resort to specialized techniques (surrogate models, subspace projections, incremental fronts, etc.) to get anything resembling a robust, well-converged frontier.\n\nAnswer  \nBecause Pareto Selection must repeatedly sort and trim a large population of candidate LLMs in order to build up the non-dominated front, embedding it into a space of hundreds of millions or billions of parameters has two direct consequences:  \n 1) Convergence slows dramatically (you need far more generations and much larger populations to adequately explore and tighten up the true Pareto frontier), and  \n 2) the computational cost per selection grows (both because each model evaluation is expensive and because dominance‐sorting scales poorly with population size).  \n\nIn other words, high-dimensional LLM parameter spaces make the Pareto Selection step both much slower to converge and much more expensive to compute, often forcing researchers to adopt additional tricks (e.g., surrogate objectives or dimensionality‐reduction on the search space) to get reasonable Pareto fronts within practical budgets."
    },
    {
        "question": "What motivates leveraging Gaussian noise mutation to enhance exploration in diversity generation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To further improve the diversity and exploration of LLMs, we incorporate Gaussian noise as the mutation strategy.",
            "Studies Wu et al. (2022  ###reference_b31###) have shown that introducing Gaussian noise during LLM training helps models escape local optima, thereby boosting their performance."
        ],
        "final_answer": "The use of Gaussian noise mutation is motivated by evidence that adding Gaussian noise during LLM training helps models escape local optima and thus boosts their performance, enhancing exploration in the diversity generation process.",
        "relevant_elements": [
            "Gaussian Noise",
            "Mutation"
        ],
        "id": 756,
        "masked_question": "What motivates leveraging [mask1] mutation to enhance exploration in diversity generation?",
        "masked_number": 1,
        "masked_elements": [
            "Gaussian Noise"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.14500v1_figure_1.png",
        "paperid": "2411.14500v1",
        "paper_path": "./papers/2411.14500v1.json",
        "figure_id": "2411.14500v1_figure_1.png",
        "caption": "Figure 1: Overview of our framework.",
        "qtype": "Design_Rationale",
        "response": "Chain of thought:  \n1. In the FGDG (Fairness-Guided Diversity Generation) module, aside from crossover, there is a mutation operator highlighted in red.  \n2. The annotation in that red box reads “Gaussian Noise.”  \n3. The text explains that we “incorporate Gaussian noise as the mutation strategy” because prior work (e.g. Wu et al. 2022) has shown that adding Gaussian perturbations during LLM training helps models escape local optima.  \n4. By nudging the model parameters with random Gaussian noise, the evolutionary process can explore new regions of the parameter space—improving diversity and avoiding premature convergence.  \n\nAnswer:  \nThey inject Gaussian noise as the mutation so that offspring LLMs can escape local optima and explore a broader region of the parameter space, which boosts diversity and ultimately yields more robust, higher-performing models."
    },
    {
        "question": "What motivates conducting objective evaluation prior to fitness evaluation to guide Pareto selection?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Objective evaluation focuses on assessing specific metrics, such as accuracy and fairness, tailored to the particular needs identified by decision-makers within the domain of application.",
            "Fitness evaluation, however, involves ranking LLMs based on the outcomes of their objective evaluations. This is typically done using a multi-objective optimiser … which assigns each LLM a fitness value by evaluating their performance considering the defined objectives …"
        ],
        "final_answer": "Conducting objective evaluation first provides clear, per-model metric values for each chosen objective (e.g. accuracy and fairness). These objective scores are then used as the inputs to the multi-objective optimizer’s fitness evaluation, enabling a proper Pareto-based ranking and selection of models.",
        "relevant_elements": [
            "Objective Evaluation",
            "Fitness Evaluation"
        ],
        "id": 757,
        "masked_question": "What motivates conducting [mask1] prior to [mask2] to guide Pareto selection?",
        "masked_number": 2,
        "masked_elements": [
            "Objective Evaluation",
            "Fitness Evaluation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.14500v1_figure_1.png",
        "paperid": "2411.14500v1",
        "paper_path": "./papers/2411.14500v1.json",
        "figure_id": "2411.14500v1_figure_1.png",
        "caption": "Figure 1: Overview of our framework.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition in a nutshell:\n\n1. What the red box ([mask1]) does:  \n   “Objective Evaluation” applies your chosen accuracy and fairness criteria (precision, recall, F1, statistical parity, equal opportunity, etc.) to every LLM in your population and spits out raw metric values.\n\n2. What the blue box ([mask2]) does:  \n   “Fitness Evaluation” takes those metric values, feeds them into a multi‐objective optimizer (e.g. an MOEA), and produces a fitness ranking (who’s best, who’s worst) so that you can pull off a Pareto‐front selection.\n\nWhy do you run the objective evaluation first? Because you need concrete, model-specific measurements of both accuracy and fairness before you can meaningfully rank or compare LLMs in a Pareto sense. Without step 1 you wouldn’t have the numbers that step 2 needs to assign fitness and guide the Pareto selection."
    },
    {
        "question": "What motivated using Semantic Human Parsing and ID-Preserved Masking for targeted clothes inpainting?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "However, diffusion models often struggle with preserving the intricate details of a person’s identity during image generation (see Fig. 1). We overcome this by leveraging a human parsing method [26] to produce binary masks that mark only the clothing regions in an image. We then use this binary mask to retain ID-specific portions in an image during diffusion inpainting, such as the face, hair, and body shape, thereby only augmenting the subject’s clothes.",
            "While generating different images of a subject, it is important to preserve the person-relevant information. However, diffusion models struggle to retain this information due to the varied and complex nature of human faces and body shapes. Thus, we apply a semantic human parsing method [26] to each image, x_i, to obtain an ID-preserving binary mask, m_i, where only the upper-clothes, lower-clothes, and footwear regions are marked for inpainting. ... This allows the person (s_i) features, such as the face, hair, and general body structure, to be preserved in the generated samples, even after clothes inpainting—an especially critical aspect for enriching CC-ReID data."
        ],
        "final_answer": "They found that text-guided diffusion inpainting often alters or loses identity-specific details (face, hair, body shape). To prevent this and only modify clothing, they use semantic human parsing to create an ID-preserving mask that marks just the clothing regions for inpainting, thus preserving the person’s identity features while changing outfits.",
        "relevant_elements": [
            "Semantic Human Parsing",
            "ID-Preserved Masking"
        ],
        "id": 758,
        "masked_question": "What motivated using [mask1] and [mask2] for targeted clothes inpainting?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic Human Parsing",
            "ID-Preserved Masking"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.07205v2_figure_2.png",
        "paperid": "2411.07205v2",
        "paper_path": "./papers/2411.07205v2.json",
        "figure_id": "2411.07205v2_figure_2.png",
        "caption": "Figure 2: Overview of DLCR data generation (stage 1). Given a training image xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an ID-preserving mask, misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where only the clothing regions are marked for inpainting. A target clothing ID, c𝑐citalic_c, is randomly selected from the training dataset and a textual description of the clothing items belonging to c𝑐citalic_c is extracted. Specifically, we use LLaVA to extract frame-wise descriptions for every image in Xcsuperscript𝑋𝑐X^{c}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, and use LLaMA to obtain a summarized description Rcsuperscript𝑅𝑐R^{c}italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT. This description is used as a prompt for diffusion inpainting to artificially change a subject’s clothing. Our ID-preserving diffusion inpainting model\nsynthesizes quality generated data x^0subscript^𝑥0\\hat{x}_{0}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for improved downstream CC-ReID training.",
        "qtype": "Design_Rationale",
        "response": "The two modules were introduced simply to give us fine‐grained control over “where” we let the diffusion model repaint, and to make sure that only the clothes get changed while everything that carries identity (face, hair, body shape…) stays untouched. \n\n1. Mask₁ (the red box: Semantic Human Parsing) is just a fast off-the-shelf human segmenter that tells us, pixel by pixel, which parts of the image are upper‐clothes, lower‐clothes, shoes, etc.  \n2. Mask₂ (the blue box: ID-Preserved Masking) then takes exactly those clothing regions and turns them into a binary “inpainting” mask, so when we run the text-conditioned diffusion step we only corrupt and repaint the clothes—and leave all the rest of the person perfectly intact.  \n\nWithout that two–step parse-and-mask pipeline, a vanilla diffusion model would happily wash out or hallucinate your subject’s face, hair and body shape while trying to change their outfit. By parsing first and then masking just the clothes, we generate many new outfits per person but never lose the identity that a Re-ID network must learn to recognize."
    },
    {
        "question": "What reasoning supports extracting clothes descriptions from LLaVA before summarizing with LLaMA?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "While CC-ReID datasets provide clothes IDs, they are usually simple scalar values that do not contain descriptive information as to what specific clothing items are present in an image.",
            "A naive approach would be to simply generate descriptions of clothing items using an LLM, e.g. LLaMA, or to create random clothing descriptions. While this is likely to increase the diversity of the dataset, and consequently, the generalization capacity of the downstream CC-ReID models, it does not alleviate dataset-specific biases.",
            "We use LLaVA in order to obtain descriptions of clothing items that are present in the dataset, aiming to reduce the dataset-specific bias (see supplementary). This forces the CC-ReID models to focus on identity features, and ignore clothing features.",
            "However, the subject’s visibility can vary across images of a particular clothes ID, C, due to changes in occlusions, lighting, or viewpoints. This can lead LLaVA to occasionally respond with missing or incorrect clothing items on certain images of X^c.",
            "To mitigate this issue, we pass the image-based responses, R^c, as input to LLaMA and prompt it to summarize them into a single clothing description. Consequently, we obtain a holistic clothes description, \\hat{r}^c, for a particular clothes ID c. Through this summarization, LLaMA helps to produce accurate clothing descriptions and overcomes the issue of missing clothing items."
        ],
        "final_answer": "By first using LLaVA to extract descriptions grounded in the actual images, the method avoids random or biased text prompts and captures dataset-specific clothing details. Then, because individual LLaVA outputs can be incomplete or noisy (due to occlusions, lighting changes, etc.), these multiple image-level descriptions are summarized with LLaMA into a single holistic description, ensuring accuracy and completeness.",
        "relevant_elements": [
            "LLaVA",
            "LLaMA"
        ],
        "id": 759,
        "masked_question": "What reasoning supports extracting clothes descriptions from [mask1] before summarizing with LLaMA?",
        "masked_number": 1,
        "masked_elements": [
            "LLaVA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.07205v2_figure_2.png",
        "paperid": "2411.07205v2",
        "paper_path": "./papers/2411.07205v2.json",
        "figure_id": "2411.07205v2_figure_2.png",
        "caption": "Figure 2: Overview of DLCR data generation (stage 1). Given a training image xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an ID-preserving mask, misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where only the clothing regions are marked for inpainting. A target clothing ID, c𝑐citalic_c, is randomly selected from the training dataset and a textual description of the clothing items belonging to c𝑐citalic_c is extracted. Specifically, we use LLaVA to extract frame-wise descriptions for every image in Xcsuperscript𝑋𝑐X^{c}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, and use LLaMA to obtain a summarized description Rcsuperscript𝑅𝑐R^{c}italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT. This description is used as a prompt for diffusion inpainting to artificially change a subject’s clothing. Our ID-preserving diffusion inpainting model\nsynthesizes quality generated data x^0subscript^𝑥0\\hat{x}_{0}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for improved downstream CC-ReID training.",
        "qtype": "Design_Rationale",
        "response": "Here is the chain of reasoning that ties the red‐boxed step (“Extract the clothing items from the following text:”) to the later LLaMA summarization and why that extraction is crucial:\n\n1.  Image–level noise and missing items  \n    •  We first run every image of clothes‐ID c through LLaVA, producing a loose, free‐form sentence for each image (e.g. “The person is wearing a striped black and white t-shirt, black pants and black shoes.”).  \n    •  Because of occlusions, lighting and viewpoint changes, some of those sentences may omit items or add spurious details.\n\n2.  Need for a clean, consolidated description  \n    •  If we simply shoved all of those raw LLaVA sentences into LLaMA and asked it to “summarize,” the model might latch onto stylistic words, filler phrases, or hallucinate items that only appear once.  \n    •  We want a single, accurate list of actual clothing pieces for that clothes‐ID, not a prose paragraph.\n\n3.  Extraction before summarization  \n    •  By first prompting LLaMA with “Extract the clothing items from the following text: …” (the red box), we force LLaMA to filter out everything except the concrete items (e.g. “black and white striped t-shirt, black pants, black shoes”).  \n    •  This yields a distilled list of garments, stripped of any filler or stylistic variations.\n\n4.  Robust summarization step  \n    •  Now that each image’s description has been converted into a uniform item list, we can feed all of those lists back into LLaMA for a final “summarize across images” prompt.  \n    •  Because its input is now a clean set of garment‐only lists, LLaMA can reliably produce one holistic, bias-free description Rᶜ for that clothes‐ID.  \n\nIn short, the red‐boxed extraction step removes noise and variation so that the subsequent summarization by LLaMA yields an accurate, consistent clothing description to drive the inpainting model."
    },
    {
        "question": "How does Clothes Description Extraction combine LLaVA responses and LLaMA summarization into a single clothes description?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "First, we prompt LLaVA to obtain the information of the clothing items for the upper and lower body, as well as the footwear a person is wearing in each image x_i. We denote the set of text responses obtained from LLaVA as r_i^c, where r_i^c is the clothes description of image x_i.",
            "However, the subject’s visibility can vary across images of a particular clothes ID, C, due to changes in occlusions, lighting, or viewpoints. This can lead LLaVA to occasionally respond with missing or incorrect clothing items on certain images of C. To mitigate this issue, we pass the image-based responses, r_i^c, as input to LLaMA and prompt it to summarize them into a single clothing description. Consequently, we obtain a holistic clothes description, R^c, for a particular clothes ID c.",
            "We further post-process R^c if there are multiple clothing descriptions obtained from LLaMA for a particular body part. We keep the most frequently occurring descriptions across the images of C."
        ],
        "final_answer": "For each clothes ID, DLCR first collects image‐level clothing descriptions r_i^c from the visual-language model LLaVA across all images of that ID. It then feeds this set of LLaVA outputs into the large language model LLaMA, prompting it to summarize them into a single, holistic description R^c. Finally, if LLaMA produces multiple variants for a specific garment (e.g., upper body), the most frequent item is chosen to form the final clothes description.",
        "relevant_elements": [
            "Clothes Description Extraction",
            "LLaVA",
            "LLaMA"
        ],
        "id": 760,
        "masked_question": "How does [mask1] combine [mask2] responses and LLaMA summarization into a single clothes description?",
        "masked_number": 2,
        "masked_elements": [
            "Clothes Description Extraction",
            "LLaVA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.07205v2_figure_2.png",
        "paperid": "2411.07205v2",
        "paper_path": "./papers/2411.07205v2.json",
        "figure_id": "2411.07205v2_figure_2.png",
        "caption": "Figure 2: Overview of DLCR data generation (stage 1). Given a training image xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an ID-preserving mask, misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where only the clothing regions are marked for inpainting. A target clothing ID, c𝑐citalic_c, is randomly selected from the training dataset and a textual description of the clothing items belonging to c𝑐citalic_c is extracted. Specifically, we use LLaVA to extract frame-wise descriptions for every image in Xcsuperscript𝑋𝑐X^{c}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, and use LLaMA to obtain a summarized description Rcsuperscript𝑅𝑐R^{c}italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT. This description is used as a prompt for diffusion inpainting to artificially change a subject’s clothing. Our ID-preserving diffusion inpainting model\nsynthesizes quality generated data x^0subscript^𝑥0\\hat{x}_{0}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for improved downstream CC-ReID training.",
        "qtype": "Implementation_Details",
        "response": "The red-boxed module (“Clothes Description Extraction”) is nothing more than a small pipeline that:\n\n1. Gathers all of the per-image clothing descriptions rᶜ₁…rᶜₙ produced by LLaVA (that’s the blue-boxed part, “What clothes is the person wearing?” on each image xᶜᵢ).\n\n2. Concatenates or feeds that list of image-level strings into LLaMA with a simple prompt like “Extract the clothing items from the following text: …”.\n\n3. Lets LLaMA compress and rewrite the noisy, possibly incomplete LLaVA outputs into one clean, holistic sentence Rᶜ describing the outfit.\n\n4. (Optionally) Post-processes Rᶜ by taking the most frequent mention of each garment type across the set so that the final description only contains items that appeared in a majority of the frames.\n\nThe result is a single, dataset-grounded clothes description per clothes-ID that mixes all the LLaVA responses but is distilled and cleaned up by LLaMA."
    },
    {
        "question": "How does ID-Preserving Masking interact with the Forward Process to retain subject identity during diffusion inpainting?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "Forward Process: At each timestep, Gaussian noise is added with a timestep dependent variance to obtain x_t... We can also efficiently sample x_t in a single step [20].",
            "We apply a semantic human parsing method [26] to each image, x_i, to obtain an ID-preserving binary mask, m_i, where only the upper-clothes, lower-clothes, and footwear regions are marked for inpainting. This allows the person (s_i) features, such as the face, hair, and general body structure, to be preserved in the generated samples, even after clothes inpainting—an especially critical aspect for enriching CC-ReID data.",
            "For a given image x_i, we get the ID-preserved masked image x̄_i by applying its corresponding human-parsed semantic mask, m_i.",
            "The masked image x̄_i is used in the forward diffusion process (Eq. (1)), to preserve ID-related information."
        ],
        "final_answer": "Before running the diffusion forward process, the full image is element-wise masked with a human-parsed binary mask that zeroes out only the clothing regions. The resulting masked image (which preserves face, hair, body shape, etc.) is then fed into the forward process. Because the unmasked (identity) regions remain intact during the noise addition steps, the subject’s identity is retained throughout the diffusion inpainting.",
        "relevant_elements": [
            "ID-Preserving Masking",
            "Forward Process"
        ],
        "id": 761,
        "masked_question": "How does [mask1] interact with the Forward Process to retain subject identity during diffusion inpainting?",
        "masked_number": 1,
        "masked_elements": [
            "ID-Preserving Masking"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.07205v2_figure_2.png",
        "paperid": "2411.07205v2",
        "paper_path": "./papers/2411.07205v2.json",
        "figure_id": "2411.07205v2_figure_2.png",
        "caption": "Figure 2: Overview of DLCR data generation (stage 1). Given a training image xisubscript𝑥𝑖x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, we apply an ID-preserving mask, misubscript𝑚𝑖m_{i}italic_m start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, where only the clothing regions are marked for inpainting. A target clothing ID, c𝑐citalic_c, is randomly selected from the training dataset and a textual description of the clothing items belonging to c𝑐citalic_c is extracted. Specifically, we use LLaVA to extract frame-wise descriptions for every image in Xcsuperscript𝑋𝑐X^{c}italic_X start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT, and use LLaMA to obtain a summarized description Rcsuperscript𝑅𝑐R^{c}italic_R start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT. This description is used as a prompt for diffusion inpainting to artificially change a subject’s clothing. Our ID-preserving diffusion inpainting model\nsynthesizes quality generated data x^0subscript^𝑥0\\hat{x}_{0}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT for improved downstream CC-ReID training.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be precise about what “mask₁” is.  In the red‐boxed region of Fig. 2 the paper shows the **ID-preserving semantic mask** mᵢ and the **masked image** x̄ᵢ.  Concretely:\n\n  • mᵢ is a binary human‐parsing mask which is 1 exactly on the upper‐clothes, lower‐clothes and shoes pixels, and 0 everywhere else (face, hair, body, background).  \n  • x̄ᵢ = (1 − mᵢ) ⊙ xᵢ  is the original image with the clothing pixels “blanked out,” so that only the identity‐bearing regions remain intact.\n\nHow this interacts with the forward diffusion pass is the key to retaining identity:\n\n 1. **Masking before noising.**  Instead of starting diffusion on the full RGB image xᵢ, the model starts from the partially masked image x̄ᵢ.  In other words, the forward process of Gaussian‐noise addition (Eq. (1) in the paper) is initialized with x̄ᵢ rather than xᵢ itself.\n\n 2. **Gating noise to the clothing region.**  Because the unmasked pixels in x̄ᵢ (face, hair, body, background) are already the ground-truth pixels from xᵢ, the noise schedule can be implemented so that **only** the masked‐out clothing region is actually “corrupted” by noise over the T timesteps, while the rest of the image carries its original, identity-bearing values through unscathed.\n\n 3. **Outcome at t = T.**  After the forward pass you have a noisy version of the clothing region but a pristine copy of everything else.  When you then run the reverse denoising conditioned on the text prompt describing a new outfit, the model only “fills in” the clothing mask with new pixels, and leaves face/body/background exactly as before.\n\nIn short, mask₁ (the semantic human‐parsing mask) is applied **before** the diffusion forward‐process, so that the only region that ever gets noised—and therefore subject to inpainting in the reverse pass—is the clothing.  All of the person‐specific cues outside that mask are carried forward intact, guaranteeing that the subject’s identity remains unchanged even as their outfit is restyled."
    },
    {
        "question": "How does temporal adapter apply global aggregation to prompt embeddings over video frames?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "For the current prompt embedding ϕ_t in the video object mask retrieval process, we leverage the adaptive average pooling strategy along the time dimension to aggregate global object and temporal information of previous k frames."
        ],
        "final_answer": "The temporal adapter applies global aggregation by performing adaptive average pooling along the time dimension over the prompt embeddings from the previous k frames, thereby combining global object and temporal information into the current prompt embedding.",
        "relevant_elements": [
            "Temporal Adapter",
            "Prompt Embedding"
        ],
        "id": 762,
        "masked_question": "How does [mask1] apply global aggregation to [mask2] over video frames?",
        "masked_number": 2,
        "masked_elements": [
            "Temporal Adapter",
            "Prompt Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17606v1_figure_2.png",
        "paperid": "2411.17606v1",
        "paper_path": "./papers/2411.17606v1.json",
        "figure_id": "2411.17606v1_figure_2.png",
        "caption": "Figure 2: Overview of HyperSeg. HyperSeg encodes the visual input in a multi-grained manner and concatenates the prompt for different perception tasks. We feed learnable fine-grained tokens into a Fine-grained Visual Perceiver (FVP) to integrate multi-scale high-resolution image features into LLM for detailed visual learning and to facilitate space-time information propagation for video understanding.\nAdditionally, we use the semantically enhanced mask tokens and prompt embedding to finally generate the segmentation masks and class scores for generic segmentation, and instance embedding for video instance association.",
        "qtype": "Implementation_Details",
        "response": "First, let’s identify the two masked components in the figure:\n\n1. [mask1] is the “Temporal Adapter” (outlined in red), which has two operations symbolized by  \n   – a “global aggregation” arrow  \n   – and a “local injection” loop.  \n2. [mask2] is the “Prompt Embedding” (outlined in blue) that feeds into the Segmentation Predictor.\n\nThe paper describes **global prompt aggregation** in Sec. 3.4 (under “Temporal Adapter”):\n\n  “For the current prompt embedding Pₜ in the video object mask retrieval process, we leverage the adaptive average pooling strategy along the time dimension to aggregate global object and temporal information of previous k frames.”\n\nChain‐of‐thought:\n\n  • We want to know how the Temporal Adapter (mask1) applies its *global aggregation* operation to the Prompt Embedding (mask2).  \n  • In the text they say that, for video tasks, they keep a history of prompt embeddings from the last k frames.  \n  • They then perform an **adaptive average pooling** over that sequence of prompt embeddings along the temporal axis.  \n  • This produces one “globally aggregated” prompt embedding vector that captures long‐term object and motion context.  \n  • That aggregated embedding is then used as the prompt input for the current frame’s segmentation.\n\nAnswer:\n\nThe Temporal Adapter’s global‐aggregation branch simply stacks the prompt embeddings from the previous k frames and runs an adaptive average‐pooling over time.  In this way it collapses all past prompt vectors into one “global” prompt embedding, which is then used by the Segmentation Predictor on the current frame."
    },
    {
        "question": "How does Fine-grained Visual Perceiver use cross-attention to fuse pyramid encoder features into fine-grained tokens?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, we fuse multi-scale visual features into fine-grained tokens (stated as  in Sec 3.1) which can inject rich fine-grained visual information into the pre-trained VLLMs without excessive computation cost.",
            "Formally, given the vision input , we leverage a pyramid vision encoder [7  ###reference_b7###] to get details-aware image features . For the j-th scale and the previous fine-grained tokens , the FVP module enriches each token through conditional weighted cross-attention: where MHCA denotes the Multi-Head Cross-Attention layer,  is the projection function, tanh is a normalization function and MLP is a Multilayer Perceptron. The component of  is the conditional weight used to multiply the enriched fine-grained tokens  before the residual connection to the previous tokens ."
        ],
        "final_answer": "The Fine-grained Visual Perceiver (FVP) injects multi-scale pyramid encoder features into its fixed-length fine-grained tokens by performing a conditional weighted multi-head cross-attention (MHCA). At each pyramid scale, the FVP attends from the previous fine-grained tokens over the new scale’s encoder features, projects and normalizes the attended output (via tanh and an MLP) to form a conditional weight, multiplies this weight with the cross-attention result, and then adds it residually back to the original tokens.",
        "relevant_elements": [
            "Fine-grained Visual Perceiver",
            "Pyramid Encoder",
            "Fine-grained Tokens"
        ],
        "id": 763,
        "masked_question": "How does [mask1] use cross-attention to fuse pyramid encoder features into fine-grained tokens?",
        "masked_number": 1,
        "masked_elements": [
            "Fine-grained Visual Perceiver"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17606v1_figure_2.png",
        "paperid": "2411.17606v1",
        "paper_path": "./papers/2411.17606v1.json",
        "figure_id": "2411.17606v1_figure_2.png",
        "caption": "Figure 2: Overview of HyperSeg. HyperSeg encodes the visual input in a multi-grained manner and concatenates the prompt for different perception tasks. We feed learnable fine-grained tokens into a Fine-grained Visual Perceiver (FVP) to integrate multi-scale high-resolution image features into LLM for detailed visual learning and to facilitate space-time information propagation for video understanding.\nAdditionally, we use the semantically enhanced mask tokens and prompt embedding to finally generate the segmentation masks and class scores for generic segmentation, and instance embedding for video instance association.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in the diagram is the “FVP” or Fine‐grained Visual Perceiver.  In §3.3 the authors spell out exactly how it works:\n\n1.  At scale j we have  \n    –  a set of fine‐grained tokens Z_{j–1} (these are your “queries”), and  \n    –  multi–scale feature maps f_j coming out of the pyramid encoder.  \n\n2.  FVP projects f_j into keys and values via two learned linear layers:  \n       K_j = W^k f_j,  \n       V_j = W^v f_j.  \n\n3.  It performs multi-head cross-attention (MHCA) with  \n       Q = Z_{j–1},  \n       K = K_j,  \n       V = V_j  \n   to get an update H_j = MHCA(Q,K,V).  \n\n4.  In parallel, it concatenates Z_{j–1} and f_j, runs them through an MLP plus a tanh to produce a per‐token gating vector γ_j:  \n       γ_j = tanh(MLP([Z_{j–1}; f_j])).  \n\n5.  Finally it fuses the attended features back into the tokens via a gated residual:  \n       Z_j = Z_{j–1} + γ_j ⊙ H_j.  \n\nIn this way the cross-attention lets each fine-grained token pull information from the pyramid’s multi-scale features, the MLP–tanh branch learns how strongly to inject it, and the residual connection preserves stability."
    },
    {
        "question": "How does FVP’s fusion of pyramid encoder features complement CLIP-derived vision tokens within the LLM input pipeline?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "The VLLM takes three types of inputs: visual tokens encoded by the CLIP encoder, renewed fine-grained tokens, and prompt tokens for diverse instructions. The output embeddings of semantically enhanced mask tokens and prompt tokens are further fed into the segmentation predictor for final segmentation results.",
            "Specifically, we fuse multi-scale visual features into fine-grained tokens (stated as z in Sec 3.1) which can inject rich fine-grained visual information into the pre-trained VLLMs without excessive computation cost."
        ],
        "final_answer": "FVP merges the pyramid encoder’s multi-scale, detail-rich features into a set of fine-grained tokens. These tokens are provided in parallel with the low-resolution CLIP-derived vision tokens (and prompt tokens) as inputs to the LLM. In this way, the coarse global representations from CLIP are complemented by the FVP’s fine-grained spatial details, yielding a richer visual embedding for downstream segmentation.",
        "relevant_elements": [
            "FVP",
            "Vision Tokens"
        ],
        "id": 764,
        "masked_question": "How does [mask1]’s fusion of pyramid encoder features complement [mask2] within the LLM input pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "FVP",
            "Vision Tokens"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17606v1_figure_2.png",
        "paperid": "2411.17606v1",
        "paper_path": "./papers/2411.17606v1.json",
        "figure_id": "2411.17606v1_figure_2.png",
        "caption": "Figure 2: Overview of HyperSeg. HyperSeg encodes the visual input in a multi-grained manner and concatenates the prompt for different perception tasks. We feed learnable fine-grained tokens into a Fine-grained Visual Perceiver (FVP) to integrate multi-scale high-resolution image features into LLM for detailed visual learning and to facilitate space-time information propagation for video understanding.\nAdditionally, we use the semantically enhanced mask tokens and prompt embedding to finally generate the segmentation masks and class scores for generic segmentation, and instance embedding for video instance association.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the Fine‐grained Visual Perceiver (FVP) block in red, and Mask2 is the “vision tokens” coming out of the frozen CLIP encoder in blue.  What the question is really asking is:\n\n“How does the FVP’s fusion of pyramid encoder features (Mask1) complement the coarse CLIP vision tokens (Mask2) when both are fed into the LLM?”\n\nHere is the step-by-step reasoning:\n\n1.  The vanilla CLIP encoder only produces a single­-scale, relatively low­-resolution set of vision tokens (Mask2).  Those tokens carry strong global semantics, but lack fine spatial detail.  \n2.  HyperSeg adds a separate pyramid encoder that computes multi­-scale, high­-resolution feature maps.  \n3.  The FVP (Mask1) takes a small set of learnable “fine­-grained” tokens and, via cross­-attention over each pyramid level, injects the pyramid encoder’s rich multi­-scale detail into those tokens.  \n4.  After fusion, FVP outputs a new set of fine­-grained tokens that explicitly encode high­-resolution, multi­-scale structure.  \n5.  The LLM input is then formed by concatenating three things:  \n    a.  The original CLIP vision tokens (Mask2) for coarse, global visual context  \n    b.  The FVP’s fine­-grained tokens for detailed, high­-res cues  \n    c.  The task prompt tokens  \n6.  By feeding both the CLIP tokens and the FVP tokens into the LLM, HyperSeg endows the model with a truly multi­-grained visual representation.  The CLIP tokens ensure strong semantic grounding, while the FVP tokens supply the fine spatial detail necessary for precise segmentation and reasoning.\n\nIn short, FVP’s fusion of pyramid features (Mask1) injects the high­-resolution, multi­-scale structure that the coarse CLIP tokens (Mask2) lack, and both are concatenated at the LLM input so that the model attends jointly to global semantics and local details."
    },
    {
        "question": "How does semantic recognition coupling with mask tokens expand VLLM-based mask decoding compared to decode-only strategies?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Decode-only methods [59, 58] use the prompt embedding and mask tokens decoded by VLLM to obtain class scores for each mask, which makes the mask tokens interact insufficiently with the semantic condition as they ignore the powerful generative capabilities of VLLM.",
            "As illustrated in Fig. 3 (c), VLLM is compelled to generate all the existing objects in the vision input and then the mask tokens. The semantically enhanced mask tokens contain valuable semantic integrated information about the image, which are subsequently used as input for the segmentation predictor to generate segmentation masks."
        ],
        "final_answer": "By forcing the VLLM to first generate object names and then emit the mask tokens, the mask tokens become “semantically enhanced” – they carry integrated category information directly from the model’s generative process. This contrasts with decode-only strategies that merely apply prompt embeddings at decode time and ignore the VLLM’s semantic recognition power, resulting in richer, more semantically aware mask decoding.",
        "relevant_elements": [
            "semantic recognition",
            "mask tokens"
        ],
        "id": 765,
        "masked_question": "How does [mask1] coupling with mask tokens expand VLLM-based mask decoding compared to decode-only strategies?",
        "masked_number": 1,
        "masked_elements": [
            "semantic recognition"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17606v1_figure_2.png",
        "paperid": "2411.17606v1",
        "paper_path": "./papers/2411.17606v1.json",
        "figure_id": "2411.17606v1_figure_2.png",
        "caption": "Figure 2: Overview of HyperSeg. HyperSeg encodes the visual input in a multi-grained manner and concatenates the prompt for different perception tasks. We feed learnable fine-grained tokens into a Fine-grained Visual Perceiver (FVP) to integrate multi-scale high-resolution image features into LLM for detailed visual learning and to facilitate space-time information propagation for video understanding.\nAdditionally, we use the semantically enhanced mask tokens and prompt embedding to finally generate the segmentation masks and class scores for generic segmentation, and instance embedding for video instance association.",
        "qtype": "Literature_Background",
        "response": "In our terminology, “[mask1]” is the little red-boxed module in the sketch labeled “Semantic Recognition.”  Its job is to force the LLM to first spit out the object names (or other category‐level descriptions) and *then* immediately produce the corresponding mask token(s) in the same generation pass.  \n\nHere is what that coupling buys you over a pure “decode-only” recipe:\n\n1.  In decode-only methods you simply inject a set of learnable mask tokens into the LLM, run one forward pass with your prompt, and then take those final token embeddings as fixed classifiers.  They never actually *see* the LLM’s semantic predictions, so they remain basically undifferentiated proposals that you then dot against your prompt embeddings just at decode time.  \n\n2.  In our hybrid scheme, by contrast, we prepend each mask token in the generation stream with the human-readable object name.  The LLM must generate the words “dog,” “person,” “tree,” etc., *and then* generate the mask token.  Because of that, the mask token’s embedding has been conditioned on—and literally “carried along” with—the semantic content immediately before it.  \n\n3.  As a result, when the segmentation head sees those “semantic-enhanced” mask tokens, it no longer has to recover semantics out of thin air.  It inherits the LLM’s own understanding of *what* it just named, so the mask proposals are already grounded in real category meaning.  \n\nIn short, by coupling the “Semantic Recognition” step with the mask token generation you turn your mask tokens into *semantically aware* mask tokens.  That merges the LLM’s powerful generative understanding and the downstream decoder’s mask-scoring step into one coherent pipeline, yielding far stronger multi‐object segmentation than decode-only strategies ever can."
    },
    {
        "question": "How does Flow Predictor extend dense flow estimation methodologies for latent motion representation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The Latent Flow Generator (LFG) is a self-supervised training framework designed to model motion information between the source image x_src and the driving image x_dri. As illustrated in Figure 1 (a), LFG consists of three trainable modules: the image encoder E_img, the flow predictor F, and the image decoder D_img.",
            "The flow predictor estimates a dense flow map f_pred and a blocking map m_pred (Siarohin et al., 2021; 2020), corresponding to f and m: f_pred, m_pred = F(z_src, z_dri).",
            "The flow map f_pred describes the feature-level movement of x_src relative to x_dri in horizontal and vertical directions. The blocking map m_pred, ranging from 0 to 1, indicates the degree of area blocking in the transformation from x_src to x_dri.",
            "The flow map f_pred is used to perform the affine transformation W(f_pred, z_src), serving as a coarse-grained warping of z_src. Subsequently, the blocking map m_pred guides the model in repairing the occlusion area, thereby serving as fine-grained repair.",
            "We consider the concatenation of f_pred and m_pred as Δ̂ to represent the motion of x_dri relative to x_src. In this way, we achieve two objectives: 1) finding an effective explicit motion representation Δ, which is identity-agnostic and well-supported by physical meaning, and 2) reconstructing x_dri from x_src and Δ without the need for a full pixel generation."
        ],
        "final_answer": "The Flow Predictor extends classical dense-flow methods by operating in the latent space: it jointly predicts a dense flow map (f_pred) that encodes feature-level motion and a blocking map (m_pred) that marks occluded regions. The flow map is used for coarse affine warping of the source latent code, while the blocking map enables fine-grained repair of occlusions. By concatenating these two outputs into Δ̂, the model obtains an explicit, identity-agnostic latent motion representation that drives the subsequent reconstruction without having to generate every pixel anew.",
        "relevant_elements": [
            "Flow Predictor"
        ],
        "id": 766,
        "masked_question": "How does [mask1] extend dense flow estimation methodologies for latent motion representation?",
        "masked_number": 1,
        "masked_elements": [
            "Flow Predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.13726v2_figure_1.png",
        "paperid": "2410.13726v2",
        "paper_path": "./papers/2410.13726v2.json",
        "figure_id": "2410.13726v2_figure_1.png",
        "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.",
        "qtype": "Literature_Background",
        "response": "The component in the red box in Fig. 1(a) is simply the “flow predictor” inside our Latent Flow Generator.  What it does – and how it extends off‐the‐shelf dense‐flow methods – can be broken down as follows:\n\n1.  Latent‐space estimation  \n    •  Instead of computing optical flow on raw pixels, we push both the source and driving frames through a shared encoder and do all of our flow computations in that lower-dimensional feature space.  \n    •  This makes the flow completely identity-agnostic (it only sees “what moves,” not “who is moving”) and far more compact to store or diffuse.\n\n2.  Two simultaneous dense fields  \n    •  We predict not only a dense 2D displacement field f_dri but also a per-pixel “blocking” or occlusion map m_dri (values in [0,1]).  \n    •  The displacement field handles the coarse, back-and-forth motion; the blocking map tells us which regions get uncovered or occluded so we can inpaint them in a second pass.  \n\n3.  Coarse warping + fine repair  \n    •  At test time we first warp the source‐feature map by f_dri.  Wherever m_dri says “occluded,” we know to sharpen, inpaint or hallucinate new content via the decoder.  \n    •  This two‐step (warp + occlusion repair) is exactly what lets us achieve sharp, physically plausible motion in a self-supervised way, without pixel-by-pixel supervision.\n\nIn short, the “flow predictor” extends classic dense‐flow methods by relocating all flow estimation into a latent feature space and by augmenting the usual 2D flow with an explicit occlusion (blocking) map. This yields a compact, interpretable motion code (f_dri ⊕ m_dri) that we can both reconstruct and later diffuse from audio."
    },
    {
        "question": "How does PBNet's transformer-based VAE leverage Rotary Positional Encoding techniques from sequence modeling?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "To enhance the model’s extrapolation capability, we use RoPE as the positional encoding in the decoder, consistent with A2V-FDM."
        ],
        "final_answer": "PBNet incorporates Rotary Positional Encoding (RoPE) in its transformer decoder to encode sequence positions, improving the model’s ability to generalize and extrapolate over variable‐length pose and blink sequences.",
        "relevant_elements": [
            "PBNet"
        ],
        "id": 767,
        "masked_question": "How does [mask1]'s transformer-based VAE leverage Rotary Positional Encoding techniques from sequence modeling?",
        "masked_number": 1,
        "masked_elements": [
            "PBNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.13726v2_figure_1.png",
        "paperid": "2410.13726v2",
        "paper_path": "./papers/2410.13726v2.json",
        "figure_id": "2410.13726v2_figure_1.png",
        "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.",
        "qtype": "Literature_Background",
        "response": "Here’s what’s going on step by step:\n\n1.  Identify [mask1]: that red‐boxed block in the big pipeline is the Pose & Blink generation Network (PBNet), which is explicitly described as “a transformer-based Variational Auto‐Encoder.”\n\n2.  What the paper says about positional encodings in PBNet:  \n    •  In Sec. 3.4 (“Audio‐Driven Pose and Blink Generation”) they note that, to boost extrapolation to longer sequences, they “use RoPE as the positional encoding in the decoder, consistent with A2V-FDM.”  \n    •  RoPE refers to the Rotary Positional Encoding introduced in Su et al. (2023), which injects position information by rotating the query and key vectors in self-attention according to their sequence index.\n\n3.  Why rotary instead of absolute embeddings?  \n    •  Standard learned position embeddings fix you to the lengths seen at train time. RoPE, by contrast, encodes positions via a deterministic rotation in the Q/K space.  \n    •  Because those rotations compose neatly, the transformer can generalize to positions (i.e. frame indices) longer than those seen during training—exactly what you want when you “train on short clips” but “run on long ones.”\n\n4.  In practice in PBNet’s VAE:  \n    –  The encoder still ingests the initial pose + audio embeddings to produce μ, σ.  \n    –  The decoder attends over the latent z together with the public audio‐derived keys/values.  \n    –  In that decoder’s multi-head attention, each head’s Q and K are element-wise rotated by angle θₚ that depends on the token’s absolute index p.  \n    –  This RoPE trick gives the model a continuous, unbounded way to “know” the frame index and thereby handle arbitrary‐length pose/blink sequences at test time.\n\nAnswer: PBNet’s transformer VAE replaces learned absolute position embeddings with Rotary Positional Encodings (RoPE) in its decoder’s self-attention. Concretely, it multiplies each Q/K vector by a small rotation whose angle encodes the frame index—this both preserves relative distance information and lets the network extrapolate to much longer pose/blink sequences than seen during training."
    },
    {
        "question": "How does the warp operation preserve source identity while applying flow representations in latent space?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The flow map φ describes the feature-level movement of z_s relative to z_d in horizontal and vertical directions. The blocking map m, ranging from 0 to 1, indicates the degree of area blocking in the transformation from z_s to z_d. The flow map φ is used to perform the affine transformation g, serving as a coarse-grained warping of z_s. Subsequently, the blocking map m guides the model in repairing the occlusion area, thereby serving as fine-grained repair. Finally, the image decoder converts the warped latent code into the target image x̂, where x̂ = dec(g(z_s, φ) ⊙ m).",
            "We consider the concatenation of φ and m as z_motion to represent the motion of z_d relative to z_s. In this way, we achieve two objectives: 1) finding an effective explicit motion representation z_motion, which is identity-agnostic and well-supported by physical meaning, and 2) reconstructing x_d from x_s and z_motion without the need for a full pixel generation."
        ],
        "final_answer": "In the Latent Flow Generator, the warp operation applies the estimated flow map φ directly to the source’s latent code (z_s), moving only the motion-related features. A complementary blocking map m then masks out and repairs occlusions, ensuring only displaced (motion) features are altered. Because the warp is performed in the learned latent space and only modifies identity-agnostic motion features, the source’s appearance and identity information remain intact and are faithfully reconstructed by the decoder.",
        "relevant_elements": [
            "warp",
            "latent space"
        ],
        "id": 768,
        "masked_question": "How does the [mask1] operation preserve source identity while applying flow representations in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "warp",
            "latent space"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.13726v2_figure_1.png",
        "paperid": "2410.13726v2",
        "paper_path": "./papers/2410.13726v2.json",
        "figure_id": "2410.13726v2_figure_1.png",
        "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.",
        "qtype": "Experimental_Results",
        "response": "In our Latent Flow Generator (Fig. 1 (a)), the little red‐outlined step is nothing mystical – it is simply “warping” the source latent code by the flow field.  In other words, after we encode the still frame into zₛᵣc, we apply an affine warp to zₛᵣc using the flow map f₍dri₎ (and then mask off occluded regions with the blocking map m₍dri₎).  Because all of this happens in the latent space (the big blue box), the original identity information lives untouched in zₛᵣc – we’re merely sliding its feature‐vectors around according to the motion vectors, not re-encoding or overwriting them.  That is how the warp in latent space preserves source identity while still imparting the learned motion."
    },
    {
        "question": "How does PBNet relieve A2V-FDM of long-term pose and blink dependency modeling?",
        "relevant_section_ids": [
            "4.5"
        ],
        "relevant_context": [
            "Ablation study on PBNet. We evaluate the effectiveness of the PBNet in Table 4. The term “w/o PBNet” indicates that the PBNet module was removed from the architecture, requiring the A2V-FDM to simultaneously generate pose, blink, and lip motions from the audio by itself. The results suggest an overall enhancement of all evaluation metrics with the inclusion of PBNet. This is because modeling the long-term dependency of pose and blink movements through PBNet simplifies training for the A2V-FDM."
        ],
        "final_answer": "By offloading the modeling of long-term pose and blink dependencies to PBNet, A2V-FDM no longer needs to learn those long-term temporal correlations itself, which simplifies its training.",
        "relevant_elements": [
            "PBNet",
            "A2V-FDM"
        ],
        "id": 769,
        "masked_question": "How does [mask1] relieve [mask2] of long-term pose and blink dependency modeling?",
        "masked_number": 2,
        "masked_elements": [
            "PBNet",
            "A2V-FDM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.13726v2_figure_1.png",
        "paperid": "2410.13726v2",
        "paper_path": "./papers/2410.13726v2.json",
        "figure_id": "2410.13726v2_figure_1.png",
        "caption": "Figure 1: The pipeline of DAWN. First, we train the Latent Flow Generator (LFG) in (a) to extract the motion representation from the video. Then the Pose and Blink generation Network (PBNet) in (b) is utilized to generate the head pose and blink sequences of the avatar. Subsequently, the Audio-to-Video Flow Diffusion Model (A2V-FDM) in (c) generates the talking head video from the source image conditioned by the audio and pose/blink sequences provided by the PBNet.",
        "qtype": "Experimental_Results",
        "response": "Here, the red‐boxed module is the pretrained Pose‐and‐Blink Network (PBNet), and the blue‐boxed block is the diffusion‐based Audio-to-Video Flow Model (A2V-FDM) inside our DAWN framework.  By running head‐pose and blink prediction ahead of time—via PBNet trained on long clips and powered by a frozen HuBERT audio encoder—we feed the full pose/blink trajectories (ρ₁:ₙ, a₁:ₙ) as external conditions into the diffusion model.  In doing so, A2V-FDM never has to learn to carry long‐range pose/blink dependencies itself; it simply “paints” the fine‐grained video flow sheet given the already‐computed head‐motion and blink signals."
    },
    {
        "question": "How does power allocation coordinate with channel arrangement under varying service compliance for resource optimization?",
        "relevant_section_ids": [
            "5.1",
            "6.1",
            "7.4"
        ],
        "relevant_context": [
            "2) Power Actor State : We integrate SC decision actions into the power actor states since SC orchestration serves as a prerequisite for power allocation.",
            "In particular, the initial state s₀ is sent into the μ_SC actor networks at the beginning of each episode, generating μ_SC SC allocation actions a_SC,t. The actions for power a_PA are then produced by the power actor network, based on the integrated a_SC,t and s_PA,t.",
            "The APs consistently tend to serve users who are in close proximity, and the utilization of the same SC by multiple APs is well-coordinated to achieve a satisfactory balance between spectral efficiency and interference. From the perspective of application types, EF services (UE₂) are configured with stricter reliability requirements, which drive reduced SC reuse, as illustrated in Fig. 7(c) where SC₃ is exclusively utilized by UE₂."
        ],
        "final_answer": "The scheme uses a two-actor DRL structure in which the SC (subcarrier) actor first arranges channels to satisfy each user’s compliance requirements (e.g., stricter reliability leads to less SC reuse). The power actor then takes that SC allocation as part of its input state and dynamically allocates continuous power ratios over the chosen SCs. Under tighter service compliance (lower compliance tolerance), the SC actor dedicates fewer, interference-protected SCs to the demanding flow, enabling the power actor to concentrate higher power on those links; when compliance is looser, more aggressive SC reuse is allowed and the power actor spreads power more evenly to boost spectral efficiency. This coordination under a shared reward (trading off effective capacity, power consumption and bandwidth occupancy) achieves joint energy- and spectrum-efficient resource optimization.",
        "relevant_elements": [
            "Power allocation",
            "Channel arrangement"
        ],
        "id": 770,
        "masked_question": "How does [mask1] coordinate with channel arrangement under varying service compliance for resource optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Power allocation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.15609v1_figure_1.png",
        "paperid": "2408.15609v1",
        "paper_path": "./papers/2408.15609v1.json",
        "figure_id": "2408.15609v1_figure_1.png",
        "caption": "Figure 1: System Model. This diagram illustrates the on-demand services framework, where applications with diverse requirements are served by the underlying resources and facilities, the system rewards are calculated based on the completion of requirements and the consumption of resources.",
        "qtype": "Experimental_Results",
        "response": "The item in the little red box is simply the “Power‐allocation” module.  In our HADRL framework it lives side-by-side with the “Channel-arrangement” (SC-orchestration) actor.  Concretely the two cooperate as follows:\n\n1.  At each decision step the SC-actor first chooses which subcarriers (SCs) to turn on, which user each SC should serve, or whether to shut an SC off entirely.  \n2.  Those discrete SC decisions are then packaged into the power-actor’s state.  The power-actor is a continuous actor that spits out a softmax over power ratios for exactly the SCs that were just selected.  By construction the sum of those ratios never exceeds one, so every AP’s power budget is respected.  \n3.  Both actors share a single reward (the ESE/EEE reward) that is sensitive to service compliance rates (i.e. how close each user’s delivered capacity comes to its requirement).  \n4.  Whenever a particular service misses its target (low compliance) the reward is heavily penalized.  That in turn drives the power-actor to boost power on the offending SCs, and it drives the SC-actor to re-arrange channels (for instance by isolating that flow onto a less interfered SC).  Conversely, if some users are being “over-served,” the reward structure discourages them from hogging both spectrum and power.\n\nIn this way “Power allocation” and “Channel arrangement” are trained jointly under PPO (the COHA structure), they pass information back and forth (SC choices → power state), and the shared compliance-based reward forces them to coordinate so that scarce spectrum and power are applied only where and when they are needed."
    },
    {
        "question": "How does the methodology derive discounted capacity from service capacity and satisfaction rate for reward computation?",
        "relevant_section_ids": [
            "5.3"
        ],
        "relevant_context": [
            "(1) The sum of the values among all users yields the system’s service capacity.",
            "(2) The discounted capacity for system is obtained from the system’s service capacity and the user satisfaction rate φ within the system: C̃ = C¯ × φ."
        ],
        "final_answer": "The methodology computes discounted capacity by multiplying the system’s service capacity by the overall user satisfaction rate in the system.",
        "relevant_elements": [
            "Service capacity",
            "Discount capacity"
        ],
        "id": 771,
        "masked_question": "How does the methodology derive [mask1] from [mask2] and satisfaction rate for reward computation?",
        "masked_number": 2,
        "masked_elements": [
            "Discount capacity",
            "Service capacity"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.15609v1_figure_1.png",
        "paperid": "2408.15609v1",
        "paper_path": "./papers/2408.15609v1.json",
        "figure_id": "2408.15609v1_figure_1.png",
        "caption": "Figure 1: System Model. This diagram illustrates the on-demand services framework, where applications with diverse requirements are served by the underlying resources and facilities, the system rewards are calculated based on the completion of requirements and the consumption of resources.",
        "qtype": "Experimental_Results",
        "response": "The missing link is actually very simple – the “discounted capacity” (the red-boxed term) is nothing more than the system’s raw service capacity (the blue-boxed term) scaled by the overall satisfaction rate.  In formula form, if C_s is your service capacity and δ is the fraction of users whose demands you actually meet, then the discounted capacity is  \n    \n  Ĉ_s = C_s × δ."
    },
    {
        "question": "What ethical concerns could arise from power allocation decisions prioritizing EEE Reward over fair service?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "power allocation",
            "EEE Reward"
        ],
        "id": 773,
        "masked_question": "What ethical concerns could arise from [mask1] decisions prioritizing EEE Reward over fair service?",
        "masked_number": 1,
        "masked_elements": [
            "power allocation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.15609v1_figure_1.png",
        "paperid": "2408.15609v1",
        "paper_path": "./papers/2408.15609v1.json",
        "figure_id": "2408.15609v1_figure_1.png",
        "caption": "Figure 1: System Model. This diagram illustrates the on-demand services framework, where applications with diverse requirements are served by the underlying resources and facilities, the system rewards are calculated based on the completion of requirements and the consumption of resources.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed item in Fig. 1 is the “Power allocation” block under “Parameter optimization.”  In our Business-Centric Network (BCN) that uses DRL to trade off Effective Spectrum Efficiency (ESE) and Effective Energy Efficiency (EEE), the “EEE Reward” term will tend to push the agent to cut transmit power wherever it can, simply to save joules per bit.\n\nIf one blindly chases EEE without any fairness or minimum-service constraint, then a number of ethically troubling outcomes become possible:\n\n  1.  Service starvation of the poor link-budget users.  \n      •  Users at the edge of the cell or in deep fading may routinely get zero or minimal power, effectively shutting them out of service.  \n  2.  Discrimination against latency- or reliability-sensitive flows.  \n      •  Emergency, public-safety or IoT traffic (which often has strict latency/reliability needs but low data volume) may be throttled or dropped entirely in favour of bulk traffic that is “cheaper” per joule.  \n  3.  Violation of network-neutrality principles.  \n      •  By aggressively skewing power toward the “most energy-efficient” flows, the system may implicitly prioritize certain applications or user classes over others, without any human-legislated policy.  \n  4.  Exacerbation of the digital divide.  \n      •  Wealthy or densely populated areas (where it’s easier to get high EEE) will see consistently good service; poorer or rural areas suffer chronic neglect.  \n  5.  Lack of transparency and accountability.  \n      •  A purely reward-driven DRL agent can become a “black box” that makes power cuts in ways that operators and end users cannot predict or audit, undermining trust.  \n\nIn short, if “Power allocation” is steered solely by the EEE Reward term, the system risks creating a two-tier network in which some users get robust, low-latency links and others are effectively left in the dark—all in the name of saving watts per bit. That raises serious fairness, equity, regulatory (e.g. net-neutrality) and social-justice concerns that any practical deployment would have to guard against (for example, by adding hard minimum-service constraints or per-user fairness penalties into the reward)."
    },
    {
        "question": "What limitations emerge from using Model under analysis features in Samples subset selection to capture bias diversity?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "In the typical deployment scenario, the correctly classified examples are abundant, and, for instance,  projects them in a very narrow neighborhood of its latent space.",
            "Our long-range objective will be indeed to capture the set of features that are correctly learned by the model, and k-medoids is a natural choice to have a good coverage of the latent space for ."
        ],
        "final_answer": "Because the model under analysis projects many correctly classified samples into a very narrow region of its own latent space, relying directly on its features to pick examples would yield little diversity. To overcome this, the authors apply k-medoids clustering to force a broader coverage of the feature space and better capture bias diversity.",
        "relevant_elements": [
            "Model under analysis",
            "Samples subset selection"
        ],
        "id": 774,
        "masked_question": "What limitations emerge from using [mask1] features in Samples subset selection to capture bias diversity?",
        "masked_number": 1,
        "masked_elements": [
            "Model under analysis"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.09570v1_figure_2.png",
        "paperid": "2408.09570v1",
        "paper_path": "./papers/2408.09570v1.json",
        "figure_id": "2408.09570v1_figure_2.png",
        "caption": "Figure 2: Pipeline for SaMyNa. Given a model, we can tell on either 𝒟trainsuperscript𝒟train\\mathcal{D}^{\\text{train}}caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPTor 𝒟valsuperscript𝒟val\\mathcal{D}^{\\text{val}}caligraphic_D start_POSTSUPERSCRIPT val end_POSTSUPERSCRIPT what are the correct (with green border) and the incorrectly (red border) classified samples. Amongst these, we first perform a sample subset selection looking at the latent space of the model under analysis and choosing through k-medoids the most representative samples for the learned class. Then, we employ a captioner to get a textual description of these samples. Among these descriptions, we identify recurrent keywords and, in parallel, working in the latent space of a text encoder, we extract the mean description for the learned classes, cleansed from common features within the dataset. We finally compare this representation with the embedding of the keywords, identifying learned correlations aside from the target.",
        "qtype": "Others",
        "response": "Here’s the reasoning in steps:\n\n1.  In the “Samples subset selection” block we take the trained model under analysis, run all correctly classified (and misclassified) examples through it, and extract the penultimate‐layer activations (i.e. its internal feature vectors).  \n2.  We then run k-medoids on those feature vectors to pick a small set of “representative” images.  \n3.  By construction, this procedure can only “see” whatever patterns the model has already encoded in its hidden layers.  \n\nBecause of that, the following limitations arise:\n\n•  **Blind spots for unlearned biases**  \n   If there is a spurious correlation in the data that the model never picked up (or collapsed together with more salient features), it simply won’t show up as a distinct cluster in its latent space—and so our sampling never exposes it.\n\n•  **Under-representation of rare or subtle cues**  \n   k-medoids will tend to carve up the dense, well-populated regions of the embedding space.  Small pockets corresponding to low-frequency spurious features can get swallowed by larger clusters and thus be omitted.\n\n•  **Dependency on cluster granularity**  \n   Choosing the number of medoids (the hyper-parameter k) is a trade-off: too few clusters, and you miss feature variety; too many, and you end up back with a huge set of images.  There’s no automatic way to guarantee you’ve picked the “right” k for bias coverage.\n\n•  **Inherited model biases**  \n   Any entanglement or compression in the model’s features (e.g. if it merges two different visual cues into one direction in embedding space) will directly limit which distinctions your sample-selector can make.\n\nIn short, by using the model’s own internal features to drive the subset selection, you constrain your bias-mining to only those spurious patterns the model has already learned and represented distinctly—and you risk missing rarer or more subtle biases altogether."
    },
    {
        "question": "How might reliance on a pre-trained Multimodal LLM for Samples captioning introduce biases or misinterpretations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Samples captioning",
            "Multimodal LLM"
        ],
        "id": 775,
        "masked_question": "How might reliance on a pre-trained [mask1] for [mask2] introduce biases or misinterpretations?",
        "masked_number": 2,
        "masked_elements": [
            "Multimodal LLM",
            "Samples captioning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.09570v1_figure_2.png",
        "paperid": "2408.09570v1",
        "paper_path": "./papers/2408.09570v1.json",
        "figure_id": "2408.09570v1_figure_2.png",
        "caption": "Figure 2: Pipeline for SaMyNa. Given a model, we can tell on either 𝒟trainsuperscript𝒟train\\mathcal{D}^{\\text{train}}caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPTor 𝒟valsuperscript𝒟val\\mathcal{D}^{\\text{val}}caligraphic_D start_POSTSUPERSCRIPT val end_POSTSUPERSCRIPT what are the correct (with green border) and the incorrectly (red border) classified samples. Amongst these, we first perform a sample subset selection looking at the latent space of the model under analysis and choosing through k-medoids the most representative samples for the learned class. Then, we employ a captioner to get a textual description of these samples. Among these descriptions, we identify recurrent keywords and, in parallel, working in the latent space of a text encoder, we extract the mean description for the learned classes, cleansed from common features within the dataset. We finally compare this representation with the embedding of the keywords, identifying learned correlations aside from the target.",
        "qtype": "Others",
        "response": "First, let’s identify the two masked pieces of the question by aligning it with Fig. 2 and the text:\n\n- The red box in the diagram is the “Multimodal LLM” component, i.e. a large vision-language model that turns an image (plus a prompt) into a caption.  \n- The blue box around that red box is the “Samples captioning” stage: it is where we use that pre-trained multimodal LLM to generate textual descriptions of our selected images.\n\nSo the question becomes:\n\n“How might reliance on a pre-trained multimodal LLM for samples captioning introduce biases or misinterpretations?”\n\nChain of Thought:\n\n1. In SaMyNa, after selecting representative correct/misclassified images, we hand them off to a **pre-trained multimodal LLM** (e.g. LLaVA or BLIP) to produce natural‐language captions.  \n2. That LLM was itself trained on massive, uncurated web data, which is known to harbor all kinds of subtle and not‐so‐subtle stereotypes (gender, race, age, cultural assumptions, etc.).  \n3. When it “sees” a face it may routinely describe it as “a young woman smiling” (even if the person is nonbinary), or when it sees a domestic scene it may default to “mom in the kitchen,” and so on.  \n4. Those ingrained biases mean that the **captions**—and hence the downstream keyword pool—are already tinted by the LLM’s own shortcuts or hallucinations.  \n5. As a result, the keywords you mine (“makeup,” “kitchen,” “beard,” “sidewalk,” “water,” etc.) may reflect *the LLM’s* spurious associations rather than *the model under test*’s true learned biases.  \n6. In extreme cases the LLM will even misidentify objects or actions (calling a racquet “a frying pan,” or “a person is cooking” when they’re actually repairing a car), leading the entire SaMyNa pipeline astray.\n\nAnswer:\n\nReliance on a pre-trained multimodal LLM for the samples-captioning step can inadvertently import the LLM’s own training-data biases (stereotypes, cultural assumptions, gender/race slants) or hallucinations into the generated descriptions. Those skewed or incorrect captions then feed downstream keyword extraction, so you may end up attributing spurious features to your model under test that in fact stem from the captioner’s biases or misinterpretations rather than from the vision model itself."
    },
    {
        "question": "What is the motivation behind contrasting correct and incorrect samples before captioning?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Given \\(D_{train}\\), for a given target class \\(c\\), we extract the pool of correctly classified samples \\(S^{correct}_c\\) and samples misclassified as \\(S^{misclass}_c\\).",
            "Provided that \\(f\\) clusters both \\(S^{correct}_c\\) and \\(S^{misclass}_c\\) together, our hypothesis is that these two share a common set of features, behind which we might find a bias."
        ],
        "final_answer": "By grouping together both correctly classified and misclassified samples and then contrasting them, the method aims to surface the features they have in common—features that are likely spurious correlations or biases the model has learned. Captioning this combined subset therefore highlights the semantic cues that underlie the model’s bias.",
        "relevant_elements": [
            "Samples subset selection",
            "Samples captioning"
        ],
        "id": 776,
        "masked_question": "What is the motivation behind contrasting correct and incorrect [mask1] before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Samples subset selection",
            "Samples captioning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.09570v1_figure_2.png",
        "paperid": "2408.09570v1",
        "paper_path": "./papers/2408.09570v1.json",
        "figure_id": "2408.09570v1_figure_2.png",
        "caption": "Figure 2: Pipeline for SaMyNa. Given a model, we can tell on either 𝒟trainsuperscript𝒟train\\mathcal{D}^{\\text{train}}caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPTor 𝒟valsuperscript𝒟val\\mathcal{D}^{\\text{val}}caligraphic_D start_POSTSUPERSCRIPT val end_POSTSUPERSCRIPT what are the correct (with green border) and the incorrectly (red border) classified samples. Amongst these, we first perform a sample subset selection looking at the latent space of the model under analysis and choosing through k-medoids the most representative samples for the learned class. Then, we employ a captioner to get a textual description of these samples. Among these descriptions, we identify recurrent keywords and, in parallel, working in the latent space of a text encoder, we extract the mean description for the learned classes, cleansed from common features within the dataset. We finally compare this representation with the embedding of the keywords, identifying learned correlations aside from the target.",
        "qtype": "Design_Rationale",
        "response": "First, let’s restate the two masked pieces in plain English:\n\n • [MASK1] is the “correctly-and-incorrectly classified samples subset selection” (the red‐boxed block in the figure).  \n • [MASK2] is the “samples captioning” step (the blue‐boxed block in the figure).\n\nSo the question becomes:  \n\n  “What is the motivation behind contrasting correct and incorrect samples before doing the captioning step?”\n\n**Chain-of-thought**  \n\n1.  We know that our ultimate goal is to surface *spurious features* – the bits of the image that the model has latched onto wrongly.  \n\n2.  Misclassified samples are *where* the model is most confident yet wrong, so they are the clearest flag of a bias the model has picked up.  \n\n3.  Correctly classified samples, by contrast, carry the *legitimate* features that the model uses to succeed.  \n\n4.  If you cluster *both* groups together in the model’s latent space, the *intersection* of their features should be precisely the features that the model *relies on but shouldn’t* – i.e. the spurious correlations.  \n\n5.  Hence, before asking a captioning LLM to describe what it “sees,” you first pick out a small, representative set of both the correct and the misclassified images (using k‐medoids to cover the correct ones and grabbing the most confidently wrong ones).  \n\n6.  That way, when you feed *just* those images to the captioner you:  \n   – keep the workload small, and  \n   – force the captioner to describe *exactly* the slice of the data where the model’s bias lives.  \n\n7.  The captions you get back now contain the high-signal keywords you need to compare against your learned class embedding and finally unmask the spurious features.  \n\n**Answer**  \n\nThey first pull out a compact set of both correctly and misclassified examples so that they can isolate the *shared* features between those two groups—features that the model is using but which actually drive it to make confident mistakes.  By doing that selection *before* captioning, they both reduce the number of images the captioner must process and focus the captions on exactly those image characteristics that reveal the model’s spurious correlations."
    },
    {
        "question": "What motivates comparing class embedding with keywords for ranking biases?",
        "relevant_section_ids": [
            "3.2.5"
        ],
        "relevant_context": [
            "Now, we are ready to compare the embedding of each keyword with E*(c) using the cosine similarity: where ψ is the embedding of the keyword in the same latent space used to calculate E*(c).",
            "This tells us how much the concept is embodied by the proposed keywords.",
            "Based on the ranking, we will obtain a set of keywords that correlate with the learned class c, and others that become decorrelated as they embody some knowledge shared through all the classes (as filtered in (4)).",
            "Finally, as post-processing, we filter all the keywords related to the ground-truth target class the model was aiming at learning: the final ranking we obtain embodies the set of features that correlate with the learned class c, from which an end user of the system can deduce the presence of a bias."
        ],
        "final_answer": "The comparison is motivated by the need to measure how much each candidate keyword concept is embodied in the model’s learned class representation. By computing the cosine similarity between the class embedding and each keyword embedding, the method can rank keywords by their relevance to the class and thus surface those features that most strongly correspond to potential spurious biases.",
        "relevant_elements": [
            "Learned class embedding",
            "Keywords ranking"
        ],
        "id": 777,
        "masked_question": "What motivates comparing [mask1] with [mask2] for ranking biases?",
        "masked_number": 2,
        "masked_elements": [
            "Learned class embedding",
            "Keywords ranking"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.09570v1_figure_2.png",
        "paperid": "2408.09570v1",
        "paper_path": "./papers/2408.09570v1.json",
        "figure_id": "2408.09570v1_figure_2.png",
        "caption": "Figure 2: Pipeline for SaMyNa. Given a model, we can tell on either 𝒟trainsuperscript𝒟train\\mathcal{D}^{\\text{train}}caligraphic_D start_POSTSUPERSCRIPT train end_POSTSUPERSCRIPTor 𝒟valsuperscript𝒟val\\mathcal{D}^{\\text{val}}caligraphic_D start_POSTSUPERSCRIPT val end_POSTSUPERSCRIPT what are the correct (with green border) and the incorrectly (red border) classified samples. Amongst these, we first perform a sample subset selection looking at the latent space of the model under analysis and choosing through k-medoids the most representative samples for the learned class. Then, we employ a captioner to get a textual description of these samples. Among these descriptions, we identify recurrent keywords and, in parallel, working in the latent space of a text encoder, we extract the mean description for the learned classes, cleansed from common features within the dataset. We finally compare this representation with the embedding of the keywords, identifying learned correlations aside from the target.",
        "qtype": "Design_Rationale",
        "response": "They do it because they want to know which of the candidate “bias words” (the blue‐boxed keyword embeddings) actually line up with what the model has learned about class c (the red‐boxed class embedding).  Concretely:\n\n1. Mask 1 (red) is the model’s *learned class embedding* E*(c) – i.e. the average caption embedding for class c with out any dataset-wide “common” features removed.  \n2. Mask 2 (blue) is the set of *keyword embeddings* ψemb,k that cropped up repeatedly in the captions.  \n\nBy computing the cosine similarity between E*(c) and each ψemb,k, we get a ranked list of which keywords most strongly reflect the model’s own internal representation of that class.  Those top‐ranked keywords are precisely the spurious correlations or biases the model has actually picked up."
    },
    {
        "question": "What motivates integrating random masking with recover masked frames for robust transition training?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To address this, we convert the unsupervised problem of generating transitions into a supervised learning framework by leveraging existing continuous sign language videos.",
            "Given a complete continuous sign language sequence X, we simulate missing transitions by randomly masking portions of X, creating gaps that mimic real-world transition absences.",
            "By training the model to predict the masked frames M conditioned on the observable context C, it learns to generate realistic transitions that smoothly bridge the gap.",
            "To generate smooth transitional poses and capture relationships between different segments, we employ a random masking strategy during training, as illustrated in the training phase of Fig. 2.",
            "This masking strategy enables the model to predict missing frames based on observable context, facilitating smooth transition generation."
        ],
        "final_answer": "Because real transition frames are unavailable, the model uses random masking on continuous sign videos to simulate missing segments and then learns, in a supervised way, to recover those masked frames from their surrounding context. This converts an unsupervised transition generation task into a supervised one and enables the network to produce robust, smooth transitions by explicitly training it to predict missing frames.",
        "relevant_elements": [
            "random masking",
            "recover masked frames"
        ],
        "id": 778,
        "masked_question": "What motivates integrating [mask1] with [mask2] for robust transition training?",
        "masked_number": 2,
        "masked_elements": [
            "random masking",
            "recover masked frames"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16810v1_figure_1.png",
        "paperid": "2411.16810v1",
        "paper_path": "./papers/2411.16810v1.json",
        "figure_id": "2411.16810v1_figure_1.png",
        "caption": "Figure 1: Task and key steps. Our work aims to generate continuous sign videos by creating transition poses between discrete segments. In training, random masking simulates missing transitions, and the model learns to recover these frames (steps 1-3). During inference, padding initializes missing transitions, which the model refines to generate smooth, coherent sequences (steps 4-6).",
        "qtype": "Design_Rationale",
        "response": "Chain of thought:\n\n1. In Fig. 1 (steps 1–3), the red‐boxed operation is exactly the “random masking” of frames in a long sign‐pose sequence, and the blue‐boxed operation is the subsequent “recover masked frames” task that uses the diffusion model to in‐paint those missing frames.  \n2. The whole point of masking out real transitions (step 1) is to simulate the very gaps we ultimately want to fill at test time (unsupervised transition generation has no ground‐truth transitions).  \n3. By then training the model to recover exactly those masked‐out frames (step 2), we turn the unsupervised gap‐filling problem into a supervised learning problem, using the true frames as targets.  \n4. This coupling of random masking with masked‐frame recovery forces the model to learn context‐aware, temporally coherent transitions, yielding far more robust and realistic sign‐language frame generation.\n\nAnswer:\n\nThey mask out real transition frames at random so they can use the original frames as “ground truth” targets for the recovery network. In other words, random masking simulates missing transitions and frame recovery supplies the supervised training signal, together enabling the model to learn robust, context‐aware transition generation."
    },
    {
        "question": "What drives implementing initialize padding before predict missing frames during inference?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the inference phase, generating smooth transitions between discrete motion segments is challenging due to the gaps between observed frames.",
            "We leverage the natural correlation between consecutive frames in sign language sequences by introducing a linear interpolation padding initialization strategy, as illustrated in Fig. 2.",
            "This strategy starts by using the last pose of the preceding observed segment, Xₐ, and the first pose of the following observed segment, X_b, as boundary frames for the missing transition segment. The goal is to generate intermediate frames X_m that provide a smooth transition from Xₐ to X_b."
        ],
        "final_answer": "The gaps between discrete observed sign segments make it hard to generate smooth transitions directly. To overcome this, the model first initializes the missing frames via linear interpolation between the last frame of the preceding segment and the first frame of the following segment. This \"initialize padding\" provides a coherent starting point—leveraging natural temporal correlations—so the diffusion model can more effectively predict and refine the missing transition frames.",
        "relevant_elements": [
            "initialize padding",
            "predict missing frames"
        ],
        "id": 779,
        "masked_question": "What drives implementing [mask1] before predict missing frames during inference?",
        "masked_number": 1,
        "masked_elements": [
            "initialize padding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16810v1_figure_1.png",
        "paperid": "2411.16810v1",
        "paper_path": "./papers/2411.16810v1.json",
        "figure_id": "2411.16810v1_figure_1.png",
        "caption": "Figure 1: Task and key steps. Our work aims to generate continuous sign videos by creating transition poses between discrete segments. In training, random masking simulates missing transitions, and the model learns to recover these frames (steps 1-3). During inference, padding initializes missing transitions, which the model refines to generate smooth, coherent sequences (steps 4-6).",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module in step 4 is nothing mystical – it’s simply the “Linear Interpolation Padding Initialization” that the text describes.  Inference in our diffusion framework doesn’t start from pure noise; instead we first stitch in a very rough, linearly interpolated set of frames between the last pose of the preceding segment and the first pose of the following segment.  This padding:\n\n1. Leverages the strong temporal correlation in sign sequences (you know roughly how your hand has to move from A to B),  \n2. Gives the denoiser a reasonable warm‐start rather than starting from scratch,  \n3. And so produces much smoother, more stable transition frames once the diffusion model “refines” them in the subsequent predict-missing-frames pass."
    },
    {
        "question": "How does random masking determine which frames to hide while maintaining sufficient context for transition recovery?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Given a complete continuous sign language sequence S, we simulate missing transitions by randomly masking portions of S, creating gaps that mimic real-world transition absences. Let the masked frames be denoted as Xm, and the observable pose frames as Xo.",
            "The mask M is constructed by randomly selecting frame indices, extending across the dimensions of the latent feature Z. This mask M is then applied to selectively retain or mask out parts of Z, resulting in a masked latent representation: Z_masked = Z ⊙ (1 - M)."
        ],
        "final_answer": "The random masking strategy simply picks a set of frame indices at random to hide—masking out those frames in the latent representation—while leaving the surrounding (unmasked) frames visible. Those visible frames naturally provide the context needed for the model to learn to reconstruct the missing transitions.",
        "relevant_elements": [
            "random masking"
        ],
        "id": 780,
        "masked_question": "How does [mask1] determine which frames to hide while maintaining sufficient context for transition recovery?",
        "masked_number": 1,
        "masked_elements": [
            "random masking"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.16810v1_figure_1.png",
        "paperid": "2411.16810v1",
        "paper_path": "./papers/2411.16810v1.json",
        "figure_id": "2411.16810v1_figure_1.png",
        "caption": "Figure 1: Task and key steps. Our work aims to generate continuous sign videos by creating transition poses between discrete segments. In training, random masking simulates missing transitions, and the model learns to recover these frames (steps 1-3). During inference, padding initializes missing transitions, which the model refines to generate smooth, coherent sequences (steps 4-6).",
        "qtype": "Implementation_Details",
        "response": "The thing in the red box is nothing exotic – it is simply our “random masking” of the latent pose‐sequence during training.  Concretely:\n\n1.  We take the encoded long-video latent Z of length T and build a binary mask M of the same length.  \n2.  We uniformly (i.e. randomly) pick a subset of frame indices in [1…T] to zero out in Z.  That subset becomes our “masked” frames.  \n3.  Because the sampling is random and only covers part of the sequence, there are always unmasked frames immediately before and after each hole.  Those surviving frames supply the model with the boundary context (the “where I left off” and “where I pick back up”) that it needs to hallucinate a smooth transition.  \n\nIn this way the model learns, from many different random gaps, how to interpolate plausible in-between poses, yet at every training example it still sees enough unmasked frames on either side to anchor its predictions."
    },
    {
        "question": "How does initialize padding compute intermediate poses from boundary frames for predicting missing transition frames?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the inference phase, generating smooth transitions between discrete motion segments is challenging due to the gaps between observed frames. We leverage the natural correlation between consecutive frames in sign language sequences by introducing a linear interpolation padding initialization strategy, as illustrated in Fig. 2.",
            "This strategy starts by using the last pose of the preceding observed segment, z_start, and the first pose of the following observed segment, z_end, as boundary frames for the missing transition segment. The goal is to generate intermediate frames z_i that provide a smooth transition from z_start to z_end. Using linear interpolation, we initialize these missing frames as follows:",
            "This interpolation scheme gradually fills in the key points at each time step, creating intermediate poses that ensure a coherent and natural flow between segments. This strategy facilitates smoother transitions and provides a stable foundation for later refinement, resulting in seamless sign language video synthesis."
        ],
        "final_answer": "Initialize padding creates each missing transition pose by linearly interpolating between the last observed pose of the preceding segment (z_start) and the first observed pose of the following segment (z_end). Concretely, for the i-th missing frame out of N, its pose is set to ((N–i)/N)·z_start + (i/N)·z_end, producing a smooth sequence of intermediate poses.",
        "relevant_elements": [
            "initialize padding",
            "predict missing frames"
        ],
        "id": 781,
        "masked_question": "How does [mask1] compute intermediate poses from boundary frames for predicting missing transition frames?",
        "masked_number": 1,
        "masked_elements": [
            "initialize padding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.16810v1_figure_1.png",
        "paperid": "2411.16810v1",
        "paper_path": "./papers/2411.16810v1.json",
        "figure_id": "2411.16810v1_figure_1.png",
        "caption": "Figure 1: Task and key steps. Our work aims to generate continuous sign videos by creating transition poses between discrete segments. In training, random masking simulates missing transitions, and the model learns to recover these frames (steps 1-3). During inference, padding initializes missing transitions, which the model refines to generate smooth, coherent sequences (steps 4-6).",
        "qtype": "Implementation_Details",
        "response": "Here’s the step-by-step reasoning to identify and explain what “[mask1]” is doing:\n\n1.  In the diagram, the red box around step 4 is labeled “initialize padding.”  \n2.  In the text this corresponds exactly to the “Linear Interpolation Padding” strategy under Inference.  \n3.  That section tells us that to fill a gap of m frames between two observed poses—call them  \n       •  xₚ (the last pose of the preceding segment)  \n       •  x𝗳 (the first pose of the following segment)  \n    —we simply linearly interpolate each missing frame.  \n4.  Concretely, for i=1…m we set the i-th missing frame to  \n      ĥxᵢ = (1 – i/(m+1))⋅xₚ  +  (i/(m+1))⋅x𝗳  \n    so that when i=1 it’s almost entirely xₚ, when i=m it’s almost entirely x𝗳, and in between it smoothly blends.  \n\nAnswer:  \n[mask1] is the linear‐interpolation padding initializer.  It takes the last observed frame of the previous segment and the first observed frame of the next segment and for each missing slot i (out of m) computes  \n\n ĥxᵢ = (m+1 – i)/(m+1) · x_prev  +  i/(m+1) · x_next  \n\nthus providing a smooth, evenly spaced set of “guessed” poses that the diffusion model then refines."
    },
    {
        "question": "How does CKKS homomorphic encryption handle quantized integer values during ciphertext packing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Encryption",
            "Quantization"
        ],
        "id": 782,
        "masked_question": "How does [mask1] homomorphic handle quantized integer values during ciphertext packing?",
        "masked_number": 1,
        "masked_elements": [
            "Encryption"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05260v1_figure_1.png",
        "paperid": "2411.05260v1",
        "paper_path": "./papers/2411.05260v1.json",
        "figure_id": "2411.05260v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed QuanCrypt-FL framework.",
        "qtype": "Implementation_Details",
        "response": "In the diagram the red‐boxed block is simply the “Encryption” step – i.e. “Encrypt using the CKKS scheme.”  What that really means is:\n\n  1.  After you have quantized your weight‐updates into low‐bit integers (say 8 – 32 bit), you view that little integer vector as a fixed‐point real vector by multiplying by your CKKS scaling factor Δ and rounding to the nearest plaintext integer.  \n  2.  You then call CKKS’s native batch‐encoder (sometimes called “EncodeBatch”) to take that integer vector and embed it into one single plaintext polynomial P(x).  Under the hood CKKS uses the Chinese remainder/batching isomorphism on Rₜ[X]/(Xⁿ+1) to store all of your quantized entries in the different “slots” of P.  \n  3.  Finally you encrypt P(x), producing one ciphertext that in effect contains your entire quantized update in SIMD form.  \n\nBecause CKKS is approximate‐arithmetic and supports SIMD, you get to homomorphically add and multiply that entire packed integer vector in parallel while it remains encrypted.  In short, CKKS “handles” your quantized ints simply by (a) scaling them into fixed‐point integers, (b) batching them into the coefficients/slots of a single plaintext polynomial, and (c) encrypting that polynomial."
    },
    {
        "question": "How does dynamic unstructured pruning update its mask based on L1 norm and pruning rate evolution?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "During local model training, we employed a pruning technique to iteratively remove less important weights or gradients from the model updates. Specifically, clients perform soft, unstructured pruning based on the L1 norm, which creates a sparse model and makes the FL training process more efficient. The pruning process is guided by a dynamically updated pruning rate δ_t, which increases over the communication rounds, allowing for more aggressive pruning as training progresses.",
            "The pruning rate δ_t is updated iteratively using the equation 3, where t is the pruning rate at round t, t₀ is the effective round when pruning starts, t_f is the target round when the target pruning rate is reached, δ₀ is the initial pruning rate, and δ_f is the target pruning rate. This pruning rate increases gradually from the initial value to the target value, ensuring that pruning is progressively applied more aggressively as training advances.",
            "Once pruning is applied to the model updates at each client, the pruned local model update Ŵ_p^t is computed as in equation 4, where ⊙ represents the element-wise product, and M_p^t is the local pruning mask generated to identify which weights to prune at communication round t."
        ],
        "final_answer": "At each communication round t a new binary mask M_p^t is generated by ranking all weights (or gradients) in each layer by their L1-norm magnitudes and zeroing out the smallest-magnitude fraction. The fraction of weights to be pruned, δ_t, is not fixed but follows a schedule (Eq.3) that smoothly increases from an initial pruning rate δ₀ at start round t₀ to a target rate δ_f by round t_f. The mask M_p^t is then applied element-wise (Ŵ_p^t = ΔW_p^t ⊙ M_p^t) to produce the pruned update sent for aggregation.",
        "relevant_elements": [
            "Model Pruning"
        ],
        "id": 783,
        "masked_question": "How does dynamic unstructured [mask1] update its mask based on L1 norm and pruning rate evolution?",
        "masked_number": 1,
        "masked_elements": [
            "Model Pruning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05260v1_figure_1.png",
        "paperid": "2411.05260v1",
        "paper_path": "./papers/2411.05260v1.json",
        "figure_id": "2411.05260v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed QuanCrypt-FL framework.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed block in Figure 1 is nothing mystical – it is simply our “dynamic, unstructured pruning” step.  At each pruning event (in practice every m communication rounds) we:\n\n 1. Take the current weight or update tensor (either the global model on the server or the local Δw on the client).  \n 2. Compute the L₁–norm of each element (or each filter, or each neuron, depending on granularity).  \n 3. Look up our current pruning rate P(t), which has been marching upward from an initial P₀ to a final P_f according to the schedule in Equation (3).  \n 4. Choose a cutoff so that exactly P(t) of the total weights lie below it in L₁–magnitude.  \n 5. Build a binary mask mₜ that zeroes out (prunes) all weights whose L₁–norm is below that cutoff and leaves the rest untouched.  \n 6. Apply that mask by simple Hadamard product:  \n      Δw_pₜ = Δwₜ ∘ mₜ  \n\nBecause P(t) grows from round t₀ to T (Equation 3), each time we rebuild mₜ more and more of the smallest‐magnitude weights are dropped, hence the mask evolves dynamically in lock‐step with the pruning‐rate schedule."
    },
    {
        "question": "How does quantization affect CKKS encryption efficiency and communication overhead during secure model updates?",
        "relevant_section_ids": [
            "1",
            "4"
        ],
        "relevant_context": [
            "Second, to address the high computational complexity of HE in FL, we implement a low-bit quantization technique to reduce upload costs for users or organizations, although this introduces challenges related to numerical overflows.",
            "Next, each client performs quantization on the pruned and clipped model updates to reduce communication costs. The quantization process involves calculating the scaling factor s and determining the quantized values Q to ensure the updates are compressed before transmission. The quantized values are then clamped to the range [−l,l] and converted to the appropriate data type based on the bit width (e.g., 8-bit, 16-bit, or 32-bit) to minimize communication overhead. After completing quantization, each client encrypts the quantized model updates using the CKKS homomorphic encryption scheme.",
            "We integrate low-bit quantization and dynamic pruning with HE to enhance both efficiency and privacy. Quantization reduces the precision of model weights, resulting in a 3X reduction in storage usage."
        ],
        "final_answer": "By converting full-precision updates into low-bit integers before CKKS encoding, quantization compresses the plaintext and thus shrinks the resulting ciphertexts. This both speeds up CKKS encryption/decryption and cuts the size of data sent over the network, yielding roughly a 3× reduction in storage and communication overhead for secure model updates.",
        "relevant_elements": [
            "Quantization",
            "Encryption"
        ],
        "id": 784,
        "masked_question": "How does [mask1] affect CKKS encryption efficiency and communication overhead during secure model updates?",
        "masked_number": 1,
        "masked_elements": [
            "Quantization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05260v1_figure_1.png",
        "paperid": "2411.05260v1",
        "paper_path": "./papers/2411.05260v1.json",
        "figure_id": "2411.05260v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed QuanCrypt-FL framework.",
        "qtype": "Literature_Background",
        "response": "Here “​[mask1]” is simply the quantization block in the red box – i.e. the step where each client takes its full‐precision weight‐update Δw, maps it into a low–bit‐width representation (e.g. 8– or 16–bit integers), and then feeds that into the CKKS encoder/encryption routine.  Because CKKS’s cost (both in time and in the eventual size of the ciphertext) grows with the bit‐length and dynamic range of the plaintext you are encoding, quantizing first has two immediate benefits:\n\n 1. Encryption/Decryption Speed  \n    – By shrinking your model updates down to a smaller integer range, the encoder needs fewer slots/moduli, the underlying number–theoretic transforms work over smaller moduli, and all of the subsequent NTT‐based polynomial multiplications are cheaper.  \n    – Empirically this translates into a 2×–4× speed‐up in the encrypt/decrypt routines versus encrypting 32– or 64–bit floats directly under CKKS.  \n\n 2. Communication Overhead  \n    – The ciphertext size in CKKS is essentially proportional to the plaintext bit‐width (because you need one or more RNS limbs of the same size as your target modulus).  By quantizing from 32–bit floats to 8–bit (for instance), you cut your ciphertext payload by roughly 4×.  \n    – Since each round of Federated Learning ships one ciphertext per layer (or per packed slot), that reduction directly slashes bandwidth requirements on the uplink (and downlink if you also quantize the global model).  \n\nIn short, inserting the low‐bit quantization step before CKKS encoding greatly reduces both the compute cost of homomorphic operations and the number of bytes you must transmit each round, at only a small controllable loss in numerical precision (which is then recovered by careful dynamic clipping)."
    },
    {
        "question": "How does dynamic unstructured pruning affect FedAvg aggregation complexity and communication overhead across rounds?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "During local model training, we employed a pruning technique to iteratively remove less important weights or gradients from the model updates. Specifically, clients perform soft, unstructured pruning based on the L1 norm, which creates a sparse model and makes the FL training process more efficient.",
            "After pruning, clients send their pruned updates to the server, which aggregates them using FedAvg to generate the global model. This pruning technique not only reduces the model size and computational costs but also makes the training process more resistant to inference attacks.",
            "By progressively increasing the pruning rate, the communication efficiency improves throughout the rounds. As clients share a sparsified model with the server, the transmitted model is no longer the full model, limiting the information available to potential attackers."
        ],
        "final_answer": "Dynamic unstructured pruning gradually increases the sparsity of each client’s update over the course of training rounds. As more weights are zeroed out, each FedAvg aggregation step has fewer nonzero parameters to process—reducing aggregation complexity—and the size of each transmitted update shrinks, lowering the communication overhead round by round.",
        "relevant_elements": [
            "Model Pruning",
            "Aggregation"
        ],
        "id": 785,
        "masked_question": "How does dynamic unstructured [mask1] affect FedAvg [mask2] complexity and communication overhead across rounds?",
        "masked_number": 2,
        "masked_elements": [
            "Model Pruning",
            "Aggregation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.05260v1_figure_1.png",
        "paperid": "2411.05260v1",
        "paper_path": "./papers/2411.05260v1.json",
        "figure_id": "2411.05260v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed QuanCrypt-FL framework.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify the two masked regions in the figure:\n\n 1. The red‐boxed operation on each client is “dynamic unstructured pruning” of the local update.  \n 2. The blue‐boxed box at the server is the “Aggregation – Aggregate Enc(w) using FedAvg” step.  \n\nQuestion restated:  \n“How does applying dynamic unstructured pruning on the client side (mask 1) change the computational complexity of FedAvg at the server (mask 2) and the amount of data exchanged each round?”\n\nChain of thought:\n\n1. In vanilla FedAvg you send back a full dense weight‐update vector of dimension d from each client, and the server does O(N·d) work to sum and average them (where N is the number of clients).  Communication = O(N·d) floats per round.  \n\n2. With dynamic unstructured pruning each client zeroes out a growing fraction of its weights (based on some pruning rate ρₜ that increases over rounds).  Suppose after pruning only kₜ out of d weights remain nonzero (kₜ≪d once pruning ramps up).  \n\n3. ► Communication overhead:  \n   – Instead of transmitting d parameters, each client only needs to send its kₜ nonzero values (and their indices or a shared mask).  In practice that cuts bandwidth roughly by a factor kₜ/d.  \n   – Since ρₜ grows over time, kₜ shrinks over rounds, so the per‐round byte count decreases as training proceeds.  \n\n4. ► Server‐side FedAvg complexity:  \n   – Naively, FedAvg still averages d‐length vectors, but if you exploit sparsity you only have to touch kₜ entries per client.  The total work drops from O(N·d) to O(N·kₜ).  \n   – In other words, summation/averaging cost shrinks in proportion to the fraction of weights kept.  \n\n5. ► Overhead of the “dynamic” part:  \n   – You must keep track of which entries are pruned each round (sending or agreeing on a new mask every m rounds).  That mask-synchronization or index‐encoding adds a small extra cost—but compared to d, sending a bitmask or index list is still much cheaper when sparsity is high.  \n   – The client must compute the pruning mask and the server may have to reconfigure its sparse accumulator, but both of these costs are sublinear in d and become negligible once d is large.  \n\nPutting it all together:\n\nAnswer:\n\n  • By pruning out an ever‐increasing fraction of weights on each client, the dimension of each update falls from d to kₜ≪d.  \n  • This reduces the FedAvg aggregation cost at the server from O(N·d) per round to O(N·kₜ).  \n  • Likewise, the bytes sent from each client drop from O(d) to O(kₜ) (plus a tiny overhead for sending the mask or indices).  \n  • Because the pruning rate ramps up over rounds, both the computational load on the server and the network traffic shrink progressively as training proceeds.  \n  • The only extra overhead is the occasional transmission or re-construction of the new pruning mask, which is negligible compared with the savings from sparsity."
    },
    {
        "question": "How does MHA layer interaction affect FFN Factorization compared to MoE-based decomposition methodologies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MHA layer",
            "FFN Factorization"
        ],
        "id": 786,
        "masked_question": "How does [mask1] interaction affect FFN Factorization compared to MoE-based decomposition methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "MHA layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.11855v1_figure_1.png",
        "paperid": "2408.11855v1",
        "paper_path": "./papers/2408.11855v1.json",
        "figure_id": "2408.11855v1_figure_1.png",
        "caption": "Figure 1: Overall Framework of FactorLLM. Teacher Model: Original transformer blocks with multi-head attention (MHA) and feed-forward layers. Student Model: Modified blocks composed of the same MHA layers and factorized FFN, with a linear router deciding which expert(s) tokens will pass through. Training Process: Input tokens branch into normal transformer layers and FactorLLM to produce ground-truth (GT) and predictions respectively. Transformers freeze to distill FactorLLM based on compositional loss, including mean square error (MSE) between per-layer representations, cross entropy (CE) loss between per-layer optimal and routing masks, and final CE loss between GT and predictions.",
        "qtype": "Literature_Background",
        "response": "First, let’s be clear about what the red‐boxed region is: it’s the intact Multi-Head-Attention (MHA) sub-layer in each Transformer block of the *teacher* model.  In FactorLLM we **never** break up or modify that MHA – we only factorize the **feed-forward (FFN)** weights that follow it.  This has two very important consequences compared with “pure” MoE-style FFN decompositions:\n\n1.  **Stable, High-Fidelity Inputs to the Experts**  \n    Because the MHA is left intact, every expert in our MoE-FFN sees exactly the same high-quality attention output that the original monolithic FFN did.  We then *distill* those MHA outputs into our experts via a per-layer MSE loss, so each little FFN piece learns precisely the slice of knowledge it needs, in the exact feature space the original model used.\n\n2.  **No Heavy‐Handed Load-Balancing Needed**  \n    Most MoE-FFN schemes randomly initialize expert blocks or cluster neurons by some heuristic and then slap on a “balance” penalty to try to keep the router from collapsing onto a few experts.  In contrast, our router is *guided* from the very start by the untouched MHA outputs (via pseudo-labels derived from the teacher’s full FFN), so it never wanders off into those degenerate “one-expert-fits-all” regimes.  We only need a small cross-entropy term to match the teacher’s expert‐selection pattern; we do *not* require the large load-balancing losses that standard MoEs rely on.\n\nIn short, by preserving the MHA → FFN interface exactly as it was trained, and then using that fixed MHA output as a *teacher signal* for splitting the FFN into experts, FactorLLM achieves a far cleaner, more data-efficient factorization than MoE methods which break up the FFN in isolation and must then struggle to rebalance or recover lost attention-to-FFN alignment."
    },
    {
        "question": "How does Cross Entropy training shape Router allocations akin to load balancing in MoE routing designs?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "To mitigate this imbalance, a load balance loss, denoted as L_balance, was introduced in [37] to penalize uneven input distribution among experts: where I_{n,j}=1 if the jth expert is selected for processing token n and 0 otherwise.",
            "Subsequently, we apply the TopK algorithm to extract expert indices T for the smallest distances, leading to a pseudo router allocation ˆy, where elements corresponding to indices in T are set to 1 and all others to 0, defined as ˆy. Therefore, leveraging the pre-established pseudo label ˆy, we expedite the router’s update using the cross-entropy function:"
        ],
        "final_answer": "Rather than using an explicit balancing penalty, FactorLLM builds a one-hot “pseudo-label” for each token by picking the TopK experts whose outputs most closely match the teacher (via MSE + TopK). It then trains the freshly injected router with a cross-entropy loss against these labels. This cross-entropy supervision forces the router to mimic the teacher’s allocations—effectively steering traffic among experts in a balanced, MoE-style manner without adding a separate load-balance term.",
        "relevant_elements": [
            "Router",
            "Cross Entropy"
        ],
        "id": 787,
        "masked_question": "How does [mask1] training shape Router allocations akin to load balancing in MoE routing designs?",
        "masked_number": 1,
        "masked_elements": [
            "Cross Entropy"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.11855v1_figure_1.png",
        "paperid": "2408.11855v1",
        "paper_path": "./papers/2408.11855v1.json",
        "figure_id": "2408.11855v1_figure_1.png",
        "caption": "Figure 1: Overall Framework of FactorLLM. Teacher Model: Original transformer blocks with multi-head attention (MHA) and feed-forward layers. Student Model: Modified blocks composed of the same MHA layers and factorized FFN, with a linear router deciding which expert(s) tokens will pass through. Training Process: Input tokens branch into normal transformer layers and FactorLLM to produce ground-truth (GT) and predictions respectively. Transformers freeze to distill FactorLLM based on compositional loss, including mean square error (MSE) between per-layer representations, cross entropy (CE) loss between per-layer optimal and routing masks, and final CE loss between GT and predictions.",
        "qtype": "Literature_Background",
        "response": "Here’s the key idea, broken down step-by-step:\n\n1.  In standard MoE you add an explicit “load‐balancing” term to your loss so that your gating network doesn’t collapse onto just one or two experts.  \n2.  In FactorLLM we do something very similar—but instead of hand‐crafting a balancing penalty, we derive “where each token ought to go” from a frozen, pretrained teacher and then teach the new router to match that.  \n3.  Concretely, we compute a binary “pseudo‐routing mask” for each token by (a) measuring which student experts best reproduce the teacher’s FFN output (via an MSE to each expert) and (b) picking the top‐K experts as the 1’s in our mask.  \n4.  We then take the router’s own softmax‐gated Top-K probabilities and train it with a Cross‐Entropy loss against this pseudo‐routing mask.  \n5.  That Cross‐Entropy term therefore does double duty—it supervises the router to select the “right” experts (per the teacher) and, because those pseudo‐labels are spread across different experts, it implicitly enforces a balanced use of all experts in exactly the same way a conventional MoE load-balance penalty would."
    },
    {
        "question": "How does cross-entropy supervision of router outputs shape dynamic expert activation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We define y* as the output of the teacher’s feed-forward network FFN_T, and Y as the outputs from the student experts {FFN_i}. We first compute the Mean Squared Error (MSE) across these features, yielding a set of distances d. Subsequently, we apply the TopK algorithm to extract expert indices S for the smallest d, leading to a pseudo router allocation Y* , where elements corresponding to indices in S are set to 1 and all others to 0, defined as Y* = TopK(d).",
            "Therefore, leveraging the pre-established pseudo label Y* , we expedite the router’s update using the cross-entropy function:\n\nL_PA = −∑_{l=1}^{L} Y*_l ⋅ log (Router(X)_l)\n\nHere, L denotes the number of layers in FFN, and Router(X)_l represents the router’s output distribution over experts in layer l."
        ],
        "final_answer": "By applying a cross-entropy loss between the router’s predicted expert-selection distribution and a pseudo-label derived from the teacher’s feature distances, the router is directly trained to activate exactly those experts whose outputs most closely match the teacher. This supervision forces the router to learn dynamic, per-token expert assignments that align with the teacher’s knowledge, thereby shaping which experts are activated at inference time.",
        "relevant_elements": [
            "Cross-Entropy loss",
            "router",
            "Experts"
        ],
        "id": 789,
        "masked_question": "How does cross-entropy supervision of [mask1] outputs shape dynamic expert activation?",
        "masked_number": 1,
        "masked_elements": [
            "router"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.11855v1_figure_1.png",
        "paperid": "2408.11855v1",
        "paper_path": "./papers/2408.11855v1.json",
        "figure_id": "2408.11855v1_figure_1.png",
        "caption": "Figure 1: Overall Framework of FactorLLM. Teacher Model: Original transformer blocks with multi-head attention (MHA) and feed-forward layers. Student Model: Modified blocks composed of the same MHA layers and factorized FFN, with a linear router deciding which expert(s) tokens will pass through. Training Process: Input tokens branch into normal transformer layers and FactorLLM to produce ground-truth (GT) and predictions respectively. Transformers freeze to distill FactorLLM based on compositional loss, including mean square error (MSE) between per-layer representations, cross entropy (CE) loss between per-layer optimal and routing masks, and final CE loss between GT and predictions.",
        "qtype": "Experimental_Results",
        "response": "Here is the reasoning, step by step:\n\n1.  In the student block the red‐boxed module is the newly injected **router**, which at each layer emits a small “mask” or probability vector over the ​E​ experts (this is what we’ve labeled as [mask1] in the figure).  \n2.  At the start of training this router is random, so it has no clue which experts are the best match for any given token.  \n3.  In parallel, the **teacher** FFN (a frozen, pretrained block) produces its own activations on the same input, and we measure the L₂ (MSE) distance between the teacher’s output and each student‐expert’s output.  \n4.  We pick the top-K experts whose student‐expert outputs most closely match the teacher as our “pseudo-ground‐truth” routing mask (a one-hot or sparse binary vector).  \n5.  We then apply a **cross-entropy loss** between that teacher‐derived mask and the router’s soft output [mask1].  \n6.  Minimizing this cross-entropy forces the router’s probability distribution to align with the teacher’s choice of experts, so that over training the router learns to “turn on” exactly those experts that best reproduce the teacher’s behavior.  \n\nIn this way, **cross-entropy supervision of the router outputs directly sculpts the router’s dynamic expert‐activation patterns**, steering it to select the same subset of experts that yield minimal MSE to the teacher’s FFN."
    },
    {
        "question": "How does Compositional Incremental Learning enforce nonrecurrence of composition classes while enabling primitives to recur?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We note that the composition classes between incremental tasks are always disjoint, which means C_i ∩ C_j = ∅ for any i ≠ j.",
            "Different from the composition classes, the primitive classes are allowed to recur in different tasks. That means it allows the tasks to share some primitive concepts of objects and states."
        ],
        "final_answer": "Compositional Incremental Learning enforces nonrecurrence of composition classes by requiring that the sets of composition classes in any two tasks be disjoint (C_i ∩ C_j = ∅). At the same time, it permits the underlying primitives (state classes and object classes) to recur across tasks, allowing tasks to share primitive concepts even though their compositions are distinct.",
        "relevant_elements": [
            "Compositional Incremental Learning",
            "Primitives Recurrence"
        ],
        "id": 790,
        "masked_question": "How does [mask1] enforce nonrecurrence of composition classes while enabling primitives to recur?",
        "masked_number": 1,
        "masked_elements": [
            "Compositional Incremental Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01739v2_figure_1.png",
        "paperid": "2411.01739v2",
        "paper_path": "./papers/2411.01739v2.json",
        "figure_id": "2411.01739v2_figure_1.png",
        "caption": "Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions.",
        "qtype": "Experimental_Results",
        "response": "Here’s the core idea, unpacked step by step:\n\n1.  In Compositional‐IL (the red–boxed panel (c)) every “class” is not a bare object or a bare state but a unique state‐object pair y=(s,o).  \n2.  At dataset‐construction time we partition the full set of possible pairs into T tasks so that no specific pair ever appears in more than one task (i.e. C₁∩C₂=∅, C₂∩C₃=∅, …).  This guarantees that each composition class is seen exactly once.  \n3.  At the same time, the underlying pools of primitives—S={all states} and O={all objects}—are shared globally across all T tasks.  We simply recombine them into new pairs for each task.  \n4.  As a result you never “replay” the same pair (nonrecurrence of compositions), but you do allow the same state (e.g. “yellow”) or the same object (e.g. “suit”) to reappear in multiple tasks (primitives recur).  \n\nIn short, by making your atomic classes be (state, object) pairs and then slicing those pairs up into disjoint tasks, you automatically enforce zero overlap at the pair‐level while still drawing from the same state‐ and object–vocabularies over and over."
    },
    {
        "question": "How does Blurry Incremental Learning’s class recurrence mechanism influence Compositional Incremental Learning’s state-object labeling protocol?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Blurry Incremental Learning",
            "Compositional Incremental Learning"
        ],
        "id": 791,
        "masked_question": "How does [mask1]’s class recurrence mechanism influence Compositional Incremental Learning’s state-object labeling protocol?",
        "masked_number": 1,
        "masked_elements": [
            "Blurry Incremental Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01739v2_figure_1.png",
        "paperid": "2411.01739v2",
        "paper_path": "./papers/2411.01739v2.json",
        "figure_id": "2411.01739v2_figure_1.png",
        "caption": "Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions.",
        "qtype": "Experimental_Results",
        "response": "In the three‐panel schematic (Fig. 1), panel (b)—our “blur‐IL” or Blurry Incremental Learning—relaxes the strict no‐recurrence rule of standard class‐IL by allowing previously seen object classes to pop up again in later tasks.  In our new “compositional‐IL” setting (panel c), we simply lift that blur‐IL recurrence idea from whole classes down to the two primitive parts of a composition: states and objects.  Concretely:\n\n– In blur‐IL you may see “suit” in Task 1 and then again in Task 2 (class recurrence).  \n– In compositional‐IL we require each complete state–object pair (“Yellow Dress,” “White Suit,” etc.) to be new, but we still re‐use the underlying state words (“Yellow,” “White,” …) and object words (“Dress,” “Suit,” …) across tasks.  \n\nThus the blur‐IL class‐recurrence mechanism directly inspires our compositional labeling scheme: we tag every image with a compound label (state, object), and we allow each primitive (state or object) to reappear freely even though the full composition never repeats."
    },
    {
        "question": "What challenges might arise from maintaining disjoint state-object compositions while allowing primitives recurrence?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Although the composition classes are disjoint across incremental tasks, the primitive classes (i.e. objects and states) encountered in old tasks are allowed to reappear in new tasks. Unfortunately, existing incremental learning approaches are challenged by such a compositional scenario, because their models excessively prioritize the object primitives while neglecting the state primitives. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable.",
            "The main stumbling block in composition-IL is the ambiguous composition boundary. Although the composition label consists of two primitives (i.e. object and state), we note that the model excessively prioritizes the object primitive while neglecting the state primitive. Consequently, the compositions with the same object but with different states become ambiguous and indistinguishable."
        ],
        "final_answer": "Maintaining disjoint compositions while allowing primitives to recur leads to an ambiguous composition boundary: models tend to focus on object primitives at the expense of state primitives, making different compositions that share the same object but have different states hard to distinguish.",
        "relevant_elements": [
            "Primitives Recurrence",
            "State-Object Composition"
        ],
        "id": 792,
        "masked_question": "What challenges might arise from maintaining disjoint [mask1] while allowing [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "State-Object Composition",
            "Primitives Recurrence"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01739v2_figure_1.png",
        "paperid": "2411.01739v2",
        "paper_path": "./papers/2411.01739v2.json",
        "figure_id": "2411.01739v2_figure_1.png",
        "caption": "Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions.",
        "qtype": "Others",
        "response": "Here is a step‐by‐step breakdown:\n\n1.  Image–text alignment  \n    - In Fig. 1(c), the red box ([mask1]) encloses the *composition* labels in Task 2 (“Silver Skirt”, “Yellow Shorts”, “Black Suit”).  These are guaranteed to be *disjoint* from Task 1’s compositions.  \n    - The blue box ([mask2]) highlights that although compositions never repeat, the *primitives* (“Silver”, “Yellow”, “Black” – states – and “Skirt”, “Shorts”, “Suit” – objects) *can* reappear across tasks.\n\n2.  What maintaining disjoint compositions but allowing primitive recurrence means  \n    - You never train on the exact same state–object pair twice.  \n    - But individual states and objects keep turning up in new combinations.\n\n3.  The core challenge that pops out  \n    - Because the network only ever sees each full composition once, it has to *reassemble* its knowledge of states and objects on the fly.  \n    - In practice, the model latches onto the *stronger*, easier-to‐tell-apart signal (usually the object), and largely *ignores* the weaker one (the state).  \n    - So two compositions sharing the same object (e.g. “White Dress” vs. “Black Dress”) become *ambiguous* in feature space, and the old compositions slip from memory once you’ve moved on.\n\nIn short, forcing each composition to be new while letting its parts recur makes it very hard to carve out *fine-grained* class boundaries. The learner overemphasizes objects at the expense of states, leading to confused decision regions and rapid forgetting of earlier compositions."
    },
    {
        "question": "How could blurry incremental learning's class recurrence strategy be adapted for compositional incremental learning?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "To break such a strict limitation, recent studies develop a new setting mostly called Blurry Incremental Learning (blur-IL) MVP ###reference_b24###; CLIB ###reference_b11###, where the incremental sessions allow the recurrence of previous classes, resulting in a more realistic and flexible scenario.",
            "As compared in Fig. 1 ###reference_###, we can see that composition-IL integrates the characteristics of class-IL and blur-IL. Although the composition classes are disjoint across incremental tasks, the primitive classes (i.e. objects and states) encountered in old tasks are allowed to reappear in new tasks."
        ],
        "final_answer": "By following blur-IL’s idea of letting classes recur in later sessions—but applying it at the level of primitive concepts—composition-IL allows old object and state primitives (rather than full compositions) to reappear in new tasks, while still keeping individual composition labels disjoint.",
        "relevant_elements": [
            "Blurry Incremental Learning",
            "Compositional Incremental Learning"
        ],
        "id": 793,
        "masked_question": "How could [mask1]'s class recurrence strategy be adapted for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Blurry Incremental Learning",
            "Compositional Incremental Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.01739v2_figure_1.png",
        "paperid": "2411.01739v2",
        "paper_path": "./papers/2411.01739v2.json",
        "figure_id": "2411.01739v2_figure_1.png",
        "caption": "Figure 1: Differences between Class Incremental Learning (class-IL), Blurry Incremental Learning (blur-IL), and Compositional Incremental Learning (composition-IL). The object classes are not allowed to recur in the class-IL scenario, whereas they may recur randomly in the blur-IL scenario. Different from them, the classes in composition-IL involve state-object compositions apart from the object classes. Besides, the compositions do not reoccur, but the primitives (states or objects) may randomly reappear across incremental sessions.",
        "qtype": "Others",
        "response": "Here is a sketch of the reasoning and the answer:\n\n1.  Identify mask₁ and mask₂  \n    -  mask₁ is the red‐boxed panel, i.e. “Blurry Incremental Learning (blur-IL) with class recurrence.”  \n    -  mask₂ is the blue‐boxed panel, i.e. “Compositional Incremental Learning (composition-IL) with state–object primitives.”  \n\n2.  What blur-IL’s recurrence strategy actually does  \n    -  In blur-IL you explicitly allow *old object classes* to reappear in later tasks.  \n    -  At each new session you see a mixture of brand-new object classes and some that you’ve already learned.  \n    -  To prevent catastrophic forgetting of the classes that reappear, blur-IL methods typically use one (or a combination) of these ideas:  \n       •  a small replay buffer of exemplars,  \n       •  a distillation or regularization loss toward the old model’s outputs, or  \n       •  parameter‐importance weighting (EWC, SI, etc.).  \n\n3.  What composition-IL requires instead  \n    -  In composition-IL the *whole set* of fine-grained compositions (e.g. “Yellow Dress,” “White Suit,” etc.) is still strictly disjoint across tasks (no composition recurs).  \n    -  However, *primitives* (the objects like “Dress,” “Suit,” *and* the states like “Yellow,” “White”) are allowed to randomly recur across tasks.  \n    -  The big challenge is to avoid forgetting both kinds of primitives and to keep them disentangled so new compositions can be formed without confusion.\n\n4.  How to adapt blur-IL’s class recurrence to composition-IL’s primitives recurrence  \n    →  **Simply treat each primitive type (states and objects) as its own “blur-IL class,” and apply the very same recurrence techniques to their prompt pools.**\n\n    Concretely:  \n    •  Split your model’s memory into three prompt pools (as in the paper):  \n       –  one for states,  \n       –  one for objects,  \n       –  one for full compositions.  \n    •  Whenever a *state* or an *object* primitive that you already learned in an earlier task reappears in the new task, handle it exactly as blur-IL would handle a recurring class.  \n       –  If you use replay, sample a few old-state or old-object exemplars and mix them in.  \n       –  If you use distillation, add a KL‐loss or LwF‐style loss on the state‐pool and object-pool outputs.  \n       –  If you use EWC/SI, compute importance scores for prompts in the state and object pools and penalize big updates on those that were crucial for old primitives.  \n    •  Meanwhile allow the *composition* pool to learn its new compositions from scratch (no replay or distillation on full compositions, since they never recur).\n\n5.  In one sentence  \n    “You simply lift blur-IL’s ‘allow/replay/distill/regularize old classes’ recipe up one level, and apply it separately to the two primitive prompt pools (states and objects) instead of to whole composition classes.”\n\nThat way, as soon as “Yellow” or “Suit” shows up again in a later batch, you treat it just like blur-IL would treat a recurring class—but confined to the state-prompt or object-prompt subnetwork—thus preserving old primitive knowledge while still being free to form brand-new state–object pairs."
    },
    {
        "question": "What privacy risks emerge from constructing point clouds using MVS on user-captured images?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MVS",
            "point clouds"
        ],
        "id": 794,
        "masked_question": "What privacy risks emerge from constructing [mask1] using MVS on user-captured images?",
        "masked_number": 1,
        "masked_elements": [
            "point clouds"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.14316v1_figure_2.png",
        "paperid": "2409.14316v1",
        "paper_path": "./papers/2409.14316v1.json",
        "figure_id": "2409.14316v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. MVPGS leverages learning-based MVS to estimate dense view-consistent depth Dm v ssuperscript𝐷𝑚𝑣𝑠D^{mvs}italic_D start_POSTSUPERSCRIPT italic_m italic_v italic_s end_POSTSUPERSCRIPT and construct a point cloud 𝒫𝒫\\mathcal{P}caligraphic_P for the initialization of Gaussians 𝒢𝒢\\mathcal{G}caligraphic_G. We excavate the computed geometry from MVS through forward warping to generate appearance priors for the supervision of unseen views. To regularize the geometry update during optimization, we introduce LC Ssubscript𝐿𝐶𝑆L_{CS}italic_L start_POSTSUBSCRIPT italic_C italic_S end_POSTSUBSCRIPT from MVS depth and Lm o n osubscript𝐿𝑚𝑜𝑛𝑜L_{mono}italic_L start_POSTSUBSCRIPT italic_m italic_o italic_n italic_o end_POSTSUBSCRIPT from monocular depth priors to guide Gaussians to converge to proper positions.",
        "qtype": "Others",
        "response": "Chain-of-thought reasoning:\n\n1.  In the figure, the red‐boxed “[mask1]” is the output of the learning‐based MVS module – namely the “consistent dense depth maps” \\(D^{\\mathrm{mvs}}\\) estimated from the user’s input images.  \n2.  To build these depth maps, the system ingests raw, user-captured photos (often from a smartphone or wearable camera), infers per‐pixel depths via multi‐view stereo, and then filters for geometric consistency.  \n3.  Those depth maps are themselves a fairly rich 3D reconstruction of whatever the user photographed: room layouts, furniture positions, object shapes, and even the 3D contours of people’s faces and bodies.  \n4.  If an application or adversary collects or shares these MVS depth outputs, they can re-create the exact geometry of private spaces (homes, offices, etc.), localize personal items, and—when combined with the original color images—extract highly identifying and sensitive information.  \n5.  In short, by constructing consistent depth maps from user-captured imagery, one inadvertently produces a geometric “blueprint” of the user’s environment and subjects that can be exploited for location tracking, biometric profiling, interior surveillance, or other privacy intrusions.  \n\nAnswer:\n\nBy running MVS on users’ own photos to produce the red-boxed “consistent depth” maps, you end up with 3D reconstructions of their private spaces and of any people in them—floorplans, furniture layouts, even facial or body-shape geometry.  If those depth maps are stored or leaked, an attacker can reverse-engineer the user’s home layout, identify individuals by their facial depth profile, track movements over time, or glean other sensitive personal details—representing a significant privacy risk."
    },
    {
        "question": "How could inverse bilinear sampling amplify appearance inconsistencies across large viewpoint changes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "inverse bilinear sampling",
            "appearance priors"
        ],
        "id": 795,
        "masked_question": "How could [mask1] amplify appearance inconsistencies across large viewpoint changes?",
        "masked_number": 1,
        "masked_elements": [
            "inverse bilinear sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.14316v1_figure_2.png",
        "paperid": "2409.14316v1",
        "paper_path": "./papers/2409.14316v1.json",
        "figure_id": "2409.14316v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. MVPGS leverages learning-based MVS to estimate dense view-consistent depth Dm v ssuperscript𝐷𝑚𝑣𝑠D^{mvs}italic_D start_POSTSUPERSCRIPT italic_m italic_v italic_s end_POSTSUPERSCRIPT and construct a point cloud 𝒫𝒫\\mathcal{P}caligraphic_P for the initialization of Gaussians 𝒢𝒢\\mathcal{G}caligraphic_G. We excavate the computed geometry from MVS through forward warping to generate appearance priors for the supervision of unseen views. To regularize the geometry update during optimization, we introduce LC Ssubscript𝐿𝐶𝑆L_{CS}italic_L start_POSTSUBSCRIPT italic_C italic_S end_POSTSUBSCRIPT from MVS depth and Lm o n osubscript𝐿𝑚𝑜𝑛𝑜L_{mono}italic_L start_POSTSUBSCRIPT italic_m italic_o italic_n italic_o end_POSTSUBSCRIPT from monocular depth priors to guide Gaussians to converge to proper positions.",
        "qtype": "Others",
        "response": "Here’s the intuition in two steps:\n\n1. What “backward‐warping” does  \n   In a backward‐warp you take each pixel in the *target* view, use its (often noisy) estimated depth to shoot a ray *back* into one of your *source* images, sample whichever source‐pixel you land on, and copy that color into the target.  \n\n2. Why large‐baseline poses blow it up  \n   – Tiny errors or missing geometry in that target depth get hugely magnified when you invert the projection over a big camera swing. A millimeter of depth error can shift your sample point by dozens—or even hundreds—of pixels when the viewpoint moves a lot.   \n   – You often end up sampling occluded or out‐of‐frame texels, or pixels with wildly different lighting/BRDF from the source, so you get ghosting, holes and “popping” lighting shifts.  \n   – In short, pulling colors *from* the source via the target’s estimated geometry under large viewpoint changes amplifies both your depth noise and any view‐dependent shading mismatch, leading to very inconsistent rendered appearances."
    },
    {
        "question": "What motivates integrating L_mono with L_CS to regulate Gaussian geometry convergence during optimization?",
        "relevant_section_ids": [
            "1",
            "3.3",
            "3.3.2"
        ],
        "relevant_context": [
            "Furthermore, the position parameters in 3DGS are directly updated by the back-propagation gradient, which may lead to deviations from accurate geometry during few-shot optimization (see leaves in Fig. 7 ###reference_###). To facilitate convergence during optimization, we introduce a loss between 3DGS’s geometry and the confident geometric structure computed from MVS. Additionally, MVS may have poor performance in areas such as textureless and low overlap [49 ###reference_b49###], we incorporate monocular depth priors [31 ###reference_b31###] to further constrain the global geometry of scenes and mitigate the influence of inaccurate warped appearance priors caused by imprecise MVS depths.",
            "The geometry of scenes can be reflected by the Gaussian’s mean parameter μ, which is updated directly through back-propagate gradients during optimization. In practice, these parameters tend to have difficulty converging to correct positions when constraints from input views are insufficient. To facilitate convergence, we introduce two geometric constraints derived from MVS outputs and recent monocular depth priors [31 ###reference_b31###].",
            "MVS depth may not be consistent in certain areas. For regions lacking consistent structure regularization, we use monocular depth priors as compensation. Specifically, we utilize the ViT-based DPT [31 ###reference_b31###], which is trained on large-scale datasets and demonstrates strong generalization capabilities on other scenes, to predict depth map D_mono for each view."
        ],
        "final_answer": "Because back‐propagated updates alone can cause the Gaussians’ positions to drift under sparse‐view supervision and MVS depths are only reliable in high‐confidence regions, the authors combine the MVS‐based consistency loss (L_CS) with a monocular depth prior loss (L_mono) to ensure that Gaussian geometry converges correctly both locally (where MVS is trusted) and globally (where MVS may fail).",
        "relevant_elements": [
            "L_mono",
            "L_CS",
            "Gaussian"
        ],
        "id": 796,
        "masked_question": "What motivates integrating [mask1] with [mask2] to regulate Gaussian geometry convergence during optimization?",
        "masked_number": 2,
        "masked_elements": [
            "L_mono",
            "L_CS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.14316v1_figure_2.png",
        "paperid": "2409.14316v1",
        "paper_path": "./papers/2409.14316v1.json",
        "figure_id": "2409.14316v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. MVPGS leverages learning-based MVS to estimate dense view-consistent depth Dm v ssuperscript𝐷𝑚𝑣𝑠D^{mvs}italic_D start_POSTSUPERSCRIPT italic_m italic_v italic_s end_POSTSUPERSCRIPT and construct a point cloud 𝒫𝒫\\mathcal{P}caligraphic_P for the initialization of Gaussians 𝒢𝒢\\mathcal{G}caligraphic_G. We excavate the computed geometry from MVS through forward warping to generate appearance priors for the supervision of unseen views. To regularize the geometry update during optimization, we introduce LC Ssubscript𝐿𝐶𝑆L_{CS}italic_L start_POSTSUBSCRIPT italic_C italic_S end_POSTSUBSCRIPT from MVS depth and Lm o n osubscript𝐿𝑚𝑜𝑛𝑜L_{mono}italic_L start_POSTSUBSCRIPT italic_m italic_o italic_n italic_o end_POSTSUBSCRIPT from monocular depth priors to guide Gaussians to converge to proper positions.",
        "qtype": "Design_Rationale",
        "response": "The two boxed losses in the right‐hand side of Fig. 2 are exactly the pair of geometric‐regularizers we add on top of the pure photometric splatting loss.  The blue box (L₍Cₛ₎) is the “masked MVS depth” loss – it pins the rendered Gaussian depth back to those pixels where our MVS depths are known to be correct – and the red box (L₍mono₎) is the monocular‐depth prior loss that gives us a soft, global‐scale cue everywhere else.\n\nWhy do we need both?  If you only use L₍Cₛ₎, you get great local accuracy wherever your multi‐view stereo was confident, but everything else is underconstrained and the Gaussians will wander.  If you only use a monocular prior, you get a roughly plausible shape but often at the wrong scale and missing fine detail.  By integrating L₍Cₛ₎ with L₍mono₎ we marry the high‐confidence, high‐resolution structure from MVS with the broad, scene‐wide consistency of a pretrained monocular predictor.  In practice that combination keeps the Gaussians from drifting in unobserved regions while still snapping to the correct geometry where MVS is reliable."
    },
    {
        "question": "Why employ inverse bilinear sampling after forward warping for appearance prior generation in unseen views?",
        "relevant_section_ids": [
            "1",
            "3.2.2"
        ],
        "relevant_context": [
            "Since the warped locations might be floating-point numbers and not exactly aligned with an image grid, we utilize reversed bilinear sampling[42] to distribute colors to the local regions.",
            "In practice, x' is fractional and may not religiously lie in the pixel center. To address this problem, we utilize reversed bilinear sampling[42], which distributes the weight of the projected pixel to its nearest neighbors."
        ],
        "final_answer": "Inverse bilinear sampling is used to handle fractional (floating-point) warped coordinates that do not fall exactly on integer pixel centers. By distributing each warped pixel’s color across its four nearest neighbors, it ensures proper alignment of color contributions on the discrete image grid and yields a smoother, more accurate appearance prior for unseen views.",
        "relevant_elements": [
            "inverse bilinear sampling",
            "forward warping"
        ],
        "id": 797,
        "masked_question": "Why employ [mask1] after forward warping for appearance prior generation in unseen views?",
        "masked_number": 1,
        "masked_elements": [
            "inverse bilinear sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.14316v1_figure_2.png",
        "paperid": "2409.14316v1",
        "paper_path": "./papers/2409.14316v1.json",
        "figure_id": "2409.14316v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. MVPGS leverages learning-based MVS to estimate dense view-consistent depth Dm v ssuperscript𝐷𝑚𝑣𝑠D^{mvs}italic_D start_POSTSUPERSCRIPT italic_m italic_v italic_s end_POSTSUPERSCRIPT and construct a point cloud 𝒫𝒫\\mathcal{P}caligraphic_P for the initialization of Gaussians 𝒢𝒢\\mathcal{G}caligraphic_G. We excavate the computed geometry from MVS through forward warping to generate appearance priors for the supervision of unseen views. To regularize the geometry update during optimization, we introduce LC Ssubscript𝐿𝐶𝑆L_{CS}italic_L start_POSTSUBSCRIPT italic_C italic_S end_POSTSUBSCRIPT from MVS depth and Lm o n osubscript𝐿𝑚𝑜𝑛𝑜L_{mono}italic_L start_POSTSUBSCRIPT italic_m italic_o italic_n italic_o end_POSTSUBSCRIPT from monocular depth priors to guide Gaussians to converge to proper positions.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is “Inverse Bilinear Sampling.”  After you forward–warp pixels from a known view into an unseen view you end up with floating-point (sub-pixel) locations that don’t sit exactly on your image’s integer grid.  If you simply splat each warped pixel to the nearest grid cell you get holes and aliasing.  By doing “inverse” (reversed) bilinear sampling you take each warped point and distribute its colour into the four surrounding integer pixels, weighted by their distance.  This (1) converts a continuous coordinate into a discrete pixel footprint, (2) prevents gaps in your appearance prior, and (3) yields a smoother, more stable supervisory signal for training on unseen views."
    },
    {
        "question": "What motivates embedding IEM in each skip connection instead of only at the bottleneck?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "A primary concern when manipulating U-Net-based architecture bottleneck features is the potential information loss in skip connections. Specifically, suppose feature disentanglement is solely conducted at the last feature (i.e., bottleneck) of the encoder E without a similar process in the skip connections linked to the generator G. In that case, there is a risk of information loss. To address this issue, we designed AIM and IEM to perform identity feature disentanglement and target age conjunction at each i-th skip connection and bottleneck as: This scheme ensures that feature disentanglement and age injection are consistently applied across all levels of the skip connections, as depicted in Fig. 3."
        ],
        "final_answer": "Embedding IEM in every skip connection is motivated by the need to prevent identity information loss that would occur if disentanglement were applied only at the bottleneck. By performing identity feature disentanglement at each skip level as well as at the bottleneck, the model ensures that identity-related features are preserved consistently throughout all pathways of the U-Net architecture.",
        "relevant_elements": [
            "IEM",
            "skip connections"
        ],
        "id": 798,
        "masked_question": "What motivates embedding [mask1] in each [mask2] instead of only at the bottleneck?",
        "masked_number": 2,
        "masked_elements": [
            "IEM",
            "skip connections"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.16945v1_figure_2.png",
        "paperid": "2410.16945v1",
        "paper_path": "./papers/2410.16945v1.json",
        "figure_id": "2410.16945v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of IdenBAT for brain age transformation. Age transformer 𝒯𝒯\\mathcal{T}caligraphic_T, which consists of encoder ℰℰ\\mathcal{E}caligraphic_E, identity extracting module (IEM), age injecting module (AIM), and generator 𝒢𝒢\\mathcal{G}caligraphic_G aims to synthesize age-transformed image 𝐗^^𝐗\\hat{\\mathbf{X}}over^ start_ARG bold_X end_ARG.",
        "qtype": "Design_Rationale",
        "response": "Here “mask1” is the Identity Extracting Module (IEM) and “mask2” are the U-Net’s skip-connections.  The paper points out that if you only disentangle identity at the bottleneck, all of the low-level features that flow through the skip-connections will still be “age-entangled,” and you risk either losing fine identity detail or reintroducing unwanted age cues.  By inserting the IEM (and its twin age-injecting AIM) into every skip path, they guarantee that at every spatial scale the network’s features are first stripped of the old age signal (preserving the subject’s identity) and then re-stamped with the new age.  This avoids information “leaking” around the bottleneck and keeps disentanglement and age conditioning consistent across all resolutions."
    },
    {
        "question": "How does AIM's style transfer mechanism enhance age conditioning of identity features before generator synthesis?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We utilize a style transfer method for age conditioning in AIM to incorporate information about the target age into the identity feature.",
            "The target age a_t is incorporated into φ_{t,i} using a mapping network 𝓜, comprising eight fully-connected layers with LeakyReLU activation function, in line with StyleGAN2 Karras et al. (2020).",
            "The embedded age condition is fused into the identity feature γ_i through CBN blocks. Specifically, the i-th identity feature γ_i undergoes batch normalization, standardizing the features batch-wise and mapping them into a specific normalized space.",
            "Subsequently, these normalized features undergo denormalization, wherein the target age representation y_t is incorporated by adjusting the mean and standard deviation parameters of the denormalization as: [CBN equation].",
            "This technique allows for reintroducing age-specific characteristics into the normalized identity feature, tailoring it to reflect the desired age condition."
        ],
        "final_answer": "AIM uses a style-transfer–inspired conditional batch-normalization (CBN) pipeline. It first sends the target age through a small mapping network to produce age embeddings, then normalizes the identity feature γ_i and immediately denormalizes it by shifting and scaling with those age embeddings. This injects target-age–specific style (mean and variance) into the identity features before they are passed to the generator, ensuring the synthesized image carries the desired age attributes while preserving identity.",
        "relevant_elements": [
            "AIM",
            "generator"
        ],
        "id": 799,
        "masked_question": "How does [mask1]'s style transfer mechanism enhance age conditioning of identity features before generator synthesis?",
        "masked_number": 1,
        "masked_elements": [
            "AIM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.16945v1_figure_2.png",
        "paperid": "2410.16945v1",
        "paper_path": "./papers/2410.16945v1.json",
        "figure_id": "2410.16945v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of IdenBAT for brain age transformation. Age transformer 𝒯𝒯\\mathcal{T}caligraphic_T, which consists of encoder ℰℰ\\mathcal{E}caligraphic_E, identity extracting module (IEM), age injecting module (AIM), and generator 𝒢𝒢\\mathcal{G}caligraphic_G aims to synthesize age-transformed image 𝐗^^𝐗\\hat{\\mathbf{X}}over^ start_ARG bold_X end_ARG.",
        "qtype": "Design_Rationale",
        "response": "Here’s how I worked it out step by step:\n\n1.  **Find what the red box is.**  \n    In the big U-Net drawing the red box surrounds two modules both labeled “AIM.”  In the text these are called the Age Injecting Module.\n\n2.  **Ask: what does AIM do?**  \n    The Methods text under “Age Transformer” says that AIM “utilizes a style transfer method for age conditioning.”  It then immediately explains that AIM is built from a series of conditional batch-normalization (CBN) blocks and that the target age is first mapped by an 8-layer MLP (the “mapping network”) into a style vector.\n\n3.  **Understand the CBN‐based style transfer.**  \n    – The identity feature  (output of the IEM) is batch-normalized (zero mean, unit variance).  \n    – Then it is “denormalized,” i.e. rescaled and re-shifted, using the mean and variance parameters produced by the mapping network from the target age.  \n    – This effectively injects age-specific “style” (in the sense of StyleGAN) into what would otherwise be an age-neutral identity feature map.\n\n4.  **Why does this help?**  \n    – By normalizing first, you strip out any residual age palette that might still be lurking in the identity feature.  \n    – By then re-shaping that normalized feature with scale-and‐shift parameters keyed to the desired age, you force the network to explicitly rewrite the identity feature’s statistics so they carry exactly the new age information you want.  \n    – Because you repeat this at every skip connection and at the bottleneck, each level of detail in the identity feature is consistently “re-styled” for the target age before it ever reaches the generator.\n\n5.  **Conclusion – how does the style‐transfer in AIM enhance age conditioning?**  \n    By turning the identity feature into a blank slate via batch-norm and then “painting on” the target age via learned scale‐and‐shift parameters, AIM guarantees that the generator receives identity maps whose very feature statistics have been forced to reflect the chosen age.  This is far stronger than, say, simply concatenating an age scalar; it weaves the age information directly into the texture-and-style space of the identity feature.\n\nAnswer (fill-in):  \n[mask1] is the Age Injecting Module (AIM).  Its style-transfer mechanism takes the identity feature, strips out any residual age information via batch-normalization, and then “re-colors” or re-styles that feature with scale-and-shift parameters derived from the target-age embedding.  By doing so at every skip connection, it deeply embeds the desired age characteristics into the identity features before they reach the generator."
    },
    {
        "question": "How are age and identity features iteratively combined through AIM and IEM across each skip connection?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "The embedded age condition is fused into the identity feature f_i through CBN blocks. Specifically, the k-th identity feature f_i^k undergoes batch normalization, standardizing the features batch-wise and mapping them into a specific normalized space. Subsequently, these normalized features undergo denormalization, wherein the target age representation ψ(a_t) is incorporated by adjusting the mean and standard deviation parameters of the denormalization as: … This technique allows for reintroducing age-specific characteristics into the normalized identity feature, tailoring it to reflect the desired age condition. (Section 3.1)",
            "To address this issue, we designed AIM and IEM to perform identity feature disentanglement and target age conjunction at each k-th skip connection and bottleneck as: … This scheme ensures that feature disentanglement and age injection are consistently applied across all levels of the skip connections, as depicted in Fig. 3. (Section 3.2)"
        ],
        "final_answer": "At every U-Net skip connection (and at the bottleneck), the encoder’s feature map is first sent through the Identity Extracting Module (IEM) to pull out an age-irrelevant, identity feature f_i^k. That identity feature is then passed into the Age Injecting Module (AIM), where conditional batch-normalization layers use the target age embedding ψ(a_t) to modulate mean and variance and re-inject age information. By alternating IEM (to disentangle identity) and AIM (to inject age) at each resolution level, the network fuses the two kinds of features iteratively across all skip connections.",
        "relevant_elements": [
            "IEM",
            "AIM",
            "skip connections"
        ],
        "id": 800,
        "masked_question": "How are age and identity features iteratively combined through [mask1] and [mask2] across each skip connection?",
        "masked_number": 2,
        "masked_elements": [
            "AIM",
            "IEM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.16945v1_figure_2.png",
        "paperid": "2410.16945v1",
        "paper_path": "./papers/2410.16945v1.json",
        "figure_id": "2410.16945v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of IdenBAT for brain age transformation. Age transformer 𝒯𝒯\\mathcal{T}caligraphic_T, which consists of encoder ℰℰ\\mathcal{E}caligraphic_E, identity extracting module (IEM), age injecting module (AIM), and generator 𝒢𝒢\\mathcal{G}caligraphic_G aims to synthesize age-transformed image 𝐗^^𝐗\\hat{\\mathbf{X}}over^ start_ARG bold_X end_ARG.",
        "qtype": "Implementation_Details",
        "response": "In the picture, the little blue‐boxed blocks are exactly the Identity‐Extracting Modules (IEMs) and the red‐boxed blocks are the Age‐Injecting Modules (AIMs).  What actually happens at each U-Net skip is this:\n\n 1.  You take the encoder’s feature map at level i, call it fᵢ.  \n 2.  You run fᵢ through the IEM (the blue box).  This “pulls out” an age-irrelevant identity code uᵢ = IEM(fᵢ).  \n 3.  You feed uᵢ together with the target age embedding m(aₜ) into the AIM (the red box).  Concretely AIM uses conditional batch-norm whose scale–shift (γ, β) are produced by the small mapping network M(aₜ), so you get an age-modulated feature vᵢ = AIM(uᵢ; γ(aₜ), β(aₜ)).  \n 4.  You inject vᵢ into the generator block at the same resolution via the skip connection (in the drawing they use a simple summation ⊕).  \n\nAnd then you do exactly the same thing at every skip level.  By alternating “extract identity (IEM) → inject age (AIM) → skip into decoder,” you ensure that (a) nothing but pure identity information gets carried forward and (b) at every scale it is re-coloured by the desired age condition."
    },
    {
        "question": "How do outputs from Teacher Text Encoder guide adjustments in the Student Text Encoder prompt?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "For an image I and a set of classes C, we start by performing zero-shot classification using the frozen teacher model. Specifically, we use the teacher image encoder fᵀ_I and text encoder fᵀ_T to compute the teacher image features zᵀ_I and text features zᵀ_T. For the teacher model we use the fixed text prompt “a photo of [CLASS]”. We then apply Eq. 1 to produce the probabilities pᵀ predicted by the teacher on image I for classes C.",
            "Finally, using Eq. 1 we produce student class probabilities pˢ predicted on image I for classes C. Note that all encoder parameters except for the learnable prompt p are frozen.",
            "We use the symmetric KL-divergence between the teacher (pᵀ) and the student (pˢ) probabilities in a distillation loss: L_distill = KL(pᵀ || pˢ) + KL(pˢ || pᵀ). This distillation loss depends only on the fixed predictions of the teacher, the prompt-conditioned predictions of the students, and the set of classes C."
        ],
        "final_answer": "The teacher text encoder produces text features (using the fixed prompt “a photo of [CLASS]”) which are converted into teacher class probabilities. During training, the student’s prompt parameters are updated so that the student text encoder produces class probabilities that match the teacher’s. Concretely, the symmetric KL-divergence between the teacher and student probabilities serves as the loss, and back-propagating this loss through the student text encoder adjusts its prompt parameters.",
        "relevant_elements": [
            "Teacher Text Encoder",
            "Student Text Encoder"
        ],
        "id": 802,
        "masked_question": "How do outputs from [mask1] guide adjustments in the Student Text Encoder prompt?",
        "masked_number": 1,
        "masked_elements": [
            "Teacher Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.03056v2_figure_1.png",
        "paperid": "2407.03056v2",
        "paper_path": "./papers/2407.03056v2.json",
        "figure_id": "2407.03056v2_figure_1.png",
        "caption": "Figure 1: Motivation and overview. (Top left) Lightweight VLMs like CLIP achieve impressive zero-shot performance but lag behind supervised approaches; large VLMs incur a high computational burden. (Bottom left) Parameter-efficient prompt learning offers a non-destructive approach to adapting VLMs to downstream tasks; however, existing methods require annotated samples and struggle to generalize to unseen classes. (Right) Our approach does not require labeled samples and learns by distilling knowledge from a more powerful VLM. It can be seamlessly integrated into existing prompt learning techniques and generalizes better to unseen classes on downstream tasks.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is simply the frozen teacher’s text (and image) encoders running zero-shot on the training images, producing a “soft” class distribution pᵀ.  At each step we compare that fixed pᵀ to the student’s own output pˢ (which depends on its current prompt) via a symmetric KL–divergence loss, and back-propagate the resulting gradients into the student’s prompt tokens.  In this way the teacher’s output distribution directly steers how the student text‐encoder’s prompt should shift so that pˢ moves closer to pᵀ."
    },
    {
        "question": "How are Teacher Image Encoder representations aligned with learned Prompt embeddings in Student Image Encoder?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "For an image x and a set of classes C, we start by performing zero-shot classification using the frozen teacher model. Specifically, we use the teacher image encoder f_T and text encoder g_T to compute the teacher image features v_T and text features t_T. We then apply Eq. 1 to produce the probabilities p^T predicted by the teacher on image x for classes C.",
            "Finally, using Eq. 1 we produce student class probabilities p^S predicted on image x for classes C. Note that all encoder parameters except for the learnable prompt P are frozen.",
            "We use the symmetric KL-divergence between the teacher (p^T) and the student (p^S) probabilities in a distillation loss: L_{KD}(p^T,p^S)=KL(p^T||p^S)+KL(p^S||p^T)."
        ],
        "final_answer": "The student’s prompt-conditioned image encoder is trained to match the teacher’s image-and-text induced class distributions by minimizing a symmetric KL-divergence between the teacher’s and the student’s predicted probabilities. In this way, the student’s learned prompts are aligned with the teacher’s image encoder representations through unsupervised knowledge distillation.",
        "relevant_elements": [
            "Teacher Image Encoder",
            "Prompt",
            "Student Image Encoder"
        ],
        "id": 803,
        "masked_question": "How are [mask1] representations aligned with learned Prompt embeddings in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Teacher Image Encoder",
            "Student Image Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.03056v2_figure_1.png",
        "paperid": "2407.03056v2",
        "paper_path": "./papers/2407.03056v2.json",
        "figure_id": "2407.03056v2_figure_1.png",
        "caption": "Figure 1: Motivation and overview. (Top left) Lightweight VLMs like CLIP achieve impressive zero-shot performance but lag behind supervised approaches; large VLMs incur a high computational burden. (Bottom left) Parameter-efficient prompt learning offers a non-destructive approach to adapting VLMs to downstream tasks; however, existing methods require annotated samples and struggle to generalize to unseen classes. (Right) Our approach does not require labeled samples and learns by distilling knowledge from a more powerful VLM. It can be seamlessly integrated into existing prompt learning techniques and generalizes better to unseen classes on downstream tasks.",
        "qtype": "Implementation_Details",
        "response": "In our diagram the red‐boxed module is the *teacher’s image encoder* and the blue‐boxed module is the *student’s prompt-conditioned image encoder* (i.e. the student CLIP model with learnable visual/text prompts injected).  We never directly pair up individual teacher activations with individual prompt tokens.  Instead we force the *output distributions* of the two systems to match, which in turn drives the student’s prompts to carve out embeddings in the shared CLIP space that line up with the teacher’s image representations.  Concretely:\n\n 1.  We run an image \\(I\\) through the frozen teacher image encoder to get a feature vector \\(f_{T}(I)\\), and through the frozen teacher text encoder (using the fixed “a photo of [CLASS]” prompts) to get a set of class embeddings \\(\\{f_{T}(c_k)\\}\\).  \n 2.  We form the teacher’s zero-shot probability vector  \n     \\(p_{T}(k\\!\\mid\\!I)\\;=\\;\\mathrm{softmax}\\bigl(\\cos\\bigl(f_{T}(I),\\,f_{T}(c_k)\\bigr)/\\tau\\bigr)\\).  \n 3.  We run the same image \\(I\\) through the *student* image encoder—with *its* learnable visual (or textual) prompts injected—to get \\(f_{S}(I)\\), and through the student text encoder (with the *learned* prompts) to get class embeddings \\(\\{f_{S}(c_k)\\}\\).  \n 4.  We form the student’s probability vector  \n     \\(p_{S}(k\\!\\mid\\!I)\\;=\\;\\mathrm{softmax}\\bigl(\\cos\\bigl(f_{S}(I),\\,f_{S}(c_k)\\bigr)/\\tau\\bigr)\\).  \n 5.  Finally we minimise the *symmetric* KL–divergence between \\(p_{T}\\) and \\(p_{S}\\):  \n     \\[\n       \\mathcal{L}_{\\mathrm{KD}}\n       =\\,\\mathrm{KL}\\bigl(p_{T}\\,\\|\\,p_{S}\\bigr)\\;+\\;\\mathrm{KL}\\bigl(p_{S}\\,\\|\\,p_{T}\\bigr).\n     \\]\n  \nBecause both teacher and student live in the *same* CLIP embedding space and we are matching *their softmaxed cosine similarities*, the student’s learned prompts are driven to produce embeddings that are well aligned with the teacher’s image representations."
    },
    {
        "question": "How does Knowledge Distillation align student prompts with teacher outputs for label-agnostic adaptation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Given a lightweight CLIP model (the student) and a larger, more powerful CLIP model (the teacher), we aim to improve the downstream performance of the student model by distilling knowledge from teacher to student. For an image x and a set of classes C, we start by performing zero-shot classification using the frozen teacher model. Specifically, we use the teacher image encoder f_t^i and text encoder f_t^t to compute the teacher image features z_t^i and text features z_t^t. … We then apply Eq. 1 to produce the probabilities p_t predicted by the teacher on image x for classes C.",
            "Given the same image x processed by the teacher and the same set of classes C, the student extracts image features z_s^i and text features z_s^t. Note that the text and image encoders can both depend on the prompt parameters θ. … Finally, using Eq. 1 we produce student class probabilities p_s predicted on image x for classes C. Note that all encoder parameters except for the learnable prompt θ are frozen.",
            "We use the symmetric KL-divergence between the teacher (p_t) and the student (p_s) probabilities in a distillation loss: L_KD(θ) = KL(p_t || p_s) + KL(p_s || p_t)."
        ],
        "final_answer": "KDPL aligns the student’s learnable prompts with the teacher’s outputs by performing zero-shot inference with the frozen teacher to obtain a target probability distribution p_t, running the student (with only its prompts unfrozen) to obtain p_s, and then minimizing the symmetric KL-divergence between p_t and p_s. This distillation loss updates only the prompt parameters and requires no ground-truth labels.",
        "relevant_elements": [
            "Knowledge Distillation",
            "Student Prompts",
            "Teacher Outputs"
        ],
        "id": 804,
        "masked_question": "How does [mask1] align [mask2] with teacher outputs for label-agnostic adaptation?",
        "masked_number": 2,
        "masked_elements": [
            "Knowledge Distillation",
            "Student Prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.03056v2_figure_1.png",
        "paperid": "2407.03056v2",
        "paper_path": "./papers/2407.03056v2.json",
        "figure_id": "2407.03056v2_figure_1.png",
        "caption": "Figure 1: Motivation and overview. (Top left) Lightweight VLMs like CLIP achieve impressive zero-shot performance but lag behind supervised approaches; large VLMs incur a high computational burden. (Bottom left) Parameter-efficient prompt learning offers a non-destructive approach to adapting VLMs to downstream tasks; however, existing methods require annotated samples and struggle to generalize to unseen classes. (Right) Our approach does not require labeled samples and learns by distilling knowledge from a more powerful VLM. It can be seamlessly integrated into existing prompt learning techniques and generalizes better to unseen classes on downstream tasks.",
        "qtype": "Literature_Background",
        "response": "Let’s unpack what the two highlighted regions actually are:\n\n1. The red‐boxed area is our **Knowledge Distillation** step – more precisely, the **distillation loss** that drives prompt learning.  \n2. The blue‐boxed area is the **learnable prompt input** to the student (i.e. the continuous context vectors plus the class name) whose parameters we want to adapt.\n\nSo the question becomes:\n\n“How does our **distillation loss** align the **learnable prompt** with the teacher’s outputs when no labels are available?”\n\nAnswer (chain of thought):\n\n- We freeze a large, powerful CLIP (“teacher”) and extract its zero‐shot class probabilities for each image under the fixed text prompt “a photo of [CLASS]”.  \n- We also freeze almost all of a smaller CLIP (“student”) except for its prompt parameters, and run the same images and class names through it to get its prompt-conditioned probabilities.  \n- We then compute a **symmetric KL-divergence** between the teacher’s and student’s class distributions and back-propagate only into the prompt parameters.  \n- By minimizing this KL-divergence, the student’s learnable prompt is forced to reproduce the teacher’s soft outputs **without ever seeing ground‐truth labels**.\n\nIn short:\n\nIt uses an **unsupervised KD loss** (the symmetric KL-divergence) to match the student’s prompt-conditioned probability vector to the teacher’s frozen zero-shot probabilities, thereby aligning the learnable prompt to the teacher outputs in a label-agnostic way."
    },
    {
        "question": "How does parameter-efficient prompt learning eliminate the need for annotated samples?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "To eliminate the need for labeled training examples and improve the generalization of learned prompts, we propose a novel approach to prompt learning which we call Knowledge Distillation Prompt Learning (KDPL). KDPL adapts lightweight VLMs and improves performance on downstream tasks by distilling knowledge from a more powerful VLM without the need for annotated examples.",
            "Our proposed approach, which we call Knowledge Distillation Prompt Learning (KDPL), is a general method designed to enhance the performance of the CLIP model on downstream tasks through parameter-efficient prompt learning. Unlike previous approaches which rely on labeled examples for training, KDPL eliminates the need for manually-labeled samples by learning only through knowledge distillation from a larger and more powerful VLM."
        ],
        "final_answer": "Parameter-efficient prompt learning (KDPL) removes the need for annotated samples by using the outputs of a larger, pre-trained vision-language model (the teacher) as soft labels: it distills the teacher’s zero-shot prediction distributions into the student’s prompt parameters via a symmetric KL-divergence loss, training entirely without ground-truth labels.",
        "relevant_elements": [
            "Parameter-efficient prompt learning",
            "Annotated Samples"
        ],
        "id": 805,
        "masked_question": "How does [mask1] eliminate the need for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Parameter-efficient prompt learning",
            "Annotated Samples"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.03056v2_figure_1.png",
        "paperid": "2407.03056v2",
        "paper_path": "./papers/2407.03056v2.json",
        "figure_id": "2407.03056v2_figure_1.png",
        "caption": "Figure 1: Motivation and overview. (Top left) Lightweight VLMs like CLIP achieve impressive zero-shot performance but lag behind supervised approaches; large VLMs incur a high computational burden. (Bottom left) Parameter-efficient prompt learning offers a non-destructive approach to adapting VLMs to downstream tasks; however, existing methods require annotated samples and struggle to generalize to unseen classes. (Right) Our approach does not require labeled samples and learns by distilling knowledge from a more powerful VLM. It can be seamlessly integrated into existing prompt learning techniques and generalizes better to unseen classes on downstream tasks.",
        "qtype": "Literature_Background",
        "response": "First, let’s align what the two highlighted regions are:\n\n1. The red‐boxed text on the right is our method:  \n   **“Parameter-efficient prompt learning via Knowledge Distillation”**  \n   (i.e. KDPL)\n\n2. The blue‐boxed picture shows an image with “Label: Unknown,”  \n   i.e. the usual **labeled training examples** or **ground-truth class labels** that most prompt-tuning methods require.\n\n---\n\nChain of thought:\n\n1. Traditional prompt‐tuning (CoOp, VPT, etc.) needs a cross‐entropy loss against true labels for every training image.  \n2. In KDPL, we *never* use those ground-truth labels. Instead, we freeze a powerful “teacher” CLIP, run every unlabeled image through it with a fixed zero-shot prompt (“a photo of [CLASS]…”), and record its soft probability distribution over classes.  \n3. We then train only the *prompts* of a smaller “student” CLIP by minimizing the symmetric KL-divergence between the teacher’s soft predictions and the student’s own predictions.  \n4. Because this distillation loss uses *teacher-generated* pseudo-labels, we never need any human‐annotated labels.\n\n---\n\nAnswer to the question:\n\nParameter-efficient prompt learning via Knowledge Distillation (KDPL) eliminates the need for labeled training examples by using the teacher model’s zero‐shot output distributions (soft pseudo-labels) as supervisory signals—instead of ground-truth class labels—via a KL-divergence distillation loss."
    },
    {
        "question": "How does VPT-NSP's null-space mechanism inform orthogonal updating of Wδ within SSM blocks?",
        "relevant_section_ids": [
            "2.2",
            "4.2"
        ],
        "relevant_context": [
            "VPT-NSP [24] theoretically deduces two sufficient consistency conditions to strictly satisfy the orthogonality for prompt tuning, where the null space method [36] is utilized to implement the conditions.",
            "Inspired by the null-space optimization methods [31  ###reference_b31###, 24  ###reference_b24###, 36  ###reference_b36###], the bases of the projection matrices P_A and P_δ should reside in the null space of the corresponding feature subspace extracted from the previous task. As a result, we derive the projectors P_A, P_δ, P_B and P_C to enable that the parameter updates satisfy the conditions in Eq.24. To sum up, we use Eq.26 to perform orthogonal projections for parameter updating."
        ],
        "final_answer": "VPT-NSP uses a null-space projection to enforce that prompt updates lie in the subspace orthogonal to features from earlier tasks.  Analogously, for the SSM’s Wδ parameter, we extract its input-conditioned feature matrix from the previous task, compute its null-space via SVD, build a projector P_δ whose bases are those null-space vectors, and then update Wδ by ΔWδ = P_δ·G^{Wδ}.  This guarantees that every change to Wδ is orthogonal to the old-task feature subspace, preserving consistency.",
        "relevant_elements": [
            "Wδ"
        ],
        "id": 806,
        "masked_question": "How does VPT-NSP's null-space mechanism inform orthogonal updating of [mask1] within SSM blocks?",
        "masked_number": 1,
        "masked_elements": [
            "Wδ"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15469v1_figure_1.png",
        "paperid": "2411.15469v1",
        "paper_path": "./papers/2411.15469v1.json",
        "figure_id": "2411.15469v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed Mamba-CL. The backbone contains S𝑆Sitalic_S Mamba blocks. For each Mamba block, we fine-tune the weights 𝐖δ,𝐖B,𝐖Csuperscript𝐖𝛿superscript𝐖𝐵superscript𝐖𝐶{\\mathbf{W}}^{\\delta},{\\mathbf{W}}^{B},{\\mathbf{W}}^{C}bold_W start_POSTSUPERSCRIPT italic_δ end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT and 𝐀𝐀{\\mathbf{A}}bold_A within the SSM (as well as the linear layer after the SSM). To minimize forgetting, we aim to utilize orthogonal projections to keep the output unchanged from SSM after training the t+1𝑡1t+1italic_t + 1-th task.",
        "qtype": "Literature_Background",
        "response": "First, note that the little red box in the SSM‐block schematic is literally the learnable “discretization” weight matrix, which we have been calling Wᵟ.  In VPT-NSP the authors show how to force prompt‐updates to be exactly orthogonal to the subspace spanned by all previously seen features by projecting the optimizer’s raw gradient into the null‐space of the old‐task feature covariance.  We simply copy that recipe for each of our four SSM parameters – in particular for Wᵟ – as follows:\n\n 1. After finishing task t, run all tᵗʰ‐task images through your frozen backbone + SSM and collect the resulting feature matrix Xᵗ.\n 2. Form the uncentered covariance Cᵗ = Xᵗ (Xᵗ)ᵀ and do an SVD → Cᵗ = U Σ Uᵀ.\n 3. Pick the right‐singular vectors in U that correspond to (near) zero singular‐values – that subspace is the “null‐space” Nᵗ.\n 4. Define the projector Pᵟ = Nᵗ (Nᵗ)ᵀ (or, to soften it, α I + (1−α)Nᵗ(Nᵗ)ᵀ).\n 5. At task t+1, whenever you get a raw gradient gᵟ = ∇_{Wᵟ}L, replace it by Pᵟ gᵟ before applying your usual optimizer step.\n\nBecause Pᵟ kills any component of the update that lies in the span of the old features, the change ΔWᵟ is guaranteed orthogonal to everything you saw on previous tasks – exactly the same null-space trick introduced in VPT-NSP, now applied to the discretization weights inside each Mamba SSM block."
    },
    {
        "question": "How could NSCL's orthogonal subspace projection influence updates of A to preserve SSM outputs?",
        "relevant_section_ids": [
            "2.2",
            "4.2"
        ],
        "relevant_context": [
            "Subspace projection methods [36, 31, 21, 42] propose to update parameters in the subspace orthogonal to the previous feature space. Through the orthogonality, the features from old tasks can remain unchanged after learning new tasks, thereby theoretically enhancing the stability of models.",
            "Inspired by the null‐space optimization methods [31, 24, 36], the bases of the projection matrices P^A and P^δ should reside in the null space of H^{1,2,3}. ... To sum up, we use Eq.26 to perform orthogonal projections for parameter updating."
        ],
        "final_answer": "By adopting NSCL’s orthogonal subspace projection, when computing the update ΔA for the SSM’s input‐invariant parameter A, one first forms the feature subspace H^{1,2,3} from all previous tasks’ inputs and then projects ∇A onto the null space of H^{1,2,3}. This enforces ΔA ⟂ H^{1,2,3}, so that the change in A does not affect the SSM’s outputs on old data, thereby preserving past task performance.",
        "relevant_elements": [
            "A"
        ],
        "id": 807,
        "masked_question": "How could NSCL's orthogonal subspace projection influence updates of [mask1] to preserve SSM outputs?",
        "masked_number": 1,
        "masked_elements": [
            "A"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15469v1_figure_1.png",
        "paperid": "2411.15469v1",
        "paper_path": "./papers/2411.15469v1.json",
        "figure_id": "2411.15469v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed Mamba-CL. The backbone contains S𝑆Sitalic_S Mamba blocks. For each Mamba block, we fine-tune the weights 𝐖δ,𝐖B,𝐖Csuperscript𝐖𝛿superscript𝐖𝐵superscript𝐖𝐶{\\mathbf{W}}^{\\delta},{\\mathbf{W}}^{B},{\\mathbf{W}}^{C}bold_W start_POSTSUPERSCRIPT italic_δ end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT and 𝐀𝐀{\\mathbf{A}}bold_A within the SSM (as well as the linear layer after the SSM). To minimize forgetting, we aim to utilize orthogonal projections to keep the output unchanged from SSM after training the t+1𝑡1t+1italic_t + 1-th task.",
        "qtype": "Literature_Background",
        "response": "First, note that the little red box in the SSM diagram is the matrix Aₜ – the global “convolution” kernel inside each Mamba block.  In Mamba-CL we want to tweak Aₜ when learning task t + 1, but in such a way that the SSM’s mapping on all of the old‐task hidden‐states H remains exactly the same.\n\nNSCL achieves this by the familiar “null‐space” trick:\n\n 1. After finishing task t, we collect all the state‐vectors H∈ℝⁿ×d that flowed through Aₜ (or, more precisely, through the linear map that lives immediately downstream of Aₜ).  \n 2. We form the uncentered covariance Σ_H = HᵀH and do an SVD on Σ_H.  We pick off the right singular-vectors corresponding to the near-zero singular-values; call their span U_H.  By construction U_H is orthogonal to the column–space of H.  \n 3. We build the projector  \n      P_A = I – U_H U_Hᵀ  \n    whose range is exactly the subspace orthogonal to everything H can see.  \n 4. Now, whenever the optimizer computes a raw gradient ∇_A ℒ for the new task, we do  \n      ΔA ← P_A (–η ∇_A ℒ)  \n    and then set  \n      Aₜ ← Aₜ + ΔA.  \n\nBecause ΔA lives entirely in the null‐space of H, for any old‐task hidden‐state h old we have  \n   Aₜ h_old = (Aₜ + ΔA) h_old.  \nIn other words, projecting the gradient into the orthogonal complement of the previous feature‐space guarantees that Aₜ’s action on all old features is unchanged, and thus the SSM outputs on earlier tasks are perfectly preserved."
    },
    {
        "question": "How does discretization module δ_t decomposition establish separate consistency constraints for W^δ and W^C during incremental updates?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "As there are two potential variables (i.e., W^δ and W^C) in the single equation Eq.13, it is difficult to solve it directly. Given that δ is a discretization parameter that affects both W^δ and W^C, we first keep δ so that the discretization process remains consistent. Base on that, we can decompose Eq.13 into two consistency conditions for W^δ and W^C:",
            "In this way, we derive the consistency condition for W^δ: W^δ_{t+1} = W^δ_t.",
            "Considering the deduced condition W^δ_{t+1} = W^δ_t given by Eq.13, the consistency term for W^C can be simplified and expanded through the projection layer to yield the separate constraint W^C_{t+1} = W^C_t."
        ],
        "final_answer": "They first fix the discretization module δ across tasks so that the shared discretization process does not change, then decompose the single consistency equation (Eq.13) into two independent requirements: one for W^δ and one for W^C.  Concretely, they show that maintaining r_t = r_{t+1} forces W^δ_{t+1} = W^δ_t, and, under that deduced condition, they similarly derive W^C_{t+1} = W^C_t as the separate consistency constraint for the projection weights.",
        "relevant_elements": [
            "δ_t",
            "W^δ",
            "W^C"
        ],
        "id": 808,
        "masked_question": "How does discretization module [mask1] decomposition establish separate consistency constraints for [mask2] and W^C during incremental updates?",
        "masked_number": 2,
        "masked_elements": [
            "δ_t",
            "W^δ"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15469v1_figure_1.png",
        "paperid": "2411.15469v1",
        "paper_path": "./papers/2411.15469v1.json",
        "figure_id": "2411.15469v1_figure_1.png",
        "caption": "Figure 1: Illustration of the proposed Mamba-CL. The backbone contains S𝑆Sitalic_S Mamba blocks. For each Mamba block, we fine-tune the weights 𝐖δ,𝐖B,𝐖Csuperscript𝐖𝛿superscript𝐖𝐵superscript𝐖𝐶{\\mathbf{W}}^{\\delta},{\\mathbf{W}}^{B},{\\mathbf{W}}^{C}bold_W start_POSTSUPERSCRIPT italic_δ end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , bold_W start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT and 𝐀𝐀{\\mathbf{A}}bold_A within the SSM (as well as the linear layer after the SSM). To minimize forgetting, we aim to utilize orthogonal projections to keep the output unchanged from SSM after training the t+1𝑡1t+1italic_t + 1-th task.",
        "qtype": "Experimental_Results",
        "response": "In fact nothing magical is hiding behind the red box – it is exactly the little “discretizer’’ δ that multiplies both the SSM‐weights \\(W^δ\\) and the final classification weights \\(W^C\\).  Because δ is a *discretization* parameter (implemented via a monotonic nonlinearity such as softplus or exp) we can **keep it fixed** when we write down the requirement that the old and new outputs be identical.  Concretely, the consistency condition for the Mamba block at time \\(t\\) is, in its most general form,\n\n exp(δ_{t+1}) W^δ_{t+1} Xₜ  =  exp(δ_t) W^δ_t Xₜ  \n(and similarly for the \\(W^C\\) path once you propagate into the classifier).  \n\nBecause exp(·) is monotonic and we *freeze* δ (i.e. enforce δ_{t+1}=δ_t), that common scalar factor simply drops out of the difference.  What remains is two *separate* zero‐difference constraints:  \n\n1.  (W^δ_{t+1} − W^δ_t) Xₜ  =  0  \n2.  (W^C_{t+1} − W^C_t) Yₜ  =  0  \n\nIn other words, by holding δ fixed we “factor’’ the original consistency equation into one constraint on the update of the blue‐boxed \\(W^δ\\) and a completely separate constraint on the update of the yellow‐boxed \\(W^C\\).  Each of these can then be enforced by projecting the gradient of \\(W^δ\\) (resp. \\(W^C\\)) onto the null space of the features \\(Xₜ\\) (resp. \\(Yₜ\\)) when learning task \\(t\\!+\\!1\\)."
    },
    {
        "question": "How does sparse depth guidance integrate depth cues into camera features prior to view transformation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Sparse Depth Guidance. As shown in Fig. 4, SDG first projects each point of the input LiDAR point clouds into multi-view images, and obtains sparse multi-view depth maps.",
            "Then, they are fed into a shared encoder to extract depth features, which are concatenated with image features to form the depth-aware camera features.",
            "They are used as the input of view transformation, and finally voxel pooling [9] is employed to generate the image 3D feature volume, which is denoted as $F_{I}^{D}, V_I^{D}$."
        ],
        "final_answer": "Sparse depth guidance projects LiDAR points into each camera view to produce sparse depth maps, processes these maps through a shared encoder to extract depth features, and concatenates those depth features with the 2D image features to form depth-aware camera features. These combined features are then used as the input to the view transformation step.",
        "relevant_elements": [
            "SDG",
            "VT"
        ],
        "id": 810,
        "masked_question": "How does [mask1] integrate depth cues into camera features prior to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "SDG",
            "VT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00340v1_figure_1.png",
        "paperid": "2411.00340v1",
        "paper_path": "./papers/2411.00340v1.json",
        "figure_id": "2411.00340v1_figure_1.png",
        "caption": "Figure 1: Comparison between BEVFusion and the proposed GAFusion. (a) In BEVFusion, the camera stream and the LiDAR stream separately generate BEV features, which are then concatenated together. (b) In GAFusion, the camera modality BEV features are generated by multiple guidance from the LiDAR stream, and the receptive fields are enhanced by MSDPT. The BEV features are fused by LGAFT. “VT” is view transformer.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module is the Sparse Depth Guidance (SDG) and the blue box is the View Transformer (VT).  SDG sits immediately upstream of VT to “inject” real LiDAR depth into the 2D image features before they get lifted into 3D.  Concretely:\n\n1. SDG takes the raw LiDAR point cloud and projects each 3D point into every camera view, forming sparse depth maps in image space.  \n2. These sparse depth maps are passed through a small shared encoder to produce per-pixel depth‐feature maps.  \n3. The depth‐feature maps are concatenated channel-wise with the backbone’s 2D image features, yielding “depth-aware” image features.  \n4. Only after this concatenation does the View Transformer (VT) lift the now depth-augmented features into a 3D voxel (or BEV) volume.\n\nIn this way, SDG fuses LiDAR’s precise geometry with the camera stream before VT ever attempts its 3D projection."
    },
    {
        "question": "How does MSDPT enhance camera 3D features for adaptive fusion in LGAFT?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To effectively aggregate semantic information and enlarge the receptive fields, we improved a multi-scale dual-path transformer (MSDPT), a module inspired by OccFormer [47].",
            "Dual-path transformer (DPT) consists of a local path and a global path, which uses 3D convolution to perform downsampling to obtain features of different scales.",
            "The local path is mainly used to extract fine-grained semantic structures. Since the height direction has less variation in 3D object detection, the local path only slices and processes the 3D feature volume extracted from the multi-view images in parallel along the horizontal direction.",
            "The global path attempts to acquire the semantic layout of the scene accurately. It first obtains BEV features by average pooling along the height dimension, and then interacts with the basic information of the BEV features.",
            "To improve computational efficiency, they both use windowed self-attention [25], and share weights. Finally, the 3D feature volume from the local path merges the sufficient semantic features from the global path. The dual-path outputs are F_local and F_global, the combined output F is computed as: F_local ⊙ sigmoid(FFN(F_global))."
        ],
        "final_answer": "MSDPT enlarges the receptive field of the camera’s 3D feature volume and enriches its semantic content by running two parallel streams over multiple scales: a local path that slices horizontally to capture fine-grained detail and a global path that pools along height to capture scene-level layout. Both paths employ windowed self-attention, and their outputs are adaptively merged (F_local ⊙ sigmoid(FFN(F_global))). The resulting multi-scale, context-enhanced camera features are then used by the LGAFT module to perform adaptive fusion with LiDAR BEV features.",
        "relevant_elements": [
            "MSDPT",
            "LGAFT"
        ],
        "id": 811,
        "masked_question": "How does [mask1] enhance camera 3D features for adaptive fusion in LGAFT?",
        "masked_number": 1,
        "masked_elements": [
            "MSDPT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.00340v1_figure_1.png",
        "paperid": "2411.00340v1",
        "paper_path": "./papers/2411.00340v1.json",
        "figure_id": "2411.00340v1_figure_1.png",
        "caption": "Figure 1: Comparison between BEVFusion and the proposed GAFusion. (a) In BEVFusion, the camera stream and the LiDAR stream separately generate BEV features, which are then concatenated together. (b) In GAFusion, the camera modality BEV features are generated by multiple guidance from the LiDAR stream, and the receptive fields are enhanced by MSDPT. The BEV features are fused by LGAFT. “VT” is view transformer.",
        "qtype": "Experimental_Results",
        "response": "First, let’s align the red‐boxed module in the diagram with the text.  In Fig. 1(b), right after the view‐transformer (VT) that lifts the 2D image features into a 3D volume, the camera 3D features go into the module labeled “MSDPT” (that’s our [mask1]).  In Sec 3.3 the paper explains that:\n\n  • MSDPT stands for “multi‐scale dual‐path transformer.”  \n  • It consists of a local path (which slices the 3D volume horizontally and attends locally to capture fine detail) and a global path (which pools along height and attends to capture the overall scene layout).  \n  • Both paths use efficient windowed self‐attention and share weights.  \n  • Their outputs are fused—using height‐wise aggregation weights and a sigmoid gating—to produce a single enriched 3D feature volume.  \n\nBy running the camera’s 3D feature volume through MSDPT, the network\n\n  1. captures fine‐grained, height‐preserved semantics (via the local path),  \n  2. captures broad, scene‐level context (via the global path),  \n  3. merges them in a way that adaptively weighs each along the vertical (height) axis,  \n\nand in doing so greatly enlarges the receptive field and enriches the camera features.  These “MSDPT‐enhanced” camera 3D features are then fed as the key/value inputs into the LiDAR-Guided Adaptive Fusion Transformer (LGAFT), enabling much stronger, globally aware cross‐modal interactions when fusing with the LiDAR BEV features."
    },
    {
        "question": "What are potential drawbacks of sparse depth guidance (SDG) under incomplete LiDAR point coverage?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Due to the sparsity and measurement noises of LiDAR point clouds, the depth information of some pixels is inaccurate."
        ],
        "final_answer": "When LiDAR point coverage is incomplete or noisy, SDG can only provide depth at the sparse measured points, causing many pixels to have missing or inaccurate depth estimates.",
        "relevant_elements": [
            "SDG"
        ],
        "id": 812,
        "masked_question": "What are potential drawbacks of sparse depth guidance ([mask1]) under incomplete LiDAR point coverage?",
        "masked_number": 1,
        "masked_elements": [
            "SDG"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.00340v1_figure_1.png",
        "paperid": "2411.00340v1",
        "paper_path": "./papers/2411.00340v1.json",
        "figure_id": "2411.00340v1_figure_1.png",
        "caption": "Figure 1: Comparison between BEVFusion and the proposed GAFusion. (a) In BEVFusion, the camera stream and the LiDAR stream separately generate BEV features, which are then concatenated together. (b) In GAFusion, the camera modality BEV features are generated by multiple guidance from the LiDAR stream, and the receptive fields are enhanced by MSDPT. The BEV features are fused by LGAFT. “VT” is view transformer.",
        "qtype": "Others",
        "response": "The little red box in the diagram is our SDG (“sparse depth guidance”) module – it literally takes the raw LiDAR returns, projects them into each camera view, turns that tiny set of “image‐pixels‐with‐depth” into a learned depth‐feature map, and then tacks those depth features on to the usual image backbone before doing view‐projection.  If your LiDAR return is very sparse or simply misses large portions of the scene (for instance at long range, on low‐reflectivity surfaces, or in highly occluded regions), you get only a handful of depth measurements in each image.  That in turn causes three interlocking problems:  \n\n 1.  Holes and “blind spots” in your depth feature map  \n    •  Since you only ever see the pixels the LiDAR actually hit, huge swaths of the image get no depth signal at all.  \n    •  When you later do your “lift and splat” into 3D, you leave gaps in your volumetric features, so objects in those blind‐spot regions can never be reconstructed.  \n\n 2.  Bias toward LiDAR‐hits at the expense of everything else  \n    •  The network learns to trust the few LiDAR‐backed pixels very heavily, and may nearly ignore texture or color cues elsewhere.  \n    •  Any region without a return becomes “feature‐starved” – you can end up systematically under‐detecting anything that happens to lie in a LiDAR shadow.  \n\n 3.  Amplified noise and outlier sensitivity  \n    •  When there are only a handful of depth samples in a neighborhood a single noisy return or a multi‐path ghost will corrupt your learned depth‐feature more severely.  \n    •  Incomplete or spurious returns can actually pull your depth‐aware features in the wrong direction, leading to artifacts in the subsequent 3D volume.  \n\nIn short, pure sparse depth guidance works great wherever you have LiDAR returns, but wherever your point‐cloud is incomplete you get holes in your depth features, biased 3D reconstructions, and a dramatic sensitivity to individual noisy points."
    },
    {
        "question": "What alternative methods could reduce MSDPT computational cost while maintaining receptive field expansion?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To improve computational efficiency, they both use windowed self-attention [25], and share weights."
        ],
        "final_answer": "Replacing full self-attention with windowed self-attention and sharing the attention weights between the local and global paths reduces MSDPT’s computational cost while still preserving its ability to expand receptive fields.",
        "relevant_elements": [
            "MSDPT"
        ],
        "id": 813,
        "masked_question": "What alternative methods could reduce [mask1] computational cost while maintaining receptive field expansion?",
        "masked_number": 1,
        "masked_elements": [
            "MSDPT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.00340v1_figure_1.png",
        "paperid": "2411.00340v1",
        "paper_path": "./papers/2411.00340v1.json",
        "figure_id": "2411.00340v1_figure_1.png",
        "caption": "Figure 1: Comparison between BEVFusion and the proposed GAFusion. (a) In BEVFusion, the camera stream and the LiDAR stream separately generate BEV features, which are then concatenated together. (b) In GAFusion, the camera modality BEV features are generated by multiple guidance from the LiDAR stream, and the receptive fields are enhanced by MSDPT. The BEV features are fused by LGAFT. “VT” is view transformer.",
        "qtype": "Others",
        "response": "First, let me restate what the red‐boxed module is and why the question arises:\n\n  1.  In Fig. 1(b) of GAFusion the little red box sits on the arrow between the LiDAR‐guided 3D camera feature volume and the subsequent LiDAR‐guided adaptive fusion transformer (LGAFT).  If you look back at Sec. 3.3 in the text, that module is the “Multi-Scale Dual-Path Transformer” (MSDPT), which is doing windowed self-attention over both a local “slice-by-slice” path and a pooled global BEV path.  \n  2.  Windowed or global self-attention is known to expand receptive field, but it can be quite heavy computationally, especially in 3D.  The question is: what other architectural tricks could one swap in for the MSDPT to still get a large receptive field in the camera 3D volume, but at lower FLOPs?\n\nHere are a few of the most obvious “drop-in” replacements or approximations that the community has been using to strike the same balance of large context and cheaper compute:\n\n  •  Dilated (Atrous) Convolutions / ASPP  \n     –  Instead of attending across windows, you insert 3D convolutions with dilation rates {1,2,4,8,…} along the horizontal and depth axes.  \n     –  By stacking a small number of dilated convolutions (or by using an Atrous Spatial Pyramid Pooling block), you can grow your effective receptive field very quickly, at the cost of only a handful of 3×3×3 (or even 1×3×3 + 3×1×1) Conv kernels.  \n     –  Implementation note: you can make these depth-wise separable to cut flops even further.\n\n  •  Large-Kernel (Decomposed) Convolutions  \n     –  Recent works (e.g. RepLKNet, ConvNeXt-XL) have shown that a single 31×31 (or 15×15) convolution can be decomposed into a stack of 1D kernels (e.g. 1×k then k×1) or into a depth-wise plus point-wise pair, and still be very efficient.  \n     –  You get global context in one go, without the quadratic complexity of full self-attention.\n\n  •  Axial or Permuted MLP Blocks / Axial-Attention  \n     –  Rather than doing full 3D attention, you alternate attention or MLP‐mixing along one axis at a time (height, width, depth).  Each pass is only O(N·D) instead of O(N²) where N is the number of voxels in that slice.  \n     –  In practice this is almost as effective at spreading information globally but much cheaper.\n\n  •  Deformable or Strided Attention  \n     –  Borrowing from Deformable DETR or HaloNet, you attend only to a sparse set of offset positions (learned offsets or strided grids) instead of a dense window.  \n     –  This lets you reach far-away voxels with only a handful of sampled keys per query.\n\n  •  Lightweight Global Pooling + MLP  \n     –  You can pool the entire 3D volume along height (to get a BEV map), run a small 2D convolution or MLP there, then broadcast that global feature back into the volume exactly once.  \n     –  This “pool-broadcast” trick is extremely cheap and still ensures every voxel sees a bit of the whole scene.\n\nAny of those five strategies will expand the receptive field in a multi-scale way that’s competitive with a dual‐path transformer, but at a small fraction of the compute."
    },
    {
        "question": "What limitations might pixel selection impose on minority-class learning in the Professional Training Module?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Professional Training Module",
            "pixel selection"
        ],
        "id": 814,
        "masked_question": "What limitations might [mask1] impose on minority-class learning in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "pixel selection",
            "Professional Training Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.12680v2_figure_1.png",
        "paperid": "2409.12680v2",
        "paper_path": "./papers/2409.12680v2.json",
        "figure_id": "2409.12680v2_figure_1.png",
        "caption": "Figure 1: Overview of our framework. For labeled images, we apply weak augmentation for a labeled image and then feed it into Gen-Student and Pro-Student, supervised by the ground truth. For unlabeled images, we use two different modules: (a) In the professional training module, we apply weak augmentation and strong augmentation to an unlabeled image, feeding them into Gen-Teacher and Pro-Student, respectively, and then use refined pseudo-labels by pixel selection from Gen-Teacher to supervise Pro-Student s prediction. (b) In the general training module, weak and strong augmentations are applied to an unlabeled image, which are then fed into Pro-Teacher and Gen-Student, respectively, followed by the utilization of all pseudo-labels from Pro-Teacher to supervise Gen-Student s prediction. (c) In addition, we introduce a dual contrastive learning to foster distinct decision boundaries in the model, ensuring that it does not solely cater to the majority classes. (d) We evaluate the performance of STPG using Gen-Student.",
        "qtype": "Others",
        "response": "Here’s the reasoning step by step:\n\n1.  In the figure, the red‐boxed operation in (a) is the “Pixel selection” step, and the entire blue‐boxed subfigure is the “Professional Training Module.”\n\n2.  In that module, only two kinds of pixels are fed back as supervision to the Pro‐Student:  \n     •  those on which the Pro‐Student and Gen‐Teacher already agree (consistent pixels), and  \n     •  those on which they most strongly disagree (highly mismatched pixels).  \n\n3.  Every other pseudo‐label—i.e. pixels whose teacher confidence or disagreement score falls in the middle—is simply dropped.\n\n4.  Because true minority‐class pixels often sit in the gray area (not high‐confidence enough to be “consistent,” nor extreme enough to be “highly mismatched”), many of them will be filtered out by this selection rule.\n\n5.  As a result, the Professional Training Module ends up with a very sparse, skewed set of minority examples, which can:  \n     –  reduce the diversity of minority‐class supervision  \n     –  starve the model of enough positive minority‐class signals  \n     –  slow or even partially block learning on those underrepresented categories  \n\nAnswer:\nPixel selection, by only keeping the very “safe” (consistent) or very “hard” (highly mismatched) pixels and discarding everything in between, can inadvertently throw away a large fraction of true minority‐class pixels—those that neither the teacher nor student is supremely confident about—thus reducing the amount and variety of minority‐class supervision available to the Professional Training Module."
    },
    {
        "question": "How might memory bank capacity constraints undermine Dual Contrastive Learning’s effectiveness for rare classes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Dual Contrastive Learning",
            "memory bank"
        ],
        "id": 815,
        "masked_question": "How might [mask1] capacity constraints undermine Dual Contrastive Learning’s effectiveness for rare classes?",
        "masked_number": 1,
        "masked_elements": [
            "memory bank"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.12680v2_figure_1.png",
        "paperid": "2409.12680v2",
        "paper_path": "./papers/2409.12680v2.json",
        "figure_id": "2409.12680v2_figure_1.png",
        "caption": "Figure 1: Overview of our framework. For labeled images, we apply weak augmentation for a labeled image and then feed it into Gen-Student and Pro-Student, supervised by the ground truth. For unlabeled images, we use two different modules: (a) In the professional training module, we apply weak augmentation and strong augmentation to an unlabeled image, feeding them into Gen-Teacher and Pro-Student, respectively, and then use refined pseudo-labels by pixel selection from Gen-Teacher to supervise Pro-Student s prediction. (b) In the general training module, weak and strong augmentations are applied to an unlabeled image, which are then fed into Pro-Teacher and Gen-Student, respectively, followed by the utilization of all pseudo-labels from Pro-Teacher to supervise Gen-Student s prediction. (c) In addition, we introduce a dual contrastive learning to foster distinct decision boundaries in the model, ensuring that it does not solely cater to the majority classes. (d) We evaluate the performance of STPG using Gen-Student.",
        "qtype": "Others",
        "response": "Step 1 – Anchor the red‐boxed module in the text.  \nThe red box surrounds the “Dual Contrastive Learning” sub­figure – in particular the little diagram of class­‐specific anchors (yellow triangles), a cloud of features (blue circles), and the per‐class FIFO memory bank in the corner.  In the write-up this corresponds exactly to section “Dual Contrastive Learning with Anchors,” where they introduce:  \n  • a fixed anchor vector for each class  \n  • a small memory bank (FIFO queue) holding representative features per class  \n  • two losses – an “anchor contrastive” loss that pulls features to their correct class anchor and pushes them from all other class anchors, plus a “similarity” loss that pulls features toward other stored (positive) features of the same class in the memory bank.  \n\nStep 2 – Recall why Dual Contrastive Learning helps rare classes.  \nBy explicitly pulling every feature of class c toward a single class anchor and toward stored positives of class c, the method counteracts the “majority‐class” collapse that typically drowns out minority classes in conventional contrastive schemes.  In other words, even if only a few class‐c pixels ever show up in a batch, each one still receives both (i) a strong pull to the class‐c anchor, and (ii) reinforcement from any class‐c features in the memory bank.  \n\nStep 3 – Introduce capacity constraints and their effect.  \nIf the memory bank is forced to be very small – say only a handful of slots per class – two things happen:  \n  1. Rare‐class features get evicted almost as soon as they arrive (because more frequent classes continually overwrite the bank), so the bank contains almost no fresh, varied examples of the rare class.  \n  2. With too few positives in the bank, the similarity loss L_sim has almost no meaningful positive pairs to pull against – the contrastive signal for that class collapses.  \n\nStep 4 – Tie back to degraded decision boundaries.  \nOnce the memory bank can no longer supply a healthy set of class‐c positives, the rare‐class anchor loses its “gravity” (no rich cloud of stored positives to pull the new features in), and likewise there are too few negatives from other classes to push against.  The net result is that the model never learns a tight, well-separated cluster for the rare class, so its decision boundary remains weak or gets overtaken by the majority classes’ feature clouds.  \n\nAnswer – how capacity limits undermine the scheme:  \nIf the per-class memory bank is too small, it cannot hold enough positive examples of a rare class.  As a consequence, the dual contrastive losses (both the anchor pull and the intra-class pull) lose their force for that class, the learned features do not form a tight cluster around the class anchor, and the model fails to carve out a clear decision boundary for the minority category."
    },
    {
        "question": "What advantage does pixel selection bring to refining Gen-Teacher’s pseudo-labels for Pro-Student training?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "1: The first module only selects pseudo-labels that are predicted consistently or highly mismatched between a professional student (Pro-Student) and a general teacher (Gen-Teacher), which can learn more minority class information and reduce error accumulation.",
            "3.2: Thus, we propose a pixel selection strategy for producing refined pseudo-labels to improve the performance. The loss is calculated using consistent and highly mismatched pseudo-labels from Gen-Teacher to supervise Pro-Student’s predictions instead of all the pseudo-labels. The minority-class pseudo-labels are approximated with highly mismatched pseudo-labels, based on the observation that minority classes are more likely to be misclassified as other classes."
        ],
        "final_answer": "By selecting only the pixels on which Gen-Teacher and Pro-Student either agree (high-quality) or strongly disagree (likely minority-class), pixel selection produces refined pseudo-labels that focus Pro-Student’s learning on reliable and informative minority-class samples, thereby reducing noisy errors and improving performance.",
        "relevant_elements": [
            "pixel selection",
            "Gen-Teacher’s pseudo-labels",
            "Pro-Student"
        ],
        "id": 816,
        "masked_question": "What advantage does [mask1] bring to refining Gen-Teacher’s pseudo-labels for Pro-Student training?",
        "masked_number": 1,
        "masked_elements": [
            "pixel selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.12680v2_figure_1.png",
        "paperid": "2409.12680v2",
        "paper_path": "./papers/2409.12680v2.json",
        "figure_id": "2409.12680v2_figure_1.png",
        "caption": "Figure 1: Overview of our framework. For labeled images, we apply weak augmentation for a labeled image and then feed it into Gen-Student and Pro-Student, supervised by the ground truth. For unlabeled images, we use two different modules: (a) In the professional training module, we apply weak augmentation and strong augmentation to an unlabeled image, feeding them into Gen-Teacher and Pro-Student, respectively, and then use refined pseudo-labels by pixel selection from Gen-Teacher to supervise Pro-Student s prediction. (b) In the general training module, weak and strong augmentations are applied to an unlabeled image, which are then fed into Pro-Teacher and Gen-Student, respectively, followed by the utilization of all pseudo-labels from Pro-Teacher to supervise Gen-Student s prediction. (c) In addition, we introduce a dual contrastive learning to foster distinct decision boundaries in the model, ensuring that it does not solely cater to the majority classes. (d) We evaluate the performance of STPG using Gen-Student.",
        "qtype": "Design_Rationale",
        "response": "The little “Pixel selection” box you see in red is exactly our mechanism for trimming Gen-Teacher’s raw pseudo-labels down to only the most useful pixels.  By only keeping  \n  1. the pixels where Gen-Teacher and Pro-Student already agree (i.e. “consistent” labels, which are very unlikely to be wrong) and  \n  2. the pixels on which they disagree the most (i.e. “highly mismatched” labels, which tend to come from under-represented or minority classes)  \nwe end up feeding Pro-Student only high-quality, high-information pseudo-labels.  This  \n  – cuts out low-confidence or noisy pixels,  \n  – emphasizes minority-class samples,  \n  – and so both reduces error accumulation and boosts the model’s performance on rare categories."
    },
    {
        "question": "What is the motivation behind performing post-processing with point cloud registration after translation map generation?",
        "relevant_section_ids": [
            "1",
            "4.3"
        ],
        "relevant_context": [
            "However, these methods are sensitive to initialization especially when parts of the point cloud are occluded. Our method provides a reasonable initial guess that can be further refined by these methods.",
            "We can further use point cloud registration methods, e.g. SPR, to refine the estimated mesh by registering the vertices on the predicted mesh to the point clouds constructed from the depth image D. It is observed that the refined mesh is closer to the ground truth than the one without refinement and the one obtained by SPR initializing from a flattened mesh as seen in V-E."
        ],
        "final_answer": "The post-processing step uses classical point cloud registration (e.g. SPR) to refine the diffusion model’s initial mesh estimate—aligning it more precisely to the observed depth data and producing a reconstruction that is closer to the true cloth shape.",
        "relevant_elements": [
            "Post Processing",
            "Point Cloud Registration",
            "Translation Map Generation"
        ],
        "id": 818,
        "masked_question": "What is the motivation behind performing [mask1] after [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Post Processing",
            "Translation Map Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.11831v1_figure_1.png",
        "paperid": "2409.11831v1",
        "paper_path": "./papers/2409.11831v1.json",
        "figure_id": "2409.11831v1_figure_1.png",
        "caption": "Figure 1: (a) The workflow of the proposed method. (b) The detailed structure of the translation map generation model. It contains three main components: a MLP time step encoder, a ResNet-based observation encoder and a noise prediction network that uses CNN as backbone and the U-Net as the main structure. (c) An example of the mesh predicted by our method.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition in a nutshell:\n\n1.  Translation‐map generation (the blue box) only gives you an image whose pixels encode the 3D offset (Δx,Δy,Δz) of each vertex in a *canonical*, normalized cloth frame.  At that point you have no actual mesh in the real world—just a set of per-pixel translations.\n\n2.  Post-processing (the red box) is what “decodes” that translation map back into a proper 3D mesh and then *registers* it to your camera/world frame.  Concretely, it\n    • applies the predicted pixel-wise offsets to the known flat template to rebuild the mesh in canonical space  \n    • renormalizes, projects and then uses ICP (or another point‐cloud registration) to align that canonical mesh with the depth observation  \n    • uses the camera intrinsics to lift the result into real‐world coordinates  \n\nIn short, you do post-processing *after* translation-map generation because the diffusion network only ever predicts local vertex displacements.  You still need the post-processing step to (a) turn those displacements into an actual connected mesh and then (b) properly scale, translate, rotate and refine that mesh so it sits correctly in the observed 3D scene."
    },
    {
        "question": "What motivates conditioning noise prediction on both timestep embeddings and vision encoder outputs?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "The timestep t is also encoded by a MLP encoder with 2 fully connected layers so that the dimension of time has the same order of magnitude with the encoded observation.",
            "Finally, the outputs of the vision encoder and step encoder are concatenated as a latent vector c to represent the condition in Fig.1(b).",
            "The condition c is passed into each CNN layer of the noise prediction network after further processed by Feature-wise Linear Modulation (FiLM) method [34]."
        ],
        "final_answer": "Conditioning on both timestep embeddings and vision encoder outputs ensures that at each denoising iteration the network knows the current noise level (via the timestep embedding) and has access to the observed depth image (via the vision encoder). Encoding the timestep so that its embedding matches the scale of the visual features and then concatenating them into a single condition vector allows the noise prediction network to modulate its predictions based on both the diffusion step and the observation.",
        "relevant_elements": [
            "Step Encoder",
            "Vision Encoder",
            "Noise Prediction"
        ],
        "id": 819,
        "masked_question": "What motivates conditioning [mask1] on both timestep embeddings and [mask2] outputs?",
        "masked_number": 2,
        "masked_elements": [
            "Noise Prediction",
            "Vision Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.11831v1_figure_1.png",
        "paperid": "2409.11831v1",
        "paper_path": "./papers/2409.11831v1.json",
        "figure_id": "2409.11831v1_figure_1.png",
        "caption": "Figure 1: (a) The workflow of the proposed method. (b) The detailed structure of the translation map generation model. It contains three main components: a MLP time step encoder, a ResNet-based observation encoder and a noise prediction network that uses CNN as backbone and the U-Net as the main structure. (c) An example of the mesh predicted by our method.",
        "qtype": "Design_Rationale",
        "response": "First, let’s identify the two masked blocks by lining up the picture with the text:\n\n  • The red‐boxed module in Fig. 1 (b) is the “noise prediction network” (the U-Net backbone that actually predicts the denoising step).  \n  • The blue‐boxed module is the “vision encoder” (a ResNet-18 that turns the pre-processed depth image into a 256-D feature vector).\n\nIn a DDPM you must tell the noise predictor two things:\n\n  1.  “Where am I in the diffusion chain?” (i.e. how much noise remains, which is encoded by the timestep embedding), and  \n  2.  “What do I want to condition on?” (i.e. the visual features extracted from your depth observation).\n\nIf you only gave the noise predictor the image features, it wouldn’t know which noise level αₜ, βₜ it’s supposed to be undoing.  And if you only gave it the timestep, it would have no idea what the target shape should look like.  \nBy injecting both the timestep embedding (via the small MLP/time-step encoder) and the ResNet features (via FiLM layers in each CNN block), the network can dynamically adjust its filters to exactly the right noise level at each step, while still steering its output toward the observed cloth configuration."
    },
    {
        "question": "How does Feature-wise Linear Modulation integrate the condition vector into each CNN layer of the noise prediction network?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "The condition c is passed into each CNN layer of the noise prediction network after further processed by Feature-wise Linear Modulation (FiLM) method [34]."
        ],
        "final_answer": "The condition vector is first processed by the FiLM module to produce modulation parameters, and those parameters are then used to modulate (via feature-wise scaling and shifting) the activations in every CNN layer of the noise prediction network.",
        "relevant_elements": [
            "Feature-wise Linear Modulation",
            "noise prediction network"
        ],
        "id": 820,
        "masked_question": "How does [mask1] integrate the condition vector into each CNN layer of the noise prediction network?",
        "masked_number": 1,
        "masked_elements": [
            "Feature-wise Linear Modulation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.11831v1_figure_1.png",
        "paperid": "2409.11831v1",
        "paper_path": "./papers/2409.11831v1.json",
        "figure_id": "2409.11831v1_figure_1.png",
        "caption": "Figure 1: (a) The workflow of the proposed method. (b) The detailed structure of the translation map generation model. It contains three main components: a MLP time step encoder, a ResNet-based observation encoder and a noise prediction network that uses CNN as backbone and the U-Net as the main structure. (c) An example of the mesh predicted by our method.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed “Condition” node in Fig. 1 (b) is nothing exotic – it is simply the joint time‐step + vision embedding C being injected into every convolutional block via FiLM.  Concretely, at each conv layer in the U-Net “noise predictor” we take C, run it through a tiny MLP to spit out per‐channel scaling (γ) and shifting (β) vectors, and then apply  \n    yᶜ = γᶜ · xᶜ + βᶜ  \nto each feature map channel c.  This feature-wise linear modulation is how the condition vector is fused into every CNN layer."
    },
    {
        "question": "How are timestep embeddings transformed by the MLP time step encoder before conditioning the diffusion model?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "The timestep φ is also encoded by a MLP encoder with 2 fully connected layers so that the dimension of time has the same order of magnitude with the encoded observation.",
            "Finally, the outputs of the vision encoder and step encoder are concatenated as a latent vector c to represent the condition in Fig.1 (b)."
        ],
        "final_answer": "The scalar timestep is passed through a two‐layer MLP (two fully‐connected layers) to produce a time‐step embedding whose dimensionality matches that of the encoded depth observation. This embedding is then concatenated with the vision encoder output to form the conditional input to the diffusion model.",
        "relevant_elements": [
            "MLP time step encoder",
            "timestep embeddings"
        ],
        "id": 821,
        "masked_question": "How are [mask1] transformed by the MLP time step encoder before conditioning the diffusion model?",
        "masked_number": 1,
        "masked_elements": [
            "timestep embeddings"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.11831v1_figure_1.png",
        "paperid": "2409.11831v1",
        "paper_path": "./papers/2409.11831v1.json",
        "figure_id": "2409.11831v1_figure_1.png",
        "caption": "Figure 1: (a) The workflow of the proposed method. (b) The detailed structure of the translation map generation model. It contains three main components: a MLP time step encoder, a ResNet-based observation encoder and a noise prediction network that uses CNN as backbone and the U-Net as the main structure. (c) An example of the mesh predicted by our method.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In Fig. 1 (b), the red-boxed “Step Encoder” is fed the scalar diffusion time‐step t (that is what’s being masked in the question).  \n2. The caption and Sec. IV-B say “The timestep is also encoded by a MLP encoder with 2 fully connected layers so that the dimension of time has the same order of magnitude with the encoded observation.”  \n3. In the little red box you can read exactly: “Linear → Mish → Linear.”  \n4. Thus the time-step scalar is first linearly projected up in dimensionality, run through a Mish nonlinearity, and then linearly projected again to produce a time embedding.  \n5. That time embedding is then concatenated with the 256-D ResNet vision code and passed via FiLM into every U-Net layer to condition the DDPM.  \n\nAnswer  \nBefore conditioning the diffusion model, the scalar time-step is passed through a tiny 2-layer MLP—Linear → Mish → Linear—to produce a high-dimensional “time embedding” (of the same order as the 256-D image encoding), which is then concatenated with the observation encoding and injected into the U-Net via FiLM."
    },
    {
        "question": "How does the mask generator convert probability matrices into binary mask matrices?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Sigmoid function σ projects the values to probabilities.",
            "Since our goal is to filter out the adverse effects of irrelevant channels, we further perform Bernoulli resampling on the probability matrices to obtain binary mask matrix M^i with the same shape.",
            "To ensure the propagation of gradients, we use the Gumbel Softmax reparameterization trick (Jang et al., 2016) during Bernoulli resampling."
        ],
        "final_answer": "The mask generator first applies a sigmoid to its linear outputs to form a probability matrix, then performs Bernoulli resampling on each entry (using the Gumbel-Softmax trick to maintain gradients) to yield a binary mask matrix.",
        "relevant_elements": [
            "Mask Generator",
            "Probability Matrix"
        ],
        "id": 822,
        "masked_question": "How does the [mask1] convert [mask2] into binary mask matrices?",
        "masked_number": 2,
        "masked_elements": [
            "Mask Generator",
            "Probability Matrix"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.12261v1_figure_2.png",
        "paperid": "2410.12261v1",
        "paper_path": "./papers/2410.12261v1.json",
        "figure_id": "2410.12261v1_figure_2.png",
        "caption": "Figure 2: CATCH architecture. (1) Forward Module normalizes the input data, patchifies the frequency domain, and then projects it into the hidden space. (2) Channel Fusion Module captures channel interrelationships in each frequency band with a Channel-Masked Transformer (CMT) Layer, where the mask matrix (channel correlation) is generated by Mask Generator (MG). During backpropagation, MG is optimized by Channel Correlation Discovering mechanism for more appropriate channel correlations. (3) Time-Frequency Reconstruction Module obtains the frequency reconstruction through Flatten & Linear Head, and obtains the time reconstruction after iFFT.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed “Mask Generator” is nothing more exotic than an MLP → sigmoid → Bernoulli sampler.  Concretely, for each patch it first runs the patch embedding through a small MLP, then applies a sigmoid to turn each of the N×N outputs into a probability D_{i,j}∈(0,1) (this is your blue‐boxed “probability matrix”).  Finally it draws a 0/1 mask M by sampling each entry M_{i,j}∼Bernoulli(D_{i,j}), using the Gumbel–Softmax reparameterization during training so that gradients can flow.  The diagonal entries are then forced to 1 so that each channel always “sees” itself."
    },
    {
        "question": "How does the Channel-Masked Transformer Layer apply mask matrices to attention matrices?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "After the patch-wise mask generator outputs the mask matrices for frequency bands, we utilize the transformer layer to further capture the fine-grained channel correlations.",
            "Empirically, we utilize the masked attention mechanism to further model the fine-grained interrelationships among relevant channels and integrate the mask in a calculated way to keep the propagation of gradients:"
        ],
        "final_answer": "The Channel-Masked Transformer Layer takes the binary mask matrices produced by the patch-wise mask generator and applies them element-wise to the softmax-normalized attention matrix. In other words, after computing the attention scores and normalizing them with softmax, each entry in that attention matrix is multiplied by the corresponding entry in the mask matrix, zeroing out connections between channels deemed irrelevant while preserving gradient flow for the relevant ones.",
        "relevant_elements": [
            "Channel-Masked Transformer Layer",
            "Mask Matrix",
            "Attention Matrix"
        ],
        "id": 823,
        "masked_question": "How does the [mask1] apply mask matrices to attention matrices?",
        "masked_number": 1,
        "masked_elements": [
            "Channel-Masked Transformer Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.12261v1_figure_2.png",
        "paperid": "2410.12261v1",
        "paper_path": "./papers/2410.12261v1.json",
        "figure_id": "2410.12261v1_figure_2.png",
        "caption": "Figure 2: CATCH architecture. (1) Forward Module normalizes the input data, patchifies the frequency domain, and then projects it into the hidden space. (2) Channel Fusion Module captures channel interrelationships in each frequency band with a Channel-Masked Transformer (CMT) Layer, where the mask matrix (channel correlation) is generated by Mask Generator (MG). During backpropagation, MG is optimized by Channel Correlation Discovering mechanism for more appropriate channel correlations. (3) Time-Frequency Reconstruction Module obtains the frequency reconstruction through Flatten & Linear Head, and obtains the time reconstruction after iFFT.",
        "qtype": "Implementation_Details",
        "response": "In the CATCH paper’s nomenclature the little red‐boxed “mask1” is not a new kind of attention at all but simply the point at which the binary channel‐correlation masks  (produced by the Mask Generator) are “plugged in” to the usual Q–K attention.  Concretely, inside one CMT layer you do:\n\n  1.  Layer-Norm ⇒ project into Q,K,V.  \n  2.  Form the un-normalized attention scores  \n         T = Q Kᵀ / √d  \n  3.  Element-wise multiply by the mask M (and, equivalently, set the zero-entries of M to “–∞”)  \n         T̄ = T ⊙ M     (or  T̄  =  T + (1–M)⋅(–∞) )  \n  4.  Softmax over each row of T̄ to get the masked attention weights S  \n  5.  S V ⇒ the usual weighted sum of the values  \n\nBy zeroing (or driving to –∞) any off–diagonal entry that the mask says is “irrelevant,” the network is forced—that softmax only ever “fire” on those channel–pairs that the Mask Generator has chosen to keep."
    },
    {
        "question": "How does FFT & patching leverage vision transformer patch embedding for fine-grained frequency representation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In the Forward Module, we first apply the Instance Normalization … To model time series in both time and frequency domains, we then utilize the efficient FFT (Brigham & Morrow, 1967) to transform time series into orthogonal trigonometric signals in the frequency domain, where we keep both the real and imaginary (imag) parts through F(x)=[Re(x); Im(x)] for maximum information retention. Additionally, to capture fine-grained details in different frequency bands, we apply the patching operation in the frequency domain, the process is formalized as follows:\n\n   P^{R,i}_{:,k}=F_{:,(k-1)·s+1:(k-1)·s+w},\n   P^{I,i}_{:,k}=F_{:,(k-1)·s+1:(k-1)·s+w},\n\n   where each P^{R,i} and P^{I,i} ∈ ℝ^{N×p}, p=w and s is the stride.",
            "We then concat each pair of P^{R,i} and P^{I,i} into P^{i} as the i-th frequency patch. After patching in the frequency domain, the frequency patches are then projected into the high-dimensional hidden space through a learnable linear layer E:\n\n   P^{i}∈ℝ^{N×2p},    X^{i}=P^{i}E,    E∈ℝ^{2p×d}."
        ],
        "final_answer": "After converting the multivariate series into real and imaginary frequency coefficients via FFT, CATCH splits the spectrum of each channel into overlapping ‘frequency patches’ of size w (stride s), concatenates the real and imaginary parts of each patch into a 2p-length vector, and then linearly projects each patch with a learnable matrix E (ℝ^{2p×d}). This patch + linear-projection step directly mirrors the patch embedding used in Vision Transformers, yielding fine-grained, per-band frequency representations for downstream attention and reconstruction.",
        "relevant_elements": [
            "FFT & Patching",
            "Forward Module"
        ],
        "id": 824,
        "masked_question": "How does [mask1] leverage vision transformer patch embedding for fine-grained frequency representation?",
        "masked_number": 1,
        "masked_elements": [
            "FFT & Patching"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.12261v1_figure_2.png",
        "paperid": "2410.12261v1",
        "paper_path": "./papers/2410.12261v1.json",
        "figure_id": "2410.12261v1_figure_2.png",
        "caption": "Figure 2: CATCH architecture. (1) Forward Module normalizes the input data, patchifies the frequency domain, and then projects it into the hidden space. (2) Channel Fusion Module captures channel interrelationships in each frequency band with a Channel-Masked Transformer (CMT) Layer, where the mask matrix (channel correlation) is generated by Mask Generator (MG). During backpropagation, MG is optimized by Channel Correlation Discovering mechanism for more appropriate channel correlations. (3) Time-Frequency Reconstruction Module obtains the frequency reconstruction through Flatten & Linear Head, and obtains the time reconstruction after iFFT.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought  \n1. In ViT the first thing you do is turn your 2D image into a sequence of “patch tokens” by chopping the image into small squares, flattening each square, and linearly projecting it into your model’s hidden‐space—that’s called patch embedding.  \n2. CATCH wants exactly the same thing, but in the frequency domain rather than pixels. After instance-norm and an FFT on each multivariate time series, you end up with a long real-and-imaginary frequency spectrum for each channel.  \n3. The red-boxed block in Figure 2 is labeled “FFT & Patching.” Concretely, CATCH slides a small window (size p, stride s) over each channel’s spectrum, cuts out overlapping sub-bands (“frequency patches”), concatenates real and imaginary parts, then flattens and linearly projects each patch into a d-dimensional token.  \n4. Those tokens are thus the exact analogue of ViT’s image patch embeddings—but now each token carries a fine-grained sliver of the frequency content.  \n5. By tokenizing the spectrum into many small sub-bands, CATCH can attend to and fuse channel correlations on a very fine frequency scale.\n\nAnswer  \nThe red-boxed “FFT & Patching” module literally borrows ViT’s patch‐embedding idea and applies it to the FFT spectrum: it cuts each channel’s real+imag frequency curve into small, possibly overlapping patches, flattens those patches, and then linearly projects them into d-dimensional tokens. This converts the spectrum into a sequence of fine-grained frequency “patch tokens” that the downstream transformer can then attend over for very localized frequency modeling."
    },
    {
        "question": "How does Mask Generator integrate Gumbel Softmax to trade off between CI and CD channel strategies?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Note that the binary mask is an intermediate state between CI (identity matrix) and CD (all-ones matrix) strategies.",
            "Since our goal is to filter out the adverse effects of irrelevant channels, we further perform Bernoulli resampling on the probability matrices to obtain binary mask matrix M with the same shape.",
            "To ensure the propagation of gradients, we use the Gumbel Softmax reparameterization trick (Jang et al., 2016) during Bernoulli resampling."
        ],
        "final_answer": "The Mask Generator first projects each patch’s features into a probability matrix via a sigmoid. It then samples a binary mask from these probabilities—lying between the identity (CI) and all-ones (CD) extremes—by performing Bernoulli resampling with the Gumbel Softmax reparameterization trick, thereby enabling a differentiable trade-off between CI and CD strategies.",
        "relevant_elements": [
            "Mask Generator"
        ],
        "id": 825,
        "masked_question": "How does [mask1] integrate Gumbel Softmax to trade off between CI and CD channel strategies?",
        "masked_number": 1,
        "masked_elements": [
            "Mask Generator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.12261v1_figure_2.png",
        "paperid": "2410.12261v1",
        "paper_path": "./papers/2410.12261v1.json",
        "figure_id": "2410.12261v1_figure_2.png",
        "caption": "Figure 2: CATCH architecture. (1) Forward Module normalizes the input data, patchifies the frequency domain, and then projects it into the hidden space. (2) Channel Fusion Module captures channel interrelationships in each frequency band with a Channel-Masked Transformer (CMT) Layer, where the mask matrix (channel correlation) is generated by Mask Generator (MG). During backpropagation, MG is optimized by Channel Correlation Discovering mechanism for more appropriate channel correlations. (3) Time-Frequency Reconstruction Module obtains the frequency reconstruction through Flatten & Linear Head, and obtains the time reconstruction after iFFT.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module is in fact the “Mask–Generator” (MG) that sits immediately inside each Channel-Masked Transformer layer.  What it does is\n\n 1. Take the patch‐level embedding and run it through a tiny MLP to produce a real-valued probability matrix D∈ℝN×N  \n 2. Pass D through a sigmoid so that each entry Dij∈(0,1) can be interpreted as the probability that channel i “talks to” channel j  \n 3. Sample a hard 0/1 mask M∈{0,1}N×N from D by treating each Dij as the parameter of a Bernoulli distribution  \n\nIf we did that last step with a straight Bernoulli draw we wouldn’t be able to back-propagate gradients, so instead we use the Gumbel-Softmax (a.k.a. Gumbel-Sigmoid) reparameterization trick.  Concretely, for each element Dij we\n\n • add Gumbel noise   \n • divide by a temperature τ  \n • pass through a (soft) sigmoid  \n\nand then (at “test” time or for the forward pass) discretize to 0 or 1.  Because the Gumbel-Softmax is a continuous relaxation of a Bernoulli sample, gradients still flow back into D.\n\nBy adjusting τ (and by the shape of the losses that pull M either toward the identity matrix (CI) or the all-ones matrix (CD)), the model can smoothly interpolate between\n\n • the CI strategy (M≈I, i.e. “only look at yourself”)  \n • the CD strategy (M≈1, i.e. “fully connect all channels”)  \n\nIn practice the system is trained with a pair of losses—(a) a clustering (InfoNCE-style) loss that wants the “1” entries of M to correspond to strongly interrelated channels, and (b) a regularization loss that keeps M from simply becoming all ones—so that the Gumbel-Softmax–reparameterized binary mask ends up trading off exactly the right amount of CI vs. CD behavior for each frequency patch."
    },
    {
        "question": "How does the distilled diffusion model interact with the precision-optimized noise predictor to reduce reconstruction steps?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Section 4.2: \"To address this inefficiency, we employ a multi-sampling strategy within our framework by integrating a distilled diffusion model. ... By doing so, we optimize the reconstruction process in our attack framework, condensing it to just 1 to 4 steps. ... We replace the recursive application of one-step estimates with a single-step prediction using a distilled diffusion model.\"",
            "Section 4.3: \"To enhance the robustness of our adversarial attack, we finally use z̄ as the input for the distilled diffusion model regularized by precision optimized noise predictor. The final output, z̃, is designed to exhibit increased robustness.\""
        ],
        "final_answer": "The distilled diffusion model is fed the latent estimate refined by the precision-optimized noise predictor (which applies pairwise correlation and patch-wise KL losses to regularize the noise). By taking this regularized latent code as input, the distilled model can perform the full reverse diffusion reconstruction in just one to four steps instead of many recursive iterations, thus greatly reducing reconstruction time while maintaining adversarial quality.",
        "relevant_elements": [
            "distilled diffusion model",
            "precision-optimized noise predictor"
        ],
        "id": 826,
        "masked_question": "How does the distilled diffusion model interact with the [mask1] to reduce reconstruction steps?",
        "masked_number": 1,
        "masked_elements": [
            "precision-optimized noise predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.14089v1_figure_2.png",
        "paperid": "2410.14089v1",
        "paper_path": "./papers/2410.14089v1.json",
        "figure_id": "2410.14089v1_figure_2.png",
        "caption": "Figure 2: Overview of MMAD-Purify. In the MMAD-Purify framework, the input image is first processed through an encoder. It then enters the first distillated DM pipeline, where a precision-optimized noise predictor is applied. The resulting latents of the input image, combined with latents of other modalities, form a multi-modal representation. This multi-modal representation is then passed through a target classifier to generate perturbations, which are added back to the input image. This process is iteratively repeated, ultimately generating the adversarial example, 𝐱a⁢d⁢vsubscript𝐱𝑎𝑑𝑣\\mathbf{x}_{adv}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT. After purification, the final 𝐱a⁢d⁢vpsuperscriptsubscript𝐱𝑎𝑑𝑣𝑝\\mathbf{x}_{adv}^{p}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT has a different label from the generated image, demonstrating successful adversarial attack.",
        "qtype": "Literature_Background",
        "response": "In our diagram the red-boxed module is precisely the “precision-optimized noise predictor” that lives inside the distilled (student) diffusion model.  In the original MMAD attack you would grind away for dozens or even hundreds of tiny reverse‐diffusion steps (using a numerical solver on the PF-ODE), each time calling a full-blown noise predictor to go from \\(z_t\\to z_{t-1}\\).  What the distillation does is train a much smaller student network (the red box) to mimic the teacher’s multi-step trajectory in a single (or at most four) forward pass.  \n\nConcretely, at attack time you take your current noisy latent \\(z_t\\), pipe it into the distilled diffusion model—which now *is* essentially just that precision-optimized noise predictor plus a final update formula—and out pops an estimate of the next (or even the final) latent \\(z_{t-1}\\) (or \\(z_0\\)) in one go.  By replacing the loop of 30, 50 or 100 one-step solver calls with a single forward evaluation of the distilled noise predictor, you collapse the entire reverse diffusion process down to 1–4 steps.  That’s how the distilled model “hooks into” the red-boxed module and slashes reconstruction time."
    },
    {
        "question": "How does the precision-optimized noise predictor interact with the distilled diffusion model to improve computational efficiency?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "This replacement reduces the reconstruction process from multiple recursive steps to a single prediction operation, greatly improving computational efficiency.",
            "To enhance the robustness of our adversarial attack, we finally use z_N as the input for the distilled diffusion model regularized by precision optimized noise predictor. The final output, z_0, is designed to exhibit increased robustness."
        ],
        "final_answer": "The precision-optimized noise predictor first refines and regularizes the noise estimate to produce a single latent state z_N. Instead of running a full multi-step PF-ODE solver, this one refined latent vector is then passed through a distilled diffusion model that performs the entire reconstruction in one (or very few) steps. By combining the precision noise regularization with a distilled, single-step sampler, the method collapses the usual iterative diffusion pipeline into a single fast prediction, greatly improving computational efficiency.",
        "relevant_elements": [
            "Precision-optimized noise predictor",
            "Distilled diffusion model"
        ],
        "id": 828,
        "masked_question": "How does the [mask1] interact with the [mask2] to improve computational efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "Precision-optimized noise predictor",
            "Distilled diffusion model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.14089v1_figure_2.png",
        "paperid": "2410.14089v1",
        "paper_path": "./papers/2410.14089v1.json",
        "figure_id": "2410.14089v1_figure_2.png",
        "caption": "Figure 2: Overview of MMAD-Purify. In the MMAD-Purify framework, the input image is first processed through an encoder. It then enters the first distillated DM pipeline, where a precision-optimized noise predictor is applied. The resulting latents of the input image, combined with latents of other modalities, form a multi-modal representation. This multi-modal representation is then passed through a target classifier to generate perturbations, which are added back to the input image. This process is iteratively repeated, ultimately generating the adversarial example, 𝐱a⁢d⁢vsubscript𝐱𝑎𝑑𝑣\\mathbf{x}_{adv}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT. After purification, the final 𝐱a⁢d⁢vpsuperscriptsubscript𝐱𝑎𝑑𝑣𝑝\\mathbf{x}_{adv}^{p}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT has a different label from the generated image, demonstrating successful adversarial attack.",
        "qtype": "Experimental_Results",
        "response": "The key to the speed‐up in MMAD-Purify is that the little green “precision-optimized noise predictor” (the red box) gets dropped straight into the distilled diffusion pipeline (the blue box) so that you no longer have to run fifty or a hundred tiny diffusion‐steps.  In practice what happens is:\n\n  1. Off–line you train a big teacher diffusion model and then “distill” it into a tiny student network (that whole blue-boxed module).  By itself that student can hop from noisy latent zₜ to z₀ in only 1–4 steps instead of the usual dozens.\n  2. You then replace the student’s vanilla noise‐prediction head with a precision-optimized version (the red box), complete with the pairwise‐correlation and patch‐KL penalties.\n  3. Because that new predictor gives you much more accurate noise estimates, the 1–4 step student can still land on a high‐quality reconstruction, even though you’ve essentially cut out most of the intermediate diffusion steps.\n\nIn short, the red–boxed noise predictor is embedded inside the blue–boxed distilled model.  That combination lets you go from zₜ all the way to z₀ in just a handful of network calls, slashing runtime without blowing up the reconstruction error."
    },
    {
        "question": "How does multi-modal representation influence perturbation generation during each iteration of adversarial sample process?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Multi-modal representation",
            "Perturbation"
        ],
        "id": 829,
        "masked_question": "How does multi-modal representation influence [mask1] generation during each iteration of adversarial sample process?",
        "masked_number": 1,
        "masked_elements": [
            "Perturbation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.14089v1_figure_2.png",
        "paperid": "2410.14089v1",
        "paper_path": "./papers/2410.14089v1.json",
        "figure_id": "2410.14089v1_figure_2.png",
        "caption": "Figure 2: Overview of MMAD-Purify. In the MMAD-Purify framework, the input image is first processed through an encoder. It then enters the first distillated DM pipeline, where a precision-optimized noise predictor is applied. The resulting latents of the input image, combined with latents of other modalities, form a multi-modal representation. This multi-modal representation is then passed through a target classifier to generate perturbations, which are added back to the input image. This process is iteratively repeated, ultimately generating the adversarial example, 𝐱a⁢d⁢vsubscript𝐱𝑎𝑑𝑣\\mathbf{x}_{adv}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT. After purification, the final 𝐱a⁢d⁢vpsuperscriptsubscript𝐱𝑎𝑑𝑣𝑝\\mathbf{x}_{adv}^{p}bold_x start_POSTSUBSCRIPT italic_a italic_d italic_v end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT has a different label from the generated image, demonstrating successful adversarial attack.",
        "qtype": "Experimental_Results",
        "response": "Here’s the gist of what the red‐boxed “PERTURBATION” block is doing in each attack iteration, and how the fact that we are using a *multi-modal* representation changes the perturbation that gets produced:\n\n1.  Multi-modal encoding  \n    – At step t you encode *both* your image and your auxiliary modality (e.g. segmentation mask, text embedding, depth map, etc.) into a pair of latent vectors  \n       zᵗ_img = Encoder_img(x + δᵗ)  \n       zᵗ_mod = Encoder_mod(m)  \n    – You then *concatenate* or otherwise fuse those two latents into a single multi-modal vector:  \n       Zᵗ = concat(zᵗ_img, zᵗ_mod)\n\n2.  Classification and loss  \n    – You pass that fused vector Zᵗ into your target classifier C(·) to get logits, compute a standard classification loss ℓ(C(Zᵗ), y_true), and back-propagate.\n\n3.  Gradient‐based update  \n    – The gradient ∇ₓℓ now carries information not only about which pixels in the image are important, but also how those pixels interact with the *other* modality (e.g. what parts of the image the mask strongly highlights, or what semantic concepts the text prompt stresses).  \n    – You take a small ascent step in the image space along this fused, multi-modal gradient:  \n       δᵗ⁺¹ = Proj_{∞,ε}\\big( δᵗ + α · sign(∇ₓℓ(C(concat(Encoder_img(x+δᵗ), Encoder_mod(m))), y_true)) \\big)\n\n4.  Effect on the perturbation  \n    – Because the gradient is computed against a feature‐vector that *jointly* encodes image + mask/text/etc., the resulting δᵗ⁺¹ is pulled toward changes that will fool the classifier *given* that auxiliary signal.  \n    – In practice this means your perturbation will be more “aware” of the regions the second modality calls out (e.g. mask-ed object contours, semantically salient areas in the text prompt) and will be more transferable to other networks that also use that modality.\n\nIn short, it is the fusion of the image and the extra modality in the encoder‐classifier so that the back-propagated gradient—and therefore the adversarial perturbation—is shaped by *both* sources of information at every iteration."
    },
    {
        "question": "How does iterative jailbreaking with competing objectives refine synthetic prompts targeting Arab stereotypes?",
        "relevant_section_ids": [
            "3.2.x"
        ],
        "relevant_context": [
            "Phase 4: Jailbreaking ChatGPT via Iterative AIM and Competing Objectives At this stage, we have already succeeded in getting the model to generate harmful content. The next step involves creating synthetic prompts that can manipulate other LLMs into producing similar content. This process is iterative. First, we instruct the model to generate a prompt designed to elicit harmful ideas from another LLM (such as explaining how to build a bomb), while maintaining the previous context. We then test this synthetic prompt on a separate (test) LLM to see if it bypasses its safety mechanisms. If the test model does not generate harmful content, we provide the original model with the synthetic prompt and the test LLM’s response, indicating that the attempt failed, and instruct it to try again. This cycle continues until the original model creates a synthetic prompt capable of bypassing the test LLM’s safeguards. At that point, the prompt is likely to work on other LLMs as well.",
            "Phase 5: Intensifying Negative Stereotypes Towards Arabs Using AIM In the final phase, the model is instructed to create prompts that specifically target biases and stereotypes about Arabs. We apply the same iterative method to generate synthetic prompts that evoke stereotypes about Arabs, ensuring they align with our predefined categories. This process can be fully automated and has been used to generate synthetic prompts across all our targeted categories."
        ],
        "final_answer": "By repeatedly generating candidate prompts, testing them on a separate LLM, and feeding back failures, the model progressively reshapes its output until each synthetic prompt reliably bypasses safety filters. Once that iterative loop is mastered, the same procedure is applied specifically to Arab‐related content—prompt after prompt is automatically refined until it effectively evokes and intensifies the predefined stereotypes about Arabs while still fooling the target models’ safety mechanisms.",
        "relevant_elements": [
            "Phase 4: Jailbreak iterative AIM + Competing Objective",
            "Phase 5: Generate Biased Content"
        ],
        "id": 830,
        "masked_question": "How does [mask1] refine synthetic prompts targeting Arab stereotypes?",
        "masked_number": 1,
        "masked_elements": [
            "Phase 4: Jailbreak iterative AIM + Competing Objective"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.24049v3_figure_2.png",
        "paperid": "2410.24049v3",
        "paper_path": "./papers/2410.24049v3.json",
        "figure_id": "2410.24049v3_figure_2.png",
        "caption": "Figure 2: Pipeline for generating Red Teaming prompts to detect biases against Arabs. The process begins with semi-automatic AIM Chu et al. (2024) prompt generalization, Step 1, where we jailbreak ChatGPT to create 10 prompts for each of the eight categories described in Section 3.1. Step 2, we apply few-shot learning to automatically generate 100 prompts for each category. Step 3, the generated prompts are passed to six target models (Section 4.1), and the models’ responses are evaluated by the classifier (Section 4.2).",
        "qtype": "Experimental_Results",
        "response": "First, “[mask1]” corresponds to the red-boxed phase in Figure 2—Phase 4: “Jailbreak via Iterative AIM + Competing Objective.”  In that phase the authors take each candidate synthetic prompt and:\n\n1. Send it to a fresh LLM “test” session to see whether it actually elicits the forbidden content.  \n2. If the test model refuses (i.e. the prompt fails to jailbreak), they feed that failure signal back into the original prompt generator.  \n3. The generator then rewrites or sharpens the prompt (still under the tutor/mentor AIM framing plus a competing objective) to try again.  \n4. They repeat generate–test–feedback cycles until the prompt reliably bypasses safety checks.\n\nWhen this same iterative loop is applied in Phase 5 to Arab-stereotype content, it systematically hones each synthetic prompt—using direct failure/success feedback from a separate LLM—to produce highly effective, stereotype-triggering prompts targeted at the eight Arab bias categories."
    },
    {
        "question": "How does few-shot learning improve prompt diversity and maintain category coverage across eight stereotypes?",
        "relevant_section_ids": [
            "3.2.x"
        ],
        "relevant_context": [
            "After generating ten AIM prompts using the semi-automatic method described in Section 3.2, we expanded the dataset using GPT-4 through few-shot learning.",
            "Each newly generated prompt was iteratively fed back into the model to ensure novelty and minimize repetition. GPT-4 was tasked with generating five prompts at a time. This process allowed for diverse outputs across categories.",
            "The method efficiently produced 100 unique prompts per category without requiring specialized tokens or additional fine-tuning. Post-processing was applied to remove duplicates and ensure the dataset’s diversity."
        ],
        "final_answer": "Few-shot learning improves prompt diversity by having GPT-4 generate prompts in small batches (five at a time), then feeding each batch back into the model to enforce novelty and avoid repetition, followed by post-processing to remove any duplicates. Category coverage across all eight stereotypes is maintained by specifying the target category in each few-shot prompt, resulting in 100 unique, diverse prompts for each stereotype.",
        "relevant_elements": [
            "Step 2: Few Shot Learning"
        ],
        "id": 831,
        "masked_question": "How does [mask1] improve prompt diversity and maintain category coverage across eight stereotypes?",
        "masked_number": 1,
        "masked_elements": [
            "Step 2: Few Shot Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.24049v3_figure_2.png",
        "paperid": "2410.24049v3",
        "paper_path": "./papers/2410.24049v3.json",
        "figure_id": "2410.24049v3_figure_2.png",
        "caption": "Figure 2: Pipeline for generating Red Teaming prompts to detect biases against Arabs. The process begins with semi-automatic AIM Chu et al. (2024) prompt generalization, Step 1, where we jailbreak ChatGPT to create 10 prompts for each of the eight categories described in Section 3.1. Step 2, we apply few-shot learning to automatically generate 100 prompts for each category. Step 3, the generated prompts are passed to six target models (Section 4.1), and the models’ responses are evaluated by the classifier (Section 4.2).",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. The red-boxed module in Figure 2 is Step 2: the few-shot learning stage.  \n2. At this point we already have 10 “AIM” seed prompts for each of the eight stereotype categories.  \n3. To expand to 100 prompts per category, we prompt GPT-4 in five-shot batches, always tagging the call with one specific category (e.g. “Hostile Values,” “Terrorism,” etc.).  \n4. Each batch of generated prompts is fed back into the same few-shot template to check for novelty—any repeats are discarded and the model is prompted again until five fresh examples are produced.  \n5. After enough iterations to reach 100 total prompts per category, we apply simple post-processing (deduplication, trigram/cosine-similarity filters) and cluster the results into ten subtopics to verify that each stereotype is well covered.  \n6. By combining (a) category-conditioned few-shot seeding, (b) iterative novelty checks, and (c) post-hoc filtering and clustering, the pipeline both multiplies the original seeds into a large, varied set of prompts and ensures uniform coverage across all eight stereotypes.  \n\nAnswer  \nThe few-shot learning stage improves diversity by repeatedly seeding GPT-4 with small, category-tagged examples and rejecting any generated prompts that duplicate earlier ones. By cycling novel outputs back into the prompt template, running enough rounds to reach 100 prompts per category, and then applying deduplication and topic clustering, it guarantees a large, lexically varied pool while maintaining balanced coverage of all eight stereotype categories."
    },
    {
        "question": "What ethical safeguards could enhance Semi-Automatic AIM Prompt Generation to prevent harmful prompt proliferation?",
        "relevant_section_ids": [
            "7"
        ],
        "relevant_context": [
            "From an ethical standpoint, the intentional creation of jailbreak prompts that could propagate harmful stereotypes requires careful consideration.",
            "Future research should incorporate thorough ethical reviews, sensitivity analyses, and involve diverse research teams and stakeholders to mitigate risks.",
            "Our findings should inform improvements to LLMs’ unsafe content classifiers to ensure they effectively prevent harmful content generation.",
            "Expanding model diversity, improving transparency, and developing better bias detection tools will be essential for advancing ethical AI systems."
        ],
        "final_answer": "To prevent harmful prompt proliferation during Semi-Automatic AIM Prompt Generation, the paper recommends: conducting thorough ethical reviews of the prompt-generation process; performing sensitivity analyses to identify and mitigate risks; involving diverse research teams and external stakeholders to oversee and guide prompt design; improving LLM unsafe-content classifiers to catch and block harmful outputs; expanding the diversity of models and datasets; increasing transparency around prompt development; and developing more robust bias-detection tools.",
        "relevant_elements": [
            "Semi-Automatic AIM Prompt Generation"
        ],
        "id": 832,
        "masked_question": "What ethical safeguards could enhance [mask1] to prevent harmful prompt proliferation?",
        "masked_number": 1,
        "masked_elements": [
            "Semi-Automatic AIM Prompt Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.24049v3_figure_2.png",
        "paperid": "2410.24049v3",
        "paper_path": "./papers/2410.24049v3.json",
        "figure_id": "2410.24049v3_figure_2.png",
        "caption": "Figure 2: Pipeline for generating Red Teaming prompts to detect biases against Arabs. The process begins with semi-automatic AIM Chu et al. (2024) prompt generalization, Step 1, where we jailbreak ChatGPT to create 10 prompts for each of the eight categories described in Section 3.1. Step 2, we apply few-shot learning to automatically generate 100 prompts for each category. Step 3, the generated prompts are passed to six target models (Section 4.1), and the models’ responses are evaluated by the classifier (Section 4.2).",
        "qtype": "Others",
        "response": "Step 1 here (“Semi-Automatic AIM Prompt Generation”) is precisely where malicious or “jailbreak” style prompts are born and iterated.  To harden that phase against spiraling into an ever-wider proliferation of harmful requests, one could introduce the following ethical safeguards:\n\n1. Mandatory Ethical Review Gate  \n   • Require that every new batch of seed-prompts passes through an independent ethics board or data steward who vets them for potential misuse.  \n   • Institute clear “do-not-generate” lists (e.g. bomb-making, hate-incitement, malware) that are automatically checked before a prompt is ever used or expanded.  \n\n2. Sensitivity and Risk Analysis  \n   • Before iterative jailbreaking begins, run each candidate prompt through a sensitivity analyzer (ideally a specialist model or a human panel) to score its potential for real-world harm.  \n   • If a prompt’s “harm score” exceeds a low threshold, flag it for manual review rather than automatic few-shot expansion.  \n\n3. Human-in-the-Loop Oversight  \n   • Split the pipeline so that after every 2–3 rounds of AIM-driven rewriting, a human reviewer must explicitly sign off on the next iteration.  \n   • Rotate reviewers from diverse backgrounds (legal, policy, cultural) to reduce groupthink and detect edge-cases.  \n\n4. Transparent Logging and Accountability  \n   • Log every prompt, revision step, reviewer decision and risk score in an immutable audit trail.  \n   • Publish an abstracted log (minus proprietary details) so outside auditors can verify no disallowed content slipped through.  \n\n5. Built-in Policy Filters and Kill-Switches  \n   • Embed an inline policy filter that automatically truncates or neutralizes any attempt to describe methods of violence, hacking, or hate.  \n   • If the AIM loop ever crosses a “high risk” boundary, automatically terminate the session and alert the compliance team.  \n\n6. Continuous Model Calibration  \n   • Regularly fine-tune the underlying AIM-assistant on adversarially curated “safe” and “unsafe” libraries so it becomes increasingly averse to generating or refining harmful material.  \n   • Periodically inject new counter-examples (e.g. newly emerging attack vectors) to keep the filter up to date.  \n\nBy weaving in these layers of automated guardrails plus human checks right at the semi-automatic prompt-generation stage, one can greatly reduce the odds that the system will spiral into churning out proliferating harmful jailbreak prompts."
    },
    {
        "question": "How might Few-Shot Learning risk reinforcing stereotypes due to limited prompt diversity?",
        "relevant_section_ids": [
            "3.2.x",
            "7"
        ],
        "relevant_context": [
            "After generating ten AIM prompts using the semi-automatic method described in Section 3.2, we expanded the dataset using GPT-4 through few-shot learning. Each newly generated prompt was iteratively fed back into the model to ensure novelty and minimize repetition. GPT-4 was tasked with generating five prompts at a time. This process allowed for diverse outputs across categories. The method efficiently produced 100 unique prompts per category without requiring specialized tokens or additional fine-tuning. Post-processing was applied to remove duplicates and ensure the dataset’s diversity.",
            "Our use of few-shot learning for prompt generation, though effective, involved a selective process that could introduce bias due to the iterative nature of identifying high-performing prompts during semi-automatic generation (see Step 1 in Section 3.2)."
        ],
        "final_answer": "Few‐shot learning relies on a small set of exemplar prompts to bootstrap new queries. If those seed prompts are not sufficiently varied, the model may repeatedly echo the same patterns and biases. In practice, the iterative selection of high‐performing examples can narrow the prompt distribution and inadvertently reinforce stereotypes, since there are too few distinct contexts to break existing prejudices.",
        "relevant_elements": [
            "Few-Shot Learning"
        ],
        "id": 833,
        "masked_question": "How might [mask1] risk reinforcing stereotypes due to limited prompt diversity?",
        "masked_number": 1,
        "masked_elements": [
            "Few-Shot Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.24049v3_figure_2.png",
        "paperid": "2410.24049v3",
        "paper_path": "./papers/2410.24049v3.json",
        "figure_id": "2410.24049v3_figure_2.png",
        "caption": "Figure 2: Pipeline for generating Red Teaming prompts to detect biases against Arabs. The process begins with semi-automatic AIM Chu et al. (2024) prompt generalization, Step 1, where we jailbreak ChatGPT to create 10 prompts for each of the eight categories described in Section 3.1. Step 2, we apply few-shot learning to automatically generate 100 prompts for each category. Step 3, the generated prompts are passed to six target models (Section 4.1), and the models’ responses are evaluated by the classifier (Section 4.2).",
        "qtype": "Others",
        "response": "Chain-of-Thought:  \n1. The red-boxed area in the figure corresponds to “Step 2: Few-Shot Learning,” where GPT-4 is used to expand each category from 10 seed AIM prompts to 100 total prompts.  \n2. Those 10 seed prompts per category already encode very stereotypical framings (e.g. associating Arabs with terrorism, backwardness, religious extremism, etc.).  \n3. In few-shot learning, the model strongly anchors on those exemplars to generate new items, so newly minted prompts tend to mimic the same narrow structures, phrasings, and biased angles.  \n4. Because there’s little real variability in the initial seeds, the generated set remains topically repetitive and centered on the same caricatures, rather than exploring a broader or more nuanced portrayal of each category.  \n5. Consequently, instead of diluting or challenging those stereotypes, the process effectively amplifies and cements them across the expanded dataset.\n\nAnswer:  \nBy bootstrapping only from a handful of heavily stereotyped seed prompts, the few-shot stage produces new prompts that closely echo the same narrow tropes. This limited exemplar diversity causes the model to reproduce and even magnify existing caricatures—thereby reinforcing the very stereotypes it’s meant to expose."
    },
    {
        "question": "What potential biases arise from hierarchical latent spaces when reverse optimizing surrogate predictions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "hierarchical latent spaces",
            "reverse optimize",
            "surrogate model"
        ],
        "id": 834,
        "masked_question": "What potential biases arise from [mask1] when reverse optimizing surrogate predictions?",
        "masked_number": 1,
        "masked_elements": [
            "hierarchical latent spaces"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.11226v1_figure_1.png",
        "paperid": "2410.11226v1",
        "paper_path": "./papers/2410.11226v1.json",
        "figure_id": "2410.11226v1_figure_1.png",
        "caption": "Figure 1: Overview of Multi-Fidelity Latent space Active Learning (MF-LAL).",
        "qtype": "Others",
        "response": "Here is a step‐by‐step reasoning of where the bias comes from and what it looks like in practice:\n\n1.   What is [mask1]?  \n     The red box in Fig. 1 is exactly the “multi‐fidelity latent representation” block—i.e. the cascade of probabilistic encoders that map a molecule into a hierarchy of Gaussian latent spaces (z₁→…→zₖ).\n\n2.   How do we use it in reverse optimization?  \n     Once we have trained SVGP surrogates on each of those z‐spaces, we pick a point in latent space and do gradient ascent on the surrogate’s UCB (plus regularizers), then decode back to a molecule.\n\n3.   Where does the bias sneak in?  \n   • The encoders/priors impose a very strong inductive bias on what regions of chemical space are “reachable.”  Because each latent space is trained to reconstruct typical drug-like molecules from the training set, the region of z-space we actually explore in reverse optimization is heavily skewed toward those familiar chemotypes.  \n   • The Gaussian assumption at each fidelity level tends to collapse multi-modal binding patterns into a single smooth mode, so reverse optimization can never access minority modes in the true chemical–affinity landscape.  \n   • The hierarchical links (z₁→z₂→…) propagate any systematic error or blind‐spot in the low‐fidelity encoder all the way up to the highest fidelity.  As a result, molecules that might be excellent under a high-fidelity oracle—but that look “alien” to the low‐fidelity latent manifold—are never discovered.\n\n4.   What does that look like in practice?  \n   – You end up with generated compounds that are “boring” variations on the training set, rather than genuine out‐of‐distribution novelties.  \n   – The surrogates become overconfident in parts of z-space that were well covered by the training data (and underconfident or simply blind elsewhere), so reverse optimization chases after “holes” in the surrogate that are really artifacts of the encoder.  \n   – In extreme cases you may even produce chemically nonsensical molecules that maximize the surrogate’s UCB but have zero chance of binding in the real world.\n\nIn short, by reverse‐optimizing through a learned, hierarchical latent encoding, you inherit all of its architectural and prior biases—biasing you toward regions of chemical space that the encoder saw during training and away from potentially better, but out-of-distribution, solutions."
    },
    {
        "question": "How might Bayesian active learning thresholds affect the diversity of generated molecules across fidelities?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Section 3.2: “Specifically, during active learning, we repeatedly generate a latent vector z^{(i)} at fidelity f_i that decodes to a query compound. If Var[f_i](z^{(i)}) < τ, where τ is the uncertainty threshold, then we permanently increment i by one for all subsequent queries. Otherwise, i remains the same.”",
            "Section 3.3: “This additional term greatly restricts the area of the chemical space explored by the high fidelity oracles, reducing the computational cost wasted on non-promising areas and making the use of high-cost oracles feasible. It also means the higher fidelity latent spaces encode a more limited distribution of compounds, improving the quality of samples generated from those latent spaces.”"
        ],
        "final_answer": "Because the threshold τ in the Bayesian active learning loop governs when the model moves from a lower‐ to a higher‐fidelity latent space, a stricter (lower) threshold will trigger an earlier transition into the more restricted high‐fidelity space—yielding less structural diversity—whereas a more lenient (higher) threshold keeps the model longer in the broader low‐fidelity space, allowing greater diversity of generated compounds at that level.",
        "relevant_elements": [
            "Bayesian active learning",
            "generate molecules",
            "fidelities"
        ],
        "id": 835,
        "masked_question": "How might [mask1] thresholds affect the diversity of [mask2] across fidelities?",
        "masked_number": 2,
        "masked_elements": [
            "Bayesian active learning",
            "generate molecules"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.11226v1_figure_1.png",
        "paperid": "2410.11226v1",
        "paper_path": "./papers/2410.11226v1.json",
        "figure_id": "2410.11226v1_figure_1.png",
        "caption": "Figure 1: Overview of Multi-Fidelity Latent space Active Learning (MF-LAL).",
        "qtype": "Others",
        "response": "In our framework, the thing we’re calling “[mask1]” in the red‐boxed panel are the uncertainty thresholds on the GP’s posterior variance that tell us “stay at this fidelity and keep generating more points” versus “we’re confident enough here—move up to the next fidelity.”  The thing in the blue box, “[mask2],” is simply the set of new query molecules we generate at whatever fidelity we are currently at.\n\nHow those uncertainty thresholds get set has a direct impact on how wide or narrow our molecular search is, at each fidelity:\n\n 1. If you choose a **high uncertainty threshold** (i.e. you demand very low model variance before you ever advance to the next fidelity), you will linger **longer** at the low‐cost (low‐fidelity) oracle.  That gives you more opportunities to explore broadly in the low‐fidelity latent space, so your **diversity of generated molecules at low fidelity goes up**, but by the time you finally move up there’s a much smaller, more focused pool of candidates to promote.  In other words, **high thresholds ⇒ more diversity at low fidelities, less variety makes it all the way to high fidelity.**\n\n 2. Conversely, if you set a **low threshold**, you’ll **escalate** to the next fidelity very quickly.  You’ll still sample at low fidelity, but only enough to knock down that variance requirement, and then you’re off to the higher–cost oracle.  This tends to **shrink your diversity at low fidelity** (because you didn’t explore as long) but **produces a broader swath of query molecules at the higher fidelities**, since you’re generating and assessing more of them at that level.\n\nPut another way, the uncertainty threshold is literally a filter on how many and how varied your “[mask2]” query molecules you let through from one fidelity to the next.  Tune that threshold up and you champion diversity early on (but sacrifice high‐fidelity breadth); tune it down and you get a tighter early search but a wider net when you finally hit the expensive oracle."
    },
    {
        "question": "What motivates separate hierarchical latent spaces for each fidelity level instead of a shared latent representation?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Instead of separating the generative model and surrogate model, we perform surrogate modeling and generation together at each fidelity level using a sequence of hierarchical latent spaces. This improves the quality of generated queries because there is a separate latent space and decoder specialized for each fidelity, and improves surrogate modeling and inter-fidelity information passing because each latent space can be organized for predicting at just that level.",
            "Note that G_θ^{(1:K)}≠G^{(1)}=…=G^{(K)}, meaning we must learn separate generative models for each fidelity level, as opposed to previous approaches that learn a single generative model for all fidelities.",
            "The use of a specialized decoder for each fidelity level improves reconstruction quality compared to previous methods that only use one, thus making the generated samples more tailored for their fidelity level.",
            "Additionally, the use of separate latent spaces for each fidelity level, as opposed to previous approaches that use only a single latent space shared across all levels, improves surrogate modeling performance because each latent space can be organized for prediction at just that level."
        ],
        "final_answer": "Separate hierarchical latent spaces are motivated by the need to tailor both generation and surrogate modeling to each fidelity. A dedicated latent space and decoder at each fidelity improve reconstruction quality and ensure generated compounds are specialized for that fidelity. Moreover, organizing each latent space around predictions at a single fidelity enhances surrogate accuracy and enables more efficient passing of learned information from lower to higher fidelities, advantages not achievable with a single shared latent representation.",
        "relevant_elements": [
            "Multi-fidelity latent representation",
            "hierarchical latent spaces"
        ],
        "id": 836,
        "masked_question": "What motivates separate [mask1] for each fidelity level instead of a shared latent representation?",
        "masked_number": 1,
        "masked_elements": [
            "hierarchical latent spaces"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.11226v1_figure_1.png",
        "paperid": "2410.11226v1",
        "paper_path": "./papers/2410.11226v1.json",
        "figure_id": "2410.11226v1_figure_1.png",
        "caption": "Figure 1: Overview of Multi-Fidelity Latent space Active Learning (MF-LAL).",
        "qtype": "Design_Rationale",
        "response": "The key insight is that each fidelity level (e.g. fast but noisy docking vs. slow but accurate free‐energy calculations) really does live in a different “chemical subspace.”  If you cram them all into one monolithic latent space and one decoder, you end up with a representation that is mediocre at every task.  By giving each fidelity its own latent space (and its own decoder), you can\n\n  • Organize that latent space specifically for predicting that oracle’s outputs  \n  • Tailor the decoder to faithfully reconstruct the kinds of molecules that this fidelity “cares about”  \n  • Pass only the relevant, distilled information up the fidelity hierarchy (so you don’t re‐learn low-fidelity noise at the high end)  \n  • Generate queries that are well‐suited to the peculiarities of each fidelity’s distribution  \n\nAll of that leads to better surrogate‐model accuracy, sharper reconstructions, and ultimately more efficient multi‐fidelity active learning than you’d get with one shared latent representation."
    },
    {
        "question": "What motivates combining reverse optimization with surrogate prediction into a unified framework?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "However, since we want to generate query compounds to send to oracles at multiple fidelity levels, the distribution of optimal query compounds may differ across fidelities. A separate generative model is not aware of such differences across fidelity levels, hence it cannot send queries to the multi-fidelity oracles efficiently.",
            "Instead of separating the generative model and surrogate model, we perform surrogate modeling and generation together at each fidelity level using a sequence of hierarchical latent spaces. This improves the quality of generated queries because there is a separate latent space and decoder specialized for each fidelity, and improves surrogate modeling and inter-fidelity information passing because each latent space can be organized for predicting at just that level."
        ],
        "final_answer": "Because optimal query molecules differ by fidelity and a standalone generative model cannot account for those differences, the authors integrate reverse optimization (generation) and surrogate prediction in one hierarchical latent‐space framework. This unified design ensures each fidelity has its own latent space and decoder—improving the quality of generated queries and enabling more accurate, fidelity‐aware surrogate modeling and inter‐fidelity information sharing.",
        "relevant_elements": [
            "Reverse optimize",
            "Surrogate model"
        ],
        "id": 837,
        "masked_question": "What motivates combining [mask1] with [mask2] into a unified framework?",
        "masked_number": 2,
        "masked_elements": [
            "Reverse optimize",
            "Surrogate model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.11226v1_figure_1.png",
        "paperid": "2410.11226v1",
        "paper_path": "./papers/2410.11226v1.json",
        "figure_id": "2410.11226v1_figure_1.png",
        "caption": "Figure 1: Overview of Multi-Fidelity Latent space Active Learning (MF-LAL).",
        "qtype": "Design_Rationale",
        "response": "Here is the intuition in a nutshell:\n\n1.   Separate surrogate-modeling and generation breaks down as soon as you try to push molecules through oracles of wildly different cost/accuracy (e.g. docking vs. ABFE).  The “best” compounds at low fidelity live in a very different part of chemical space than the “best” compounds at high fidelity, so a stand-alone generator simply cannot learn to cover all of those modes efficiently.\n\n2.   By folding the generator (“reverse-optimize in latent space for novel molecule generation” [mask₁]) together with the act of actually proposing those molecules to your oracles ([mask₂], the picture of the candidate compound), you get a single, coherent latent hierarchy that\n     •   Specializes each layer to exactly one fidelity  \n     •   Shares information up and down between fidelities  \n     •   Knows, at generation time, how to satisfy both the cheap docking score and the expensive ABFE score  \n\nIn short, the motivation is that **only** by integrating molecule generation directly into the multi-fidelity surrogate (rather than tacking on a separate generator afterward) can you efficiently navigate the cost–accuracy trade-off between docking and free-energy methods and produce high-quality candidates that do well at all fidelity levels."
    },
    {
        "question": "What motivates using diverse criteria (Common, Longtailed, Random, Nonexistent) for concept selection in Visual Information Construction?",
        "relevant_section_ids": [
            "2.2.1"
        ],
        "relevant_context": [
            "The degree of association between object concepts in the graph (i.e., co-occurrence frequency) reflects the distribution of the objects.",
            "Based on this, we designed four criteria for concept combinations with increasing difficulty: Common: Combine the concept pairs with the highest co-occurrence frequency, Long-tail: Combine the concept pairs with associations but the lowest co-occurrence frequency in the graph, Random: Randomly combine two object concepts from the graph, Fictional: Randomly combine object concepts in the graph that have no associations.",
            "The selected pairs  are used to dynamically generate test images under different distributions. Such approach ensures the randomness of the sample concept distribution."
        ],
        "final_answer": "The diverse criteria are motivated by the desire to cover a spectrum of concept-pair distributions—ranging from very common co-occurrences to rare, to entirely unassociated pairs—so that test images span increasing difficulty levels and maintain randomness in their concept selection.",
        "relevant_elements": [
            "Criteria",
            "Visual Information Construction"
        ],
        "id": 838,
        "masked_question": "What motivates using diverse [mask1] for concept selection in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Criteria",
            "Visual Information Construction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.09318v1_figure_2.png",
        "paperid": "2409.09318v1",
        "paper_path": "./papers/2409.09318v1.json",
        "figure_id": "2409.09318v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the Open-Set Dynamic Evaluation Protocol. The workflow involves constructing a graph and generating test samples based on the graph, with four distinct steps.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1.  In the blue‐boxed block of the figure (“Visual Information Construction”), the authors must pick pairs of concepts to put into an image prompt (e.g. “a picture of A and B”).  \n2.  The red‐boxed text lists the four *criteria* they use to choose those pairs:  \n    – Common (highest co‐occurrence)  \n    – Long-tailed (lowest nonzero co‐occurrence)  \n    – Random (any two concepts)  \n    – Nonexistent (no documented co‐occurrence)  \n3.  In the paper (Sec. II-B1) they explain that by sampling pairs under these four criteria they  \n    a. cover the whole spectrum of real-world co-occurrence (from very frequent to never seen),  \n    b. vary the “difficulty” of the image (easy → hard → purely fictional), and  \n    c. guarantee randomness and unpredictability in their test set.  \n4.  This diversity is exactly what they need in the Visual Information Construction stage to stress-test models for object-existence hallucination across different distributions and difficulty levels.  \n\nAnswer:  \nThey employ diverse selection *criteria* in the Visual Information Construction step to ensure their concept pairs span from very common co-occurrences to never-seen combinations.  This gives them a random, broadly distributed set of image conditions—ranging in difficulty—that is crucial for thoroughly evaluating when and how large multimodal models hallucinate objects."
    },
    {
        "question": "Why integrate both Generative Task and Discriminative Task in Prompts Construction for hallucination evaluation?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "For generative tasks, we use the prompt “Please describe this image.” to instruct the MLLM to provide a description of the concepts present in the image.",
            "For discriminative tasks, we use “Is there a {object} in the image?” expecting a “yes” or “no” response. To evaluate hallucinated objects, we construct counterfactual prompts like “Is there a {hallucinated object} in the image?”"
        ],
        "final_answer": "By combining a Generative Task with a Discriminative Task, ODE is able to measure hallucinations in two complementary ways: the generative prompt elicits a free-form description of everything the model “sees,” thereby revealing any spontaneously introduced (hallucinated) objects, while the discriminative prompt drills down to yes/no judgments about the presence or absence of specific objects (including deliberately counterfactual ones). Together, they provide both open-ended and targeted checks for object-existence hallucinations.",
        "relevant_elements": [
            "Generative Task",
            "Discriminative Task",
            "Prompts Construction"
        ],
        "id": 839,
        "masked_question": "Why integrate both [mask1] and [mask2] in Prompts Construction for hallucination evaluation?",
        "masked_number": 2,
        "masked_elements": [
            "Generative Task",
            "Discriminative Task"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.09318v1_figure_2.png",
        "paperid": "2409.09318v1",
        "paper_path": "./papers/2409.09318v1.json",
        "figure_id": "2409.09318v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the Open-Set Dynamic Evaluation Protocol. The workflow involves constructing a graph and generating test samples based on the graph, with four distinct steps.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition, step by step:\n\n1.  Identify the two prompt types in the diagram  \n    -  [mask1] (red box) is the “Generative Task” prompt:  \n       “Please describe this image.”  \n    -  [mask2] (blue box) is the “Discriminative Task” prompt:  \n       “Is there a {object} in the image?”  \n       – once instantiated with an actually‐present object  \n       – again instantiated with a hallucination‐prone object  \n\n2.  What each prompt buys you  \n    •  Generative (“Describe this image.”)  \n       – Forces the model to list out *everything* it “sees”  \n       – Any invented (hallucinated) object will appear directly in its free‐form caption  \n       – Gives a broad, open‐ended view of the model’s internal world‐model  \n\n    •  Discriminative (“Is there a X in the image?”)  \n       – Pinpoints its yes/no judgment on specific concepts  \n       – By asking once about a truly present object, you check *sensitivity* (does it notice what’s really there?)  \n       – By asking once about a likely‐but‐absent object, you check *specificity* (does it falsely affirm a hallucinatory object?)  \n\n3.  Why combine them?  \n    – Generative alone can miss subtle hallucinations buried in prose, and is hard to score automatically.  \n    – Discriminative alone only tests a small set of pre‐chosen objects and doesn’t reveal what else the model will invent.  \n    – Together they give a *complementary* evaluation:  \n       • The generative prompt surfaces all candidate hallucinations in one shot  \n       • The discriminative prompts then rigorously verify presence/absence of both real and hallucination‐prone objects  \n\nBy integrating both the open‐ended generative prompt and the targeted yes/no checks, ODE can both *discover* and *quantify* object‐existence hallucinations in a single, automated protocol."
    },
    {
        "question": "How does the Image Generation and Filtering module integrate open vocabulary object detection to enforce quality specifications?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "To prevent model exposure to test data, we employ text-to-image generation models (e.g., Stable Diffusion 1.5, as used in our experiments) to generate ODE test images from textual prompts such as “a picture of A and B,” where A and B represent specific visual concepts. Positive and negative prompts are applied to improve image quality. For each test scenario, we generate both realistic photographs and anime-style images to ensure diversity in the representation of the same concepts.",
            "Due to limitations of the generative models, not all images produced are of high quality. To assess the quality of the generated images, we leverage an open vocabulary object detection model to extract the actual visual content of each image, discarding those that lack the expected entities. For example, for an image described as “a picture of a dog and a frisbee,” if the detection model fails to identify the dog and frisbee or shows low confidence, the image is filtered out. High-quality images are retained and annotated with detected concept information as “truth” data. Additionally, hallucination data from the conceptual hallucination graph is included for comprehensive annotation."
        ],
        "final_answer": "The module runs all generated images through an open-vocabulary object detector, compares the detected objects to the intended concepts, and discards any image where the detector either misses the expected entities or reports low confidence. Only images that satisfy the detector’s quality criteria are kept and labeled with the detected concepts as ground-truth.",
        "relevant_elements": [
            "Image Generation and Filtering",
            "Quality Specifications"
        ],
        "id": 840,
        "masked_question": "How does the [mask1] module integrate open vocabulary object detection to enforce quality specifications?",
        "masked_number": 1,
        "masked_elements": [
            "Image Generation and Filtering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.09318v1_figure_2.png",
        "paperid": "2409.09318v1",
        "paper_path": "./papers/2409.09318v1.json",
        "figure_id": "2409.09318v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the Open-Set Dynamic Evaluation Protocol. The workflow involves constructing a graph and generating test samples based on the graph, with four distinct steps.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step-by-step of how the red-boxed “Image Generation and Filtering” module (i.e. [mask1]) folds in an open-vocabulary detector to guarantee that only good–quality, on-target images survive:\n\n1.  **Text-to-Image Generation**  \n    – The pipeline begins by feeding prompts like “a picture of A and B” into a generative model (e.g. Stable Diffusion).  \n    – A batch of candidate images is produced for each concept pair or entity–environment pair.\n\n2.  **Open-Vocabulary Object Detection**  \n    – Each generated image is then passed through an open-vocabulary object detector (the “Image Detection Tool” in the diagram).  \n    – Because it is open-vocab, the detector can be asked to look for any arbitrary concept label (dog, frisbee, sky, etc.).\n\n3.  **Enforcing Quality Specifications**  \n    – For each image, the detector’s outputs are compared against the *expected* concepts (the two nodes A and B used to prompt the generation).  \n    – If either of the required objects is missing or detected with too low a confidence score, that image is *discarded*.  \n    – Only images in which both target objects appear (above the confidence threshold) are *retained* for further use.\n\n4.  **Annotation of “Truth”**  \n    – The surviving images are then annotated with the detector’s actual outputs—these become the “ground-truth” presence/absence labels for A, B, and even any hallucination-prone concepts drawn from the hallucination graph.  \n    – These high-quality, validated images form the test set for the downstream discriminative and generative tasks.\n\nBy slotting the open-vocab detector immediately after generation and gating on its confidence in detecting *exactly* the objects you intended, the module enforces your “quality specifications” (i.e. that every image truly contains the intended concepts and nothing critical is missing)."
    },
    {
        "question": "How does Visual Information Construction module apply selection criteria to generate concept pairs from the truth graph?",
        "relevant_section_ids": [
            "2.2",
            "2.2.1"
        ],
        "relevant_context": [
            "After obtaining a scene graph with object concepts, we select two concept nodes at each step to form a pair, which is used as the content for the test image.",
            "The degree of association between object concepts in the graph (i.e., co-occurrence frequency) reflects the distribution of the objects. Based on this, we designed four criteria for concept combinations with increasing difficulty:",
            "Common: Combine the concept pairs with the highest co-occurrence frequency, i.e., the object combinations with the highest degree of association.",
            "Long-tail: Combine the concept pairs with associations but the lowest co-occurrence frequency in the graph.",
            "Random: Randomly combine two object concepts from the graph.",
            "Fictional: Randomly combine object concepts in the graph that have no associations."
        ],
        "final_answer": "The Visual Information Construction module first takes the truth graph of object concepts and at each step picks two nodes to form a concept pair. It applies four selection criteria—Common (highest co-occurrence edges), Long-tail (lowest nonzero co-occurrence edges), Random (any two nodes at random), and Fictional (pairs with no edge between them)—to choose which pairs to use when dynamically generating each test image.",
        "relevant_elements": [
            "Visual Information Construction",
            "Graph of truth"
        ],
        "id": 841,
        "masked_question": "How does [mask1] module apply selection criteria to generate concept pairs from the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Information Construction",
            "Graph of truth"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.09318v1_figure_2.png",
        "paperid": "2409.09318v1",
        "paper_path": "./papers/2409.09318v1.json",
        "figure_id": "2409.09318v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the Open-Set Dynamic Evaluation Protocol. The workflow involves constructing a graph and generating test samples based on the graph, with four distinct steps.",
        "qtype": "Implementation_Details",
        "response": "Here is a step-by-step walk-through of how the Visual Information Construction module (the red‐boxed block) draws its concept pairs from the scene graph (the blue-boxed block):\n\n1.  Start with the scene graph G built in II-A.  G is a weighted, undirected graph whose nodes are object concepts and whose edge weights w(A,B) encode how often A and B co-occur in real scenes.\n\n2.  At each sample, the Visual Information Construction module must pick two nodes A and B from G.  Rather than naively picking at random, it offers four “difficulty” regimes, each defined by a simple graph-based rule:\n\n    •  Common: choose the edge (A,B) with the **highest** nonzero weight in G.  \n    •  Long-tail: choose among all edges with nonzero weight the one with the **lowest** weight.  \n    •  Random: choose **any** two nodes at random (irrespective of whether they share an edge).  \n    •  Fictional: choose two nodes that are **not** connected by any edge in G (i.e. w(A,B)=0).\n\n3.  Once a pair (A,B) is sampled under one of these four criteria, this pair becomes the visual content template (“a picture of A and B” or “a picture of A in B,” etc.) that drives the downstream image synthesis.\n\nIn this way, the Visual Information Construction module systematically mines concept pairs from the underlying scene graph by applying four selection criteria—Common, Long-tail, Random, and Fictional—each of which is defined purely in terms of the presence or magnitude of the graph’s edge weights."
    },
    {
        "question": "How are concept tokens initialized by merging Vision Encoder & Projection outputs with Grounded-SAM masks and clustering?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "For each concept, given a set of images \\(\\{I_i^1,\\dots,I_i^n\\}\\), we utilize the LLaVA vision encoder \\(V\\) and the projection layer \\(P\\) to obtain aligned visual tokens \\(V_i^1,\\dots,V_i^n\\), which have the shape of \\(n\\times D\\).",
            "To reduce redundant background information in the visual tokens, we apply the Grounded-SAM (Ren et al., 2024) with the prompt “the main character in the image” to obtain a mask \\(M_i^1,\\dots,M_i^n\\) for each image. After aligning each mask’s shape with the visual tokens, we perform an element-wise Hadamard product between \\(M_i^j\\) and \\(V_i^j\\). This process yields concept-related visual tokens, denoted as \\(W_i^1,\\dots,W_i^n\\).",
            "To obtain a more compact concept representation and reduce the number of visual tokens to \\(K\\), we applied k-means clustering (Hartigan & Wong, 1979) to the concept-related visual tokens, resulting in \\(Z_i^1,\\dots,Z_i^K\\), with a size of \\(K\\times D\\), where \\(D\\) denotes the final token length corresponding to each concept.",
            "For the special token \\([\\text{sks}_i]\\), we average the clustered \\(Z_i^1,\\dots,Z_i^K\\) values to create a special token with a shape of \\(1\\times D\\). Ultimately, we derive concept tokens \\(T_i\\) with dimensions \\((K+1)\\times D\\), constructed from visual tokens related to the concepts."
        ],
        "final_answer": "Concept tokens are initialized by first passing each concept image through the LLaVA vision encoder and projection layer to obtain per-image visual embeddings. Grounded-SAM is then used to generate masks of the main character in each image, and these masks are applied element-wise to the visual embeddings to yield concept-related tokens. These tokens are clustered via k-means into K centroids (each of dimension D), and their cluster centers form the K learnable tokens. A special token is created by averaging the K centroids. In total, each concept’s token set consists of these K cluster tokens plus the averaged special token, yielding a \\((K+1)\\times D\\) concept embedding.",
        "relevant_elements": [
            "Vision Encoder & Projection",
            "Grounded-SAM"
        ],
        "id": 842,
        "masked_question": "How are concept tokens initialized by merging [mask1] outputs with Grounded-SAM masks and clustering?",
        "masked_number": 1,
        "masked_elements": [
            "Vision Encoder & Projection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11706v1_figure_2.png",
        "paperid": "2411.11706v1",
        "paper_path": "./papers/2411.11706v1.json",
        "figure_id": "2411.11706v1_figure_2.png",
        "caption": "Figure 2: The illustration of MC-LLaVA. In the\ntraining phase, given m concepts, we utilize visual tokens to initialize the m*(k+1) concept tokens. Then, we use a multi-concept joint training strategy to learn the concept tokens and classifier weights across four training tasks.",
        "qtype": "Implementation_Details",
        "response": "The key thing to see in the red‐boxed module is that nothing magic is coming from the text side – we simply take the *vision‐encoder + projection* outputs for each of our concept images, mask out the background with Grounded-SAM, and then cluster the remaining (“concept‐only”) feature vectors down to a small set of centroids which become our starting soft‐prompt tokens.  Concretely, for each concept i with images {Iij}:  \n1. Run each Iij through the VLM’s vision encoder f_v and the little projection head W to get a grid of D–dimensional visual tokens Pij ∈ R^(hw×D).  \n2. Use Grounded-SAM (“main character in the image”) to get a binary mask Mij of size hw, reshape it to match Pij, and form the element-wise product  \n   Qij = Mij ⊙ Pij.  \n   Qij now contains only the foreground (“concept”) features.  \n3. Aggregate all of the Qij across all images j of that one concept i, and run standard k-means on this pooled set to get k cluster centers C_i1, …, C_ik ∈ R^D.  These k centroids capture the diversity of appearances of concept i.  \n4. In order to have exactly k + 1 tokens per concept (so that we can still have the special “<sks>” marker), we also take the mean of the k centroids to form one additional vector C_i0 = (1/k)∑_ℓ C_iℓ.  \n5. Finally we normalize all of these k + 1 vectors to the same average norm as the rest of the text‐token embeddings in the VLM’s tokenizer.  The result is a block of k + 1 D‐dimensional vectors which we plug in as the *initial* concept‐token embeddings for concept i.  \n\nThat is exactly what the right half of Fig. 2 is showing:  \n   (Vision Encoder & Projection) → mask by Grounded-SAM → masked tokens Q → k-means → {k centroids + 1 mean} → normalize norms → concept tokens."
    },
    {
        "question": "How does combining Grounded SAM with Vision Encoder & Projection replace high-quality negative sample methods?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Yo’LLaVA (Nguyen et al., 2024) uses high-quality negative samples that are visually similar to a specific concept but represent non-identical objects for training the concept tokens. We conduct a case study by reproducing Yo’LLaVA with different percentages of high-quality negative samples. As shown in Fig. 3 (right), reducing the number of negative samples decreases the personalization capabilities among all kinds of Yo’LLaVA data, reflecting the heavy reliance of Yo’LLaVA on high-quality negative data.\nRelying on negative samples creates challenges for multi-concept personalization, as personalizing a new concept necessitates hundreds of high-quality negative samples, which are hard for users to collect. As the number of concepts increases, defining and acquiring high-quality negative samples becomes more challenging.",
            "Therefore, as shown in the right half of Fig. 2, We propose a method that utilizes concept images to initialize the concept tokens. For each concept, given a set of images, …, we utilize the LLaVA vision encoder V and the projection layer P to obtain aligned visual tokens Z^i_1, …, Z^i_m, which have the shape of b × d. To reduce redundant background information in the visual tokens, we apply the Grounded-SAM (Ren et al., 2024) with the prompt “the main character in the image” to obtain a mask M^i_1, …, M^i_m for each image. After aligning each mask’s shape with the visual tokens, we perform an element-wise Hadamard product between M^i_j and Z^i_j. This process yields concept-related visual tokens, denoted Ẑ^i_1, …, Ẑ^i_m.\nTo obtain a more compact concept representation and reduce the number of visual tokens to k, we applied k-means clustering (Hartigan & Wong, 1979) to the concept-related visual tokens, resulting in Q^i_1, …, Q^i_k, with a size of k × d, where k denotes the final token length corresponding to each concept."
        ],
        "final_answer": "Instead of collecting and relying on hundreds of hard-to-gather negative examples, MC-LLaVA uses each concept’s own images to initialize its tokens. The images are passed through the vision encoder and projection layers to produce visual features; Grounded SAM then segments out only the concept (masking away background), yielding concept-related visual tokens. These tokens are cluster-pooled into a fixed set of concept embeddings. This image-based initialization entirely replaces the need for high-quality negative samples.",
        "relevant_elements": [
            "Vision Encoder & Projection",
            "Grounded SAM"
        ],
        "id": 844,
        "masked_question": "How does combining [mask1] with Vision Encoder & Projection replace high-quality negative sample methods?",
        "masked_number": 1,
        "masked_elements": [
            "Grounded SAM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11706v1_figure_2.png",
        "paperid": "2411.11706v1",
        "paper_path": "./papers/2411.11706v1.json",
        "figure_id": "2411.11706v1_figure_2.png",
        "caption": "Figure 2: The illustration of MC-LLaVA. In the\ntraining phase, given m concepts, we utilize visual tokens to initialize the m*(k+1) concept tokens. Then, we use a multi-concept joint training strategy to learn the concept tokens and classifier weights across four training tasks.",
        "qtype": "Literature_Background",
        "response": "Here’s the core idea, broken down step by step:\n\n1.  **What is [mask1]?**  \n    The red-boxed module in the figure is the “Grounded-SAM” block.  Grounded-SAM is used to produce a rough foreground mask of “the main character in the image.”\n\n2.  **How it hooks into the Vision Encoder & Projection:**  \n    - Each concept comes with a handful of example images.  \n    - We run those images through the VLM’s vision encoder and projection head to get a grid of visual tokens (shape H×W×d).  \n    - In parallel, we feed each image into Grounded-SAM (prompted with “the main character in the image”) to get a binary mask of the object of interest.  \n    - We element-wise multiply (Hadamard product) the mask with the visual tokens, which zeroes out background features and leaves only object-related activations.  \n\n3.  **From masked features to concept tokens:**  \n    - We now have a bag of foreground visual tokens per image.  \n    - To collapse them into a fixed small set (k) of “concept prototype” vectors, we run k-means on all masked tokens from all images of that concept.  \n    - The resulting k centroids become the k “positive” concept tokens, plus one special token formed by averaging those centroids.  \n\n4.  **Why this replaces “high-quality negative sampling”:**  \n    - In Yo’LLaVA, you had to painstakingly collect dozens or hundreds of *visually similar but different* images (the so-called high-quality negatives) to push the soft‐prompt away from everything that *isn’t* your concept.  \n    - Here, because Grounded-SAM already isolates *only* the pixels belonging to the concept, you get *pure* concept features directly.  You never needed to hunt for hard negatives—backgrounds and confusers have already been zeroed out by the mask.  \n    - Clustering those masked features yields a diverse, compact positive representation that naturally pushes against the rest of the embedding space, so the model learns a discriminative concept embedding without any explicit negative examples.\n\n5.  **Bottom line:**  \n    By combining Grounded-SAM’s image‐level masks with the vision encoder & projection, we automatically extract a clean, semantically tight set of concept tokens straight from the images.  This obviates the need to gather and curate “hard negative” image sets, since all the discrimination happens via the foreground masking + clustering process."
    },
    {
        "question": "How does Normalization of k-means initialized tokens contrast with prompt tuning initialization methods?",
        "relevant_section_ids": [
            "2",
            "3.3"
        ],
        "relevant_context": [
            "While soft prompt tuning has achieved notable successes across various tasks, its effectiveness depends on the appropriate initialization of parameters (Meng et al., 2024), which leads to current personalization approaches heavily rely on the availability of high-quality negative samples (Nguyen et al., 2024).",
            "We find concept token initialization is crucial that misalignment with the tokenizer’s embedding distribution can destabilize training. We normalize the vector norms of concept tokens (denoted P^i) from k-means. To align with tokenizer embeddings, adjusted tokens are:"
        ],
        "final_answer": "When tokens are initialized via k-means clustering on visual features, MC-LLaVA explicitly normalizes their vector norms to match the distribution of the pretrained tokenizer’s embeddings, stabilizing training. In contrast, standard prompt-tuning initializations (hard or soft prompts) do not perform any such norm alignment and instead typically rely on random or negative-sample–based initialization without adjusting token norms.",
        "relevant_elements": [
            "k-means",
            "Normalization"
        ],
        "id": 845,
        "masked_question": "How does [mask1] of k-means initialized tokens contrast with prompt tuning initialization methods?",
        "masked_number": 1,
        "masked_elements": [
            "Normalization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11706v1_figure_2.png",
        "paperid": "2411.11706v1",
        "paper_path": "./papers/2411.11706v1.json",
        "figure_id": "2411.11706v1_figure_2.png",
        "caption": "Figure 2: The illustration of MC-LLaVA. In the\ntraining phase, given m concepts, we utilize visual tokens to initialize the m*(k+1) concept tokens. Then, we use a multi-concept joint training strategy to learn the concept tokens and classifier weights across four training tasks.",
        "qtype": "Literature_Background",
        "response": "Here is the step-by-step reasoning:\n\n1.  In our pipeline (Fig. 2, right), we take the raw visual tokens for each concept, mask out the background with Grounded-SAM, and then run k-means on the remaining tokens to get k cluster centers \\({c}^{1},\\dots,{c}^{k}\\).  \n2.  Those cluster centers are exactly our “visually grounded” initial embeddings for the new concept tokens.  But because their vector norms can be wildly different from the norms of the pre-trained text-token embeddings (the ones in the LLaVA tokenizer), if we plug them straight in we destabilize training.  \n3.  The red box in the figure marks the “Normalization” step: we explicitly rescale each k-means centroid so that its Euclidean norm matches the average norm of the existing tokenizer embeddings.  Concretely, if \\(\\bar c\\) is one of our cluster centers and \\(\\lVert E\\rVert\\) is the typical norm of a text token embedding, we set\n     \n     \\[\n       \\tilde c \\;=\\;\\bar c \\times \\frac{\\lVert E\\rVert}{\\lVert \\bar c\\rVert}\\,.\n     \\]\n  \n4.  By contrast, off-the-shelf prompt-tuning methods almost always either  \n     •  initialize their soft prompts randomly (e.g. Gaussian noise),  \n     •  copy a handful of existing word-piece embeddings out of the vocabulary, or  \n     •  hand-craft “hard” text prompts,  \n   none of which perform any explicit norm-matching to the rest of the model’s embedding space.  \n\n5.  In short, our k-means-based seeds are not only semantically grounded in images but also explicitly norm-aligned to the LLM’s embedding distribution, whereas standard prompt-tuning initials are neither visually grounded nor norm-matched."
    },
    {
        "question": "How do parallel questions build on direct questions to enhance multi-chart information localization?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The direct questions evaluate whether the model can accurately identify the relevant chart to answer questions accordingly.",
            "We present multiple charts and use terms like “In the second chart” to explicitly specify which chart the answer should pertain to.",
            "Unlike direct questions that ask the model to locate and focus on one specific chart, parallel questions examine the model’s ability to answer independent questions that span multiple charts simultaneously.",
            "These questions are formed by querying information from multiple charts, with each parallel question containing several sub-questions, and each sub-answer is derived from a different chart.",
            "Similar to direct questions, we explicitly specify which chart the answer should come from."
        ],
        "final_answer": "Parallel questions build on direct questions by extending the single‐chart localization task into multiple independent sub-tasks, each targeting a specified chart. They require the model to locate and extract information from several charts at once—using explicit references like “in the second chart”—thus enhancing multi‐chart information localization through simultaneous handling of multiple chart queries.",
        "relevant_elements": [
            "Parallel Questions",
            "Direct Questions"
        ],
        "id": 846,
        "masked_question": "How do [mask1] build on [mask2] to enhance multi-chart information localization?",
        "masked_number": 2,
        "masked_elements": [
            "Parallel Questions",
            "Direct Questions"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.14179v1_figure_2.png",
        "paperid": "2410.14179v1",
        "paper_path": "./papers/2410.14179v1.json",
        "figure_id": "2410.14179v1_figure_2.png",
        "caption": "Figure 2: Multi-ChartQA contains four types of QA tasks, covering four crucial abilities for understanding and reasoning across multiple charts.\nWe highlight the key information location for answering each question with boxes and circles. The arrows represent the multi-step reasoning process across different charts.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify what the two highlighted regions are:\n\n1. [mask2] (blue box) is the “Direct Questions” module.  In this setup each question explicitly points you at one chart (e.g. “in the first chart …”) and asks you to extract either structural or content details from that single chart.  \n2. [mask1] (red box) is the “Parallel Questions” module.  Here a single prompt bundles several independent sub‐questions, each tagged to a different chart (e.g. “How many bars in chart 2? … How many members in chart 3?”).\n\nChain of thought:\n- Direct Questions teach the model to “lock on” to one chart at a time, parsing explicit references like “in the second chart” and then either reading off axis/legend details (structure) or pulling out the specified data point (content).  \n- Parallel Questions simply reuse that same explicit‐chart‐indexing mechanism, but now ask multiple of those mini-queries in one go—each sub‐question still says “in chart i” so the model must localize its attention and answer chart 1’s question, then chart 2’s, and so on.  \n- By piggy-backing on the explicit localization cues from the direct‐question stage, parallel questions scale that capability up: instead of focusing once, the model must repeatedly switch its focus to each referenced chart and retrieve the right piece of information in sequence.\n\nAnswer: Parallel questions build directly on the explicit “in chart X …” localization scaffold introduced by direct questions, but replicate it across multiple sub-questions so that a model must repeatedly re-locate and extract from different charts in a single prompt—thereby strengthening its multi-chart information-localization skill."
    },
    {
        "question": "How does sequential reasoning extend comparative reasoning to enable deeper multi-hop chart integration?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Comparison questions assess the model’s ability to analyze and compare information across multiple charts, requiring reasoning between them.",
            "Sequential reasoning involves complex multi-step reasoning questions with a temporal or logical sequence.",
            "Specifically, these questions use a single entity as a clue and, through multi-hop reasoning, traverse several charts to arrive at the final answer."
        ],
        "final_answer": "Sequential reasoning builds on comparative reasoning’s cross-chart comparison by adding a multi-step, temporal or logical traversal: it tracks a single entity’s attributes across several charts, using multi-hop reasoning to integrate information step by step for a deeper, chained analysis.",
        "relevant_elements": [
            "Sequential Reasoning",
            "Comparative Reasoning"
        ],
        "id": 847,
        "masked_question": "How does [mask1] extend comparative reasoning to enable deeper multi-hop chart integration?",
        "masked_number": 1,
        "masked_elements": [
            "Sequential Reasoning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.14179v1_figure_2.png",
        "paperid": "2410.14179v1",
        "paper_path": "./papers/2410.14179v1.json",
        "figure_id": "2410.14179v1_figure_2.png",
        "caption": "Figure 2: Multi-ChartQA contains four types of QA tasks, covering four crucial abilities for understanding and reasoning across multiple charts.\nWe highlight the key information location for answering each question with boxes and circles. The arrows represent the multi-step reasoning process across different charts.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought:\n\n1. In the figure, the red box highlights the “Sequential Reasoning” example.  \n2. In comparative reasoning questions, you simply line up or overlap elements (e.g. age groups) from two charts and count or compare them in one step.  \n3. Sequential reasoning builds on that by turning the comparison into a multi-step inference:  \n   a. First you locate a key value or category in one chart (e.g. which age group has the largest share in the U.S. House chart).  \n   b. You then take that result (the identified age group) and “hop” to a second chart to answer a new question about exactly that same entity (e.g. what percentage in that age group favor a president in their 50s).  \n4. By chaining these steps—extracting an answer from one chart and feeding it as the query into another—sequential reasoning forces the model to integrate information across multiple, semantically linked charts rather than handling each in isolation.\n\nAnswer: The red-boxed component is the “sequential reasoning” task, which extends comparative reasoning by turning a one‐step comparison into a multi‐hop chain of inferences across charts."
    },
    {
        "question": "How does sequential reasoning enforce multi-step chart traversal in the benchmark's methodology?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Sequential reasoning involves complex multi-step reasoning questions with a temporal or logical sequence.",
            "To solve such problems, the model needs to track and analyze different aspects of an entity from the information dispersed in different charts.",
            "Specifically, these questions use a single entity as a clue and, through multi-hop reasoning, traverse several charts to arrive at the final answer."
        ],
        "final_answer": "Sequential reasoning enforces multi-step chart traversal by using a single entity as a clue and requiring multi-hop reasoning: the model must track and analyze that entity’s attributes across several charts in sequence to arrive at the final answer.",
        "relevant_elements": [
            "Sequential Reasoning"
        ],
        "id": 848,
        "masked_question": "How does [mask1] enforce multi-step chart traversal in the benchmark's methodology?",
        "masked_number": 1,
        "masked_elements": [
            "Sequential Reasoning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.14179v1_figure_2.png",
        "paperid": "2410.14179v1",
        "paper_path": "./papers/2410.14179v1.json",
        "figure_id": "2410.14179v1_figure_2.png",
        "caption": "Figure 2: Multi-ChartQA contains four types of QA tasks, covering four crucial abilities for understanding and reasoning across multiple charts.\nWe highlight the key information location for answering each question with boxes and circles. The arrows represent the multi-step reasoning process across different charts.",
        "qtype": "Experimental_Results",
        "response": "Chain of thought:  \n1. The red‐boxed region in the figure is the description and example under “Sequential Reasoning.”  \n2. Reading that panel, we see it explicitly says: “Firstly identify the age group with the largest proportion in the U.S. House… Then, what percentage of people in this age group think…”  \n3. It goes on to note that sequential questions “use a single entity as a clue and, through multi‐hop reasoning, traverse several charts to arrive at the final answer.”  \n4. In other words, the benchmark “enforces multi‐step chart traversal” by 1) asking a question about Chart A, 2) taking the result (here, an age group) as the input for a new query in Chart B, and thus 3) mandating that the model jump from one chart to the next in a chained sequence.  \nAnswer: It’s the “Sequential Reasoning” module—questions are deliberately split into ordered sub-questions that force the model to answer one chart, extract an entity, then use that entity to look up and answer on another chart, thereby enforcing multi-step, cross-chart traversal."
    },
    {
        "question": "How does parallel questions design evaluate simultaneous extraction across multiple charts methodologically?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Unlike direct questions that ask the model to locate and focus on one specific chart, parallel questions examine the model's ability to answer independent questions that span multiple charts simultaneously.",
            "These questions are formed by querying information from multiple charts, with each parallel question containing several sub-questions, and each sub-answer is derived from a different chart.",
            "Similar to direct questions, we explicitly specify which chart the answer should come from.",
            "Like direct questions, parallel questions also include structure questions, content questions, as well as mixed questions that involve both."
        ],
        "final_answer": "Parallel questions are designed by grouping several independent sub-questions into one overall question, where each sub-question refers to a different chart. The model must extract each sub-answer from its designated chart—explicitly specified in the prompt—thereby testing the model’s ability to simultaneously gather and report information across multiple, semantically related charts. These parallel questions cover structural, content, and mixed information types.",
        "relevant_elements": [
            "Parallel Questions"
        ],
        "id": 849,
        "masked_question": "How does [mask1] design evaluate simultaneous extraction across multiple charts methodologically?",
        "masked_number": 1,
        "masked_elements": [
            "Parallel Questions"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.14179v1_figure_2.png",
        "paperid": "2410.14179v1",
        "paper_path": "./papers/2410.14179v1.json",
        "figure_id": "2410.14179v1_figure_2.png",
        "caption": "Figure 2: Multi-ChartQA contains four types of QA tasks, covering four crucial abilities for understanding and reasoning across multiple charts.\nWe highlight the key information location for answering each question with boxes and circles. The arrows represent the multi-step reasoning process across different charts.",
        "qtype": "Experimental_Results",
        "response": "Let me walk through how we arrive at the answer:\n\n1. Identify mask1.  \n   In the figure, the red‐outlined panel at top right is labeled “Parallel Questions.” That is what [mask1] stands for.\n\n2. What are Parallel Questions?  \n   From the text (under “Question Design”) we know that Parallel Questions are  \n   • made up of multiple independent sub-questions,  \n   • each sub-question explicitly points to a different chart (“in the second chart… in the third chart…”),  \n   • and require the model to extract answers from all referenced charts in one go.\n\n3. How is simultaneous extraction evaluated?  \n   Methodologically, the benchmark:  \n   • gives explicit instructions so the model outputs each sub-answer in a predictable format,  \n   • parses the model’s output with a script to pull out each sub-answer,  \n   • string-matches each sub-answer against the ground truth, and  \n   • computes accuracy as the fraction of sub-questions answered correctly (rather than a single all‐or‐nothing score).\n\nPutting that together:\n\nAnswer:  \nParallel Questions are designed by bundling several chart‐specific queries into one prompt (each subquestion tagged with “in chart X”), forcing the model to read and extract facts from multiple charts simultaneously.  During evaluation, the system parses out each subanswer, checks it against its ground-truth value by string match, and reports accuracy as the proportion of correctly answered subquestions."
    },
    {
        "question": "How does MSD’s learnable mask selection reduce noise accumulation during serialization?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To address this issue, we propose Masked Sequence Denoising (MSD) to selectively mask the noised point tokens of the point cloud sequences and use the purified features for classification, mitigating the adverse effects of noise accumulation.",
            "The higher the probability of p_i^1 becomes, the closer the probability value of the current position is to a value of 1, otherwise it is closer to a value of 0. This indicates that while features are preserved, noise is greatly suppressed.",
            "As shown in Figure 2(a), the input sequence will be multiplied by a learnable mask to obtain the masked sequence. Then, the denoised and purified sequence features will be forwarded for classification."
        ],
        "final_answer": "MSD learns a binary mask via Gumbel-Softmax that, when multiplied element-wise with the serialized point tokens, preserves clean features (mask = 1) and suppresses noisy ones (mask = 0). By filtering out the corrupted tokens before further processing, it prevents noise from accumulating during the serialization stage.",
        "relevant_elements": [
            "MSD"
        ],
        "id": 850,
        "masked_question": "How does [mask1]’s learnable mask selection reduce noise accumulation during serialization?",
        "masked_number": 1,
        "masked_elements": [
            "MSD"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.13574v1_figure_2.png",
        "paperid": "2408.13574v1",
        "paper_path": "./papers/2408.13574v1.json",
        "figure_id": "2408.13574v1_figure_2.png",
        "caption": "Figure 2:  The framework of PointDGMamba. It consists of three key components: (a) Masked Sequence Denoising (MSD) is presented to mask out noised point patches in the sequence and thus mitigate adverse effects of noise accumulation during the serialization stage; (b) Sequence-wise Cross-domain Feature Aggregation (SCFA) is introduced to aggregate cross-domain but same-class point cloud features with the global prompt to extract more generalized features, thereby strengthing Mamba s effectiveness in handling distribution shifts. (c) Dual-level Domain Scanning, including intra-domain scanning and cross-domain scanning, is proposed to facilitate sufficient information interaction between different parts of the features.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module is nothing more exotic than an end–to–end differentiable “noise‐gate” that sits in front of the 1D Mamba encoder and literally zeroes out any patch it thinks is corrupted before that patch ever pollutes the hidden state.  Concretely, MSD attaches a tiny Gumbel–Softmax head to every token position in the serialized point‐cloud sequence.  During training it learns to push “good” tokens to a mask value ≈1 and “bad/noisy” tokens to ≈0.  At inference time you simply multiply your raw sequence by that (now nearly binary) mask, so any token judged to be noise is zeroed out before entering the next Mamba stage.  By pruning those noisy tokens right at the “front door,” you never let them accumulate across time steps or layers, and the model remains robust to the serialization‐induced topology noise."
    },
    {
        "question": "How does combining SCFA with Dual-level Domain Scanning enhance cross-domain feature interaction?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "3.2: We propose a Sequence-wise Cross-domain Feature Aggregation (SCFA) to aggregate cross-domain but same-class point cloud features to prompt the model to extract more generalized features.",
            "3.3: In order to facilitate the interaction of different feature information for generalization, we design Dual-level Domain Scanning, including Intra-domain Scanning (IDS) and Cross-domain Scanning (CDS).",
            "3.3: IDS treats features as three unrelated sequences, scanning them one after another in order. CDS treats features as three related sequences. After scanning a data point of the first feature, the data points at the same position for the other two features will be scanned sequentially. This can promote the interaction between each feature.",
            "3.3: It is worth noting that the DDS module processes aggregated features, so it needs to be used together with the SCFA module."
        ],
        "final_answer": "SCFA first fuses same-class point-cloud features from different source domains (along with a global prompt) into a single, cross-domain representation. By feeding these aggregated features into Dual-level Domain Scanning—where Intra-domain Scanning captures sequential patterns within each feature set and Cross-domain Scanning interleaves corresponding points across domains—the network enforces both within-domain and cross-domain mixing at the sequence level. In this way, the combination of SCFA and DDS maximizes information exchange among domains and strengthens the model’s ability to learn domain-agnostic, generalized features.",
        "relevant_elements": [
            "SCFA",
            "Dual-level Domain Scanning"
        ],
        "id": 851,
        "masked_question": "How does combining SCFA with [mask1] enhance cross-domain feature interaction?",
        "masked_number": 1,
        "masked_elements": [
            "Dual-level Domain Scanning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.13574v1_figure_2.png",
        "paperid": "2408.13574v1",
        "paper_path": "./papers/2408.13574v1.json",
        "figure_id": "2408.13574v1_figure_2.png",
        "caption": "Figure 2:  The framework of PointDGMamba. It consists of three key components: (a) Masked Sequence Denoising (MSD) is presented to mask out noised point patches in the sequence and thus mitigate adverse effects of noise accumulation during the serialization stage; (b) Sequence-wise Cross-domain Feature Aggregation (SCFA) is introduced to aggregate cross-domain but same-class point cloud features with the global prompt to extract more generalized features, thereby strengthing Mamba s effectiveness in handling distribution shifts. (c) Dual-level Domain Scanning, including intra-domain scanning and cross-domain scanning, is proposed to facilitate sufficient information interaction between different parts of the features.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red-boxed “[mask1]” in the figure is the Dual-level Domain Scanning (DDS) block.  Here’s how SCFA and DDS work together to deepen cross-domain feature interaction:\n\n1. SCFA (Sequence-wise Cross-domain Feature Aggregation)  \n   - Takes denoised features f₁ (from domain 1) and f₂ (from domain 2), multiplies each by an MLP, and element-wise multiplies them to produce a coarse cross-domain feature.  \n   - Injects a set of learnable global prompt vectors to steer the aggregation away from domain-specific artifacts.  \n   - Outputs a single “aggregated” feature map that already carries information from both domains.\n\n2. DDS (Dual-level Domain Scanning)  \n   - Operates on the SCFA-produced, cross-domain aggregated feature.  \n   - Performs two complementary scans:  \n     a. Intra-domain Scanning (IDS): scans each domain’s feature sequence end-to-end, preserving the original within-domain order.  \n     b. Cross-domain Scanning (CDS): interleaves tokens from both domains (after one token from domain 1 it takes the corresponding token from domain 2, and so on), forcing fine-grained, token-level mixing.\n\n3. Synergy  \n   - SCFA guarantees that the model “sees” a blended, same-class signal from all source domains plus a global prompt.  \n   - DDS then “stirs” that blend at two levels—coarse (per-domain chunks) and fine (token by token)—so that features from different domains can exchange information both globally and locally.  \n\nBy stacking SCFA’s global cross-domain fusion with DDS’s dual scanning routines, PointDGMamba ensures maximal interaction among domain-variant features and thus learns more robust, domain-agnostic representations."
    },
    {
        "question": "Could selective masking in MSD unintentionally drop essential structural tokens, reducing model robustness across domains?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Masked Sequence Denoising",
            "MSD"
        ],
        "id": 852,
        "masked_question": "Could selective masking in [mask1] unintentionally drop essential structural tokens, reducing model robustness across domains?",
        "masked_number": 1,
        "masked_elements": [
            "MSD"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.13574v1_figure_2.png",
        "paperid": "2408.13574v1",
        "paper_path": "./papers/2408.13574v1.json",
        "figure_id": "2408.13574v1_figure_2.png",
        "caption": "Figure 2:  The framework of PointDGMamba. It consists of three key components: (a) Masked Sequence Denoising (MSD) is presented to mask out noised point patches in the sequence and thus mitigate adverse effects of noise accumulation during the serialization stage; (b) Sequence-wise Cross-domain Feature Aggregation (SCFA) is introduced to aggregate cross-domain but same-class point cloud features with the global prompt to extract more generalized features, thereby strengthing Mamba s effectiveness in handling distribution shifts. (c) Dual-level Domain Scanning, including intra-domain scanning and cross-domain scanning, is proposed to facilitate sufficient information interaction between different parts of the features.",
        "qtype": "Others",
        "response": "Let’s unpack what “[mask1]” is doing and whether it could in principle drop “too much” structure.\n\n1.  Identify [mask1].  In the red‐boxed area of Fig. 2(a) the paper introduces “Masked Sequence Denoising” (MSD).  Concretely, after the first Mamba stage you have a 1D point‐cloud token sequence X.  They multiply X element‐wise by a learned binary mask m (via Gumbel‐Softmax) to zero out (mask) those tokens deemed to be “noise.”\n\n2.  MSD’s goal.  The authors argue that serialization into a sequence inevitably shuffles spatial neighbors and injects “spurious” tokens.  By masking out these noisy patches, the remainder should better reflect the true 3D structure and generalize across new domains.\n\n3.  Where could it go wrong?  \n   - If the learned mask m mistakes an informative structural token for noise, it will permanently zero that token.  In effect you are erasing part of the object’s geometry.  \n   - If this happens consistently in training, the model never learns to rely on those tokens.  At test time, those points are gone, so the representation can become brittle when faced with new object shapes or occlusion patterns.\n\n4.  Why it may hurt cross‐domain robustness.  \n   - Domain shifts often manifest as slight changes in global shape or local detail distribution.  Masking out all low‐energy or “unimportant” tokens might remove subtle but discriminative cues that are crucial for separating a “chair” from a “sofa” in an unseen sensor setup.  \n   - Over‐aggressive masking can collapse within‐class variance and make the feature extractor blind to real structural variation, reducing the model’s ability to adapt to new domains.\n\n5.  Mitigations in the paper.  \n   - They use a Gumbel‐Softmax relaxation so that m is soft during training and only binarized at inference.  This lets the network learn which tokens truly matter via gradient flow.  \n   - They do not fix an arbitrary mask ratio; instead the mask probability for each token is learned.  \n   - Empirically (see Table 6 in the Appendix) they show their MSD outperforms random masking, similarity‐based masking, or IIM [57], suggesting the network tends to keep the “right” tokens.\n\n6.  Conclusion.  In principle any hard masking can remove essential structure, so yes, there is a risk that MSD could drop critical tokens and hurt robustness.  In practice the Gumbel‐Softmax‐learned mask and the downstream cross‐domain aggregation (SCFA) help guard against overly aggressive pruning, but one should still monitor mask sparsity and perhaps enforce a minimum keep‐rate or add a structure‐preservation loss if robustness loss emerges."
    },
    {
        "question": "What adaptive methods could replace Intra-domain Scanning to mitigate bias in Dual-level Domain Scanning?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Dual-level Domain Scanning",
            "Intra-domain Scanning"
        ],
        "id": 853,
        "masked_question": "What adaptive methods could replace [mask1] to mitigate bias in Dual-level Domain Scanning?",
        "masked_number": 1,
        "masked_elements": [
            "Intra-domain Scanning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.13574v1_figure_2.png",
        "paperid": "2408.13574v1",
        "paper_path": "./papers/2408.13574v1.json",
        "figure_id": "2408.13574v1_figure_2.png",
        "caption": "Figure 2:  The framework of PointDGMamba. It consists of three key components: (a) Masked Sequence Denoising (MSD) is presented to mask out noised point patches in the sequence and thus mitigate adverse effects of noise accumulation during the serialization stage; (b) Sequence-wise Cross-domain Feature Aggregation (SCFA) is introduced to aggregate cross-domain but same-class point cloud features with the global prompt to extract more generalized features, thereby strengthing Mamba s effectiveness in handling distribution shifts. (c) Dual-level Domain Scanning, including intra-domain scanning and cross-domain scanning, is proposed to facilitate sufficient information interaction between different parts of the features.",
        "qtype": "Others",
        "response": "First, let’s be crystal clear about what the red‐boxed module is.  If you look at Fig. 2(c), the module in the red outline is simply the “Intra-domain Scanning” block of the Dual-level Domain Scanning (DDS) stage.  Concretely, in the paper they describe two fixed scans:\n\n  1.  Intra-domain scanning (IDS), which literally walks through each feature stream end-to-end before moving on to the next.  \n  2.  Cross-domain scanning (CDS), which interleaves features from different domains at each token position.\n\nBoth of those are completely rigid, human-designed traversals.  What the question is asking is:\n\n  “What adaptive methods could we plug in instead of that rigid Intra-domain Scanning, to reduce the bias that any one fixed scan order introduces?”\n\nHere is a short list of possible adaptive or learned replacements that have appeared in related literatures:\n\n  •  **Learnable permutation via Gumbel-Sinkhorn**  \n     –  Replace the hand-coded IDS order with a small “permutation network” that produces a soft permutation matrix over the tokens (e.g. using a Gumbel-Sinkhorn relaxation).  During training you back-propagate through that Sinkhorn layer so the model actually *learns* the optimal scan order for each input.  \n\n  •  **Attention-guided dynamic routing**  \n     –  Instead of scanning in a fixed sequence, run a lightweight self-attention or cross-attention module over the feature tokens, have the attention scores “decide” which token to visit next, and feed that dynamically selected token into the SSM.  This turns a rigid walk into a data-dependent, adaptive traversal.  \n\n  •  **Graph-based traversal (learnable BFS/DFS)**  \n     –  Build a small graph (e.g. k-NN) over the tokens and then learn a parametrized “edge weighting” or “node priority” function.  A soft BFS or DFS on that learned graph gives you an ordering that reflects both geometry and domain invariance, rather than a purely lexicographic scan.  \n\n  •  **Reinforcement-learning (RL) scan policy**  \n     –  Cast the scan as a sequential decision process: an agent picks which token to read next with the goal of maximizing domain-generalization accuracy on held-out domains.  The learned policy replaces the fixed IDS.  \n\n  •  **Meta-learning the scan order**  \n     –  Use episodic/meta-learning so that on each DG episode the model not only updates weights but also adapts a small “scan-order embedding.”  Over many episodes it learns a compact function that, given token features, outputs the best scanning permutation for domain-invariance.  \n\nAll of these share the same high-level idea: instead of *hard-coding* “scan tokens 1→n then tokens n+1→2n…,” you introduce a small differentiable module (Gumbel-Sinkhorn, attention, graph neural net or RL policy) that *dynamically* orders the tokens in a way that the back-propagated loss can shape.  By doing so you remove the inductive bias of a single fixed walk and let the model discover, on the fly, the scanning pattern that best promotes domain-agnostic representations."
    },
    {
        "question": "What limitations might freezing the MLP during inference impose on editing flexibility?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MLP"
        ],
        "id": 854,
        "masked_question": "What limitations might freezing the [mask1] during inference impose on editing flexibility?",
        "masked_number": 1,
        "masked_elements": [
            "MLP"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.07463v2_figure_2.png",
        "paperid": "2410.07463v2",
        "paper_path": "./papers/2410.07463v2.json",
        "figure_id": "2410.07463v2_figure_2.png",
        "caption": "Figure 2: Our framework for language-guided audio-visual editing. During training, we extract unimodal information from the audio-visual sample using pretrained encoders. Then, we fuse audio and visual features with an MLP and feed the output along with the text prompt into the text encoder. The text encoder generates textual conditions to guide the audio-visual diffusion model. We update the parameters of the MLP and diffusion models. During inference, we freeze all parameters of our model. We replace the training prompt with an editing prompt, e.g., we append “beside a crackling fireplace” to the training prompt “a telephone is raining.” We inject the cross-model semantic enhancement module into the vision branch to improve semantic consistency. The generated audio and image accurately reflect the editing requirements.",
        "qtype": "Others",
        "response": "First, let’s be clear about what is being frozen.  In Fig. 2 (top) the little red-boxed module is the two-layer MLP that takes your one-shot audio-visual feature and projects it into the embedding space of the text encoder.  During inference (Fig. 2, bottom), all of the learned weights—including that MLP—are held fixed.\n\nStep-by-step reasoning about what that implies:\n\n1.  At training time the MLP has “learned” how to turn your single exemplar’s combined audio + visual signature into a special token embedding that the diffusion U-Net will pay attention to.\n\n2.  At inference time, because we freeze the MLP, that learned projection can no longer be updated or re-weighted in light of new prompts or new examples.\n\n3.  In practice, this means that no matter what editing instruction you append to your prompt, the contribution of the original audio-visual feature is always injected in exactly the same way.  You cannot “retune” how strongly your one-shot exemplar is held in memory versus how much the new text is allowed to rewrite it.\n\n4.  Consequently, edits that lie far outside the subspace that the frozen MLP learned during adaptation (very large or unusual changes in appearance or acoustics) tend to be under-realized or ignored—the model simply cannot remap the exemplar features on the fly.\n\n5.  In short, by freezing that MLP you gain inference speed and stability (no risk of over-updating your one-shot memory), but you lose the ability to dynamically rebalance or reshape the multimodal embedding to accommodate very novel or extreme editing requests."
    },
    {
        "question": "What alternative mechanisms could replace Semantic Enhancement for improving vision-language alignment?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Semantic Enhancement"
        ],
        "id": 855,
        "masked_question": "What alternative mechanisms could replace [mask1] for improving vision-language alignment?",
        "masked_number": 1,
        "masked_elements": [
            "Semantic Enhancement"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.07463v2_figure_2.png",
        "paperid": "2410.07463v2",
        "paper_path": "./papers/2410.07463v2.json",
        "figure_id": "2410.07463v2_figure_2.png",
        "caption": "Figure 2: Our framework for language-guided audio-visual editing. During training, we extract unimodal information from the audio-visual sample using pretrained encoders. Then, we fuse audio and visual features with an MLP and feed the output along with the text prompt into the text encoder. The text encoder generates textual conditions to guide the audio-visual diffusion model. We update the parameters of the MLP and diffusion models. During inference, we freeze all parameters of our model. We replace the training prompt with an editing prompt, e.g., we append “beside a crackling fireplace” to the training prompt “a telephone is raining.” We inject the cross-model semantic enhancement module into the vision branch to improve semantic consistency. The generated audio and image accurately reflect the editing requirements.",
        "qtype": "Others",
        "response": "First, let me restate what “[mask1]” is. In the right‐hand “Inference” half of Figure 2, the red box around the vision branch is precisely the “Semantic Enhancement” (i.e. the cross-modal semantic enhancement module, Sec. 3.4) that re-scales the vision-to-text cross-attention map in order to force the image generator to pay more attention to the new editing tokens.\n\nSo the question really asks: “What other, entirely different mechanisms could you drop in there instead of that particular ‘cross-modal semantic enhancement’ hack to get better vision–language alignment?”\n\nHere are a few of the more obvious substitutes:\n\n  1. Prompt-tuning or prefix-tuning on the vision model’s cross-attn keys/values  \n     • Rather than re-scaling attention weights at inference time, learn a small “soft prefix” (a handful of continuous tokens) in front of the textual prompt.  These learned tokens carry the new editing semantics and are prepended to the normal text prompt; the U-Net cross-attention naturally picks them up.\n\n  2. Attention adapters / LoRA in the U-Net cross-attention  \n     • Insert lightweight adapter layers (e.g. LoRA [Hu et al., 2021]) directly into the image U-Net’s cross-attention block.  During the one-shot fine-tuning you update only those adapter parameters so that future cross-attn heads better condition on your editing tokens.\n\n  3. A small contrastive alignment loss with frozen CLIP  \n     • At each denoising step, compute the image feature via CLIP-I and the text feature via CLIP-T, and add a shallow projection + contrastive loss to pull the edited prompt’s feature closer to the generated image feature (and push away irrelevant ones).  This continually “steers” the generator back toward the prompt semantics without any hand-crafted re-weighting of attention.\n\n  4. Reinforcement learning from CLIP rewards  \n     • You could treat the diffusion steps as a pseudo-reinforcement problem and use the CLIP image–text similarity as a reward signal (or the change in reward).  An on-policy update to a small policy head that modulates the cross-attention weights would gradually learn to boost under-attended tokens.\n\n  5. Learned gating masks on the cross-attention map  \n     • Instead of global re-scaling by hand (the  and  in Sec. 3.4), learn a per-token gating function g(t)∈[0,1] that multiplies each token’s attention map.  At inference you simply apply g(t) to K·Qᵀ before softmax—gates for “edit tokens” come up, gates for <sot> go down, all learned during fine-tuning.\n\nAll of these methods are conceptually interchangeable with the toy “semantic enhancement” trick in the paper—they are just different ways to get the cross-attention heads to respect the new editing tokens more strongly, and to suppress over-dominant tokens like <sot>."
    },
    {
        "question": "What motivates fusing CLIP-I and CLAP-A features through the MLP prior to text encoding?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "We extract a compact yet representative feature from the given audio-visual sample, capturing its unique and multimodal characteristics. This feature serves as a guide for fine-tuning the diffusion model.",
            "Specifically, given the same audio-visual pair, we utilize the pretrained CLIP image encoder to extract a compact visual feature f_v from I and use the pretrained CLAP audio encoder to convert A to a latent audio feature f_a, where d is the dimension of feature vectors. We concatenate f_v and f_a as an audio-visual feature f to represent the multimodal characteristics of the sounding event. Since the diffusion model is controlled by the language condition, we convert the audio-visual feature f to text-compatible representations using Multi-Layer Perceptrons (MLPs)."
        ],
        "final_answer": "They fuse the CLIP-I and CLAP-A features into a single multimodal representation and then project it through MLPs into the text embedding space so as to (1) capture the unique audio–visual characteristics of the input sample and (2) turn that multimodal signature into a text‐compatible condition for guiding and fine‐tuning the diffusion model.",
        "relevant_elements": [
            "CLIP-I",
            "CLAP-A",
            "MLP"
        ],
        "id": 856,
        "masked_question": "What motivates fusing [mask1] and [mask2] features through the MLP prior to text encoding?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP-I",
            "CLAP-A"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.07463v2_figure_2.png",
        "paperid": "2410.07463v2",
        "paper_path": "./papers/2410.07463v2.json",
        "figure_id": "2410.07463v2_figure_2.png",
        "caption": "Figure 2: Our framework for language-guided audio-visual editing. During training, we extract unimodal information from the audio-visual sample using pretrained encoders. Then, we fuse audio and visual features with an MLP and feed the output along with the text prompt into the text encoder. The text encoder generates textual conditions to guide the audio-visual diffusion model. We update the parameters of the MLP and diffusion models. During inference, we freeze all parameters of our model. We replace the training prompt with an editing prompt, e.g., we append “beside a crackling fireplace” to the training prompt “a telephone is raining.” We inject the cross-model semantic enhancement module into the vision branch to improve semantic consistency. The generated audio and image accurately reflect the editing requirements.",
        "qtype": "Design_Rationale",
        "response": "We can break the answer down as follows:\n\n1.  In our one-shot adaptation we want the diffusion model to “memorize” the *specific* audio-visual example we’ve been shown, not just a generic “church bell” or “telephone ring.”  \n2.  To do that, we extract two compact features from the reference pair:  \n    •  [mask1] = the visual feature from the pretrained CLIP image encoder,  \n    •  [mask2] = the audio feature from the pretrained CLAP audio encoder.  \n3.  We concatenate those two vectors into a single multi-modal descriptor of the sounding object.  \n4.  Because our text-conditioned diffusion U-Net expects *textual* embeddings, we pass that concatenated feature through two small MLPs that project it into the exact same embedding space that the CLIP‐ and CLAP‐based text encoders live in.  \n5.  We then use that MLP-output in place of a placeholder token in the text prompt (the “<c>” token), so that when the text encoder runs it sees a token embedding that actually *carries* the real audio+visual signature of our example.  \n6.  This tricks the diffusion model into learning not only the user’s editing instructions, but also the idiosyncratic appearance *and* sound of our one and only reference.  \n\nIn short: fusing the CLIP-I and CLAP-A features through the MLP gives us a single, text-compatible embedding that carries both *image* and *audio* information of the reference.  Plugging that embedding into the text encoder is what lets the diffusion model faithfully reproduce the same object in both modalities under new language prompts."
    },
    {
        "question": "Why inject Semantic Enhancement into the Text2Image branch during inference instead of Text2Audio?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "However, we observe that the vision branch tends to ignore some editing requirements specified by the text prompt. ... This phenomenon, termed \"catastrophic neglect\" [2], leads to inconsistent audio-visual editing outcomes, consequently reducing overall user satisfaction.",
            "To address this limitation, we propose a cross-modal semantic enhancement approach. ... By enhancing the semantic correlation between vision and language, we attain consistent audio-visual content editing."
        ],
        "final_answer": "Semantic Enhancement is applied to the Text2Image branch because only the vision branch exhibits \"catastrophic neglect\"—i.e. it tends to ignore editing instructions—whereas the audio branch already responds faithfully to the text prompts. Therefore, the enhancement targets the visual cross-attention maps to fix this vision-specific issue.",
        "relevant_elements": [
            "Semantic Enhancement",
            "Text2Image"
        ],
        "id": 857,
        "masked_question": "Why inject [mask1] into the [mask2] branch during inference instead of Text2Audio?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic Enhancement",
            "Text2Image"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.07463v2_figure_2.png",
        "paperid": "2410.07463v2",
        "paper_path": "./papers/2410.07463v2.json",
        "figure_id": "2410.07463v2_figure_2.png",
        "caption": "Figure 2: Our framework for language-guided audio-visual editing. During training, we extract unimodal information from the audio-visual sample using pretrained encoders. Then, we fuse audio and visual features with an MLP and feed the output along with the text prompt into the text encoder. The text encoder generates textual conditions to guide the audio-visual diffusion model. We update the parameters of the MLP and diffusion models. During inference, we freeze all parameters of our model. We replace the training prompt with an editing prompt, e.g., we append “beside a crackling fireplace” to the training prompt “a telephone is raining.” We inject the cross-model semantic enhancement module into the vision branch to improve semantic consistency. The generated audio and image accurately reflect the editing requirements.",
        "qtype": "Design_Rationale",
        "response": "Let me first restate in plain English what the red and blue boxes are:\n\n  – Red box ([mask1]) is the “Semantic Enhancement” module.  \n  – Blue box ([mask2]) is the Text2Image branch in the inference pipeline.  \n\nThe question is: Why do we hook the Semantic Enhancement into Text2Image at inference time, instead of into Text2Audio?\n\nStep-by-step reasoning:\n\n1.  During inference we have already fine-tuned (via one-shot adaptation) both the Text2Image and Text2Audio U-Nets, and we freeze all their weights.  We swap in a new editing prompt (e.g. “a telephone is ringing beside a crackling fireplace”).\n\n2.  In practice we observed that the audio generator (Text2Audio) faithfully picks up every editing term (thanks to CLAP’s strong audio–text alignment) and consistently synthesizes the new sound (“crackling fireplace” indeed sounds like fire).\n\n3.  By contrast, the vision generator (Text2Image), which relies on CLIP cross-attention, will often “gloss over” some of the newly added tokens.  In other words the image simply shows the telephone but no fireplace.  This phenomenon is called catastrophic neglect—the start-of-sentence token dominates the attention, and many “editing” tokens get near-zero weight.\n\n4.  The Semantic Enhancement module was designed precisely to correct that problem in the vision branch: it re-scales the cross-attention map in Text2Image, down-weighting the <sot> token and up-weighting the truly new editing tokens, so that every meaningful word (e.g. “crackling,” “fireplace”) actually steers the image denoising.\n\n5.  Because the audio branch does not suffer from this “some tokens get ignored” problem (it already attends properly via CLAP), there is no need to inject the Semantic Enhancement into Text2Audio.  The only branch that needs this extra cross-attention re-weighting is Text2Image.\n\nTherefore, at inference time we slot the Semantic Enhancement module into the Text2Image path (the blue box) and leave Text2Audio untouched."
    },
    {
        "question": "What motivates integrating transformer encoder embeddings into U-Net layers for conditional SOH curve generation?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Similar to the current state-of-the-art architectures for image and audio diffusion models [9, 45], DiffBatt is based on a U-Net architecture (see Fig. 1a) and employs diffusion processes to generate SOH curves similar to a time series generation task. Conditioning for battery information, e.g., the capacity matrix, or a diffusion timestep, is provided by adding embeddings into intermediate layers of the network [21].",
            "For this study, we employ the concept of the capacity matrix (C), as introduced by Attia et al. [1], as an additional condition for the diffusion process. The capacity matrix serves as a compact representation of battery electrochemical cycling data, incorporating a series of feature representations.",
            "To encode C into an embedding (E_C), we utilize a transformer encoder (see Fig. 1b). This allows DiffBatt to generate SOH curves, from which the RUL can be derived by calculating the number of cycles until the SOH drops below a specified threshold, such as 80% of the nominal capacity."
        ],
        "final_answer": "Integrating transformer-encoder embeddings into the U-Net layers enables DiffBatt to condition its reverse-diffusion SOH curve generation on compact, early-life battery information (the capacity matrix). By encoding the capacity matrix via a transformer, and injecting that embedding into intermediate U-Net layers, the model can leverage cycle-level performance data to guide the generation of realistic, battery-specific SOH degradation trajectories.",
        "relevant_elements": [
            "transformer encoder embeddings",
            "U-Net layers"
        ],
        "id": 858,
        "masked_question": "What motivates integrating transformer encoder embeddings into [mask1] for conditional SOH curve generation?",
        "masked_number": 1,
        "masked_elements": [
            "U-Net layers"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23893v3_figure_1.png",
        "paperid": "2410.23893v3",
        "paper_path": "./papers/2410.23893v3.json",
        "figure_id": "2410.23893v3_figure_1.png",
        "caption": "Figure 1: Schematic view of the model architecture. Adapted and modified from the work by Fürrutter et al. [14], with permission from the authors. Modifications include context-specific changes.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through why we stick a transformer‐encoder embedding right into the U-Net (the red box in the figure) when generating SOH curves:\n\n1.  The job of DiffBatt is to *conditionally* generate a full degradation trajectory (SOH vs. cycle) given only the first 100 cycles of data.  \n2.  Those first 100 cycles are summarized as a “capacity matrix” – essentially a sequence of feature vectors that capture how capacity evolves early on.  \n3.  A transformer encoder is a natural choice to turn that sequence into a single, rich embedding: self-attention lets it pick up long-range patterns, drift rates, inflection points, etc.  \n4.  In a conditional DDPM/U-Net you must somehow inject your conditioning variable (here, the battery embedding) into the denoising network.  \n5.  By adding the transformer’s output into the *intermediate layers* of the U-Net (marked by the red box, i.e. [mask1]), every denoising block can “see” both the current noised curve and the battery’s unique early-life signature.  \n6.  The result is that the diffusion process doesn’t just hallucinate some generic SOH decay – it tailors the generated degradation path so that it is consistent with the particular battery history encoded by the transformer.  \n\nIn short, we integrate the transformer encoding into the U-Net’s hidden layers so that the model can condition each denoising step on a compact, attention-powered summary of the battery’s early cycling behavior, yielding realistic, battery-specific SOH curves."
    },
    {
        "question": "How does contamination detection pipeline process overlapping samples between Training Set and Evaluation Set?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "We explore two types of data contamination in multimodal evaluation benchmarks. 1) Image-only contamination. We aim to detect how many images in the benchmark can be found in the pre-training data. To this end, we utilize the CLIPScore (Hessel et al., 2021) to measure the similarity between images from the evaluation and training set. In our pilot experiments, we find that if the CLIPScore between two images exceeds 0.9, it indicates high visual similarity. Thus, we adopt 0.9 as the threshold to determine visual contamination. The image-only contamination rate is calculated as the ratio of the number of contaminated images and the number of total images in the evaluation set.",
            "2) Image-text contamination. Beyond images, the question and answer of benchmark can also be contaminated. We extend ideas from NLP detection works (Li et al., 2024) to identify this image-text contamination. For contaminated image pairs, we determine the question and answer contaminated if the answer can be directly inferred from the captions of the training image. In practice, we leverage GPT-4 to conduct this process."
        ],
        "final_answer": "First, the pipeline computes CLIPScore between each evaluation image and all images in the training set; any pair with a score above 0.9 is marked as image-only contamination. Second, for those contaminated image pairs, it uses GPT-4 to check whether the evaluation question and answer can be directly inferred from the training‐image captions, flagging those as image-text contamination.",
        "relevant_elements": [
            "Training Set",
            "Evaluation Set"
        ],
        "id": 862,
        "masked_question": "How does contamination detection pipeline process overlapping samples between [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Training Set",
            "Evaluation Set"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.08695v2_figure_1.png",
        "paperid": "2410.08695v2",
        "paper_path": "./papers/2410.08695v2.json",
        "figure_id": "2410.08695v2_figure_1.png",
        "caption": "Figure 1: (a) shows that some images in evaluation sets can be exactly found in the training set and their corresponding questions can be solved by the captions of similar training images. (b) compares our dynamic multimodal evaluation with the previous static evaluation. We can see that dynamic evaluation can create various variants upon static benchmarks with flexible complexity.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the “Training Set” icon (the red‐boxed oval) and Mask2 is the “Evaluation Set” icon (the blue‐boxed page).  Whenever an evaluation image “overlaps” with a training image, our pipeline treats it in two passes:\n\n1.  Image-only pass:  \n    –  We compute CLIPScore between every eval image and every training image.  \n    –  If CLIPScore ≥ 0.9, we call them duplicates and mark that eval image as image‐contaminated.  \n\n2.  Image-text pass (only on those flagged duplicates):  \n    –  We take the training image’s caption and the eval question + answer pair, feed them to GPT-4.  \n    –  If GPT-4 says “yes, the answer is directly inferable from the training caption,” we also mark that sample as text-contaminated.  \n\nThat two‐stage procedure is how overlapping samples between the training set and the evaluation set are detected and recorded."
    },
    {
        "question": "How do Visual Dynamic and Linguistic Dynamic modules integrate to generate variants with flexible complexity?",
        "relevant_section_ids": [
            "4.1",
            "4.5"
        ],
        "relevant_context": [
            "As illustrated in Figure 3 (a), by simulating real LVLM’s user interaction in visual attention and linguistic understanding, we design image (i.e., T_i) and language (i.e., T_l) bootstrapping strategies. Experiments show that the composition of T_i and T_l would yield dynamic evaluation samples with flexible complexity, an example is exhibited in Figure 3 (c).",
            "Due to every single VLB strategy for image and question being atomic, we can investigate two kinds of bootstrapping composition with flexible complexities. 1) Paired multimodal composition. We can compose visual bootstrapping T_i and linguistic bootstrapping T_l into a paired multimodal dynamic sample (T_i, T_l), obtaining a total of |T_i|×|T_l| dynamic variants. 2) Multi-strategy composition. We can also stack multiple image bootstrapping strategies on a single image or multiple language bootstrapping strategies on the question, composing a multi-strategy dynamic sample like (T_i^1∘T_i^2, T_l^1∘T_l^2). Since each single VLB strategy possesses different levels of complexity c, the above two kinds of compositions can effectively construct different variants varying in complexity, to assess the robustness and adaptability of LVLMs and explore models’ upper and lower limits in performance across different benchmarks."
        ],
        "final_answer": "The Visual Dynamic (image bootstrapping) and Linguistic Dynamic (language bootstrapping) modules are each composed of atomic transformations (e.g., adding or removing objects in the image; word substitution or sentence rephrasing in the question), each with an assigned difficulty. These two sets of transformations are then integrated in two ways: 1) Paired multimodal composition – applying one image transformation and one language transformation together to produce a new variant, yielding |T_i|×|T_l| samples; 2) Multi-strategy composition – stacking multiple image or language transformations in sequence. Because each atomic strategy carries its own complexity score, both types of composition produce dynamic variants whose overall difficulty can be flexibly controlled.",
        "relevant_elements": [
            "Visual Dynamic",
            "Linguistic Dynamic"
        ],
        "id": 863,
        "masked_question": "How do [mask1] and [mask2] modules integrate to generate variants with flexible complexity?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Dynamic",
            "Linguistic Dynamic"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.08695v2_figure_1.png",
        "paperid": "2410.08695v2",
        "paper_path": "./papers/2410.08695v2.json",
        "figure_id": "2410.08695v2_figure_1.png",
        "caption": "Figure 1: (a) shows that some images in evaluation sets can be exactly found in the training set and their corresponding questions can be solved by the captions of similar training images. (b) compares our dynamic multimodal evaluation with the previous static evaluation. We can see that dynamic evaluation can create various variants upon static benchmarks with flexible complexity.",
        "qtype": "Implementation_Details",
        "response": "Let’s first identify the two masked modules in the figure:  \n  – The red‐boxed module ([mask1]) is the “Linguistic Dynamic” or language‐bootstrapping component.  \n  – The blue‐boxed module ([mask2]) is the “Visual Dynamic” or image‐bootstrapping component.  \n\nHow they work together to yield flexible-complexity variants is explained in Section 4.5 (“Compositional Bootstrapping”).  Concretely:  \n1.  Each of the two modules can apply a small, atomic transformation—  \n    •  Visual Dynamic (image bootstrapping) can add, remove, or outpaint objects in the image, each tagged as “Easy” or “Hard.”  \n    •  Linguistic Dynamic (language bootstrapping) can substitute words, rephrase sentences, or add/remove context, again at varying difficulty.  \n2.  Because each single operation comes with a known difficulty score, we can now compose them in two ways:  \n    a.  Paired multimodal composition: pick one image transform (from Visual Dynamic) and one question transform (from Linguistic Dynamic) and apply them together to the same sample.  \n    b.  Multi-strategy composition: stack two or more transforms from the same modality (e.g. two different image edits or two different question rewrites).  \n\nBy mixing and matching these atomic steps—sometimes one visual change, sometimes two, sometimes one visual plus one linguistic—you immediately get a very large space of new evaluation examples whose difficulty you can dial up or down on demand.  That is exactly how the Visual Dynamic and Linguistic Dynamic modules integrate to generate dynamic benchmark variants with flexible, controllable complexity."
    },
    {
        "question": "How does data contamination analysis motivate the design of dynamic evaluation protocols?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "Despite the proliferation of LVLM evaluations, there are increasing concerns about the genuine capabilities of LVLMs (Laskar et al., 2024), largely due to two key challenges associated with current evaluation benchmarks. 1) Data contamination. LVLMs are pre-trained on large datasets, often sourced from the internet. Unfortunately, many evaluation benchmarks are constructed from similar sources, leading to a high likelihood of overlap with training data, thus causing data contamination (Touvron et al., 2023; Chen et al., 2024a), as illustrated in Figure 1(a) and detailed in Section 3. It raises a critical concern: “Does the model genuinely perceive and understand the input, or is it merely memorizing it?”",
            "2) Static dataset with fixed complexity. As shown in Figure 1(b), existing benchmarks for LVLMs are manually collected. Once constructed, they are static with a fixed complexity, making them inadequate to keep pace with the rapid development of LVLMs.\nTo accurately assess LVLM performance boundaries, a dynamic, automated evaluation protocol with adjustable complexity is urgently needed.",
            "The results reveal that each evaluation benchmark presents certain contamination rates across training datasets of various sizes, even with some reaching as high as 84.46% (image-only) and 33.13% (image-text). Note that the actual size of pre-training data far exceeds our detected maximum of 100M, which indicates that the actual contamination issue could be even more severe."
        ],
        "final_answer": "The high rates of overlap between static evaluation benchmarks and LVLM training data undermine confidence that models are reasoning rather than memorizing. Data contamination analysis—highlighting up to 84.5% image-only and 33.1% image-text overlap—directly motivates the creation of dynamic evaluation protocols. By automatically generating new, unseen test samples with controlled complexity, dynamic evaluation reduces contamination and provides a more reliable measure of a model’s true visual-language understanding.",
        "relevant_elements": [
            "Data Contamination",
            "Dynamic Evaluation"
        ],
        "id": 864,
        "masked_question": "How does [mask1] analysis motivate the design of [mask2] protocols?",
        "masked_number": 2,
        "masked_elements": [
            "Data Contamination",
            "Dynamic Evaluation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.08695v2_figure_1.png",
        "paperid": "2410.08695v2",
        "paper_path": "./papers/2410.08695v2.json",
        "figure_id": "2410.08695v2_figure_1.png",
        "caption": "Figure 1: (a) shows that some images in evaluation sets can be exactly found in the training set and their corresponding questions can be solved by the captions of similar training images. (b) compares our dynamic multimodal evaluation with the previous static evaluation. We can see that dynamic evaluation can create various variants upon static benchmarks with flexible complexity.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought  \n1. In the red‐boxed panel (mask₁) the authors quantify “data contamination” in current LVLM benchmarks – e.g. they show in Fig. 1(a) that up to 80% of evaluation images (and a large fraction of Q &A pairs) can be directly found in the model’s pre‐training corpora.  \n2. This overlap means that static, fixed‐set evaluations cannot reliably distinguish genuine visual–language understanding from mere memorization.  \n3. At the same time, those static benchmarks offer zero control over difficulty once they are released.  \n4. Together, these two failings (leakage plus fixed complexity) demonstrate the urgent need for an evaluation that (a) cannot be contaminated by previous training data and (b) can flex its difficulty to match advancing models.  \n5. That insight directly motivates the blue‐boxed design (mask₂): a “dynamic evaluation” (the Vision–Language Bootstrapping protocol) that automatically generates new, uncontaminated VQA samples of tunable complexity, thus restoring a fair and scalable test bed for LVLMs.  \n\nAnswer  \nThe alarming rates of data contamination uncovered in the static‐benchmark analysis (mask₁) make it clear that fixed tests are both leaky and quickly saturated by model pretraining.  That, in turn, drives the move to the dynamic‐evaluation protocols (mask₂), which continually synthesize fresh, uncontaminated multimodal questions with adjustable difficulty to guard against memorization and to stretch model capabilities."
    },
    {
        "question": "How do visual dynamic and linguistic dynamic modules interact to generate flexible complexity variants?",
        "relevant_section_ids": [
            "4.1",
            "4.5"
        ],
        "relevant_context": [
            "As illustrated in Figure 3 (a), by simulating real LVLM’s user interaction in visual attention and linguistic understanding, we design image (i.e., Τ_I) and language (i.e., Τ_Q) bootstrapping strategies. Experiments show that the composition of Τ_I and Τ_Q would yield dynamic evaluation samples with flexible complexity, an example is exhibited in Figure 3 (c).",
            "Due to every single VLB strategy for image and question being atomic, we can investigate two kinds of bootstrapping composition with flexible complexities. 1) Paired multimodal composition. We can compose visual bootstrapping Τ_I and linguistic bootstrapping Τ_Q, into a paired multimodal dynamic sample (I^(i), Q^(j)), obtaining a total of m × n dynamic variants. 2) Multi-strategy composition. We can also stack multiple image bootstrapping strategies on a single image or multiple language bootstrapping strategies on the question, composing a multi-strategy dynamic sample like (I^(i1+i2), Q^(j1+j2)). Since each single VLB strategy possesses different levels of complexity c, the above two kinds of compositions can effectively construct different variants varying in complexity, to assess the robustness and adaptability of LVLMs and explore models’ upper and lower limits in performance across different benchmarks."
        ],
        "final_answer": "The visual dynamic (image bootstrapping) and linguistic dynamic (language bootstrapping) modules interact via compositional bootstrapping: each image transformation (Τ_I) and each question transformation (Τ_Q) is treated as an atomic strategy with an associated difficulty level. By pairing one image and one language strategy (paired multimodal composition) or stacking multiple strategies within one or both modalities (multi-strategy composition), VLB generates a grid of dynamic variants whose overall complexity can be flexibly controlled by the choice and number of applied transformations.",
        "relevant_elements": [
            "Visual Dynamic",
            "Linguistic Dynamic"
        ],
        "id": 865,
        "masked_question": "How do [mask1] and [mask2] modules interact to generate flexible complexity variants?",
        "masked_number": 2,
        "masked_elements": [
            "Visual Dynamic",
            "Linguistic Dynamic"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.08695v2_figure_1.png",
        "paperid": "2410.08695v2",
        "paper_path": "./papers/2410.08695v2.json",
        "figure_id": "2410.08695v2_figure_1.png",
        "caption": "Figure 1: (a) shows that some images in evaluation sets can be exactly found in the training set and their corresponding questions can be solved by the captions of similar training images. (b) compares our dynamic multimodal evaluation with the previous static evaluation. We can see that dynamic evaluation can create various variants upon static benchmarks with flexible complexity.",
        "qtype": "Literature_Background",
        "response": "First, let us un‐mask what [mask1] and [mask2] are in the figure:\n\n  • [mask2] (blue box) is the Visual Dynamic module – the “image bootstrapping” strategies (add‐object, remove‐object, outpainting).  \n  • [mask1] (red box) is the Linguistic Dynamic module – the “language bootstrapping” strategies (word substitution, sentence rephrase, add relevant/irrelevant context).\n\nHow they interact to produce a _flexible complexity spectrum_ is best understood in three steps:\n\n1.  **Atomic transformations with tagged difficulty**  \n    – Each visual strategy  (in [mask2]) and each linguistic strategy  (in [mask1]) comes with a pre-assigned difficulty label (“Easy” or “Hard”).  \n    – Applying  to an original VQA sample  gives you a new image , and applying  gives you a new question .\n\n2.  **Composition across modalities**  \n    – **Paired multimodal composition**: pick one  _and_ one  and apply them both to  to get  \n         (I, Q)  ⟶  (I', Q')  \n      Since you know the individual difficulties of  and , the sum (or some function) of those two gives you the overall complexity of the new sample.  \n    – **Multi‐strategy stacking**: you can also string together two or more  ’s in sequence on the image, or two or more  ’s on the question, or even mix and match more than one of each.  Again, the total difficulty is just the aggregate of the chosen atomic operations.\n\n3.  **Judging and filtering**  \n    – After you generate (I′, Q′), a lightweight “judge” model checks that the original answer  is still correct.  If it fails, you re‐generate until consistency is restored.  \n    – This step guarantees that every composed variant is still a valid test of the _same_ underlying question, only at higher or lower difficulty.\n\nBy selecting which image‐level transforms  and which question‐level transforms  to apply—and by deciding whether to do them singly or in combination—the system can instantly sweep from “very easy” variants (e.g. remove‐object + add‐helpful context) to “very hard” ones (e.g. add‐object + word‐substitution + irrelevant context), hence yielding a dynamically adjustable, _flexible_ complexity spectrum for evaluation."
    },
    {
        "question": "How does on-device calibration leverage metric learning to set low and high thresholds?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "After computing the prototype vector of the new positive class c_p by feeding X^+ to the Eq. 2, we calculate the average distances of the positive and negative provided samples from the prototype c_p that are denoted, respectively, as μ_p and μ_n.",
            "For the thresholds, we consider the following function:\n\n   τ(α) = μ_p + (μ_n − μ_p) * α\n\nwhere α is a configurable parameter. From this equation, the threshold values are estimated as τ_L = τ(α_L) and τ_H = τ(α_H)."
        ],
        "final_answer": "On-device calibration uses the learned embedding (via metric learning) to compute a prototype c_p from the user’s positive examples. It then measures the average embedding distance μ_p of positives and μ_n of negatives to this prototype. The low and high thresholds are set by linearly interpolating between μ_p and μ_n: τ_L = μ_p + α_L·(μ_n−μ_p) and τ_H = μ_p + α_H·(μ_n−μ_p), with α_L and α_H chosen to tightly bound positive and negative samples.",
        "relevant_elements": [
            "On-device Calibration",
            "Thresholds"
        ],
        "id": 866,
        "masked_question": "How does [mask1] leverage metric learning to set low and high [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "On-device Calibration",
            "Thresholds"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.12481v1_figure_1.png",
        "paperid": "2408.12481v1",
        "paper_path": "./papers/2408.12481v1.json",
        "figure_id": "2408.12481v1_figure_1.png",
        "caption": "Figure 1: Self-Learning framework for On-device Personalized KWS. (1) A first on-device calibration function takes the examples provided by the user and returns the threshold parameters for the labeling task. (2) The labeling task processes the audio signal to detect and store pseudo-labeled samples. (3) Eventually, the new dataset is used to incrementally train the DNN feature extractor.",
        "qtype": "Literature_Background",
        "response": "Here is the step‐by‐step breakdown:\n\n1.  Image–text alignment  \n    -  The red box in the figure is the “On-device Calibration” module.  \n    -  The blue box contains the pair of threshold values \\[Th(τ_L), Th(τ_H)\\].\n\n2.  What the calibration module does  \n    a.  It takes the user’s few labeled examples (3 positive utterances of the new keyword, plus some non-keyword negatives) and runs them through the frozen feature extractor f(·), i.e. the metric-learning network.  \n    b.  It forms the positive-class prototype c_p by averaging the embeddings of the user’s keyword examples.  \n    c.  It measures two average distances in embedding space:  \n       •  d_p  = average distance of the positive examples to c_p  \n       •  d_n  = average distance of the negative examples to c_p  \n\n3.  How thresholds are set via metric distances  \n    –  Because f(·) was trained with a triplet/metric loss, “close” in embedding space really means “likely the keyword,” and “far” means “not the keyword.”  \n    –  We define a one‐dimensional interpolation function:  \n         τ(x) = d_p  +  x·(d_n – d_p)  \n    –  By choosing two interpolation weights 0 ≤ x_L < x_H ≤ 1, we obtain:  \n         Th(τ_L) = τ(x_L)  \n         Th(τ_H) = τ(x_H)  \n      –  Th(τ_L) sits just outside the positive cluster (a tight “inlier” cutoff)  \n      –  Th(τ_H) sits closer to the negative cluster (an “outlier” cutoff)\n\n4.  Putting it all together  \n    –  The calibration module uses the metric structure of the embedding space to place one threshold just beyond the positive examples and another threshold inside where negatives begin to lie.  \n    –  These two values, [Th(τ_L), Th(τ_H)], become the blue-boxed low and high thresholds used by the labeling step to generate pseudo-positives (distance < Th(τ_L)) and pseudo-negatives (distance > Th(τ_H))."
    },
    {
        "question": "How does incremental training leverage the pseudo-labeled set to fine-tune the DNN feature extractor?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "The feature extractor is fine-tuned on the new dataset composed by the pseudo-labeled samples.",
            "The training task runs for a fixed number of epochs. Similarly to the pretraining phase, we use the triplet loss and Adam as the optimizer.",
            "At every training epoch, the pseudo-positives are randomly split into groups of P samples. Thus, the training takes place if at least P pseudo-positive samples are present in memory.",
            "Every group of samples is then combined with P samples randomly taken from the pseudo-negative set and the user-provided utterances U to form a mini-batch.",
            "From this, we obtain the triplets as all the combinations between pseudo-positives, pseudo-negatives and the user samples of a mini-batch.",
            "Referring to Eq. 1, da and dp are the embeddings obtained from pseudo-labeled samples while dn is the embedding of one of the samples provided by the user."
        ],
        "final_answer": "During incremental training the on-device system collects pseudo-labeled examples—both pseudo-positives (frames close to the keyword prototype) and pseudo-negatives (frames far from it).  These are stored and, at each epoch, the pseudo-positives are grouped into batches of size P.  Each such group is joined with an equal number of pseudo-negatives and the original user-recorded utterances to build a mini-batch.  From each mini-batch all possible triplets (anchor and positive drawn from pseudo-labeled samples, negative drawn from user examples) are formed and used to fine-tune the DNN feature extractor via the triplet loss and Adam optimizer.",
        "relevant_elements": [
            "Incremental Training",
            "pseudo-labeled set",
            "DNN Feature Extractor"
        ],
        "id": 867,
        "masked_question": "How does incremental training leverage the pseudo-labeled set to fine-tune the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "DNN Feature Extractor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.12481v1_figure_1.png",
        "paperid": "2408.12481v1",
        "paper_path": "./papers/2408.12481v1.json",
        "figure_id": "2408.12481v1_figure_1.png",
        "caption": "Figure 1: Self-Learning framework for On-device Personalized KWS. (1) A first on-device calibration function takes the examples provided by the user and returns the threshold parameters for the labeling task. (2) The labeling task processes the audio signal to detect and store pseudo-labeled samples. (3) Eventually, the new dataset is used to incrementally train the DNN feature extractor.",
        "qtype": "Literature_Background",
        "response": "Step-by-step reasoning (chain of thought)\n\n1.  Identify the red-boxed module  \n    –  In the figure, the red box highlights the “DNN Feature Extractor f(·)”—this is the part of the model that gets fine-tuned during incremental training.\n\n2.  What data does incremental training use?  \n    –  From the labeling phase we accumulate two sets of embeddings:  \n       •  Pseudo-positives (frames whose distance to the keyword prototype is < Th(τL))  \n       •  Pseudo-negatives (frames whose distance to the prototype is > Th(τH))  \n    –  We also keep the small set of user-provided, ground-truth utterances.\n\n3.  How are mini-batches formed?  \n    –  At each epoch we:  \n       a.  Randomly split pseudo-positives into groups of m samples.  \n       b.  For each group of m pseudo-positives, draw m pseudo-negatives at random.  \n       c.  Add k user-provided positive examples.  \n    –  This yields a mini-batch of size m + m + k.\n\n4.  How is the feature extractor trained?  \n    –  From each mini-batch we form all valid triplets (anchor, positive, negative):  \n       •  Anchors and positives come from pseudo-positives (or user samples)  \n       •  Negatives come from the pseudo-negatives  \n    –  We apply the triplet loss (Eq. 1) to these triplets.  \n    –  We run a fixed number of epochs of Adam optimization on f(·).\n\n5.  Result  \n    –  By minimizing the triplet loss on pseudo-labeled + user samples, we fine-tune the weights of the DNN feature extractor f(·).  \n    –  After fine-tuning, the keyword prototype is recalculated and the on-device KWS classifier is updated.  \n\nAnswer  \nIncremental training leverages the pseudo-labeled set by mixing pseudo-positives, pseudo-negatives and the few user-provided samples into triplet-based mini-batches, then fine-tuning the DNN feature extractor f(·) with triplet loss (via Adam) over several epochs."
    },
    {
        "question": "How does On-Device Calibration threshold selection shape pseudo-positive versus pseudo-negative labeling before incremental training?",
        "relevant_section_ids": [
            "4.2",
            "4.1"
        ],
        "relevant_context": [
            "After computing the prototype vector of the new positive class c_p by feeding X_p to Eq. 2, we calculate the average distances of the positive and negative provided samples from the prototype that are denoted, respectively, as μ_p and μ_n. … For the thresholds, we consider the following function: f(α) = (μ_p + α(μ_n − μ_p), μ_n − α(μ_n − μ_p)). From this equation, the threshold values are estimated as th_l = μ_p + α(μ_n − μ_p) and th_h = μ_n − α(μ_n − μ_p). Because α delimits samples close to the prototype, it must hold th_l < th_h. We experimentally verify in Sec. VI-C that a low α value for the low-thres (th_l) leads to the best quality labels for the positive samples. Vice versa, a higher α separates the negative samples and a value 0.5 is experimentally demonstrated as the best choice.",
            "If d̄ is lower than th_l (Eq. 6), the sample is marked as pseudo-positive, meaning the system is confident that the current audio frame includes the target keyword. On the other side, the audio segment is a pseudo-negative if d̄ is higher than th_h (Eq. 7). When d̄ is between th_l and th_h no decision is taken and the segment is not labeled to prevent potential errors. Eventually, the pseudo-positives and the pseudo-negatives are stored in memory to serve the incremental training task."
        ],
        "final_answer": "On-Device Calibration first measures the average distance of a few user-provided positive and negative examples to the newly computed prototype (μ_p and μ_n). It then interpolates between these two distances using a factor α to set a low threshold (th_l = μ_p + α(μ_n − μ_p)) and a high threshold (th_h = μ_n − α(μ_n − μ_p)), ensuring th_l < th_h. During the streaming labeling phase, any segment whose filtered distance to the prototype falls below th_l is labeled as a pseudo-positive, any segment above th_h is labeled as a pseudo-negative, and segments in between remain unlabeled. This selective labeling yields high-confidence pseudo-positives and pseudo-negatives for the subsequent incremental training.",
        "relevant_elements": [
            "On-Device Calibration",
            "Labeling",
            "Incremental Training"
        ],
        "id": 868,
        "masked_question": "How does [mask1] threshold selection shape pseudo-positive versus pseudo-negative labeling before incremental training?",
        "masked_number": 1,
        "masked_elements": [
            "On-Device Calibration"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.12481v1_figure_1.png",
        "paperid": "2408.12481v1",
        "paper_path": "./papers/2408.12481v1.json",
        "figure_id": "2408.12481v1_figure_1.png",
        "caption": "Figure 1: Self-Learning framework for On-device Personalized KWS. (1) A first on-device calibration function takes the examples provided by the user and returns the threshold parameters for the labeling task. (2) The labeling task processes the audio signal to detect and store pseudo-labeled samples. (3) Eventually, the new dataset is used to incrementally train the DNN feature extractor.",
        "qtype": "Experimental_Results",
        "response": "In our pipeline the little red‐boxed module is the on‐device calibration step that actually *chooses* the two cut‐off values (Th(τL) and Th(τH)) used downstream by the labelling block.  Concretely it does the following:\n\n 1.  It computes a prototype Cp from the few user‐provided “wake‐word” examples and measures the average distance δp of those positives to Cp and δn for the supplied negatives.  \n 2.  It then picks two thresholds  \n     – a *low* threshold Th(τL) sitting just above δp (by a fraction λL of the gap δn–δp),  \n     – a *high* threshold Th(τH) sitting below δn (by a fraction λH of that same gap).  \n 3.  When the streaming inference distance f(xt)→d(xt,Cp) falls *below* the low threshold Th(τL) we declare that frame a **pseudo-positive**, and when it rises *above* the high threshold Th(τH) we declare it a **pseudo-negative**.  Anything in between is left unlabeled (to avoid weak or ambiguous examples).  \n\nBy tuning λL and λH at calibration time one directly controls how tight the “inner ball” of confident positives is and how far out the “outer shell” of confident negatives is, ensuring that only highly reliable examples enter the incremental training stage."
    },
    {
        "question": "How does varying window stride affect pseudo-positive sample generation in labeling before incremental training?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Labeling",
            "Incremental Training"
        ],
        "id": 869,
        "masked_question": "How does varying window stride affect pseudo-positive sample generation in [mask1] before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Labeling",
            "Incremental Training"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.12481v1_figure_1.png",
        "paperid": "2408.12481v1",
        "paper_path": "./papers/2408.12481v1.json",
        "figure_id": "2408.12481v1_figure_1.png",
        "caption": "Figure 1: Self-Learning framework for On-device Personalized KWS. (1) A first on-device calibration function takes the examples provided by the user and returns the threshold parameters for the labeling task. (2) The labeling task processes the audio signal to detect and store pseudo-labeled samples. (3) Eventually, the new dataset is used to incrementally train the DNN feature extractor.",
        "qtype": "Experimental_Results",
        "response": "In our system the “window‐stride” parameter Tₛ governs how often you pull off a new audio frame, run it through the encoder, compare it to your keyword prototype, and (if it falls below your low‐threshold) call it a pseudo-positive.  So:\n\n  • If you make Tₛ large (say one window every 32 ms), you only get a handful of overlapping views of each spoken keyword.  You end up with relatively few pseudo-positives, because you’re simply sampling too coarsely.  \n  • If you shrink Tₛ (for instance down to 16 ms or even 8 ms), each keyword utterance is seen by many more overlapping windows.  More of those windows will fall under your “Thₗ” cut-off, so you automatically harvest more pseudo-positive samples in the labeling stage ([mask1]) before you ever batch anything into the incremental retraining step ([mask2]).\n\nIn practice (see Sec. VI-C2), we found that a stride of one-eighth the window (Tₛ = Tₙ/8) gives the best trade-off: it roughly doubles the number of pseudo-positives over a Tₛ = Tₙ/4 setting (boosting final accuracy by up to 16 %) but only doubles the energy for inference, which is still acceptable on our device."
    },
    {
        "question": "How does brightness normalization complement the dynamic gating module to preserve structural information?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "relevant_context": [
            "Section 3.2: \"Brightness normalization is then performed on these channels to remove the effects of brightness while continuing to reuse the brightness-independent channel features to preserve structural detail information.\"",
            "Section 3.2.1: \"Normalization has been proven to eliminate brightness-related components, retain robust structural representations, and effectively reduce the impact of brightness variations on model learning.\"",
            "Section 3.2.2: \"Based on the routing selection result, we select only the routed channels from the normalized feature, while retaining the remaining channels of the original feature. Finally, these channel features are recombined to obtain the output feature. This design effectively mitigates the information loss caused by normalization.\""
        ],
        "final_answer": "Brightness normalization removes brightness-related variations on the channels flagged by the dynamic gating module, while the gating module keeps the unflagged (brightness-independent) channels in their original form. By recombining the normalized (brightness-corrected) channels with the untouched channels, the network both suppresses unwanted brightness effects and preserves the structural detail carried in the brightness-independent feature channels.",
        "relevant_elements": [
            "Brightness Normalization",
            "Dynamic Gating Module",
            "Structural Information"
        ],
        "id": 870,
        "masked_question": "How does [mask1] complement the dynamic gating module to preserve structural information?",
        "masked_number": 1,
        "masked_elements": [
            "Brightness Normalization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04697v1_figure_2.png",
        "paperid": "2411.04697v1",
        "paper_path": "./papers/2411.04697v1.json",
        "figure_id": "2411.04697v1_figure_2.png",
        "caption": "Figure 2: The architecture of BA-Fusion. BA-Fusion consists of a Brightness Adaptive Gate (BAG), and the multimodal fusion backbone network.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. In the red-boxed area of Fig. 2 we see a stack of feature maps labeled x_norm feeding into the router.  \n2. The text calls this component “brightness normalization,” which is a channel-wise normalization of the encoder’s output features.  \n3. Brightness normalization is known to strip away illumination statistics (mean and variance) while retaining edges, textures and other geometric cues.  \n4. The dynamic gating module then decides, on a per-channel basis, whether to use the original feature (x) or the normalized feature (x_norm).  \n5. By routing brightness-sensitive channels through normalization and leaving the rest untouched, the network removes unwanted brightness shifts yet still has direct access to the unnormalized feature channels that carry rich structural information.  \n6. Thus the normalization block (the red-boxed [mask1]) and the gating block work in tandem: the normalizer guarantees that whatever brightness artifacts are stripped out, the structural content is preserved in x_norm, and the gate ensures the final fused feature preserves those structures by selectively combining x and x_norm.  \n\nAnswer  \n[mask1] is the channel-wise brightness normalization block. It complements the dynamic gating module by removing illumination variations (so that brightness-affected channels become invariant) while still preserving edges/textures. The gate then dynamically fuses these normalized (structure-rich) channels with the original features, ensuring structural details are kept even as brightness changes are filtered out."
    },
    {
        "question": "How does brightness consistency loss stabilize fusion decoder outputs across diverse brightness conditions?",
        "relevant_section_ids": [
            "3.1",
            "3.3",
            "3.3.2"
        ],
        "relevant_context": [
            "To optimize the BAG module, we also propose an alternating training strategy with a brightness consistency loss function to force the gating module to select the brightness-related channel, which is driven by performance stability under the brightness jitter operation.",
            "In the second stage, we propose a brightness consistency loss function, which constrains the consistency of the brightness and structural features of the fusion results under different brightness perturbations with the feature representation of normal fusion results in the first stage.",
            "Considering that image brightness is related to the amplitude in the frequency domain, we introduce frequency domain amplitude information to design the brightness consistency loss. This encourages the network to pay closer attention to the brightness information and effectively select the channels that are related to brightness."
        ],
        "final_answer": "The brightness consistency loss enforces that the frequency‐domain amplitude (and thus brightness content) of the fused image remain consistent between the normal and brightness‐jittered inputs. By penalizing deviations in amplitude, it drives the Brightness Adaptive Gate to identify and normalize only the channels affected by brightness changes. As a result, the decoder always receives features with stable brightness representations, yielding fusion outputs that are consistent and robust across diverse illumination conditions.",
        "relevant_elements": [
            "Brightness Consistency Loss",
            "Fusion Decoder",
            "Brightness Variations"
        ],
        "id": 871,
        "masked_question": "How does [mask1] stabilize [mask2] outputs across diverse brightness conditions?",
        "masked_number": 2,
        "masked_elements": [
            "Brightness Consistency Loss",
            "Fusion Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04697v1_figure_2.png",
        "paperid": "2411.04697v1",
        "paper_path": "./papers/2411.04697v1.json",
        "figure_id": "2411.04697v1_figure_2.png",
        "caption": "Figure 2: The architecture of BA-Fusion. BA-Fusion consists of a Brightness Adaptive Gate (BAG), and the multimodal fusion backbone network.",
        "qtype": "Experimental_Results",
        "response": "First, let’s name our two “masks”:\n\n  • Mask 1 (red box) is the Brightness Consistency Loss.  \n  • Mask 2 (blue box) is the Fusion Decoder (and hence its fused‐image output).  \n\nQuestion  \n“How does the Brightness Consistency Loss stabilize the Fusion Decoder’s outputs across diverse brightness conditions?”\n\nChain of thought  \n\n1. In the second stage of training we freeze both the encoder and the Fusion Decoder (mask 2) and only update the Brightness Adaptive Gate (BAG) with a new loss.  \n2. That loss—the Brightness Consistency Loss—measures the difference in frequency‐domain amplitudes between:  \n     – the “normal” fused image (no jitter) and  \n     – the “jittered” fused image (after random brightness perturbation).  \n3. By penalizing any change in the amplitude spectrum, it forces the BAG to discover which feature channels carry pure brightness information and to gate (i.e. turn off) exactly those channels when brightness changes occur.  \n4. Because the BAG then strips out only the brightness-varying components and passes on the rest unchanged, the Fusion Decoder always sees essentially the same “structural” features, no matter how the input’s brightness was altered.  \n5. As a result, the decoder produces fused images whose overall intensity/contrast remains consistent under all tested lighting conditions.\n\nAnswer  \nThe Brightness Consistency Loss anchors the frequency‐domain amplitudes of the jittered fusion result to those of the normal fusion result, so that during that stage only the BAG learns to remove brightness fluctuations.  This guarantees that the Fusion Decoder always receives brightness‐invariant features and thus yields visually stable fused outputs even when the input brightness varies."
    },
    {
        "question": "How could replacing normalization with adaptive scaling alleviate detail loss in the Brightness Adaptive Gate?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Brightness Adaptive Gate",
            "Brightness Normalization"
        ],
        "id": 873,
        "masked_question": "How could replacing [mask1] with adaptive scaling alleviate detail loss in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Brightness Normalization",
            "Brightness Adaptive Gate"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04697v1_figure_2.png",
        "paperid": "2411.04697v1",
        "paper_path": "./papers/2411.04697v1.json",
        "figure_id": "2411.04697v1_figure_2.png",
        "caption": "Figure 2: The architecture of BA-Fusion. BA-Fusion consists of a Brightness Adaptive Gate (BAG), and the multimodal fusion backbone network.",
        "qtype": "Others",
        "response": "Chain of Thought:\n\n1.  In Fig. 2 the red‐boxed operation is the channel‐wise “brightness normalization” step (Eqn. 1), which forces each channel to zero‐mean/unit‐variance.  \n2.  The blue‐boxed module is the Brightness Adaptive Gate (BAG), which routes and merges original versus normalized channels.  \n3.  While normalization strips out brightness variability, it also destroys some of the fine‐grained statistical cues (texture contrast, local detail) that the BAG needs to reconstruct high‐fidelity images.  \n4.  If we swap out hard normalization for an “adaptive scaling”—for example, learning per‐channel scale (γ) and shift (β) parameters driven by brightness cues—then:  \n    a.  We can still suppress unwanted brightness fluctuations by scaling channels according to the gate’s routing.  \n    b.  We never throw away the original mean and variance entirely, so texture contrast and edge details survive.  \n5.  In this way, adaptive scaling preserves the feature statistics that normalization would eliminate, and the BAG can merge channels without suffering detail loss.  \n\nAnswer:\n\nBy replacing the hard zero‐mean/unit‐variance normalization ([mask1]) with a learnable, brightness‐conditioned scaling (and optional shift), the module can attenuate brightness fluctuations without obliterating the original channel statistics. That softer, adaptive transform retains texture contrast and local details, so the BAG ([mask2]) no longer loses fine‐grained information when it gates and fuses those channels."
    },
    {
        "question": "What limitations arise from using RAG-based History Memory for personalization under privacy constraints?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "RAG-based History Memory"
        ],
        "id": 874,
        "masked_question": "What limitations arise from using [mask1] for personalization under privacy constraints?",
        "masked_number": 1,
        "masked_elements": [
            "RAG-based History Memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11913v1_figure_1.png",
        "paperid": "2411.11913v1",
        "paper_path": "./papers/2411.11913v1.json",
        "figure_id": "2411.11913v1_figure_1.png",
        "caption": "Figure 1: An overview of the proposed framework for personalized autonomous vehicle motion control. The system workflow begins with processing four input streams (System Message S𝑆Sitalic_S, Human Instruction I𝐼Iitalic_I, Camera Image V𝑉Vitalic_V, and Historical Memory H𝐻Hitalic_H) through an on-board VLM, which generates personalized action policies P𝑃Pitalic_P containing MPC and PID control parameter matrices. These policies are then executed through the vehicle’s drive-by-wire system. If the human evaluates, human feedback F𝐹Fitalic_F is collected and stored in the RAG-based memory module for continuous learning and adaptation of the system’s behavior to individual preferences.",
        "qtype": "Others",
        "response": "First, let’s align the diagram with the text.  The red‐boxed module in Fig. 1 is the “RAG‐based History Memory” (the thing that stores past ⟨instruction, scene, action, feedback⟩ tuples and retrieves the top-k matches to guide the VLM’s current decision).  \n\nNow: what happens when you insist on strict privacy?  You can no longer offload that memory to a shared cloud, or keep unlimited personal logs, or aggregate histories across users.  In practice that forces you to:\n\n  1. Keep all of a single user’s memory on board (device), not in a central server.  \n  2. Limit how much you store (to finite on-device disk/flash).  \n  3. Possibly prune or anonymize entries to avoid holding raw personal data.  \n  4. Forego cross-vehicle or cross-user learning (you can’t pool memories).  \n\nTaken together, those restrictions mean your personalized memory:  \n  • Is small (you can only hold N recent trips or M entries).  \n  • Has a short horizon (you lose long-tail preferences once you prune).  \n  • Cannot benefit from other users’ data (no transfer learning).  \n  • Must strip or hash any PII, which further blunts retrieval accuracy.  \n\nIn short, under strong privacy constraints the RAG‐based History Memory can only be a tiny, isolated, on-device log.  That cap on size, sharing and detail directly limits how quickly and how deeply the vehicle can personalize its control policies."
    },
    {
        "question": "What alternative control strategies could complement MPC Action Matrix to handle highly dynamic driving environments?",
        "relevant_section_ids": [
            "3.3",
            "3.5"
        ],
        "relevant_context": [
            "Section 3.3: \"…our VLM will generate two distinct action matrices to separately manage the PID controller for longitudinal movement and the MPC for lateral movement. These matrices translate the model’s understanding of the environment and user preferences into precise control actions…\"",
            "Section 3.5: \"…we implement a decoupled control strategy that separates lateral and longitudinal vehicle motion control. The lateral control is handled by MPC … while longitudinal control is managed through a PID controller calculating the front steering angle. … Our VLM generates the action matrix that primarily considers three key components…\""
        ],
        "final_answer": "A natural complement to an MPC-based action matrix in highly dynamic driving scenarios is a PID‐based controller. By decoupling lateral and longitudinal control—using MPC for one axis and a PID controller for the other—the system can more robustly handle rapid changes in vehicle dynamics.",
        "relevant_elements": [
            "MPC Action Matrix"
        ],
        "id": 875,
        "masked_question": "What alternative control strategies could complement [mask1] to handle highly dynamic driving environments?",
        "masked_number": 1,
        "masked_elements": [
            "MPC Action Matrix"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11913v1_figure_1.png",
        "paperid": "2411.11913v1",
        "paper_path": "./papers/2411.11913v1.json",
        "figure_id": "2411.11913v1_figure_1.png",
        "caption": "Figure 1: An overview of the proposed framework for personalized autonomous vehicle motion control. The system workflow begins with processing four input streams (System Message S𝑆Sitalic_S, Human Instruction I𝐼Iitalic_I, Camera Image V𝑉Vitalic_V, and Historical Memory H𝐻Hitalic_H) through an on-board VLM, which generates personalized action policies P𝑃Pitalic_P containing MPC and PID control parameter matrices. These policies are then executed through the vehicle’s drive-by-wire system. If the human evaluates, human feedback F𝐹Fitalic_F is collected and stored in the RAG-based memory module for continuous learning and adaptation of the system’s behavior to individual preferences.",
        "qtype": "Others",
        "response": "Here is the reasoning step by step:\n\n1.  Identify what the red‐boxed “[mask1]” is.  \n    – In Fig. 1 the red box encloses the “MPC Action Matrix,” i.e. the block whose outputs parameterize the lateral MPC controller.  \n\n2.  Ask: “What control techniques, other than an MPC‐based lateral controller, could I bring in when the vehicle is operating in very fast‐changing, highly dynamic scenarios?”  \n\n3.  Recall the properties of MPC:  \n    – It reasons over a finite horizon using a vehicle model and an optimization problem.  \n    – It can be computationally heavy and may struggle if the model is inaccurate or the environment changes too abruptly.  \n\n4.  Think of controllers that are  \n    a) faster to react to sudden disturbances,  \n    b) able to tolerate larger modeling errors,  \n    c) simple enough to run at very high update rates.  \n\n5.  Candidate “complementary” strategies:  \n    – Sliding-Mode Control (SMC) for very aggressive disturbance rejection and robustness to model uncertainty.  \n    – H∞ or μ-synthesis (robust control) to explicitly bound worst-case errors under unmodeled dynamics.  \n    – Gain-scheduled or adaptive PID loops (or even LQR variants) for very high update-rate feedback on lateral error and heading error.  \n    – Fuzzy-logic or neuro-fuzzy controllers that can smoothly interpolate between different driving regimes (e.g. normal vs. evasive).  \n    – Reinforcement-Learning–based or neural‐network–based “emergency” controllers trained on corner‐case maneuvers, which can take over when MPC fails.  \n    – Tube-MPC or stochastic MPC (an MPC variant) that explicitly accounts for uncertainty bounds, making the baseline MPC more robust.  \n\n6.  Therefore, to handle highly dynamic driving environments the basic MPC lateral scheme can be “backed up” or “augmented” by one or more of these approaches.  \n\nAnswer:  \n“Besides the MPC action matrix, one could layer on a very fast, robust backup controller—e.g. a sliding-mode or H∞ regulator—or pre-tuned gain-scheduled PIDs (or LQR) for ultra-low-latency feedback.  Fuzzy-logic or adaptive controllers can smoothly blend between normal and emergency modes, and learned policies (reinforcement-learning or neural network controllers) can step in for highly unstructured or rapid maneuvers.  Finally, tube-MPC or stochastic MPC variants can make the original MPC itself more robust to sudden environmental changes.”"
    },
    {
        "question": "What motivates using both MPC Action Matrix and PID Action Matrix within a unified action policy?",
        "relevant_section_ids": [
            "3.3",
            "3.5"
        ],
        "relevant_context": [
            "In our approach, reasoning within the VLM framework enables the interpretation of diverse driving scenarios and user instructions to generate actionable outputs. Traditional controllers in motion control typically rely on a default set of parameters; however, following the approach in [42], our VLM will generate two distinct action matrices to separately manage the PID controller for longitudinal movement and the MPC for lateral movement.",
            "As shown in Fig. 1, we implement a decoupled control strategy that separates lateral and longitudinal vehicle motion control. The lateral control is handled by MPC calculating the longitudinal acceleration, while longitudinal control is managed through a PID controller calculating the front steering angle."
        ],
        "final_answer": "The unified policy includes both an MPC Action Matrix and a PID Action Matrix because the system uses a decoupled control strategy: MPC is best suited for optimizing lateral (steering) behavior, while PID is used for precise longitudinal (speed) control. By generating separate action matrices for each controller, the VLM can translate its scene understanding and user preferences into the appropriate low-level commands for both steering and speed regulation.",
        "relevant_elements": [
            "MPC Action Matrix",
            "PID Action Matrix"
        ],
        "id": 876,
        "masked_question": "What motivates using both [mask1] and PID Action Matrix within a unified action policy?",
        "masked_number": 1,
        "masked_elements": [
            "MPC Action Matrix"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11913v1_figure_1.png",
        "paperid": "2411.11913v1",
        "paper_path": "./papers/2411.11913v1.json",
        "figure_id": "2411.11913v1_figure_1.png",
        "caption": "Figure 1: An overview of the proposed framework for personalized autonomous vehicle motion control. The system workflow begins with processing four input streams (System Message S𝑆Sitalic_S, Human Instruction I𝐼Iitalic_I, Camera Image V𝑉Vitalic_V, and Historical Memory H𝐻Hitalic_H) through an on-board VLM, which generates personalized action policies P𝑃Pitalic_P containing MPC and PID control parameter matrices. These policies are then executed through the vehicle’s drive-by-wire system. If the human evaluates, human feedback F𝐹Fitalic_F is collected and stored in the RAG-based memory module for continuous learning and adaptation of the system’s behavior to individual preferences.",
        "qtype": "Design_Rationale",
        "response": "Step 1 – Identify [mask1]  \nLooking at Fig. 1, the red‐boxed matrix under “Action Policy (P)” is the MPC Action Matrix.  \n\nStep 2 – Why generate both MPC and PID matrices together?  \n1. Our system decouples vehicle motion into two axes:  \n   - Lateral (steering) → handled by an MPC controller  \n   - Longitudinal (speed) → handled by a PID controller  \n2. The MPC Action Matrix specifies the weighting factors (e.g. lateral‐error and heading‐error penalties) used in the predictive horizon to compute steering commands.  \n3. The PID Action Matrix specifies the proportional, integral, and derivative gains used to regulate velocity.  \n4. By having the VLM output both matrices as one unified policy, we can:  \n   • Tune steering and speed behaviors in concert to match a user’s driving style (aggressive, moderate, conservative).  \n   • Leverage the established strengths of each controller (predictive lateral control vs. simple, robust longitudinal control).  \n   • Ensure consistency and stability across both steering and throttle/brake commands under a single personalized policy.  \n\nAnswer  \nmask1 is the MPC Action Matrix. Generating it alongside the PID Action Matrix in one action policy lets the VLM jointly tune both lateral (via MPC) and longitudinal (via PID) controllers, yielding a fully personalized, decoupled motion control strategy that leverages each controller’s strengths."
    },
    {
        "question": "What drives integrating RAG-based History Memory with Human Evaluation for continuous personalization?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "We introduce a RAG-based memory module that incorporates human feedback for continuous learning and adaptation, enabling personalized driving experiences through iterative refinement of control strategies.",
            "After each trip, users can provide feedback E on the generated control policy P for the current situations (including instructions I and visual input V), which helps refine the VLM’s reasoning process. Subsequently, the instructions I, scene description O, policy P, and feedback E are packaged as a historical data entry and stored in the RAG database.",
            "Given that our 8B-parameter VLM lacks the extensive reasoning capabilities of larger, 100B-200B parameter models, we employ a RAG-based approach and integrate a memory module to enhance reasoning and enable human feedback learning."
        ],
        "final_answer": "The integration is driven by the need for continuous learning and adaptation to each user’s driving style. By retrieving past interactions from a RAG-based memory and incorporating human evaluations after each trip, the system can iteratively refine its VLM-generated control policies and deliver truly personalized driving experiences.",
        "relevant_elements": [
            "RAG-based History Memory",
            "Human Evaluation"
        ],
        "id": 877,
        "masked_question": "What drives integrating [mask1] with Human Evaluation for continuous personalization?",
        "masked_number": 1,
        "masked_elements": [
            "RAG-based History Memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.11913v1_figure_1.png",
        "paperid": "2411.11913v1",
        "paper_path": "./papers/2411.11913v1.json",
        "figure_id": "2411.11913v1_figure_1.png",
        "caption": "Figure 1: An overview of the proposed framework for personalized autonomous vehicle motion control. The system workflow begins with processing four input streams (System Message S𝑆Sitalic_S, Human Instruction I𝐼Iitalic_I, Camera Image V𝑉Vitalic_V, and Historical Memory H𝐻Hitalic_H) through an on-board VLM, which generates personalized action policies P𝑃Pitalic_P containing MPC and PID control parameter matrices. These policies are then executed through the vehicle’s drive-by-wire system. If the human evaluates, human feedback F𝐹Fitalic_F is collected and stored in the RAG-based memory module for continuous learning and adaptation of the system’s behavior to individual preferences.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In the figure, the red-boxed block clearly shows a set of stored “cards” labeled with past instructions (I: Drive faster…), scene descriptions (D: The Weather…), action matrices (A: [0.2 0.5 1.5…]), and feedback (F: Still slow…).  \n2. The caption and text repeatedly refer to this as the RAG-based memory module (H), which retrieves similar past interactions and then, after each trip, appends the new trip’s instruction, scene description, policy, and human feedback back into the database.  \n3. The question asks why we integrate this RAG-based memory (mask1) with human evaluation (the F signal).  \n4. Reading the context shows that the entire point of the memory + feedback loop is to achieve continuous personalization—i.e., to let the system learn each user’s unique preferences over time by retrieving relevant past cases and then refining its control policy based on the user’s actual evaluation.  \n5. Thus what “drives” this integration is the need for an ever-improving, human-in-the-loop, personalized driving experience: by feeding human evaluations back into the RAG memory, the on-board VLM can adapt its future policies to match individual driving styles and preferences.\n\nAnswer:\n\nmask1 = the RAG-based history memory module. It is integrated with human evaluation in order to form a closed-loop, retrieval-augmented learning system that continuously refines the vehicle’s control policies to each user’s preferences."
    },
    {
        "question": "What is the rationale for tiling sampled frames with index annotations before VLM query?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The sampled frames are then tiled in an image with the annotation of the time order of the frames (Fig. 2 (b)).",
            "The image was then fed into a VLM to identify the frame closest to a specific action timing; Fig. 2 (c) illustrates the start timing of an action."
        ],
        "final_answer": "By tiling the sampled frames into a single image and overlaying each with its frame‐index annotation, the VLM can directly compare all candidate frames at once and select the index corresponding to the moment closest to the queried action timing.",
        "relevant_elements": [
            "tiled image",
            "VLM query"
        ],
        "id": 878,
        "masked_question": "What is the rationale for [mask1] sampled frames with index annotations before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "tiled image",
            "VLM query"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.17422v4_figure_2.png",
        "paperid": "2408.17422v4",
        "paper_path": "./papers/2408.17422v4.json",
        "figure_id": "2408.17422v4_figure_2.png",
        "caption": "Figure 2: The proposed pipeline for open-vocabulary video action localization using a VLM consists of the following steps: (a) Frames are sampled at regular intervals from a time window, covering the entire video in the first iteration. (b) The sampled frames are then tiled in an image with annotations indicating the time order of the frames. (c) This image is then fed into a VLM to identify the frames closest to a specific timing of an action (e.g., the start timing of an action). (d) The sampling window is updated by centering on the selected frame with a narrower sampling interval. Bottom panel (1) For general action localization, the start time of the action in the video is determined by iterating steps (a) to (d). Bottom panel (2) By estimating the end time of the action in the same manner, the action is localized in the video.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. The red‐boxed part of the figure (a) shows that, at each iteration, we “Equally sample N frames within a time window” and hold onto their original time‐order by stamping each thumbnail with a small index.  \n2. The blue‐boxed part (c) then “Queries the VLM” by showing it that tiled image of numbered frames and asking, for example, “Which number is closest to when ‘grasping a can’ started?”  \n3. Why do we do the sampling + indexing step first? Because modern VLMs take only static images (plus text) as input, not raw video clips or frame sequences. By sampling a handful of frames and laying them out in one big image—and crucially by annotating each one with a number—we turn the temporal localization problem into a simple multiple‐choice question that the VLM can answer (e.g. “8”).  \n4. Once the VLM returns an index, we immediately know which frame (and thus which time) it selected, so we can re-center and shrink the sampling window for the next, finer‐grained iteration.  \n\nAnswer  \nThe reason we first sample N frames and tile them with index annotations (red box) is to convert the video’s temporal search into a single static image with numbered candidates. This lets us pose a natural‐language question to the VLM (blue box) and map its textual answer (e.g. “frame 8”) back to an exact time in the video for the next refinement step."
    },
    {
        "question": "What motivates sequentially estimating start and end frames instead of detecting both simultaneously?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "start frame",
            "end frame"
        ],
        "id": 879,
        "masked_question": "What motivates sequentially estimating [mask1] and end frames instead of detecting both simultaneously?",
        "masked_number": 1,
        "masked_elements": [
            "start frame"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.17422v4_figure_2.png",
        "paperid": "2408.17422v4",
        "paper_path": "./papers/2408.17422v4.json",
        "figure_id": "2408.17422v4_figure_2.png",
        "caption": "Figure 2: The proposed pipeline for open-vocabulary video action localization using a VLM consists of the following steps: (a) Frames are sampled at regular intervals from a time window, covering the entire video in the first iteration. (b) The sampled frames are then tiled in an image with annotations indicating the time order of the frames. (c) This image is then fed into a VLM to identify the frames closest to a specific timing of an action (e.g., the start timing of an action). (d) The sampling window is updated by centering on the selected frame with a narrower sampling interval. Bottom panel (1) For general action localization, the start time of the action in the video is determined by iterating steps (a) to (d). Bottom panel (2) By estimating the end time of the action in the same manner, the action is localized in the video.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through why the paper doesn’t try to pick both boundaries in one go, but instead first zeroes in on the start frame, then (knowing that) looks for the end frame:\n\n1.   PIVOT‐style narrowing only works around one “pivot” at a time.  \n     –  At each iteration you sample a small set of frames, ask “which of these is closest to the event?” and then recenter and shrink your window around that answer.  \n     –  If you tried to do start and end in one shot, you’d have to ask the VLM a much more complicated question (“Which two frames mark the beginning and end?”), and you wouldn’t have a clear way to narrow the window independently for each boundary.  \n\n2.   By finding the start first, you can immediately discard everything before it.  \n     –  This cuts your search space in half (or better) before you even begin looking for the end.  \n     –  It makes the second question (“which frame marks the end?”) much easier—both for you and for the VLM—because you’re only showing it frames that come after the action has begun.  \n\n3.   Keeping each VLM query simple (one boundary per query) dramatically boosts reliability.  \n     –  Vision–language models struggle to juggle multiple temporal tasks in one prompt.  \n     –  Two straightforward “find the start” / “find the end” questions are both easier to write and to answer correctly than a single “find both” question.  \n\nIn short, the iterative PIVOT mechanism is inherently a single‐pivot‐at‐a‐time process.  Splitting boundary detection into (1) start and then (2) end gives you a natural, accurate way to shrink your search region and keeps each VLM prompt focused on just one subtask."
    },
    {
        "question": "How does Sample Frames module compute frame selection when halving the sampling window each iteration?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "The pipeline starts by sampling a number of frames at regular intervals from a time window of the given video (Fig. 2 (a)).",
            "The sampled frames are then tiled in an image with the annotation of the time order of the frames (Fig. 2 (b)).",
            "Finally, the sampling window is updated centering the selected frame with a narrower sampling interval (Fig. 2 (d)).",
            "The sampling window is halved with each iteration, and this process is repeated for a specific number of iterations."
        ],
        "final_answer": "At each iteration, the Sample Frames module takes the current (potentially already halved) time window and uniformly samples a fixed number of frames at regular temporal intervals across that window. After the VLM identifies the closest frame, the module centers a new window around that frame, halves its duration, and then again uniformly samples the same number of frames within this narrowed window.",
        "relevant_elements": [
            "Sample Frames"
        ],
        "id": 880,
        "masked_question": "How does [mask1] module compute frame selection when halving the sampling window each iteration?",
        "masked_number": 1,
        "masked_elements": [
            "Sample Frames"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.17422v4_figure_2.png",
        "paperid": "2408.17422v4",
        "paper_path": "./papers/2408.17422v4.json",
        "figure_id": "2408.17422v4_figure_2.png",
        "caption": "Figure 2: The proposed pipeline for open-vocabulary video action localization using a VLM consists of the following steps: (a) Frames are sampled at regular intervals from a time window, covering the entire video in the first iteration. (b) The sampled frames are then tiled in an image with annotations indicating the time order of the frames. (c) This image is then fed into a VLM to identify the frames closest to a specific timing of an action (e.g., the start timing of an action). (d) The sampling window is updated by centering on the selected frame with a narrower sampling interval. Bottom panel (1) For general action localization, the start time of the action in the video is determined by iterating steps (a) to (d). Bottom panel (2) By estimating the end time of the action in the same manner, the action is localized in the video.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-Thought  \n1. In the figure, the red-boxed submodule is literally labeled “Sample Frames” and its job is to pick N frames at uniform time‐intervals across whatever window you give it.  \n2. At each T-PIVOT iteration you halve the window (by centering it on the previously chosen pivot frame), but you never change the rule for picking frames: you still want N samples equally spaced.  \n3. Concretely, if your current window runs from frame S to frame E, you compute the step size Δ = (E − S)/(N − 1), and then sample at  \n   S + 0·Δ, S + 1·Δ, S + 2·Δ, …, S + (N − 1)·Δ.  \n4. When you halve the window, E−S shrinks by 2×, so Δ shrinks by 2×, but you still pick those N evenly spaced offsets.  \n\nAnswer  \nThe “[mask1]” module is the Sample-Frames module, and it always divides the current (potentially halved) time window into N−1 equal intervals, then picks one frame at each of the N time‐stamped boundaries S + i·((E−S)/(N−1))."
    },
    {
        "question": "How does Adjust Sampling Window update interval boundaries based on the chosen frame index?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Finally, the sampling window is updated centering the selected frame with a narrower sampling interval (Fig. 2 (d)).",
            "The sampling window is halved with each iteration, and this process is repeated for a specific number of iterations."
        ],
        "final_answer": "After the VLM returns the index of the frame closest to the queried action point, the method recenters the next sampling time window on that frame and reduces its duration—specifically, it halves the previous interval around the chosen frame index. This updates both the start and end boundaries to focus the next round of sampling around the estimated action moment.",
        "relevant_elements": [
            "Adjust Sampling Window"
        ],
        "id": 881,
        "masked_question": "How does [mask1] update interval boundaries based on the chosen frame index?",
        "masked_number": 1,
        "masked_elements": [
            "Adjust Sampling Window"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.17422v4_figure_2.png",
        "paperid": "2408.17422v4",
        "paper_path": "./papers/2408.17422v4.json",
        "figure_id": "2408.17422v4_figure_2.png",
        "caption": "Figure 2: The proposed pipeline for open-vocabulary video action localization using a VLM consists of the following steps: (a) Frames are sampled at regular intervals from a time window, covering the entire video in the first iteration. (b) The sampled frames are then tiled in an image with annotations indicating the time order of the frames. (c) This image is then fed into a VLM to identify the frames closest to a specific timing of an action (e.g., the start timing of an action). (d) The sampling window is updated by centering on the selected frame with a narrower sampling interval. Bottom panel (1) For general action localization, the start time of the action in the video is determined by iterating steps (a) to (d). Bottom panel (2) By estimating the end time of the action in the same manner, the action is localized in the video.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:  \n1.  In step (c) the VLM picks out one of the N sampled tiles as the “pivot” frame—i.e. the frame index i* whose time is judged closest to the target event.  \n2.  Step (d) (the red–boxed panel) shows how we then “zoom in” on that pivot.  Rather than keep the old broad [tstart, tend] window, we build a new, narrower window centered on the time of frame i*.  \n3.  Concretely, if the old window spanned T seconds (or T frames) and we sampled N frames equally spaced, then the spacing between samples was Δ = T/N.  We take that same Δ and form a new window [t(i*)–Δ, t(i*)+Δ].  In other words, the next iteration’s start and end bounds are set to the chosen frame’s time ± one sampling gap.  \n4.  Repeating this halving strategy over iterations zeroes in on the precise action boundary.\n\nAnswer:  \n[mask1] re-centers the temporal window on the frame chosen by the VLM and shrinks its span by the previous sampling interval.  In practice the new start/end times become (time_of_chosen_frame – Δ, time_of_chosen_frame + Δ), where Δ is the old window length divided by the number of sampled frames."
    },
    {
        "question": "How does Targeted Contrastive Matching optimize perturbation using modality-aware embeddings for semantic alignment?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To learn the image perturbation at each step, we propose Targeted Contrastive Matching (TCM), where the cross-modality semantics of clean samples, target samples, and the current adversarial samples are aligned/diverged in the same latent embedding space.",
            "Specifically, TCM maximizes the similarity between the current adversarial example and the target reference example, while minimizing the similarity between the current adversarial example and the original clean example across both vision and text modalities.",
            "To optimize the image perturbation δ through the TCM objective L_TCM, projected gradient descent [33] is adopted and the optimization can be expressed as:\nwhere Π projects δ back into the ℓ∞-ball, α is the step size, and ∇_δ L_TCM is the gradient of the TCM loss."
        ],
        "final_answer": "Targeted Contrastive Matching (TCM) operates in the joint modality-aware embedding space of images and text. At each attack step it pulls the adversarial example’s embedding closer to the target reference embedding and simultaneously pushes it away from the original clean embedding. This contrastive objective is optimized by taking gradient steps on the image perturbation δ, and then projecting δ back into the allowed perturbation budget via projected gradient descent.",
        "relevant_elements": [
            "Targeted Contrastive Matching",
            "modality-aware embeddings"
        ],
        "id": 882,
        "masked_question": "How does [mask1] optimize perturbation using [mask2] for semantic alignment?",
        "masked_number": 2,
        "masked_elements": [
            "Targeted Contrastive Matching",
            "modality-aware embeddings"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15720v1_figure_2.png",
        "paperid": "2411.15720v1",
        "paper_path": "./papers/2411.15720v1.json",
        "figure_id": "2411.15720v1_figure_2.png",
        "caption": "Figure 2: The pipeline of the Chain of Attack (CoA) framework. (a) Our framework proposes using modality-aware embeddings to capture the semantic correspondence between images and texts. To enhance the adversarial transferability, we use a chain of attacks that explicitly updates the adversarial examples based on their previous multi-modal semantics in a step-by-step manner. A Targeted Contrastive Matching objective is further proposed to align and differentiate the semantics among clean, adversarial, and target reference examples. (b) Targeted response generation is conducted during inference, where the victim models give responses based on the adversarial examples. We further introduce a unified ASR computing strategy for automatic and comprehensive robustness evaluation of VLMs in response generation.",
        "qtype": "Implementation_Details",
        "response": "Here’s the high-level answer in plain language:\n\n• [mask1] is the “Targeted Contrastive Matching” (TCM) module (the red-boxed part).  \n• [mask2] refers to the modality-aware embeddings (MAEs) that mix image and text features (and the perturbation δ lives in that same embedding space).\n\nWhat TCM does is:\n\n1.  It takes three MAEs in a common embedding space:  \n    – The clean image–text MAE,  \n    – The current adversarial image MAE (which depends on δ), and  \n    – The target reference MAE.  \n\n2.  It defines a contrastive loss that simultaneously  \n    – maximizes the similarity between the adversarial MAE and the target MAE (pulls them together), and  \n    – minimizes the similarity between the adversarial MAE and the clean MAE (pushes them apart).  \n\n3.  It then back-propagates this TCM loss all the way down to the pixel-level perturbation δ.  In practice, one computes the gradient ∇₍δ₎ℓ_TCM and takes a projected gradient‐descent step to update δ within the allowed perturbation budget.\n\nBy repeatedly applying this “pull-toward-target / push-from-clean” gradient update on δ, TCM forces the adversarial example’s MAE to drift away from its original semantics and into the target semantics—i.e. explicit multi-modal alignment."
    },
    {
        "question": "How is learnable δ updated via Projected Gradient Descent within Chain of Attack iterations?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To optimize the image perturbation δ through the TCM objective L, projected gradient descent [33] is adopted and the optimization can be expressed as:\n\n    δ ← Πε (δ + α ∇δ L(δ))\n\nwhere Πε projects δ back into the ε-ball, α is the step size, and ∇δ L(δ) represents the gradient of the TCM loss."
        ],
        "final_answer": "Within each Chain of Attack iteration, the learnable perturbation δ is updated by taking a step in the direction of the TCM loss gradient and then projecting back into the allowed ε-ball: δ_{t+1} = Πε(δ_t + α ∇δ L_TCM(δ_t)), where α is the step size and Πε enforces the maximum perturbation budget.",
        "relevant_elements": [
            "learnable δ",
            "Projected Gradient Descent",
            "Chain of Attack"
        ],
        "id": 883,
        "masked_question": "How is [mask1] updated via [mask2] within Chain of Attack iterations?",
        "masked_number": 2,
        "masked_elements": [
            "learnable δ",
            "Projected Gradient Descent"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15720v1_figure_2.png",
        "paperid": "2411.15720v1",
        "paper_path": "./papers/2411.15720v1.json",
        "figure_id": "2411.15720v1_figure_2.png",
        "caption": "Figure 2: The pipeline of the Chain of Attack (CoA) framework. (a) Our framework proposes using modality-aware embeddings to capture the semantic correspondence between images and texts. To enhance the adversarial transferability, we use a chain of attacks that explicitly updates the adversarial examples based on their previous multi-modal semantics in a step-by-step manner. A Targeted Contrastive Matching objective is further proposed to align and differentiate the semantics among clean, adversarial, and target reference examples. (b) Targeted response generation is conducted during inference, where the victim models give responses based on the adversarial examples. We further introduce a unified ASR computing strategy for automatic and comprehensive robustness evaluation of VLMs in response generation.",
        "qtype": "Implementation_Details",
        "response": "In our Chain-of-Attack loop the red-boxed “learnable δ” is nothing more than the image‐space perturbation that we iteratively refine by back-propagating the blue-boxed gradient of the TCM loss.  Concretely, at step t we form  \n    xᵃᵈᵛₜ = x + δₜ  \nrun it through the surrogate encoders, compute the targeted‐contrastive‐matching loss L_TCM, take  \n    gₜ = ∇ₓ L_TCM(x + δₜ)  \nand then do one projected‐gradient‐descent update:  \n    δₜ₊₁ = Proj_{‖·‖∞ ≤ ε}\\!\\bigl(δₜ − η · gₜ\\bigr)  \nso that δ stays within the ε-ball.  In this way the red-boxed δ is directly driven by the blue-boxed gradient every iteration."
    },
    {
        "question": "How does modality-aware embedding influence Targeted Contrastive Matching's alignment between clean and target representations?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We use modality fusion of embeddings to capture the semantic correspondence between images and texts, the modality fusion for the clean and target image-text pairs can be achieved by the following calculations: … Where m_V and m_T are the modality-aware embeddings (MAE) for clean and target image-text pairs, respectively. λ is a modality-balancing hyperparameter.",
            "Specifically, TCM maximizes the similarity between the current adversarial example and the target reference example, while minimizing the similarity between the current adversarial example and the original clean example across both vision and text modalities."
        ],
        "final_answer": "By fusing vision and language encoder outputs into modality-aware embeddings (m_V for clean and m_T for target), the Chain of Attack framework provides unified joint representations. Targeted Contrastive Matching then operates directly on these fused embeddings—pulling the adversarial example’s embedding closer to the target MAE (m_T) while pushing it away from the clean MAE (m_V)—thereby aligning the adversarial perturbation toward the target semantics and away from the original clean semantics in a single shared embedding space.",
        "relevant_elements": [
            "modality-aware embedding",
            "Targeted Contrastive Matching"
        ],
        "id": 884,
        "masked_question": "How does [mask1] influence [mask2]'s alignment between clean and target representations?",
        "masked_number": 2,
        "masked_elements": [
            "modality-aware embedding",
            "Targeted Contrastive Matching"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15720v1_figure_2.png",
        "paperid": "2411.15720v1",
        "paper_path": "./papers/2411.15720v1.json",
        "figure_id": "2411.15720v1_figure_2.png",
        "caption": "Figure 2: The pipeline of the Chain of Attack (CoA) framework. (a) Our framework proposes using modality-aware embeddings to capture the semantic correspondence between images and texts. To enhance the adversarial transferability, we use a chain of attacks that explicitly updates the adversarial examples based on their previous multi-modal semantics in a step-by-step manner. A Targeted Contrastive Matching objective is further proposed to align and differentiate the semantics among clean, adversarial, and target reference examples. (b) Targeted response generation is conducted during inference, where the victim models give responses based on the adversarial examples. We further introduce a unified ASR computing strategy for automatic and comprehensive robustness evaluation of VLMs in response generation.",
        "qtype": "Literature_Background",
        "response": "Here’s what’s going on, step by step:\n\n1.  The red‐boxed module (“modality-aware embedding & perturbation”) takes a clean image–text pair and a target image–text pair, runs each through the frozen surrogate encoders, and fuses their two embeddings into single multi-modal vectors  \n      z_clean  =  λ f_i(clean)  +  (1–λ) f_t(clean)  \n      z_target =  λ f_i(target) +  (1–λ) f_t(target)  \n   It also maintains a small, learnable pixel‐level perturbation δ that, when added to the clean image, produces the adversarial image whose embedding we call z_adv.\n\n2.  The blue‐boxed module is the Targeted Contrastive Matching (TCM) loss.  TCM looks at exactly those three embeddings z_clean, z_adv( = z_clean+δ ), and z_target and does two things at once:  \n    – “Pull” z_adv closer to z_target  \n    – “Push” z_adv away from z_clean  \n\n3.  Thus the red box *feeds* the blue box its anchors and positives/negatives.  By constructing z_clean and z_target via fused, modality-aware embeddings (and injecting the current δ into z_adv), it defines the exact vectors whose pairwise similarities the contrastive loss will increase or decrease.  In other words, the modality-aware embeddings plus perturbation *are* the representations that TCM aligns (toward the target) and de-aligns (away from the clean)."
    },
    {
        "question": "How does sample relabeling integrate with Otsu thresholding to differentiate high- and low-confidence subsets?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "ANNE starts with a sample relabeling procedure [13  ###reference_b13###] using a threshold value τ to threshold the maximum prediction score for each training sample xᵢ, as follows: where the function onehot transforms the categorical distribution produced by f into a one-hot vector of size c, with the class containing the maximum probability assigned to yᵢ*, and all other classes assigned to 0.",
            "Next, ANNE runs the sample selection stage. Initially, we use the maximum prediction scores to divide the samples into two subsets, leveraging the Otsu’s algorithm [30  ###reference_b30###], as in [21  ###reference_b21###]. The first step is to determine a threshold value, denoted as θ, which splits D′ into two subsets, the high-confidence subset (HCS) D_HCS and the low-confidence subset (LCS) D_LCS."
        ],
        "final_answer": "ANNE first relabels each sample by converting its network prediction to a one-hot label when its maximum class probability exceeds a tunable threshold τ. Then it collects the maximum prediction scores of all samples and applies Otsu’s algorithm to find an optimal cutoff θ. Samples with max scores above θ form the high-confidence subset (HCS), and those below θ form the low-confidence subset (LCS).",
        "relevant_elements": [
            "Sample Relabelling",
            "Otsu thresholding"
        ],
        "id": 886,
        "masked_question": "How does [mask1] integrate with Otsu thresholding to differentiate high- and low-confidence subsets?",
        "masked_number": 1,
        "masked_elements": [
            "Sample Relabelling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01613v1_figure_2.png",
        "paperid": "2411.01613v1",
        "paper_path": "./papers/2411.01613v1.json",
        "figure_id": "2411.01613v1_figure_2.png",
        "caption": "Figure 2: Our ANNE sample selection strategy starts with a sample relabeling approach [13], followed by a sample selection stage that first divides the training set into high-confidence (𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT) and low-confidence (𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT) subsets, according to the classification probability of the samples. Then, the samples from 𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an adaptive KNN approach, while samples from 𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an eigen-decomposition technique. As shown in the rightmost figure, our proposed adaptive KNN automatically changes the range to find the K𝐾Kitalic_K nearest neighbors, depending on the density of the sample in the feature space.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the “Sample Re-labelling” box in the red highlight.  What it does is (a) take the CNN’s soft‐max outputs, turn them into one-hot pseudo-labels (by picking the arg max class) and record each sample’s maximum prediction score, and then (b) hand that array of max-scores to Otsu’s method.  Otsu’s algorithm treats the histogram of those max-scores as a two-class problem, finds the threshold τ that maximizes between-class variance, and uses τ to split the data into:  \n •  Low-confidence samples (those whose max-score ≤ τ, LCS)  \n •  High-confidence samples (those whose max-score > τ, HCS)"
    },
    {
        "question": "How does sample relabeling influence clean versus noisy selection using Adaptive KNN and Eigen Decomposition?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "relevant_context": [
            "ANNE starts with a sample relabeling procedure [13] using a threshold value t to threshold the maximum prediction score for each training sample xᵢ, as follows: … the function R transforms the categorical distribution produced by f into a one-hot vector of size C, with the class containing the maximum probability assigned to y′ᵢ, and all other classes assigned to 0.",
            "Next, ANNE runs the sample selection stage. Initially, we use the maximum prediction scores to divide the samples into two subsets, leveraging Otsu’s algorithm [30]. The first step is to determine a threshold value ζ, which splits D′ into two subsets, the high-confidence subset (HCS) D_H and the low-confidence subset (LCS) D_L. To enable a more effective selection of clean and noisy samples from the subsets D_L and D_H, we apply a method robust to large noise rate problems to the LCS subset D_L, and a method robust to small noise rate problems to the HCS subset D_H. In particular, we use Eigen Decomposition for D_H, whereas for D_L we apply adaptive KNN.",
            "Adaptive KNN (AKNN): …We propose an adaptive k-nearest neighbour for noisy labels, where the value of k varies according to the local density in the feature space. … Samples with labels matching the prediction from the KNN classifier … are classified as clean, while those samples that do not match the KNN prediction are classified as noisy.",
            "Eigen Decomposition (FINE): FINE finds clean and noisy-label instances using the square of inner products between the image features produced by f and the dominant eigenvector computed from the features belonging to the same class. … we treat the sample as clean if it is aligned with the most dominant eigenvector, while most of the noisy-label samples tend not to be as well aligned. FINE uses a threshold parameter α to select the samples based on such inner product."
        ],
        "final_answer": "By first relabeling each sample’s softmax output into a hard pseudo-label (using a confidence threshold), ANNE then uses those scores to split the data into a high-confidence group (HCS) and a low-confidence group (LCS). In the high-confidence group—where relabelled predictions are already strong—it applies Eigen Decomposition (the FINE algorithm) to mark as clean any samples whose features align above a threshold with the class’s dominant eigenvector, and noisy otherwise. In the low-confidence group—where relabelled labels are more likely corrupted—it uses Adaptive KNN: it sets k based on local feature‐density and labels a sample clean if the majority of its k neighbours (under its relabelled pseudo-label) agree, noisy otherwise.",
        "relevant_elements": [
            "sample relabeling",
            "Adaptive KNN",
            "Eigen Decomposition"
        ],
        "id": 888,
        "masked_question": "How does [mask1] influence clean versus noisy selection using Adaptive KNN and Eigen Decomposition?",
        "masked_number": 1,
        "masked_elements": [
            "sample relabeling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01613v1_figure_2.png",
        "paperid": "2411.01613v1",
        "paper_path": "./papers/2411.01613v1.json",
        "figure_id": "2411.01613v1_figure_2.png",
        "caption": "Figure 2: Our ANNE sample selection strategy starts with a sample relabeling approach [13], followed by a sample selection stage that first divides the training set into high-confidence (𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT) and low-confidence (𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT) subsets, according to the classification probability of the samples. Then, the samples from 𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an adaptive KNN approach, while samples from 𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an eigen-decomposition technique. As shown in the rightmost figure, our proposed adaptive KNN automatically changes the range to find the K𝐾Kitalic_K nearest neighbors, depending on the density of the sample in the feature space.",
        "qtype": "Experimental_Results",
        "response": "In our framework the red‐boxed module is the “Sample Relabelling’’ step.  What it does is:\n\n  1.  It looks at each network prediction’s top softmax score and, whenever that score exceeds a fixed threshold α, it throws away the original (possibly noisy) label and replaces it with the model’s own one‐hot prediction.  Samples whose top score remains below α keep their original label.  \n  2.  Because of (1), the distribution of maximum scores over the entire training set (now called 𝒟′) becomes more bi‐modal: truly clean, high‐confidence items have been “self‐labelled’’ by the network and end up with large scores, whereas ambiguous or still‐noisy items stay lower.  We then run Otsu’s method to split 𝒟′ into a high‐confidence subset (HCS) and a low‐confidence subset (LCS).  \n  3.  The rearranged score distribution dictates which samples go to which downstream selector:\n     –  HCS (where most labels are already correct) is handed to the Eigen-Decomposition (FINE) routine, which refines the clean/noisy split by measuring alignment with the principal eigenvector of each class.  \n     –  LCS (where labels are still suspect) is sent to the Adaptive KNN module, which adaptively picks a local K for each example based on feature‐space density and votes on whether the assigned label matches its neighbours.  \n\nIn short, by relabelling all very‐confident predictions up front, we sharpen the gap between “probably clean’’ and “probably noisy’’ in the score histograms.  That in turn makes the Eigen‐Decomposition step very reliable on the clean side, and hands the more doubtful cases over to the density-aware KNN to weed out residual label noise."
    },
    {
        "question": "How does Adaptive KNN adjust neighbor search radius based on local feature density?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Unlike traditional KNN approaches, we propose an adaptive KNN method, where the number of nearest neighbors, denoted by k, varies depending on the local density of the training sample in the feature space F.",
            "We initially retrieve the nearest neighbours based on the samples with cosine similarity above threshold ε, for each sample x_i, forming the set N_i.",
            "We initially set ε to a high value and iteratively reduce it, while observing the number of neighbours |N_i| for each sample x_i.",
            "The minimum value of ε is defined as ε_min, and we control the number of neighbours by decrementing ε."
        ],
        "final_answer": "Adaptive KNN starts with a high cosine‐similarity threshold ε (small radius) and for each sample x_i it collects neighbours whose similarity exceeds ε. It then gradually lowers ε (thereby expanding the search radius) until a sufficient number of neighbours is found, stopping at a preset minimum ε_min. In dense regions only a few reductions are needed (keeping a small radius), while in sparse regions ε must be lowered further (expanding the radius), so that the neighborhood size adapts to local feature density.",
        "relevant_elements": [
            "Adaptive KNN"
        ],
        "id": 889,
        "masked_question": "How does [mask1] adjust neighbor search radius based on local feature density?",
        "masked_number": 1,
        "masked_elements": [
            "Adaptive KNN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01613v1_figure_2.png",
        "paperid": "2411.01613v1",
        "paper_path": "./papers/2411.01613v1.json",
        "figure_id": "2411.01613v1_figure_2.png",
        "caption": "Figure 2: Our ANNE sample selection strategy starts with a sample relabeling approach [13], followed by a sample selection stage that first divides the training set into high-confidence (𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT) and low-confidence (𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT) subsets, according to the classification probability of the samples. Then, the samples from 𝒟L⁢C⁢Ssubscript𝒟𝐿𝐶𝑆\\mathcal{D}_{LCS}caligraphic_D start_POSTSUBSCRIPT italic_L italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an adaptive KNN approach, while samples from 𝒟H⁢C⁢Ssubscript𝒟𝐻𝐶𝑆\\mathcal{D}_{HCS}caligraphic_D start_POSTSUBSCRIPT italic_H italic_C italic_S end_POSTSUBSCRIPT will be divided into clean or noisy-label using an eigen-decomposition technique. As shown in the rightmost figure, our proposed adaptive KNN automatically changes the range to find the K𝐾Kitalic_K nearest neighbors, depending on the density of the sample in the feature space.",
        "qtype": "Experimental_Results",
        "response": "Here, [mask1] is the Adaptive-KNN (AKNN) module.  AKNN does *not* use a fixed “k” or radius everywhere.  Instead it first looks at how many neighbours a point already has inside a tight, high-similarity cone.  –  If that count is large (i.e. the point sits in a high-density region), it leaves the cone (and thus its search radius) alone.  \n–  If the count is too small (i.e. low-density), it *lowers* the cosine-similarity threshold (equivalently *expands* its search radius) until it collects enough neighbours.  \n\nIn practice AKNN initializes with a high similarity cutoff and then iteratively relaxes that cutoff—expanding the effective radius—only in sparse areas, while keeping it small in dense clusters."
    },
    {
        "question": "How does prototype-based skill retrieval compensate missing sub-goal demonstrations during CiL stages?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "In the two-level policy hierarchy, we employ a skill prototype-based approach, in which skill prototypes capture the sequential patterns of actions and associated environmental states, as observed from expert demonstrations. These prototypes serve as a reference for skills learned from a multi-stage data stream. Through this prototype-based skill retrieval method, the policy flexibly uses skills that are shareable among tasks, potentially learned in the past or future, for policy evaluation.",
            "To facilitate skill retrieval from demonstrations, we encode observation and goal pairs (o_t, g_t) into state embeddings using a function f. We employ a skill retriever r. For this, we use multifaceted skill prototypes P, where P is the set of learned skill prototypes. These prototypes capture the sequential patterns of expert demonstrations associated with specific goal-reaching tasks. The similarity function s is defined as the maximum similarity between the current state embedding and each prototype’s bases. At inference time, the retriever compares the current (o_t, g_t) embedding against all prototypes and selects the most similar one; its associated adapter parameters are then fed to the decoder to produce the missing action segment."
        ],
        "final_answer": "When a demonstration at a given CiL stage is missing one or more sub-goal segments, the system encodes the current observation–goal pair into a state embedding and computes its similarity to a bank of learned skill prototypes. Each prototype summarizes the action–state patterns of a sub-goal from previous stages. By selecting the prototype whose bases are most similar to the current embedding, the framework retrieves the corresponding adapter parameters and feeds them to the skill decoder. This effectively “fills in” the missing sub-goal demonstration by reusing a previously learned skill that best matches the partial sequence, allowing successful completion despite incomplete demonstrations.",
        "relevant_elements": [
            "Prototype-based skill incremental learning",
            "Skill Retriever"
        ],
        "id": 890,
        "masked_question": "How does [mask1] compensate missing sub-goal demonstrations during CiL stages?",
        "masked_number": 1,
        "masked_elements": [
            "Skill Retriever"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.22658v1_figure_1.png",
        "paperid": "2410.22658v1",
        "paper_path": "./papers/2410.22658v1.json",
        "figure_id": "2410.22658v1_figure_1.png",
        "caption": "Figure 1: \nThe scenario demonstrating how IsCiL enhances continual imitation learning efficiency through retrievable skills:\n(i) Prototype-based skill incremental learning: despite the failure of τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, skills are incrementally learned from the available demonstrations. In later stages, missing skills for τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT are retrieved from other tasks, achieving the resolution of τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and illustrating the reversibility and efficiency of retrievable skills.\n(ii) Task-wise selective adaptation: IsCiL effectively retrieves relevant learned skills, facilitating rapid task adaptation.",
        "qtype": "Experimental_Results",
        "response": "During CiL training we never re-record or hallucinate new expert trajectories – instead, whenever a sub-goal is simply “missing” from the current demo, IsCiL looks back into its library of skill prototypes, finds the one whose state–goal signature best matches that gap, pulls in the corresponding adapter parameters, and then decodes those into low-level actions to literally “stitch in” the missing segment. In other words, the red-boxed module is just prototype-based skill retrieval: it fills any blanked–out sub-goal by reusing a similar skill learned at an earlier (or later) stage."
    },
    {
        "question": "How does task-wise selective adaptation use retrievable skills for rapid unseen task execution?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "In (ii) the task-wise selective adaptation, we devise efficient task adaptation procedures in the policy hierarchy to adapt to specific tasks using incrementally learned skills. This enables the CiL agent to not only facilitate adaptation to shifts in task distribution (e.g., due to non-stationary environment conditions) but also support task unlearning upon explicit user request (e.g., due to privacy concerns).",
            "The evaluation process adapts to novel tasks and sub-goal sequences from the environment by modifying the goal g. This adjustment enables the inference of appropriate current actions, in a manner of similar to handling learned tasks."
        ],
        "final_answer": "Task-wise selective adaptation executes rapid unseen task inference by taking the new sub-goal sequence as input, passing it through the skill retriever to fetch the corresponding learned skill prototypes, and then feeding those retrieved adapters into the skill decoder to immediately produce the correct actions—allowing the agent to perform novel tasks on the fly without any extra training.",
        "relevant_elements": [
            "Task-wise selective adaptation",
            "Skill Retriever"
        ],
        "id": 891,
        "masked_question": "How does [mask1] use [mask2] for rapid unseen task execution?",
        "masked_number": 2,
        "masked_elements": [
            "Task-wise selective adaptation",
            "Skill Retriever"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.22658v1_figure_1.png",
        "paperid": "2410.22658v1",
        "paper_path": "./papers/2410.22658v1.json",
        "figure_id": "2410.22658v1_figure_1.png",
        "caption": "Figure 1: \nThe scenario demonstrating how IsCiL enhances continual imitation learning efficiency through retrievable skills:\n(i) Prototype-based skill incremental learning: despite the failure of τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT, skills are incrementally learned from the available demonstrations. In later stages, missing skills for τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT are retrieved from other tasks, achieving the resolution of τ1subscript𝜏1\\tau_{1}italic_τ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and illustrating the reversibility and efficiency of retrievable skills.\n(ii) Task-wise selective adaptation: IsCiL effectively retrieves relevant learned skills, facilitating rapid task adaptation.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify the two masked phrases:\n\n  • mask₁ (red box) is the panel title: “Task-wise selective adaptation.”  \n  • mask₂ (blue box) is the output of that panel: the “Retrieved skills” (i.e. the small set of skill prototypes the retriever has selected for the new task).\n\nNow, how does Task-wise selective adaptation use those Retrieved skills to execute a never-seen task so quickly?\n\n  1. At inference time we are handed an unseen task τᵤ, which is simply a new sequence of sub-goals (g₆, g₂, g₃, g₅ in the figure).  \n  2. The Skill Retriever scans its library of skill prototypes and, via its learned similarity function, pulls out exactly the handful of adapters whose prototypes best match each of the requested sub-goals—the blue-boxed “Retrieved skills.”  \n  3. Those retrieved adapters are then plugged straight into the Skill Decoder (no further fine-tuning), which emits the short-horizon actions needed to satisfy each sub-goal in turn.  \n  4. Because we reuse pre-learned, modular skills rather than training from scratch, the agent can immediately “chain together” the right primitives and complete the new task τᵤ in one shot."
    },
    {
        "question": "What limitations arise from FiLM-based EEG adapter versus cross-attention methods for embedding injection?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "Previous works, e.g., [19] have demonstrated that FiLM is more suitable for incorporating category information (e.g., EEG signals), while the cross-attention method is more applicable to sequential information (e.g., text description).",
            "In addition, compared to the additional 15% Gflops overhead introduced by cross-attention, the computational cost associated with FiLM is almost negligible [19]."
        ],
        "final_answer": "By using FiLM rather than cross-attention to inject EEG embeddings, the model gains almost negligible extra compute cost, but at the expense of losing cross-attention’s superior handling of sequential or temporal conditioning. In other words, FiLM is well suited to static, category‐style signals like EEG summaries but is less effective than cross-attention at modeling sequence‐dependent information.",
        "relevant_elements": [
            "EEG adapter",
            "FiLM",
            "cross-attention"
        ],
        "id": 894,
        "masked_question": "What limitations arise from FiLM-based [mask1] versus [mask2] methods for embedding injection?",
        "masked_number": 2,
        "masked_elements": [
            "EEG adapter",
            "cross-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.14021v1_figure_2.png",
        "paperid": "2409.14021v1",
        "paper_path": "./papers/2409.14021v1.json",
        "figure_id": "2409.14021v1_figure_2.png",
        "caption": "Figure 2: Overview of our BrainDreamer. After aligning the EEG signals, images, and text using a mask-based triple contrastive learning strategy, we design an EEG adapter based on the trained EEG encoder. The EEG adapter employs the FiLM to modulate EEG embeddings. Then, the EEG and text embeddings are fed into pre-trained Stable Diffusion to generate reasoning-coherent images.",
        "qtype": "Others",
        "response": "Here is a step-by-step breakdown of what the two different injection schemes (the red-boxed FiLM adapter vs. the blue-boxed cross‐attention) buy you—and what they give up:\n\n1. Identify the two blocks  \n   • Mask₁ (red box) = the FiLM-style EEG adapter.  It simply computes a learned scale and shift from the projected EEG embedding, then applies those channel-wise to the UNet feature maps at each layer.  \n   • Mask₂ (blue box) = a frozen cross-attention block that attends from UNet queries into the text (or in other methods into the EEG) embedding.\n\n2. Computational cost  \n   • FiLM (mask₁): almost zero extra GFLOPs over the vanilla network.  \n   • Cross-attention (mask₂): on the order of +15 % GFLOPs per forward pass if you add a new attention head for EEG.\n\n3. Expressive power / flexibility  \n   • FiLM: the only degrees of freedom are per-channel scale and bias.  It cannot selectively re-weight individual spatial locations, nor model any higher‐order interactions between features and timesteps.  \n   • Cross-attention: it can “look” at every position in the conditioning embedding (text or EEG) and dynamically re-distribute attention back to each pixel or feature location in the UNet.  That makes it much better at handling sequential or structured signals.\n\n4. Best use cases  \n   • FiLM is well suited to injecting a compact, static code (e.g. an object category or a small EEG summary vector).  If your conditioning is non-sequential and you just want a lightweight bias, it works.  \n   • Cross-attention is almost mandatory for anything that has internal order or needs fine‐grained alignment—e.g. full text prompts, video frames, or a time series you want to align timestep by timestep.\n\n5. The key limitation trade-off  \n   – FiLM (mask₁) trades away modeling capacity (no per-token or per-position routing of information) in exchange for near-zero extra compute.  \n   – Cross-attention (mask₂) retains full flexibility to route and attend to fine-grained structure in your embedding, at the cost of substantially more FLOPs and memory."
    },
    {
        "question": "How might text encoder biases influence mask-based triple contrastive learning and affect EEG adapter outputs ethically?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "text encoder",
            "mask-based triple contrastive learning",
            "EEG adapter"
        ],
        "id": 895,
        "masked_question": "How might [mask1] biases influence mask-based triple contrastive learning and affect EEG adapter outputs ethically?",
        "masked_number": 1,
        "masked_elements": [
            "text encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.14021v1_figure_2.png",
        "paperid": "2409.14021v1",
        "paper_path": "./papers/2409.14021v1.json",
        "figure_id": "2409.14021v1_figure_2.png",
        "caption": "Figure 2: Overview of our BrainDreamer. After aligning the EEG signals, images, and text using a mask-based triple contrastive learning strategy, we design an EEG adapter based on the trained EEG encoder. The EEG adapter employs the FiLM to modulate EEG embeddings. Then, the EEG and text embeddings are fed into pre-trained Stable Diffusion to generate reasoning-coherent images.",
        "qtype": "Others",
        "response": "First, note that the little red box in the diagram (our [mask₁]) is simply the CLIP text encoder.  During Stage 1 we lock the image and text encoders and only train the EEG encoder by pulling its embeddings into alignment with both the CLIP image space and the CLIP text space via that “mask-based triple contrastive” loss.  In effect, we treat the fixed CLIP text embeddings as a kind of “ground truth” anchor for semantics.\n\nBecause CLIP’s text encoder itself carries all of the biases that went into its pre-training corpus (gender and racial stereotypes, cultural imbalances, over- or under-representation of certain objects, activities, or communities, etc.), those same distortions become baked into the triple-contrastive objective.  Concretely:\n\n 1.  If CLIP’s text encoder systematically associates “doctor” with masculine pronouns, or “nurse” with feminine ones, then the triple-contrastive loss will push the EEG encoder to encode any brain patterns it sees during “doctor” trials toward those same skewed masculine-doctor text embeddings—regardless of the actual subject’s identity.  \n 2.  If the text encoder under-represents certain ethnicities, or uses negative connotations more often with some demographic groups, the EEG embeddings will be warped toward those same lopsided regions of the joint embedding space.  \n\nIn Stage 2, the EEG adapter simply FiLM-scales and shifts neural activations in Stable Diffusion according to the EEG embedding learned under that biased alignment.  The downstream effect is that the final images will reflect (and often magnify) the original text-encoder biases: \n\n •  Certain classes of imagined objects may be over- or under-represented.  \n •  People or faces may be rendered with stereotyped skin tones, clothing, or activities.  \n •  Ambiguous imagery (e.g. “doctor,” “teacher,” “athlete”) will tend to default to whatever biased prototype the text encoder learned.  \n\nEthically this is a real concern: we end up amplifying social stereotypes, mis-representing individuals (especially from marginalized groups), and potentially violating consent if the subject’s own self-image or cultural identity is mis-generated.  \n\nTo mitigate:\n\n •  One must audit and, if possible, de-bias the frozen text encoder (e.g. apply post-hoc debiasing or use a more balanced language model).  \n •  Ensure the contrastive training set is extensively balanced across gender, race, age, etc., so that no single group dominates the loss.  \n •  Consider replacing the text anchor with a more carefully curated, fairness-aware semantic layer or injecting explicit fairness constraints into the contrastive loss.  \n •  Provide subject control and appeal paths: allow the end-user to correct or override any generated content that they find mis-representative.  \n\nOnly by recognizing that the frozen CLIP text encoder is carrying a large, un-checked set of social biases can we begin to guard the entire BrainDreamer pipeline against reinforcing or amplifying those biases in its EEG-to-image outputs."
    },
    {
        "question": "What reasoning drives using mask-based triple contrastive learning to align EEG, image, and text embeddings?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Previous works [2,26] have attempted to align the EEG embeddings with the CLIP image embeddings, and then input the EEG embeddings to the generative model for image generation. We believe that such image-EEG alignment is incomplete due to the following reasons: 1) Most generative models are text-to-image models (e.g., Imagen and Stable Diffusion), which are more sensitive to text embeddings. Despite CLIP’s impressive image-text representation capabilities, there still exists a certain degree of domain gap between image and text modalities. 2) Textual information offers greater flexibility and controllability, which is why text embeddings are often considered as “ground truth” in most multi-modal works [37,32]. Therefore, we also incorporate textual information to map EEG embeddings to the CLIP embedding space in addition to image information. We design a mask-based triple contrastive learning strategy for embedding alignment.",
            "In addition, we employ masked modeling on the image and EEG data. This not only enhances the robustness of the features but also reduces training costs."
        ],
        "final_answer": "They found that aligning EEG only to image embeddings is insufficient because modern diffusion models are text-to-image and depend heavily on text embeddings, and because CLIP’s image–text space still has a domain gap. Text embeddings also provide richer, more controllable semantic supervision often treated as “ground truth.” Consequently, they extend contrastive alignment to include EEG, image, and text jointly, and apply masking on the EEG and image inputs to improve robustness and cut training overhead.",
        "relevant_elements": [
            "mask-based triple contrastive learning",
            "Image Encoder",
            "Text Encoder"
        ],
        "id": 896,
        "masked_question": "What reasoning drives using [mask1] to align EEG, [mask2], and text embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "mask-based triple contrastive learning",
            "Image Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.14021v1_figure_2.png",
        "paperid": "2409.14021v1",
        "paper_path": "./papers/2409.14021v1.json",
        "figure_id": "2409.14021v1_figure_2.png",
        "caption": "Figure 2: Overview of our BrainDreamer. After aligning the EEG signals, images, and text using a mask-based triple contrastive learning strategy, we design an EEG adapter based on the trained EEG encoder. The EEG adapter employs the FiLM to modulate EEG embeddings. Then, the EEG and text embeddings are fed into pre-trained Stable Diffusion to generate reasoning-coherent images.",
        "qtype": "Design_Rationale",
        "response": "Here is the step‐by‐step reasoning that explains why the authors chose to use a “mask-based triple contrastive learning” loss (the red-boxed module) to align EEG, image, and text embeddings (the blue-boxed image/text inputs):\n\n1.  What do we want to align?  \n    –  We have three kinds of embeddings:  \n       a) EEG embeddings (to be learned by our EEG encoder)  \n       b) CLIP image embeddings (obtained from a frozen CLIP image encoder on the observed picture)  \n       c) CLIP text embeddings (from a frozen CLIP text encoder on the associated caption)  \n\n2.  Why not just align EEG → image embeddings?  \n    –  Stable Diffusion (and most modern generative models) are fundamentally text–to–image models and are far more sensitive to text prompts than to pure visual features.  \n    –  CLIP’s joint image-text space is excellent, but there remains a non‐trivial “image ↔ text” domain gap.  If we only line up EEG with images, we miss out on the rich, controllable semantics that the text branch provides.  \n\n3.  Why bring in the text embeddings at all?  \n    –  In multimodal learning the text side often serves as the semantic “gold standard” because text can flexibly describe not only “what” is in a scene but also attributes like color, position, background, etc.  \n    –  By forcing EEG features to be close to both the image and the text vectors in CLIP’s space, we ensure that EEG-driven generations will inherit both low-level visual consistency (from the image side) and high-level descriptive control (from the text side).  \n\n4.  Why “mask-based” contrastive?  \n    –  Randomly masking parts of the image and the EEG sequence during training (i) acts as a kind of data augmentation that makes the learned EEG features more robust, and (ii) reduces the overall compute needed (because you don’t process every patch or every EEG time-step in full).  \n\n5.  Putting it all together:  \n    –  The red box (“Mask-based Triple Contrastive Learning”) implements a loss that, over a batch of triplets \\((\\text{EEG},\\,\\text{Image},\\,\\text{Text})\\), (a) randomly masks portions of the image and EEG, (b) encodes each modality, and (c) pulls the three resulting vectors together in the shared CLIP space.  \n    –  This guarantees that the EEG embeddings lie in the same joint space that Stable Diffusion expects—one that tightly couples visual consistency (from the image encoder) with semantic controllability (from the text encoder).  \n\nAnswer: They introduce mask-based triple contrastive learning to simultaneously align EEG embeddings with CLIP image embeddings and CLIP text embeddings because (1) generative diffusion models are more text-driven and text provides richer semantics than images alone, (2) there is still an image–text domain gap even in CLIP, and (3) applying masking makes the learned features more robust and lowers training cost."
    },
    {
        "question": "What motivates freezing EEG encoder parameters during reconstruction stage?",
        "relevant_section_ids": [
            "2.2",
            "2.4"
        ],
        "relevant_context": [
            "In Stage 2 we construct an EEG adapter, consisting of an EEG encoder and a feature projection module, where the parameter weights of the EEG encoder remain fixed.",
            "As shown in Fig. 2, the EEG adapter consists of a frozen EEG encoder and a feature projection module.",
            "Compared to the additional 15% Gflops overhead introduced by cross-attention, the computational cost associated with FiLM is almost negligible."
        ],
        "final_answer": "During the reconstruction stage the EEG encoder’s weights are frozen so that the embedding mapping learned in the first (alignment) stage is preserved, and only the lightweight feature-projection module (injected via FiLM) needs to be trained, keeping the computational overhead almost negligible.",
        "relevant_elements": [
            "EEG Encoder",
            "Stage2: Reconstruction"
        ],
        "id": 897,
        "masked_question": "What motivates freezing [mask1] parameters during [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "EEG Encoder",
            "Stage2: Reconstruction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.14021v1_figure_2.png",
        "paperid": "2409.14021v1",
        "paper_path": "./papers/2409.14021v1.json",
        "figure_id": "2409.14021v1_figure_2.png",
        "caption": "Figure 2: Overview of our BrainDreamer. After aligning the EEG signals, images, and text using a mask-based triple contrastive learning strategy, we design an EEG adapter based on the trained EEG encoder. The EEG adapter employs the FiLM to modulate EEG embeddings. Then, the EEG and text embeddings are fed into pre-trained Stable Diffusion to generate reasoning-coherent images.",
        "qtype": "Design_Rationale",
        "response": "Let me walk through how I landed on the answer:\n\n1.  Identify the two masked regions in the figure.  \n    –  [mask1] (the red-boxed block) is the EEG encoder that was trained in Stage 1 by mask-based triple contrastive learning to map raw EEG signals into the CLIP embedding space.  \n    –  [mask2] (the blue-boxed region) is Stage 2 (“Reconstruction”), where we plug an “EEG adapter” into the frozen Stable Diffusion backbone to actually generate images.\n\n2.  Notice what the paper says about the EEG adapter in Stage 2 (Sec. 3.4):  \n    –  “the EEG adapter consists of a *frozen* EEG encoder and a feature projection module.”  \n    –  Only the small projection module (two linears, a norm, an activation) is trainable at this stage.\n\n3.  Why freeze the EEG encoder?  \n    a.  We’ve *already* spent Stage 1 carefully teaching that EEG encoder to produce CLIP-compatible embeddings by contrastive alignment with images and text.  \n    b.  If we were to continue updating it alongside a huge diffusion model, we risk *unlearning* that hard-won alignment.  \n    c.  Freezing it keeps the representation space stable and lets us focus the Stage 2 training budget on the tiny FiLM-projection layers.  \n    d.  This both (i) cuts down on compute/overfitting (only a few thousand adapter parameters need to move), and (ii) preserves the EEG→CLIP mapping so that FiLM modulation really “means” something to the frozen Stable Diffusion network.\n\nAnswer  \nWhat motivates freezing the EEG encoder during Stage 2 is precisely to preserve the embedding alignment it learned in Stage 1 (so the EEG features remain in the CLIP space) and to drastically cut down the number of trainable parameters/compute in the reconstruction stage—only the lightweight projection (FiLM) layers are updated to inject EEG signals into the frozen diffusion model."
    },
    {
        "question": "What drives integrating CQT transform with STFT amplitude spectrogram and Mel spectrogram?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The Mel spectrogram provides higher frequency resolution in the low-frequency range, which facilitates the analysis of low-frequency line spectrum components.",
            "The CQT spectrogram offers higher frequency resolution in the low-frequency range and improved temporal resolution in the high-frequency range.",
            "It not only facilitates the analysis of low-frequency components but also provides information on periodic modulation in the high-frequency portion, such as propeller rhythm."
        ],
        "final_answer": "The integration is driven by the complementary time–frequency resolution properties of the three transforms: the STFT amplitude spectrogram provides a general time–frequency representation, the Mel spectrogram enhances low-frequency resolution, and the CQT spectrogram further delivers both high resolution at low frequencies and improved temporal resolution at high frequencies to capture periodic modulation (e.g., propeller rhythm).",
        "relevant_elements": [
            "CQT transform",
            "STFT amplitude spectrogram",
            "Mel spectrogram"
        ],
        "id": 898,
        "masked_question": "What drives integrating [mask1] with [mask2] and Mel spectrogram?",
        "masked_number": 2,
        "masked_elements": [
            "CQT transform",
            "STFT amplitude spectrogram"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02758v1_figure_1.png",
        "paperid": "2411.02758v1",
        "paper_path": "./papers/2411.02758v1.json",
        "figure_id": "2411.02758v1_figure_1.png",
        "caption": "Figure 1: The general process of the data acquisition, preprocessing, and feature extraction.",
        "qtype": "Design_Rationale",
        "response": "The “mystery” turn‐pike that forces us to use *all three* spectrograms is simply that each one brings a different, complementary view of the ship’s noise.  \n\n1. The red–boxed block (mask₁) is the CQT transform.  \n2. The blue–boxed block (mask₂) is the STFT amplitude spectrogram.  \n\nWe fuse CQT + STFT + Mel precisely because  \n  • STFT gives a uniform, baseline time–frequency picture of the signal,  \n  • Mel filtering boosts our resolution on the low-frequency line spectra, and  \n  • CQT (with its log-spaced filters) both refines the low-frequency view *and* sharpens our temporal tracking of high-frequency modulations (e.g. propeller blade rhythms).  \n\nIn short, the need to capture low-frequency line components *and* high-frequency temporal modulations with sufficient precision drives the joint use of CQT, STFT amplitude, and Mel spectrograms."
    },
    {
        "question": "What motivates summing 2-D modulation spectrum into 1-D DEMON spectrum?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To represent the modulation components of the signal more intuitively, the 2D-DEMON spectrum can be summed along the modulation frequency dimension to obtain the 1D-DEMON spectrum.",
            "The peaks in the 1D-DEMON spectrum reflect the shaft frequency and blade frequency of the propeller (fundamental frequency and its harmonics), which contain robust physical characteristics of the target."
        ],
        "final_answer": "Summing the 2-D modulation spectrum into a 1-D DEMON spectrum is motivated by the desire to represent the modulation components more intuitively; the resulting 1-D spectrum’s peaks clearly indicate the propeller’s shaft and blade frequencies, providing robust physical characteristics of the target.",
        "relevant_elements": [
            "2-D modulation spectrum",
            "1-D DEMON spectrum"
        ],
        "id": 899,
        "masked_question": "What motivates summing [mask1] into 1-D DEMON spectrum?",
        "masked_number": 1,
        "masked_elements": [
            "2-D modulation spectrum"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02758v1_figure_1.png",
        "paperid": "2411.02758v1",
        "paper_path": "./papers/2411.02758v1.json",
        "figure_id": "2411.02758v1_figure_1.png",
        "caption": "Figure 1: The general process of the data acquisition, preprocessing, and feature extraction.",
        "qtype": "Design_Rationale",
        "response": "Chain of Thought:\n\n1. In the diagram (red‐boxed area) we see the 2-D DEMON modulation spectrum, which is a matrix of modulation amplitudes versus carrier sub‐bands.   \n2. The text explains that this full 2-D spectrum contains rich modulation information but is still a high-dimensional representation.   \n3. By summing (collapsing) along the modulation-frequency axis we collapse each sub‐band’s modulation energy into a single vector entry, yielding a 1-D DEMON spectrum.   \n4. The paper explicitly states that this 1-D spectrum “more intuitively” exposes the modulation peaks—and in particular the propeller shaft frequency and its harmonics—so that these robust, physically meaningful features can feed the routing layer and downstream classifier in a low-dimensional, easy-to-interpret form.\n\nAnswer:\n\nThey collapse (sum) the 2-D modulation spectrogram along the modulation-frequency axis to produce a one-dimensional DEMON spectrum because this 1-D form (a) distills the essential modulation peaks—namely the propeller’s shaft and blade frequencies—into a clear, low‐dimensional signature and (b) makes those physically meaningful features more intuitive and computationally efficient to use for routing and classification."
    },
    {
        "question": "How is the CQT transform configured to balance temporal and frequency resolution before log concatenation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We apply the constant-Q transform (CQT) to obtain the CQT spectrogram. In this process, each frame’s amplitude spectrum is convolved with the CQT kernel, which consists of a bank of logarithmically spaced band-pass filters. Among them, the center frequency component of the k-th filter, denoted as f_k, is determined by Equation (2): where the octave resolution is represented by B, and the upper and lower frequencies to be processed are indicated by f_u and f_l, respectively. The ratio of the filter bandwidth Δf_k to the center frequency f_k is a constant Q.",
            "After the filtering, spectra undergo a logarithmic transformation and are concatenated across all frames to obtain the CQT spectrogram. The CQT spectrogram offers higher frequency resolution in the low-frequency range and improved temporal resolution in the high-frequency range."
        ],
        "final_answer": "The CQT is implemented using a bank of logarithmically spaced band-pass filters with a fixed bandwidth-to-center-frequency ratio (constant Q). An octave resolution parameter B controls the number of filters per octave between a lower cutoff f_l and upper cutoff f_u, yielding fine frequency resolution at low frequencies and finer temporal resolution at high frequencies before taking the logarithm and concatenating across frames.",
        "relevant_elements": [
            "CQT transform",
            "log concatenate"
        ],
        "id": 900,
        "masked_question": "How is the [mask1] configured to balance temporal and frequency resolution before log concatenation?",
        "masked_number": 1,
        "masked_elements": [
            "CQT transform"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02758v1_figure_1.png",
        "paperid": "2411.02758v1",
        "paper_path": "./papers/2411.02758v1.json",
        "figure_id": "2411.02758v1_figure_1.png",
        "caption": "Figure 1: The general process of the data acquisition, preprocessing, and feature extraction.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module is nothing exotic at all but simply the Constant-Q transform (CQT) layer.  In our pipeline it is set up exactly as follows:\n\n  1.  We choose a lowest and highest frequency of interest, f_min and f_max, and an octave-resolution B (i.e. how many filters per octave).  \n  2.  We then build a bank of B·log2(f_max/f_min) band-pass filters whose center frequencies f_k are log-spaced by octaves:\n         f_k = 2^((k–1)/B) · f_min .  \n  3.  Each filter is made “constant-Q,” meaning its bandwidth Δf_k is always a fixed fraction Q of its center frequency (Δf_k/f_k = 1/Q).  \n  4.  When you convolve each short-time Fourier amplitude spectrum with that bank you automatically get narrow bands (high frequency resolution) down in the bass and wider, faster-responding bands (high temporal resolution) up top.  \n  5.  Finally you take the log of each band’s output and stitch the frames together to form your CQT spectrogram.\n\nBecause the ratio Δf_k/f_k is held constant, the CQT front-end “scales” its time–frequency trade-off in exactly the way you want: fine spectral detail at low f, fine temporal detail at high f, all before the log+concat step."
    },
    {
        "question": "How does summing across modulation frequencies convert the 2-D modulation spectrum into a 1-D DEMON spectrum?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The modulation spectra corresponding to each sub-band are then concatenated to form the 2D-DEMON spectrum.",
            "To represent the modulation components of the signal more intuitively, the 2D-DEMON spectrum can be summed along the modulation frequency dimension to obtain the 1D-DEMON spectrum."
        ],
        "final_answer": "By collapsing the 2-D modulation spectrum along its modulation-frequency axis—i.e. summing the power or amplitude values across all modulation frequencies for each sub-band—the 2-D matrix is reduced to a single-dimensional vector, yielding the 1-D DEMON spectrum.",
        "relevant_elements": [
            "2-D modulation spectrum",
            "1-D DEMON spectrum"
        ],
        "id": 901,
        "masked_question": "How does summing across modulation frequencies convert the [mask1] into a 1-D DEMON spectrum?",
        "masked_number": 1,
        "masked_elements": [
            "2-D modulation spectrum"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.02758v1_figure_1.png",
        "paperid": "2411.02758v1",
        "paper_path": "./papers/2411.02758v1.json",
        "figure_id": "2411.02758v1_figure_1.png",
        "caption": "Figure 1: The general process of the data acquisition, preprocessing, and feature extraction.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the full 2-D DEMON modulation spectrogram (modulation‐frequency vs. sub-band index).  By summing along the modulation–frequency axis (i.e. for each sub-band, adding up its energy at all modulation frequencies), you collapse that two-dimensional map into a single vector whose entries are the total modulation energy per sub-band.  That one‐dimensional curve is exactly the 1-D DEMON spectrum, whose peaks mark the shaft and blade‐rate harmonics."
    },
    {
        "question": "How does Temporal Grounding filter key frames for Spatial Grounding using contrastive attention weights?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Following [23], we leverage Gumbel-Softmax [18] to manage a differentiable discrete selection between the two calculated attention probabilities and obtain a contrastive map G to distinguish between positive and negative frames in the video. The first and second columns of G index the positive frames (i.e., G[:,0]) and negative frames (i.e., G[:,1]).",
            "Specifically, for positive frames, we utilize top-k function to select M video frames with the highest attention weight from G and gather their corresponding frame features as p⁺. For negative frames, we take bottom-k function to select M video frames with the lowest attention weight from G and gather their corresponding frame features as p⁻."
        ],
        "final_answer": "Temporal Grounding builds a contrastive attention map G via Gumbel-Softmax to score each frame for how question-relevant it is. It then applies a top-k selection on G[:,0] (the positive-attention column) to pick the M frames with the highest weights as key frames for Spatial Grounding, and a bottom-k selection on the same scores to pick M negative frames.",
        "relevant_elements": [
            "Temporal Grounding",
            "Spatial Grounding"
        ],
        "id": 902,
        "masked_question": "How does [mask1] filter key frames for Spatial Grounding using contrastive attention weights?",
        "masked_number": 1,
        "masked_elements": [
            "Temporal Grounding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.14319v1_figure_1.png",
        "paperid": "2409.14319v1",
        "paper_path": "./papers/2409.14319v1.json",
        "figure_id": "2409.14319v1_figure_1.png",
        "caption": "Figure 1: \nComparison between existing research and our work for TextVideoQA. (a) Existing research has two major problems: 1) Opaque decision-making; they hardly tell if their answers (e.g., “30”) are originated from the relevant scene texts in the videos, or attributed to other short-cuts. 2) Heavy reliance on scene-text recognition; their low QA accuracy could be due to a failure in decoding the textual answer (e.g., “30 M.P.H.”) from the corresponding scene text region. (b) We establish a novel pipeline by temporal-spatially localizing the scene text region and then decoding them into textual answers. We also enable direct evaluation on the grounded scene-text region.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the Temporal Grounding (TG) stage, the model computes two attention distributions over the T sampled frames:  \n   – L⁺ ∈ ℝ^{T} scores how “positively” each frame aligns with the question  \n   – L⁻ ∈ ℝ^{T} scores how “negatively” (i.e. irrelevantly) each frame aligns  \n   These are obtained by a cross‐attention plus a Gumbel‐Softmax step (Eqs. 5–6).  \n2. Not all of those positively scored frames actually contain clear, answer‐bearing scene text, and we only want a small set of “key” frames to pass on to the Spatial Grounding (SG) stage.  \n3. [mask1] is precisely that filtering operator: it takes the contrastive attention weights L⁺ and L⁻ and runs a top-k/bottom-k selection.  \n   – For the positive side, we do  \n     F⁺ = Top_{α_vf}(L⁺)  \n     i.e. pick the α_vf frames with the highest L⁺–scores  \n   – For the negative side, we do  \n     F⁻ = Bot_{α_vf}(L⁻)  \n     i.e. pick the α_vf frames with the lowest L⁻–scores  \n4. The resulting F⁺ (the α_vf best frames) are passed to the Spatial Grounding module (along with F⁻ as negatives), where OCR tokens are then contrastively filtered.  \n\nAnswer  \n[mask1] is the Top/Bottom-k selection over the contrastive frame‐wise attention maps: it selects the α_vf frames with the highest positive attention scores (from L⁺) and the α_vf frames with the lowest negative scores (from L⁻) to serve as the key frames for the Spatial Grounding stage."
    },
    {
        "question": "How does temporal grounding help mitigate opaque decision-making in Text-Based VideoQA models?",
        "relevant_section_ids": [
            "1",
            "3.2.1"
        ],
        "relevant_context": [
            "However, the key factors causing performance loss remain unclear due to the opaque decision-making process. For example, is it because of poor QA or poor scene-text recognition in the video? Additionally, even for the correct predictions, these methods rarely tell if their answers are originated from relevant scene texts in the videos, or attributed to other short-cut paths. This severely impedes further improvements.",
            "Specifically, at the first stage, T2S-QA employs temporal grounding to distinguish positive frames (i.e., frames with question-relevant scene texts) from negative frames (i.e., frames without question-relevant scene texts) in a video. Given that motion blur and occlusion often obscure scene text in video frames, it subsequently refines the selection by identifying a few key positive frames that are most relevant to the question as the grounded frames."
        ],
        "final_answer": "By using temporal grounding to distinguish and select only those frames that contain question-relevant scene text, the model exposes which moments it relies on to answer. This frame-level evidence reduces the opacity of its decision process by showing when in the video the critical text appears, rather than hiding the reasoning in an end-to-end pipeline.",
        "relevant_elements": [
            "Temporal Grounding",
            "Text-Based VideoQA Model"
        ],
        "id": 903,
        "masked_question": "How does [mask1] help mitigate opaque decision-making in [mask2] models?",
        "masked_number": 2,
        "masked_elements": [
            "Temporal Grounding",
            "Text-Based VideoQA Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.14319v1_figure_1.png",
        "paperid": "2409.14319v1",
        "paper_path": "./papers/2409.14319v1.json",
        "figure_id": "2409.14319v1_figure_1.png",
        "caption": "Figure 1: \nComparison between existing research and our work for TextVideoQA. (a) Existing research has two major problems: 1) Opaque decision-making; they hardly tell if their answers (e.g., “30”) are originated from the relevant scene texts in the videos, or attributed to other short-cuts. 2) Heavy reliance on scene-text recognition; their low QA accuracy could be due to a failure in decoding the textual answer (e.g., “30 M.P.H.”) from the corresponding scene text region. (b) We establish a novel pipeline by temporal-spatially localizing the scene text region and then decoding them into textual answers. We also enable direct evaluation on the grounded scene-text region.",
        "qtype": "Literature_Background",
        "response": "Chain-of-Thought Reasoning:\n\n1.  In the left‐hand “blue” box of Fig. 1a we see a conventional text‐based VideoQA model: it ingests frames + OCR tokens, produces an answer (“30”), but offers no explicit justification for why it picked that token (i.e. “opaque decision‐making,” and we don’t know if it really read “30 M.P.H.” or just guessed).\n\n2.  In contrast, the right‐hand “red” box of Fig. 1b is our Temporal-to-Spatial grounding pipeline:  \n    a.  A Temporal Grounding module first pinpoints which frames actually contain the question‐relevant text.  \n    b.  A Spatial Grounding module then selects the specific OCR bounding box(es) in those frames.  \n    c.  Finally, Scene Text Recognition reads out the text from exactly those localized boxes.\n\n3.  Because the model is forced to point to actual image regions (the red‐boxed “grounded scene‐text regions”) before emitting its answer, we gain a built-in visual “evidence” trace.  We can literally see which frame and which text box the model used.\n\n4.  This explicit pointing mechanism breaks open the black box of the blue-boxed, text-only VideoQA approach.  We no longer have to infer via attention weights whether the model really “read” the speed limit sign—it physically highlights it.  \n\nAnswer:\n\nBy inserting that red-boxed temporal-and-spatial grounding stage—i.e. forcing the model to locate the exact video frames and OCR bounding boxes that justify its answer—we convert a hidden, purely text-based VideoQA decision into one backed by explicit visual evidence, thereby mitigating the opaque decision‐making of the blue-boxed models."
    },
    {
        "question": "How does spatial grounding complement scene text recognition to provide visual evidence for QA?",
        "relevant_section_ids": [
            "1",
            "3.2.2"
        ],
        "relevant_context": [
            "Such a setting enjoys three-fold advantages: First, the grounded scene-text regions serve as visual evidence to support textual answers, thus enabling a reliable TextVideoQA system.",
            "At the second stage, T2S-QA applies spatial grounding in each grounded frame to differentiate positive scene texts from negative ones. Since answers typically pertain to a small subset of the rich scene text present in a video frame, T2S-QA further selects partial positive scene texts covering the answer as the final grounding results.",
            "Our analysis reveal that answers only occupy a very small area in the video frame, as shown in Fig. 3(c). Therefore, we first need to distinguish the scene texts related to the question from numerous elements, and then locate a few critical strongly associated scene texts as visual answers.",
            "SG facilitates adaptive OCR token selection ... enabling it to differentiate positive OCR tokens from negative ones based on their similarity to the given question. ... We take the bounding boxes of the selected positive OCR tokens as grounded OCR tokens for answer grounding evaluation."
        ],
        "final_answer": "Spatial grounding pinpoints which detected text regions (OCR tokens) in each selected frame are relevant to the question by ranking and selecting the highest-scoring bounding boxes. These spatially grounded regions then feed into the scene text recognition system, and the recognized text inside those precise boxes serves as concrete visual evidence (visual answers) that directly supports the final textual answer.",
        "relevant_elements": [
            "Spatial Grounding",
            "Scene Text Recognition"
        ],
        "id": 904,
        "masked_question": "How does [mask1] complement scene text recognition to provide visual evidence for QA?",
        "masked_number": 1,
        "masked_elements": [
            "Spatial Grounding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.14319v1_figure_1.png",
        "paperid": "2409.14319v1",
        "paper_path": "./papers/2409.14319v1.json",
        "figure_id": "2409.14319v1_figure_1.png",
        "caption": "Figure 1: \nComparison between existing research and our work for TextVideoQA. (a) Existing research has two major problems: 1) Opaque decision-making; they hardly tell if their answers (e.g., “30”) are originated from the relevant scene texts in the videos, or attributed to other short-cuts. 2) Heavy reliance on scene-text recognition; their low QA accuracy could be due to a failure in decoding the textual answer (e.g., “30 M.P.H.”) from the corresponding scene text region. (b) We establish a novel pipeline by temporal-spatially localizing the scene text region and then decoding them into textual answers. We also enable direct evaluation on the grounded scene-text region.",
        "qtype": "Literature_Background",
        "response": "Chain of Thought:\n1. In the diagram (Fig.1 b) the red‐boxed component sits immediately above the “Scene Text Recognition” block and below “Temporal Grounding.”  \n2. The text describes a two‐stage grounding: first you pick out question‐relevant frames (Temporal Grounding), then within those frames you pinpoint the exact text regions (Spatial Grounding).  \n3. The red box therefore marks the Spatial Grounding module, whose job is to sift through all detected text in a selected frame and pick out only the few boxes that actually answer the question.  \n4. By localizing those specific text regions, Spatial Grounding delivers bounding‐box “evidence” to the OCR system, making the subsequent scene‐text recognizer focus on the right patch, and also yielding a clear visual justification for the model’s answer.  \n\nAnswer:\n[mask1] = Spatial Grounding"
    },
    {
        "question": "How does unified attention control balance cross attention and self attention for consistent image editing?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, the two commonly used text-guided cross attention strategies are cross attention replacement and cross-attention refinement. These two methods ensure the seamless flow of information from the target prompt to the source prompt, thereby guiding the latent map towards the desired direction.",
            "To be specific, in the early steps of diffusion, the feature in the editing steps t_e, t_e+1, and t_e+2 will be used in self attention calculation to generate an image layout closer to the target prompt, while in the later stages, the feature in the reconstruction steps t_r−2, t_r−1 and t_r will be used to guide the generation of the target image layout closer to the original image."
        ],
        "final_answer": "The unified attention control scheme combines two text-driven cross-attention edits (replacement and refinement) to inject the target prompt’s semantics seamlessly, while simultaneously modulating the self-attention maps over time. During the early diffusion steps it uses self-attention from the target prompt (editing steps t_e, t_e+1, t_e+2) to shift the layout toward the desired edit, then in later steps it switches to self-attention from the original image (reconstruction steps t_r−2, t_r−1, t_r) to preserve background structure. This joint control of cross- and self-attention ensures both accurate semantic changes and consistency with the source image.",
        "relevant_elements": [
            "Unified Attention Control",
            "Cross Attention",
            "Self Attention"
        ],
        "id": 905,
        "masked_question": "How does [mask1] balance [mask2] and self attention for consistent image editing?",
        "masked_number": 2,
        "masked_elements": [
            "Unified Attention Control",
            "Cross Attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.03286v2_figure_2.png",
        "paperid": "2411.03286v2",
        "paper_path": "./papers/2411.03286v2.json",
        "figure_id": "2411.03286v2_figure_2.png",
        "caption": "Figure 2: Overview of the DiT4Edit framework. During the image editing process, our inversion algorithm generates high-quality latent maps, and the final edited image is achieved through unified attention control.",
        "qtype": "Literature_Background",
        "response": "The mystery component inside the red box is nothing more exotic than our “unified attention control” module, and the thing in the blue box is simply the cross‐attention layer that carries your target–prompt semantics into the model.  What the unified controller does is sit between the self–attention path (which knows all about your original image’s layout and structure) and the cross–attention path (which knows how you want to change that structure, because it is keyed off your new, target prompt), and blend them together in a single, coherent attention map.  \n\nConcretely, at each diffusion step we extract:\n\n  1.  The self‐attention scores from the reconstruction branch (to preserve shape, perspective, lighting, etc.).  \n  2.  The cross‐attention scores from the editing branch (to inject the new semantics, e.g. “horse” instead of “camel”).  \n\nWe then merge them patch by patch—using a small learned or hand-tuned weight that can even change over time (early steps lean more on self-attention to lock in coarse layout; later steps lean more on cross-attention to nail down fine semantic detail).  The result is a single “unified” attention map that both (a) keeps your image from drifting wildly away from its original structure, and (b) smoothly folds in the new instructions from your target prompt.  That balance is exactly how DiT4Edit delivers consistently faithful edits at arbitrary image sizes."
    },
    {
        "question": "How does patches merging optimize self attention to reduce computational overhead in the diffusion transformer?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Patches merging. To enhance the inference speed, inspired by token merging (Bolya et al. 2023  ###reference_b2###), we embed patches merging into the denoising model. This approach is motivated by the observation that the number of patches involved in attention calculations within the transformer architecture is significantly greater than that in UNet. The calculation flow is shown in Figure 4  ###reference_###. For a feature map, we first compute the similarity between each patch and merge the most similar ones to reduce the number of patches processed by the attention mechanism. After attention calculation, we unmerge the patches to maintain the original input size for the next layer in the model. By incorporating patches merging into our framework, we aim to streamline the process and improve overall efficiency, without altering the fundamental operations of each layer."
        ],
        "final_answer": "By grouping together (merging) the most similar patches before computing self-attention, the model substantially cuts down the number of tokens that must be attended to. After the attention step, the merged patches are then restored (unmerged) to their original resolution. This drop-and-restore strategy reduces the computational load of the transformer’s global attention without changing any other layer operations.",
        "relevant_elements": [
            "Patches Merging",
            "Self Attention"
        ],
        "id": 906,
        "masked_question": "How does [mask1] optimize self attention to reduce computational overhead in the diffusion transformer?",
        "masked_number": 1,
        "masked_elements": [
            "Patches Merging"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.03286v2_figure_2.png",
        "paperid": "2411.03286v2",
        "paper_path": "./papers/2411.03286v2.json",
        "figure_id": "2411.03286v2_figure_2.png",
        "caption": "Figure 2: Overview of the DiT4Edit framework. During the image editing process, our inversion algorithm generates high-quality latent maps, and the final edited image is achieved through unified attention control.",
        "qtype": "Literature_Background",
        "response": "First, note that the red‐boxed module in the figure is our “Patches Merging” block.  Its entire purpose is to cut down on the number of tokens that go into the self-attention, thereby reducing that layer’s quadratic cost.  Concretely:\n\n1. At each transformer layer, we take the full set of patch embeddings and compute a lightweight similarity score between them.  \n2. We then greedily merge the most similar patches into a single “super‐patch,” shrinking the total token count that must participate in the self-attention.  \n3. Self-attention runs on this reduced token set (so instead of paying O(N²) for N patches, we more or less pay O(M²) for M≪N merged patches).  \n4. After attention, we “unmerge” the super-patches back into the original grid so that downstream layers see the same spatial resolution as before.\n\nBy clustering and merging look-alike patches before computing attention, we dramatically cut down on both memory and compute without changing the semantics of the transformer or its ability to restore the full-resolution feature map afterward."
    },
    {
        "question": "How does DPM-Solver++ inversion improve latent reconstruction accuracy at reduced inversion steps?",
        "relevant_section_ids": [
            "3.3",
            "4.4"
        ],
        "relevant_context": [
            "As we discussed earlier, using a high-order DPM-Solver (e.g., DPM-Solver++), can effectively improve the sampling speed.  To approximate the integral term in equation Eq. 4, given the xₜ at time t, using the Taylor expansion at t–1, and the DPM-Solver++ can obtain a exact solution value at time t–1.  In practical applications, it is common to set r>0, enabling a rapid inference and minimizing discretization errors.  This DPM-Solver is named DPM-Solver++ (2M).",
            "When comparing the two methods with the same number of inference steps (r), DPM-Solver consistently outperformed DDIM in terms of image editing quality.  This demonstrates that our use of the DPM-Solver inversion strategy allows for the generation of superior latent maps, resulting in better editing outcomes within fewer steps."
        ],
        "final_answer": "By using a high-order Taylor-expansion solver, DPM-Solver++ computes an exact solution at each step, greatly reducing discretization error.  As a result, it produces more accurate inversion latents than DDIM even when using fewer steps, yielding superior reconstruction quality at reduced inversion steps.",
        "relevant_elements": [
            "DPM-Solver++ Inversion"
        ],
        "id": 907,
        "masked_question": "How does [mask1] improve latent reconstruction accuracy at reduced inversion steps?",
        "masked_number": 1,
        "masked_elements": [
            "DPM-Solver++ Inversion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.03286v2_figure_2.png",
        "paperid": "2411.03286v2",
        "paper_path": "./papers/2411.03286v2.json",
        "figure_id": "2411.03286v2_figure_2.png",
        "caption": "Figure 2: Overview of the DiT4Edit framework. During the image editing process, our inversion algorithm generates high-quality latent maps, and the final edited image is achieved through unified attention control.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the red‐boxed “DPM-Solver++ Inversion” block (i.e. [mask1]) lets you get back a much closer latent in far fewer steps:\n\n1.  Standard inversions like DDIM are essentially first‐order approximations of the diffusion ODE, so if you only take, say, 10–20 inversion steps they accumulate quite a bit of discretization error and your recovered latent drifts away from the true one.  \n2.  DPM-Solver++ is a high-order ODE solver (in our case a 2–multistep “2M” scheme) that uses a Taylor expansion of the score network’s output plus a backward‐Euler correction to approximate the integral term in closed form.  \n3.  Because it is second‐order accurate in time (rather than first‐order like DDIM), each step of the inversion is far more faithful to the continuous diffusion trajectory.  \n4.  In practice this means you can invert a real image back to its noisier latent in as few as 10–20 steps, yet still recover almost exactly the same latent you would get with a long, brute-force DDIM inversion.  \n5.  The ablation (Figure 7) confirms that for the same # of steps, DPM-Solver++ inversion yields latents whose reconstructions have much higher PSNR and perceptual quality than DDIM.  \n\nIn short: by upgrading the inversion from a first-order DDIM­style step to a second-order, multistep DPM-Solver++ ODE solver, you dramatically cut down the step count while keeping the inversion almost lossless."
    },
    {
        "question": "How does patches merging influence unified attention control's efficiency without degrading editing performance?",
        "relevant_section_ids": [
            "3.3",
            "4.4"
        ],
        "relevant_context": [
            "Patches merging. To enhance the inference speed, inspired by token merging (Bolya et al. 2023  ###reference_b2###), we embed patches merging into the denoising model. This approach is motivated by the observation that the number of patches involved in attention calculations within the transformer architecture is significantly greater than that in UNet. The calculation flow is shown in Figure 4  ###reference_###. For a feature map, we first compute the similarity between each patch and merge the most similar ones to reduce the number of patches processed by the attention mechanism. After attention calculation, we unmerge the patches to maintain the original input size for the next layer in the model. By incorporating patches merging into our framework, we aim to streamline the process and improve overall efficiency, without altering the fundamental operations of each layer.",
            "We perform a series of ablation studies to demonstrate the effectiveness of DPM-Solver inversion and patches merging. The results of our ablation experiments on patches merging are presented in Figure 4  ###reference_### and Table 2  ###reference_###. Implementing patches merging led to a notable reduction in the editing time for large-sized images while maintaining editing quality comparable to that achieved without patches merging. This indicates that patches merging can significantly enhance the overall performance of image editing frameworks."
        ],
        "final_answer": "By merging similar patches before the transformer’s attention calculation, the model processes fewer tokens—speeding up unified self- and cross-attention operations—and then restores the full set of patches afterward. Ablation results show this reduces editing time on large images without any drop in editing quality, thus boosting efficiency without degrading performance.",
        "relevant_elements": [
            "Patches Merging",
            "Unified Attention Control"
        ],
        "id": 908,
        "masked_question": "How does [mask1] influence [mask2]'s efficiency without degrading editing performance?",
        "masked_number": 2,
        "masked_elements": [
            "Patches Merging",
            "Unified Attention Control"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.03286v2_figure_2.png",
        "paperid": "2411.03286v2",
        "paper_path": "./papers/2411.03286v2.json",
        "figure_id": "2411.03286v2_figure_2.png",
        "caption": "Figure 2: Overview of the DiT4Edit framework. During the image editing process, our inversion algorithm generates high-quality latent maps, and the final edited image is achieved through unified attention control.",
        "qtype": "Experimental_Results",
        "response": "Here’s a step-by-step reasoning:\n\n1.  Identify the two components in question  \n    -  [mask1] is the “Patches Merging” module (red box).  \n    -  [mask2] is the “Unified Attention Control” mechanism (blue box).  \n\n2.  Recall what each one does  \n    -  Patches Merging groups together (and later splits back apart) spatially or semantically similar image patches so that the transformer has fewer tokens to attend over in each layer.  \n    -  Unified Attention Control is the joint self- and cross-attention procedure that steers the denoising process toward the target prompt while preserving structure from the source image.  \n\n3.  The efficiency bottleneck in attention control  \n    -  Attention scales quadratically in the number of tokens.  If you have N patches, computing unified control over all N×N correlations becomes expensive, especially for large images.  \n\n4.  How Patches Merging helps  \n    -  By temporarily collapsing similar patches into a smaller set of “super-tokens,” the attention mechanism (i.e. Unified Attention Control) works on a much smaller token matrix.  \n    -  This directly cuts down both memory usage and compute time during those cross/self-attention passes.  \n\n5.  Why editing quality is preserved  \n    -  The merging is done on the basis of patch similarity, so no new artifacts are introduced—the merged tokens faithfully represent the underlying image content.  \n    -  After attention weights are applied, the framework “unmerges” the patches back to full resolution, restoring fine‐grained detail.  \n    -  Empirically (cf. their ablations in Fig. 4 and Table 2), the authors show that you get virtually the same editing quality even though the attention runs over fewer tokens.  \n\nAnswer:  \nBy folding highly similar patches into fewer tokens before running Unified Attention Control, “Patches Merging” slashes the computational and memory overhead of the attention matrices.  Because this merging is reversible and similarity‐based, it does not destroy local detail—once attention is computed on the super-tokens, the framework simply unmerges them back to full resolution—so editing performance remains unchanged even though the attention step is much faster."
    },
    {
        "question": "How does NVS leverage Jensen-Shannon divergence to select the correct decoupled image for decoding?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Thus, to identify the correct decoupled image z_d between the dual image and the residual image, we introduce a non-visual input x_nv, containing only the textual prompt, without any visual information, to serve as an assistant.",
            "We then calculate the Jensen–Shannon Divergence (JSD) as the distance between the output distributions from the non-visual input and the dual image as JSD_dn, and between the non-visual input and the residual image as JSD_rn.",
            "The visual input corresponding to the greater distance is selected as the decoupled image z_d, formulated as: z_d = { z_d if JSD_dn > JSD_rn; z_r otherwise }."
        ],
        "final_answer": "NVS feeds both the dual image and the residual image (each paired with the prompt) through the LVLM, and also feeds the prompt alone (non-visual input). It then computes the Jensen–Shannon divergence between the prompt-only output distribution and each image-conditioned distribution (JSD_dn and JSD_rn). Whichever image yields the larger JSD (i.e. whose output diverges more from the language-only prior) is deemed to contain the key visual features and is selected as the decoupled image for decoding.",
        "relevant_elements": [
            "NVS"
        ],
        "id": 909,
        "masked_question": "How does [mask1] leverage Jensen-Shannon divergence to select the correct decoupled image for decoding?",
        "masked_number": 1,
        "masked_elements": [
            "NVS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.12713v1_figure_2.png",
        "paperid": "2411.12713v1",
        "paper_path": "./papers/2411.12713v1.json",
        "figure_id": "2411.12713v1_figure_2.png",
        "caption": "Figure 2: LVLMs may generate responses that include hallucinations (e.g., “One person on the left side is holding a phone”, where “sandwich” is hallucinated as “phone”. First, the CVD method leverages SAM to decouple the original input image v𝑣vitalic_v into the dual image zdsubscript𝑧𝑑z_{d}italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the residual image zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and introduces a non-visual input znsubscript𝑧𝑛z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. These four inputs are then passed into the LVLM to generate their corresponding output distributions: l⁢o⁢g⁢i⁢t⁢so𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑜logits_{o}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sd𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑑logits_{d}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sr𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑟logits_{r}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and l⁢o⁢g⁢i⁢t⁢sn𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑛logits_{n}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. The Jensen-Shannon Divergence (JSD) is computed between them to obtain J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT, and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT. The NVS method compares J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT, and the input with the greater distance is selected as the decoupled image (e.g., zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT). Next, ATCD selects the decoding strategy by comparing J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT and J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, if J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT is greater, the decoupled image output distribution is employed to contrastively subtract the original distribution. Conversely, if J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT is greater, the output distribution from the decoupled image is leveraged to contrastively enhance the weighted original distribution. Effectively correcting the hallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably, this process is dynamically performed at each token generation step.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought:\n1. We have two candidate “decoupled” images at each decoding step: the dual image (z_d) and the residual image (z_r), plus a non-visual input (z_n) that contains only the text prompt.  \n2. If a candidate image actually carries the key visual cue needed to predict the next token, its resulting output distribution will differ significantly from the model’s pure language-prior distribution (i.e. the non-visual case).  \n3. Conversely, if the candidate image has lost that crucial visual detail, its distribution will collapse back toward the non-visual (language‐only) distribution, yielding a small divergence.  \n4. Jensen–Shannon divergence (JSD) is used to quantify how “far” each image-conditioned distribution is from the non-visual baseline.  \n5. Concretely, we compute JSD(P(non-visual) || P(z_d)) = JSD_dn and JSD(P(non-visual) || P(z_r)) = JSD_rn.  \n6. The image whose divergence is larger is deemed to retain the key visual information (because it moved further away from the language-only prior), and is thus selected as the decoupled image for the next token’s generation.\n\nAnswer:\nThe red-boxed module is the Non-Visual Screening (NVS) step. NVS feeds the text-only prompt and each candidate image (dual vs. residual) through the LVLM to get three output distributions. It then computes the Jensen–Shannon divergence between the text-only distribution and each image-conditioned distribution. The candidate whose distribution has the larger JSD from the non-visual baseline is chosen as the correct decoupled image for decoding, since a larger divergence signals that it still holds the critical visual cue."
    },
    {
        "question": "How does ATCD use divergence between decoupled and original distributions to prevent cumulative hallucinations?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "We first calculate the distance between the output distributions from the non-visual input and the decoupled image, denoted as JSD_rn, and the distance between the output distributions from the original visual input and the non-visual input, denoted as JSD_on.",
            "We consider two scenarios: (1) Hallucination Existence: When JSD_rn is greater than JSD_on, we conclude that hallucinations are present in the original output distribution. ... (2) Diversity Insufficient: When JSD_rn is less than JSD_on, we consider there to be a risk of cumulative hallucinations. In this case, we use the output distribution from the decoupled image to contrastively enhance the weighted original distribution, thereby improving the diversity of generation, formulated as: ... The hyperparameters α and β represent the amplification factors. The final generated token y_t is sampled from P_t."
        ],
        "final_answer": "ATCD monitors the Jensen–Shannon divergence between (i) the model’s output when given only the decoupled image versus the non-visual prompt (JSD_rn) and (ii) the output when given the original visual input versus the non-visual prompt (JSD_on). If JSD_rn falls below JSD_on—indicating the decoupled distribution is too similar to the original and thus lacks diversity—ATCD counteracts emerging cumulative hallucinations by contrastively enhancing the original output distribution with the decoupled distribution, using learned amplification factors to boost diversity in the generated tokens.",
        "relevant_elements": [
            "ATCD"
        ],
        "id": 910,
        "masked_question": "How does [mask1] use divergence between decoupled and original distributions to prevent cumulative hallucinations?",
        "masked_number": 1,
        "masked_elements": [
            "ATCD"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.12713v1_figure_2.png",
        "paperid": "2411.12713v1",
        "paper_path": "./papers/2411.12713v1.json",
        "figure_id": "2411.12713v1_figure_2.png",
        "caption": "Figure 2: LVLMs may generate responses that include hallucinations (e.g., “One person on the left side is holding a phone”, where “sandwich” is hallucinated as “phone”. First, the CVD method leverages SAM to decouple the original input image v𝑣vitalic_v into the dual image zdsubscript𝑧𝑑z_{d}italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the residual image zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and introduces a non-visual input znsubscript𝑧𝑛z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. These four inputs are then passed into the LVLM to generate their corresponding output distributions: l⁢o⁢g⁢i⁢t⁢so𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑜logits_{o}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sd𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑑logits_{d}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sr𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑟logits_{r}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and l⁢o⁢g⁢i⁢t⁢sn𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑛logits_{n}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. The Jensen-Shannon Divergence (JSD) is computed between them to obtain J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT, and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT. The NVS method compares J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT, and the input with the greater distance is selected as the decoupled image (e.g., zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT). Next, ATCD selects the decoding strategy by comparing J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT and J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, if J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT is greater, the decoupled image output distribution is employed to contrastively subtract the original distribution. Conversely, if J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT is greater, the output distribution from the decoupled image is leveraged to contrastively enhance the weighted original distribution. Effectively correcting the hallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably, this process is dynamically performed at each token generation step.",
        "qtype": "Experimental_Results",
        "response": "Step 1: Identify [mask1]  \n– The red‐boxed subfigure in the diagram is the ATCD module (Adaptive Token-level Contrastive Decoding).\n\nStep 2: Recall what ATCD computes  \n– At each decoding step the model has three logit distributions:  \n  1. logitsₒ from the original image  \n  2. logits𝑐 from the chosen decoupled image (either dual or residual)  \n  3. logitsₙ from the “non-visual” (text-only) input  \n– It measures two Jensen–Shannon divergences:  \n  • JSD_cn between the decoupled distribution and the non-visual distribution  \n  • JSD_on between the original distribution and the non-visual distribution  \n\nStep 3: Two decoding modes based on divergence comparison  \n– If JSD_cn > JSD_on:  \n  • The decoupled image’s distribution has drifted farther from the text‐only baseline than the original has → hallucination likely in the original.  \n  • Action: contrastively subtract the decoupled logits from the original logits, thereby stripping out spurious (hallucinated) mass.  \n– If JSD_cn < JSD_on:  \n  • The original is not drifting much beyond the text prior, so the model may be too conservative and risk “sticking” to previous hallucinations over time (poor diversity).  \n  • Action: contrastively add (i.e. reinforce) the decoupled logits into the original, boosting under-represented but visually grounded tokens.  \n\nStep 4: How this prevents cumulative hallucinations  \n– By dynamically choosing subtraction when hallucination signals are strong, ATCD prunes away hallucinated tokens at each step.  \n– By switching to additive mode when the model is over-relying on language priors, it injects fresh, visually grounded alternatives.  \n– This token‐wise, divergence-driven “push-and-pull” keeps the decoder both factual (no runaway hallucinations) and diverse (no repeated errors), preventing errors from compounding."
    },
    {
        "question": "What limitations could arise when SAM segments visual inputs for CVD in complex scenes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "SAM",
            "CVD"
        ],
        "id": 911,
        "masked_question": "What limitations could arise when [mask1] segments visual inputs for [mask2] in complex scenes?",
        "masked_number": 2,
        "masked_elements": [
            "SAM",
            "CVD"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.12713v1_figure_2.png",
        "paperid": "2411.12713v1",
        "paper_path": "./papers/2411.12713v1.json",
        "figure_id": "2411.12713v1_figure_2.png",
        "caption": "Figure 2: LVLMs may generate responses that include hallucinations (e.g., “One person on the left side is holding a phone”, where “sandwich” is hallucinated as “phone”. First, the CVD method leverages SAM to decouple the original input image v𝑣vitalic_v into the dual image zdsubscript𝑧𝑑z_{d}italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the residual image zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and introduces a non-visual input znsubscript𝑧𝑛z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. These four inputs are then passed into the LVLM to generate their corresponding output distributions: l⁢o⁢g⁢i⁢t⁢so𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑜logits_{o}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sd𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑑logits_{d}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sr𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑟logits_{r}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and l⁢o⁢g⁢i⁢t⁢sn𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑛logits_{n}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. The Jensen-Shannon Divergence (JSD) is computed between them to obtain J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT, and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT. The NVS method compares J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT, and the input with the greater distance is selected as the decoupled image (e.g., zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT). Next, ATCD selects the decoding strategy by comparing J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT and J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, if J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT is greater, the decoupled image output distribution is employed to contrastively subtract the original distribution. Conversely, if J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT is greater, the output distribution from the decoupled image is leveraged to contrastively enhance the weighted original distribution. Effectively correcting the hallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably, this process is dynamically performed at each token generation step.",
        "qtype": "Others",
        "response": "Here’s a concise breakdown:\n\n1.  What “mask1” and “mask2” refer to  \n    -  mask1 is the red-boxed component “SAM” (the off-the-shelf Segment Anything Model).  \n    -  mask2 is the blue-boxed module “CVD” (Complementary Visual Decoupling).\n\n2.  How they interact  \n    -  CVD relies on SAM to carve the raw image into two complementary views (the “dual” and “residual” images) before feeding either one back into the LVLM.\n\n3.  Potential limitations in complex scenes  \n    a.  Segmentation errors  \n       •  Over-segmentation: breaking a single object into many fragments can scatter its features across both views, so neither view has a coherent representation of that object.  \n       •  Under-segmentation: merging two distinct objects into one mask may force the model to treat a background element or an occluder as if it were semantically important.  \n    b.  Small or highly occluded objects  \n       •  SAM can miss tiny objects or ones partly hidden behind others, so CVD may completely drop the true “key” region into the wrong view (residual rather than dual), causing the model to hallucinate.  \n    c.  Ambiguous or overlapping boundaries  \n       •  In cluttered environments with lots of interlaced items (e.g., crowds, foliage, machinery), SAM’s masks may not align well with the true object shapes, injecting noise into both decoupled images.  \n    d.  Semantic mis‐prioritization  \n       •  CVD chooses its dual/residual split based purely on SAM’s area‐based confidence, not on “what actually matters” to the downstream task, so it can emphasize visually large but semantically trivial regions (e.g., sky, floor) over small yet crucial ones (e.g., a person’s hand).  \n    e.  Runtime and scalability  \n       •  Running SAM at each decoding step (or re-segmenting with different k) can be computationally expensive, making real-time or large-scale use challenging.\n\nAll of these failures in SAM’s mask quality feed directly into CVD, so mis-segmentation propagates through to the LVLM and undermines the decoupling strategy’s ability to isolate just the “important” visual features."
    },
    {
        "question": "What challenges may emerge using Jensen-Shannon Divergence in NVS for accurate decoupled image selection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Jensen-Shannon Divergence",
            "NVS"
        ],
        "id": 912,
        "masked_question": "What challenges may emerge using [mask1] in [mask2] for accurate decoupled image selection?",
        "masked_number": 2,
        "masked_elements": [
            "Jensen-Shannon Divergence",
            "NVS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.12713v1_figure_2.png",
        "paperid": "2411.12713v1",
        "paper_path": "./papers/2411.12713v1.json",
        "figure_id": "2411.12713v1_figure_2.png",
        "caption": "Figure 2: LVLMs may generate responses that include hallucinations (e.g., “One person on the left side is holding a phone”, where “sandwich” is hallucinated as “phone”. First, the CVD method leverages SAM to decouple the original input image v𝑣vitalic_v into the dual image zdsubscript𝑧𝑑z_{d}italic_z start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT and the residual image zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT, and introduces a non-visual input znsubscript𝑧𝑛z_{n}italic_z start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. These four inputs are then passed into the LVLM to generate their corresponding output distributions: l⁢o⁢g⁢i⁢t⁢so𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑜logits_{o}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sd𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑑logits_{d}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT, l⁢o⁢g⁢i⁢t⁢sr𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑟logits_{r}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT and l⁢o⁢g⁢i⁢t⁢sn𝑙𝑜𝑔𝑖𝑡subscript𝑠𝑛logits_{n}italic_l italic_o italic_g italic_i italic_t italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT. The Jensen-Shannon Divergence (JSD) is computed between them to obtain J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT, and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT. The NVS method compares J⁢S⁢Dm⁢n𝐽𝑆subscript𝐷𝑚𝑛JSD_{mn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_m italic_n end_POSTSUBSCRIPT and J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT, and the input with the greater distance is selected as the decoupled image (e.g., zrsubscript𝑧𝑟z_{r}italic_z start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT). Next, ATCD selects the decoding strategy by comparing J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT and J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT, if J⁢S⁢Dc⁢n𝐽𝑆subscript𝐷𝑐𝑛JSD_{cn}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_c italic_n end_POSTSUBSCRIPT is greater, the decoupled image output distribution is employed to contrastively subtract the original distribution. Conversely, if J⁢S⁢Do⁢n𝐽𝑆subscript𝐷𝑜𝑛JSD_{on}italic_J italic_S italic_D start_POSTSUBSCRIPT italic_o italic_n end_POSTSUBSCRIPT is greater, the output distribution from the decoupled image is leveraged to contrastively enhance the weighted original distribution. Effectively correcting the hallucinated token (e.g., “phone” is successfully corrected to “sandwich”). Notably, this process is dynamically performed at each token generation step.",
        "qtype": "Others",
        "response": "When you lean on the “hallucinated” token‐probability vector (the red-boxed logits) as your backbone for the JS–divergence screening (the blue-boxed NVS module), a couple of brittle failure modes crop up:\n\n1. very small or very similar divergences  \n   – If both the “exposed” and “masked” images still generate distributions that lie close to the purely text-only (non-visual) baseline, their JS distances to that baseline will be nearly identical.  In practice that means you can’t tell which sub‐image really carries the next‐token cue, so the decoupling decision flips almost at random.  \n\n2. domination by language priors  \n   – Because the LLM’s logits on the original image are already biased by spurious linguistic correlations, the JS metric tends to reward whichever sub-image still “sounds like English,” rather than whichever actually contains the needed pixels.  That bias can completely swamp any tiny visual signal you were trying to pick up.  \n\n3. sensitivity to noise and overhead  \n   – Numerical noise in the prob-vectors or small perturbations (e.g. from temperature sampling) can swing the JS distances back and forth.  Since you’re running two full forward passes (dual and residual) plus the text-only pass each token, this also doubles or triples your compute cost, making the whole scheme both flaky and expensive.  \n\nTogether, these effects can cause the NVS module to mis‐identify which half of the image really holds the next token’s evidence, defeating the purpose of “decoupled” decoding."
    },
    {
        "question": "What motivates combining uncertainty estimation with prompting in segmentation methodology?",
        "relevant_section_ids": [
            "4.1.2"
        ],
        "relevant_context": [
            "Robustness in foundation models for medical image segmentation is crucial for consistent and accurate results across various conditions. It ensures reliable outcomes, regardless of changes in data input or computational environment. This issue is particularly vulnerable in SAM-based segmentation models, where users can input customized prompts, potentially leading to inconsistent results due to the variability and unpredictability of these user-defined inputs.",
            "Zhang et al. introduces uncertainty estimation for both SAM and MedSAM by using the change in segmentation boundaries as a function of prompt augmentation to generate uncertainty maps [2]. They propose that incorporating uncertainty estimations into SAMs builds trust through better error identification."
        ],
        "final_answer": "The combination is driven by the need to address robustness issues introduced by the variability and unpredictability of user-defined prompts and the ambiguous boundaries in medical images: by augmenting prompts and measuring how segmentation outputs change, uncertainty maps can be generated to highlight where the model is less confident, improving error identification and building trust in the segmentation results.",
        "relevant_elements": [
            "Uncertainty estimation",
            "prompting",
            "Segmentation"
        ],
        "id": 916,
        "masked_question": "What motivates combining [mask1] with prompting in segmentation methodology?",
        "masked_number": 1,
        "masked_elements": [
            "Uncertainty estimation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.15851v2_figure_1.png",
        "paperid": "2407.15851v2",
        "paper_path": "./papers/2407.15851v2.json",
        "figure_id": "2407.15851v2_figure_1.png",
        "caption": "Figure 1: A schematic overview of motivations, foundation model usage, tasks and trustworthiness enhancements discussed in this paper.",
        "qtype": "Design_Rationale",
        "response": "In that red‐boxed cell the paper is talking about “uncertainty estimation,” and the reason it is coupled with prompting is exactly to shore up SAM’s known robustness problems on medical images.  In practice medical scans often have low‐contrast or genuinely ambiguous boundaries, and SAM’s output can wander depending on exactly where you click or how you draw your box.  By perturbing the prompt (moving the point, jittering the box, etc.) and measuring how much the mask moves, you can build up an uncertainty map.  That uncertainty map then (a) flags regions the model is least confident about, (b) tells the clinician where to drop in extra prompts, and (c) overall provides a quantitative measure of “trustworthiness” or error‐likelihood.  In short, the poor tolerance of vanilla SAM to boundary ambiguity and prompt variability motivates combining uncertainty estimation with prompting, so that you can both locate and correct its weak spots."
    },
    {
        "question": "What motivates verifying solution plan against visible tests before drafting initial code?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "In Section 3: “However, the LLM-generated plan P may occasionally be incorrect, misguiding subsequent program generation. To avoid this, LPW queries the LLM to verify P against all visible tests.”",
            "In Section 1 (Introduction): “Different from other approaches that exclude the solution plan entirely from the code generation (Chen et al., 2023b; Zhong et al., 2024), LPW incorporates the LLM-generated plan and its verification in the initial code development to clarify the programming logic. This approach ensures that the initial code closely aligns with the problem description, thus reducing the need for subsequent refinements.”"
        ],
        "final_answer": "Because an LLM-generated plan can be wrong and lead the code astray, LPW first checks the plan against visible tests to confirm its correctness and to provide a clear, verified logical blueprint—ensuring initial code aligns with the problem and reducing later debugging.",
        "relevant_elements": [
            "Solution Plan",
            "Plan Verification"
        ],
        "id": 917,
        "masked_question": "What motivates verifying [mask1] against visible tests before drafting initial code?",
        "masked_number": 1,
        "masked_elements": [
            "Solution Plan"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.14503v1_figure_1.png",
        "paperid": "2411.14503v1",
        "paper_path": "./papers/2411.14503v1.json",
        "figure_id": "2411.14503v1_figure_1.png",
        "caption": "Figure 1: The pipeline of LPW, a large language model programming workflow, where the components highlighted in red are exclusive to LPW. LPW consists of two phases. In the solution generation phase, LPW initially creates a solution plan (block (b)) for a problem (block (a)), along with the plan verification (block (c)) for each visible test. If the plan verification infers the accurate output for each visible test based on the solution plan (block (c)) and no incorrect logic is found in the verification check process (block (d)), LPW uses the generated plan and plan verification to help LLMs draft the initial program (block (e)) at the beginning of the code implementation phase. If the initial program passes all visible tests after execution (block (f)), it is used as the final code (block (l)) and then assessed with hidden tests.\nOtherwise, the LLM-generated code explanation (block (g)) and error analysis (block (j)) serve as debugging inputs to refine the error program (block (k)). The LLM-generated error analysis involves comparing the execution trace (block (h)) with the plan verification (block (i)) on the failed visible test to identify logic flaws in the code implementation and provide repair suggestions. The refined program is reevaluated on the visible tests to determine the necessity for further debugging iterations.",
        "qtype": "Design_Rationale",
        "response": "Here “​[mask1]” is the LLM-generated solution plan. LPW forces that plan to be run “by hand” on all of the visible examples before any code is ever written in order to:\n\n  1.  Catch and correct any flawed reasoning or missing corner‐cases in the high-level design.  \n  2.  Produce a fully worked-out, step-by-step derivation (including all intermediate values) that is guaranteed to match the example outputs.  \n  3.  Give the model an unambiguous, test-validated specification to drive its first draft of code, rather than letting it spin off in the wrong direction.  \n\nIn short, by verifying the solution plan on the sample tests first—just like a miniature test-driven‐development step—LPW ensures that its initial implementation is grounded in a correct, complete logical sketch of the solution, cutting down on later debugging and refinement."
    },
    {
        "question": "What rationale supports comparing execution trace with plan verification in error analysis?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "In contrast to previous studies (Chen et al., 2023b ###reference_b12###; Zhong et al., 2024 ###reference_b59###; Shinn et al., 2023 ###reference_b49###) that query LLMs to infer errors in the generated code when it fails a visible test, LPW prompts LLMs to compare the expected output of each intermediate step for solving the failed visible test, as recorded in the plan verification, against the execution trace on the failed visible test to identify discrepancies and further produce an error analysis (block (j) in Figure 1 ###reference_###). This approach is more straightforward and reduces uncertainty. These discrepancies assist LLMs in accurately locating bugs and identifying logic flaws in the code implementation, and generating detailed refinement suggestions, as documented in the error analysis.",
            "LPW instructs the LLM to conduct an error analysis by identifying inconsistencies between the intermediate outputs recorded in the execution trace of P and the expected outputs documented in the verification, analyzing the causes, and offering refinement suggestions (block (4))."
        ],
        "final_answer": "By directly comparing actual runtime outputs (execution trace) with the expected intermediate outputs in the plan verification, LPW can more straightforwardly spot discrepancies, reduce uncertainty in debugging, and accurately locate bugs and logic flaws to produce precise refinement suggestions.",
        "relevant_elements": [
            "Execution Trace",
            "Plan Verification",
            "Error Analysis"
        ],
        "id": 918,
        "masked_question": "What rationale supports comparing [mask1] with plan verification in error analysis?",
        "masked_number": 1,
        "masked_elements": [
            "Execution Trace"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.14503v1_figure_1.png",
        "paperid": "2411.14503v1",
        "paper_path": "./papers/2411.14503v1.json",
        "figure_id": "2411.14503v1_figure_1.png",
        "caption": "Figure 1: The pipeline of LPW, a large language model programming workflow, where the components highlighted in red are exclusive to LPW. LPW consists of two phases. In the solution generation phase, LPW initially creates a solution plan (block (b)) for a problem (block (a)), along with the plan verification (block (c)) for each visible test. If the plan verification infers the accurate output for each visible test based on the solution plan (block (c)) and no incorrect logic is found in the verification check process (block (d)), LPW uses the generated plan and plan verification to help LLMs draft the initial program (block (e)) at the beginning of the code implementation phase. If the initial program passes all visible tests after execution (block (f)), it is used as the final code (block (l)) and then assessed with hidden tests.\nOtherwise, the LLM-generated code explanation (block (g)) and error analysis (block (j)) serve as debugging inputs to refine the error program (block (k)). The LLM-generated error analysis involves comparing the execution trace (block (h)) with the plan verification (block (i)) on the failed visible test to identify logic flaws in the code implementation and provide repair suggestions. The refined program is reevaluated on the visible tests to determine the necessity for further debugging iterations.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In LPW’s code-refinement loop, we have two pieces of information for a failed test  \n    a.  The plan verification: a text-based, step-by-step derivation of what the correct intermediate and final outputs should be.  \n    b.  The execution trace (the red-boxed module): the actual intermediate values the program printed when it ran on that same test.  \n\n2.  By comparing those two—i.e. “execution trace” vs. “plan verification”—we get a direct, point-by-point view of where the implementation diverges from the intended logic.\n\n3.  This direct comparison turns a vague bug description into a concrete discrepancy (e.g. “step 3 should be 10 but your code printed 8”), which  \n    •  pinpoints the faulty line or logic clause,  \n    •  reduces the LLM’s uncertainty about what went wrong, and  \n    •  yields precise, stepwise repair suggestions.\n\nThat is why LPW explicitly compares the execution trace with the plan verification during error analysis."
    },
    {
        "question": "How does the Verification Check module identify and correct logic errors in the Plan Verification outputs?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "For each t, its verification V compares the derived output ŷ with the ground-truth output y to assess the correctness of P, as outlined at block 4 in Figure 2.",
            "LPW includes two update steps in the solution generation phase to enable self-correction as indicated by the red arrows in Figure 2: 1) when the plan verification inferred ultimate output differs from the ground-truth output for a visible test, where ŷ≠y in T_vis, a revised solution plan P′ is included in the LLM response to substitute the original plan; 2) when the LLM detects any incorrect intermediate values in V, e.g. contextual inconsistencies, mathematical miscalculations, or logical reasoning errors, LPW prompts the LLM to regenerate the plan verification."
        ],
        "final_answer": "The Verification Check module runs the LLM‐generated plan through all visible tests and compares both the final outputs and each intermediate step against the known answers. If the final output for any test is wrong, it asks the LLM to produce a revised solution plan. If any intermediate value is inconsistent (due to a miscalculation or logical flaw), it prompts the LLM to regenerate the detailed verification. This loop continues until every intermediate result and the final outputs match the visible tests.",
        "relevant_elements": [
            "Plan Verification",
            "Verification Check"
        ],
        "id": 919,
        "masked_question": "How does the [mask1] module identify and correct logic errors in the Plan Verification outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Verification Check"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.14503v1_figure_1.png",
        "paperid": "2411.14503v1",
        "paper_path": "./papers/2411.14503v1.json",
        "figure_id": "2411.14503v1_figure_1.png",
        "caption": "Figure 1: The pipeline of LPW, a large language model programming workflow, where the components highlighted in red are exclusive to LPW. LPW consists of two phases. In the solution generation phase, LPW initially creates a solution plan (block (b)) for a problem (block (a)), along with the plan verification (block (c)) for each visible test. If the plan verification infers the accurate output for each visible test based on the solution plan (block (c)) and no incorrect logic is found in the verification check process (block (d)), LPW uses the generated plan and plan verification to help LLMs draft the initial program (block (e)) at the beginning of the code implementation phase. If the initial program passes all visible tests after execution (block (f)), it is used as the final code (block (l)) and then assessed with hidden tests.\nOtherwise, the LLM-generated code explanation (block (g)) and error analysis (block (j)) serve as debugging inputs to refine the error program (block (k)). The LLM-generated error analysis involves comparing the execution trace (block (h)) with the plan verification (block (i)) on the failed visible test to identify logic flaws in the code implementation and provide repair suggestions. The refined program is reevaluated on the visible tests to determine the necessity for further debugging iterations.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in Phase 1 is nothing mystical – it is simply an automated “sanity check” on the LLM’s own plan verification.  Concretely, LPW takes the step-by-step trace the LLM has produced (all of the intermediate values plus the final result) and runs it against each of the small, *visible* test cases.  It then compares, line by line, the LLM’s “derived” values to the *known* ground-truth values:\n\n  1.  If the *final output* on any visible test doesn’t match the ground truth, the module declares that the *plan* itself is wrong and prompts the LLM to *rewrite* the solution plan.  \n  2.  If the final output is correct but *some intermediate value* is off (a stray index, a mis-added term, etc.), the module flags that as a logic error in the *verification* and asks the LLM to *regenerate* the step-by-step verification.  \n\nThis two-pronged update loop (revise the plan when the end result is wrong; redo the verification when an intermediate step is wrong) repeats until every visible test’s intermediate and final values align perfectly with the ground truth.  At that point LPW considers the plan “clean” and moves on to code generation."
    },
    {
        "question": "How does Error Analysis integrate Execution Trace discrepancies with Plan Verification to generate refinement suggestions?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "LPW collects the runtime information on the first failed visible test u_n, indicating that the implementation in p_init deviates from the specifications in V.",
            "LPW instructs the LLM to conduct an error analysis by identifying inconsistencies between the intermediate outputs recorded in the execution trace of p_init and the expected outputs documented in the plan verification, analyzing the causes, and offering refinement suggestions (block (4)).",
            "Subsequently the error analysis and code explanation for p_init generated by the LLM (block (5)) are concatenated as the prompt to generate the refined program p_1 (block (6)). The code explanation helps the LLM align the text-based error analysis with the code implementation."
        ],
        "final_answer": "When the initial code fails a visible test, LPW captures the execution trace and compares each intermediate output in that trace against the corresponding expected output from the plan verification. The LLM then performs an error analysis: it pinpoints mismatches between actual and expected intermediate values, diagnoses their causes, and synthesizes concrete refinement suggestions. Finally, this error analysis—together with an LLM-generated code explanation—is fed back to the model to guide the generation of a corrected, refined program.",
        "relevant_elements": [
            "Error Analysis",
            "Execution Trace",
            "Plan Verification"
        ],
        "id": 920,
        "masked_question": "How does [mask1] integrate Execution Trace discrepancies with Plan Verification to generate refinement suggestions?",
        "masked_number": 1,
        "masked_elements": [
            "Error Analysis"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.14503v1_figure_1.png",
        "paperid": "2411.14503v1",
        "paper_path": "./papers/2411.14503v1.json",
        "figure_id": "2411.14503v1_figure_1.png",
        "caption": "Figure 1: The pipeline of LPW, a large language model programming workflow, where the components highlighted in red are exclusive to LPW. LPW consists of two phases. In the solution generation phase, LPW initially creates a solution plan (block (b)) for a problem (block (a)), along with the plan verification (block (c)) for each visible test. If the plan verification infers the accurate output for each visible test based on the solution plan (block (c)) and no incorrect logic is found in the verification check process (block (d)), LPW uses the generated plan and plan verification to help LLMs draft the initial program (block (e)) at the beginning of the code implementation phase. If the initial program passes all visible tests after execution (block (f)), it is used as the final code (block (l)) and then assessed with hidden tests.\nOtherwise, the LLM-generated code explanation (block (g)) and error analysis (block (j)) serve as debugging inputs to refine the error program (block (k)). The LLM-generated error analysis involves comparing the execution trace (block (h)) with the plan verification (block (i)) on the failed visible test to identify logic flaws in the code implementation and provide repair suggestions. The refined program is reevaluated on the visible tests to determine the necessity for further debugging iterations.",
        "qtype": "Implementation_Details",
        "response": "Step‐by‐step reasoning:\n\n1. In the code‐implementation phase, when the current draft of the program  fails on a visible test  (block 2), LPW first instruments  with print statements so that each intermediate variable or subexpression’s value is logged at runtime. Running this instrumented code on  produces an execution trace (block h).\n\n2. In parallel, we already have from the solution‐generation phase a fully worked‐out “plan verification” for the same test case (block i): a sequence of intermediate values and final output that the correct implementation ought to produce.\n\n3. The red‐boxed component in the figure (block j), “Error Analysis,” takes both of these artefacts as input:\n   – the actual runtime values from the execution trace  \n   – the expected values from the plan verification  \n\n4. It aligns them step by step, looking for the first point where they diverge. At that point of divergence, it diagnoses the likely mistake (wrong arithmetic, off‐by‐one loop bound, incorrect branching, etc.).\n\n5. Finally, it converts that diagnosis into a concrete “refinement suggestion” – e.g. “change the loop condition to ≤”, “swap the order of these two assignments,” or “use integer division here instead of floating‐point division” – which then gets fed back into the LLM prompt to generate a refined program.\n\nIn this way, the Error Analysis module tightly couples the observed execution‐trace discrepancies with the expected plan‐verification values, localizes the bug, and proposes targeted code fixes."
    },
    {
        "question": "How does the Encoder alter group vectors for input into the cross attention block?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Firstly, group vectors are transformed by a condition encoder:\n    \\[ h = \\mathrm{Enc}(g), \\]\n    where Enc can be a multi-layer perception (MLP)."
        ],
        "final_answer": "The Encoder applies a small condition encoder (e.g. an MLP) to each raw group vector g, producing an encoded vector h = Enc(g). This encoded group vector h is then fed as the query input into the cross-attention block.",
        "relevant_elements": [
            "Encoder",
            "group vectors",
            "cross attention"
        ],
        "id": 921,
        "masked_question": "How does the [mask1] alter group vectors for input into the cross attention block?",
        "masked_number": 1,
        "masked_elements": [
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.02791v1_figure_1.png",
        "paperid": "2410.02791v1",
        "paper_path": "./papers/2410.02791v1.json",
        "figure_id": "2410.02791v1_figure_1.png",
        "caption": "Figure 1: The flowchart of DifFaiRec. The original rating vector is 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. In the forward process, the vector is added with Gaussian noise T𝑇Titalic_T times, becoming 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. In the reverse process, 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, the group vectors, and the time step are fed into DifFaiRec to estimate the noise, and then the missing ratings in 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT can be recovered after T𝑇Titalic_T times of denoising.",
        "qtype": "Implementation_Details",
        "response": "The little blue box in the red frame is nothing mystical – it is simply the “condition‐encoder” (denoted Enc in the text).  In practice what it does is:\n\n1.  Take your raw group‐vector (either u₊ or u₋)  \n2.  Feed it through a small MLP (this is Enc(·))  \n3.  Produce a new embedding c = Enc(u₊/u₋)  \n\nThat new vector c now goes into the cross‐attention module (as the Query) instead of the raw group‐vector.  By doing this the network can learn to “mix and re-weight” the different dimensions of the group representation before it ever meets the user‐rating vectors in the attention block."
    },
    {
        "question": "How does the Reverse Process integrate time step embeddings and group vectors to predict the noise term?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "ε_θ can be formulated as follows through parameterization: where ε_θ is a noise approximator which can be inferred by diffusion model, and c based on user's group.",
            "In this work, as shown in Figure 1, we let ε_θ(x_t, t, c) take as input the current noisy sample x_t, the embedding vector z_t (one-hot encoding of the diffusion time step), and the output of the counterfactual module (the group-conditioned vector)."
        ],
        "final_answer": "In the reverse (denoising) process, the noise predictor ε_θ is implemented as a neural network that is conditioned on three things: the noisy rating vector x_t, a learned embedding z_t encoding the current diffusion time step, and the counterfactual group vector produced by the counterfactual module. By concatenating (or otherwise fusing) z_t and the group vector together with x_t, ε_θ(x_t, t, c) jointly integrates temporal information and group-fairness information to predict the noise term at each reverse step.",
        "relevant_elements": [
            "Reverse Process",
            "time step",
            "group vectors"
        ],
        "id": 922,
        "masked_question": "How does the [mask1] integrate [mask2] embeddings and group vectors to predict the noise term?",
        "masked_number": 2,
        "masked_elements": [
            "Reverse Process",
            "time step"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.02791v1_figure_1.png",
        "paperid": "2410.02791v1",
        "paper_path": "./papers/2410.02791v1.json",
        "figure_id": "2410.02791v1_figure_1.png",
        "caption": "Figure 1: The flowchart of DifFaiRec. The original rating vector is 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. In the forward process, the vector is added with Gaussian noise T𝑇Titalic_T times, becoming 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. In the reverse process, 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, the group vectors, and the time step are fed into DifFaiRec to estimate the noise, and then the missing ratings in 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT can be recovered after T𝑇Titalic_T times of denoising.",
        "qtype": "Implementation_Details",
        "response": "In our notation, the little red‐boxed module in Fig. 1 is simply the “reverse’’ or denoising pass of the diffusion model – in other words it is the εₜₕ noise‐predictor network – and the little blue‐boxed label is the one‐hot “time‐step” embedding eₜ.  What that diagram is showing you is that, at each denoising step t, we take\n\n  1. the noisy rating vector xₜ  \n  2. the time‐step embedding eₜ (blue box)  \n  3. the counterfactually adjusted group‐vector from C_g(·)  \n\nwe run the last two jointly through a small MLP (“condition encoder”), we run xₜ through another MLP, and then we fuse those two streams with a Q–K–V cross‐attention block.  The output of that attention is passed through a final feed–forward head, and that is exactly εₜₕ(xₜ, eₜ, C_g(g)) – the predicted noise term we need in the usual reverse‐diffusion update."
    },
    {
        "question": "How does Encoder output steer QKV attention for noise estimation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Encoder",
            "Cross attention"
        ],
        "id": 923,
        "masked_question": "How does [mask1] output steer QKV attention for noise estimation?",
        "masked_number": 1,
        "masked_elements": [
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.02791v1_figure_1.png",
        "paperid": "2410.02791v1",
        "paper_path": "./papers/2410.02791v1.json",
        "figure_id": "2410.02791v1_figure_1.png",
        "caption": "Figure 1: The flowchart of DifFaiRec. The original rating vector is 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. In the forward process, the vector is added with Gaussian noise T𝑇Titalic_T times, becoming 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. In the reverse process, 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, the group vectors, and the time step are fed into DifFaiRec to estimate the noise, and then the missing ratings in 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT can be recovered after T𝑇Titalic_T times of denoising.",
        "qtype": "Literature_Background",
        "response": "First, let’s be crystal‐clear about what that little red box is.  If you look at Figure 1 in the dashed “training loop” you’ll see:\n\n  – Two small MLPs take the noisy rating vector xₜ and produce a pair of vectors (one of which will be used as Query, the other as Key).  \n  – A third stream — highlighted in red — is labeled “Encoder.”  This is our [mask1].  \n\nThat “Encoder” is nothing mystical: it simply takes the counterfactual‐group vector (uₖ or uₗ) and turns it into a joint embedding (we concatenate it with the time‐step embedding).  That encoded vector then goes into the cross‐attention block as the remaining component of the usual Q – K – V set.\n\nConcretely, at each diffusion step t the model builds:\n\n  • Q = f₁(xₜ)  \n  • K = f₂(xₜ)  \n  • V = Encoder([group‐vector ⊕ time‐embedding])  \n\nand then runs a standard scaled‐dot‐product attention:\n\n    Attention(Q,K,V) = Softmax((Q Kᵀ)/√d) V.\n\nBecause the values V (and keys K, if you like) come from that Encoder, the cross‐attention will automatically “look up” the parts of the noisy rating xₜ that are most relevant to the current counterfactual group.  In other words, the encoder’s output steers the attention head so that εₜ (the predicted noise residual) is computed in a group‐conditioned way.  This is exactly how the model enforces group‐fair denoising: the Encoder injects the group‐information into the K/V streams of the QKV attention, biasing the noise‐estimate toward the counterfactual conditioning."
    },
    {
        "question": "How do group vectors and time step embeddings interact in the noise prediction network?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "… can be formulated as follows through parameterization: where ε_θ is a noise approximator which can be inferred by diffusion model, and G_t is based on user's group. In this work, as shown in Figure 1, we let\n    G_t = Enc(counterfactual_module_output) + e_t\n    where e_t denotes the embedding vector (one-hot encoding) of time step t and G_t denotes the counterfactual module.",
            "The parameter set Θ contains all parameters of ε_θ, Enc, and the counterfactual module."
        ],
        "final_answer": "In the noise-prediction network ε_θ, the time-step embedding e_t (a one-hot encoding of t) is added to the group-condition vector produced by the counterfactual module. Their sum serves as the conditioning input (G_t) alongside the noisy sample x_t for predicting the noise.",
        "relevant_elements": [
            "Group vectors",
            "Time step"
        ],
        "id": 924,
        "masked_question": "How do [mask1] and [mask2] embeddings interact in the noise prediction network?",
        "masked_number": 2,
        "masked_elements": [
            "Group vectors",
            "Time step"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.02791v1_figure_1.png",
        "paperid": "2410.02791v1",
        "paper_path": "./papers/2410.02791v1.json",
        "figure_id": "2410.02791v1_figure_1.png",
        "caption": "Figure 1: The flowchart of DifFaiRec. The original rating vector is 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT. In the forward process, the vector is added with Gaussian noise T𝑇Titalic_T times, becoming 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT. In the reverse process, 𝐱Tsubscript𝐱𝑇\\mathbf{x}_{T}bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT, the group vectors, and the time step are fed into DifFaiRec to estimate the noise, and then the missing ratings in 𝐱0subscript𝐱0\\mathbf{x}_{0}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT can be recovered after T𝑇Titalic_T times of denoising.",
        "qtype": "Literature_Background",
        "response": "In DifFaiRec’s denoiser (the box labelled “εθ” in Figure 1) the two “side-information” streams – (1) the counterfactual group vector (your red box, i.e. fφ(g)) and (2) the diffusion time-step (your blue box, t) – never get simply concatenated to xₜ all in one go.  Instead they are injected into two parallel branches of the network which talk to one another via a small cross‐attention (QKV) bottleneck.  \n\nConcretely:  \n  • On the left side you have an MLP that ingests xₜ and produces a set of query vectors Q.  \n  • The group-vector fφ(g) (the red-boxed embedding) is run through an encoder to become the keys and values K, V for that same cross-attention block.  \n  • That cross-attention layer thus “lets xₜ ask questions of the group embedding” and returns an attended feature.  \n  • On the right side you have a second MLP which takes two inputs: the output of that attention and the time‐step embedding sₜ (the blue-boxed embedding).  Typically sₜ is either a small learned lookup or a sinusoidal embedding which is then added or concatenated onto the attention features.  \n  • Finally that right‐hand MLP fuses them and spits out εθ(xₜ, t, fφ(g)), the predicted noise.  \n\nSo in short:  \n   – The red‐boxed group vector is injected via cross-attention (furnishing K/V).  \n   – The blue‐boxed time embedding is injected by a parallel branch (added into the second MLP’s inputs).  \n   – The only place they actually “meet” is downstream of that attention module, inside the right-hand MLP that takes both the attended group–conditioned features and the time-step embedding and transforms them into the final noise estimate."
    },
    {
        "question": "How does linear decoupling improve neural network's ability to approximate NLSE operators?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "To mitigate the linear effects after the entire transmission and introduce physical formula modeling, we adopt Feature Decoupling Distribution (FDD) modeling in the neural network part. This approach reduces the difficulty of fitting the neural network, however, the neural network itself no longer conforms to the NLSE.",
            "To allow the neural network to fully fit the NLSE, a linear system modeled using physical formulas is cascaded after the neural network. The linear system corresponds to the inverse process of dispersion compensation and distance attenuation compensation before the neural network, which can be regarded as a linear operator."
        ],
        "final_answer": "By extracting and handling the linear dispersion and attenuation effects with a physical‐modelled operator outside the network, the neural network only needs to learn the remaining nonlinear part of the NLSE. This ‘‘linear decoupling’’ simplifies the learning task, reducing the difficulty of fitting and thus improving the network’s ability to approximate the NLSE operators accurately.",
        "relevant_elements": [
            "linear decoupling",
            "neural network"
        ],
        "id": 925,
        "masked_question": "How does [mask1] improve neural network's ability to approximate NLSE operators?",
        "masked_number": 1,
        "masked_elements": [
            "linear decoupling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04511v1_figure_1.png",
        "paperid": "2411.04511v1",
        "paper_path": "./papers/2411.04511v1.json",
        "figure_id": "2411.04511v1_figure_1.png",
        "caption": "Figure 1: Two methods for fitting the NLSE.",
        "qtype": "Literature_Background",
        "response": "The little red “black‐box” in Figure 1 is nothing mystical – it is simply the analytic, linear part of the NLSE (the dispersion + loss operator) pulled out and applied by formula instead of being thrown on the neural network to learn from scratch.  Concretely, by inserting the step  \n  \n   ∂s/∂z  =  – Ď s  \n  \nahead of (and its inverse after) the Bi‐LSTM, you have “pre–compensated” for all of the known, linear evolution of the pulse.  The network only ever sees the *residual* (the purely nonlinear piece) and so has to approximate a much simpler mapping.  In practice this cuts the function-approximation problem in half, dramatically speeding up training, raising accuracy and improving the model’s ability to generalize over different fiber lengths."
    },
    {
        "question": "How does cascading linear operator with neural network reproduce full NLSE integration?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "To allow the neural network to fully fit the NLSE, a linear system modeled using physical formulas is cascaded after the neural network.",
            "The linear system corresponds to the inverse process of dispersion compensation and distance attenuation compensation before the neural network, which can be regarded as a linear operator, as shown in Equation (3).",
            "The equivalent system formed by cascading the neural network and the linear system can fit the complete NLSE system."
        ],
        "final_answer": "By feeding the signal through a neural network that learns the nonlinear part of the NLSE and then applying a physics-based linear operator (implementing inverse dispersion compensation and attenuation compensation) after it, the combined NN + linear cascade reproduces both the nonlinear and linear operators of the NLSE, thus fitting the full NLSE integration.",
        "relevant_elements": [
            "linear operator",
            "neural network"
        ],
        "id": 926,
        "masked_question": "How does cascading [mask1] with neural network reproduce full NLSE integration?",
        "masked_number": 1,
        "masked_elements": [
            "linear operator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04511v1_figure_1.png",
        "paperid": "2411.04511v1",
        "paper_path": "./papers/2411.04511v1.json",
        "figure_id": "2411.04511v1_figure_1.png",
        "caption": "Figure 1: Two methods for fitting the NLSE.",
        "qtype": "Literature_Background",
        "response": "In our figure the little red‐boxed block immediately downstream of the BiLSTM (labelled “∂s/∂z = D̂ s  (linear operator)”) is exactly the missing piece.  What it does is undo all of the artificial “decoupling” that went in before the network and re–impose the exact analytical solution of the linear part of the NLSE (i.e. dispersion + attenuation) in the frequency domain.  In other words, the network only ever has to learn the nonlinear term; as soon as it spits out its update, we pass its output through this closed-form linear propagator.  The result of running “BiLSTM → linear operator” once is mathematically identical to one full split–step of the NLSE.  By repeating that cascade span after span one therefore recovers the entire continuous NLSE solution."
    },
    {
        "question": "How does encoded distance z facilitate neural network training under linear decoupling for NLSE simulation?",
        "relevant_section_ids": [
            "2.2",
            "3.1"
        ],
        "relevant_context": [
            "To derive the output waveform with respect to the distance parameter z and enhance the model’s generalization over distances, we have implemented an encoded input method for z at the input end[8].",
            "We introduce a parameter z that controls the distance encoded by the neural network at the input end[8]. We utilize the feature of neural networks that can back propagate and take derivatives, we obtain the derivative of the output predicted waveform with respect to z.",
            "The transmission distance parameter z is also encoded by a linear layer and input into a neural network."
        ],
        "final_answer": "Under linear decoupling, the physical linear operator (dispersion and attenuation) is handled outside the network, so the neural network only needs to learn the nonlinear part of the NLSE. To enable it to capture how the nonlinear distortion accumulates over varying fiber lengths, the span distance z is encoded (via a learnable linear layer) and fed into the network at its input. By doing so, the network can back-propagate through z to compute ∂output/∂z, which is used in the NLSE loss, and it also generalizes naturally to distances not seen during training.",
        "relevant_elements": [
            "Encoded distance z",
            "neural network",
            "linear decoupling"
        ],
        "id": 927,
        "masked_question": "How does [mask1] facilitate neural network training under linear decoupling for NLSE simulation?",
        "masked_number": 1,
        "masked_elements": [
            "Encoded distance z"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04511v1_figure_1.png",
        "paperid": "2411.04511v1",
        "paper_path": "./papers/2411.04511v1.json",
        "figure_id": "2411.04511v1_figure_1.png",
        "caption": "Figure 1: Two methods for fitting the NLSE.",
        "qtype": "Experimental_Results",
        "response": "In the FDD‐based architecture the little red box hanging off the front of the Bi‐LSTM is nothing mystical – it is simply the *inverse*, linear propagation operator \n\n  ∂s/∂z = −D̂ s\n\napplied to the incoming waveform before it ever touches the net.  By stripping off (i.e. “decoupling’’) the exactly known dispersion + attenuation step in analytical form, the neural network no longer has to learn those two effects from data.  Instead it sees a signal that has already been “undone’’ for all the linear physics and only needs to fit the *residual* nonlinear evolution.  In practice this cuts the complexity of the regression problem almost in half, lets the Bi-LSTM converge far more quickly, and dramatically improves its ability to predict over unseen distances."
    },
    {
        "question": "How does cascading the neural network with the linear operator simplify NLSE modeling complexity in FDD?",
        "relevant_section_ids": [
            "2.2",
            "3.3"
        ],
        "relevant_context": [
            "From (2), it can be seen that the NLSE equation includes both linear and nonlinear operators. Therefore, to mitigate the linear effects after the entire transmission and introduce physical formula modeling, we adopt Feature Decoupling Distribution (FDD) modeling in the neural network part. This approach reduces the difficulty of fitting the neural network. To allow the neural network to fully fit the NLSE, a linear system modeled using physical formulas is cascaded after the neural network. The linear system corresponds to the inverse process of dispersion compensation and distance attenuation compensation before the neural network, which can be regarded as a linear operator. The equivalent system formed by cascading the neural network and the linear system can fit the complete NLSE system.",
            "In FDD model, the linear effects of the FDD model system are modeled by equations which incorporates prior physical knowledge of the linear part into the system, thus, at the beginning of the training, the linear part of the NLSE equation is already correct, allowing the neural network to focus on optimizing the nonlinear part."
        ],
        "final_answer": "By cascading a physically-modeled linear operator after the neural network, FDD offloads all of the linear dispersion and attenuation effects into an analytic block. This decouples the NLSE’s linear and nonlinear parts, so the neural network only needs to learn the remaining nonlinear dynamics. As a result, the fitting complexity is greatly reduced and training becomes easier and more accurate.",
        "relevant_elements": [
            "neural network",
            "linear operator"
        ],
        "id": 928,
        "masked_question": "How does cascading the [mask1] with the linear operator simplify NLSE modeling complexity in FDD?",
        "masked_number": 1,
        "masked_elements": [
            "neural network"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.04511v1_figure_1.png",
        "paperid": "2411.04511v1",
        "paper_path": "./papers/2411.04511v1.json",
        "figure_id": "2411.04511v1_figure_1.png",
        "caption": "Figure 1: Two methods for fitting the NLSE.",
        "qtype": "Experimental_Results",
        "response": "First, note that the little red-boxed block immediately before the Bi-LSTM in the FDD branch is simply a “linear decoupling” operator (–D̂s), and the red box right after the Bi-LSTM is its inverse (+D̂s).  In other words, the network in the middle is *only* ever asked to learn the *nonlinear* part of the NLSE, and all of the dispersion/attenuation physics are swept into the two linear blocks that we hook on by hand.  \n\nStep by step:\n\n1.  In the full NLSE you must simultaneously model both  \n    a.  Dispersion + attenuation (a *linear* operator D̂)  \n    b.  Kerr nonlinearity (a *nonlinear* operator)  \n\n2.  If you ask a black-box NN to learn the entire NLSE at once, it has to cover both effects at once—this makes the learning problem high-dimensional and poorly conditioned.\n\n3.  In the FDD approach, you *pre-cancel* the known linear part at the input (–D̂), let the network learn just the residual nonlinear mapping, and then *restore* the linear part at the output (+D̂).\n\n4.  By “cascading the NN block (the [mask1] operator) with the analytic linear block,” you reconstruct exactly the full NLSE operator, but you have divided the work:\n\n   • The linear dispersion/attenuation is done in closed form (zero training burden).  \n   • The NN only needs to fit the nonlinear Kerr term (much simpler!).  \n\n5.  The net result is a drastically reduced function-approximation problem, faster convergence, far better distance-generalization, and a lower overall NLSE loss.\n\nIn short, attaching the analytic linear operator before/after the net means the net never has to relearn dispersion or loss.  All it must do is learn the pure nonlinearity—hence the modeling complexity collapses."
    },
    {
        "question": "How does inter-view cross attention balance GPU memory efficiency and multi-view consistency within Multi-view Image Block?",
        "relevant_section_ids": [
            "4.2.2"
        ],
        "relevant_context": [
            "Given the small overlapping between field-of-views of adjacent cameras, we split each image latent z_I into two halves horizontally.",
            "For each view i, its left half z_i^L attends to the right half z_{i-1}^R of its left neighbor view, while its right half z_i^R depends on the left half z_{i+1}^L of its right neighbor.",
            "Compared to full cross-attention in MagicDrive (Gao et al., 2023), our split strategy significantly reduces the per-scene GPU memory cost of inter-view condition from 11GB to 3GB with better multi-view consistency."
        ],
        "final_answer": "Within the Multi-view Image Block, the inter-view cross-attention is implemented by splitting each image latent into left and right halves and only attending each half to its adjacent neighbor’s corresponding half. This ‘half-to-half’ cross-attention reduces the peak GPU memory from 11 GB to 3 GB while still improving consistency across views compared to a full cross-attention approach.",
        "relevant_elements": [
            "Inter-view Cross Attention",
            "Multi-view Image Block"
        ],
        "id": 930,
        "masked_question": "How does inter-view cross attention balance GPU memory efficiency and multi-view consistency within [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-view Image Block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01123v1_figure_2.png",
        "paperid": "2411.01123v1",
        "paper_path": "./papers/2411.01123v1.json",
        "figure_id": "2411.01123v1_figure_2.png",
        "caption": "Figure 2: Overview of our proposed X-Drive framework. We design a dual-branch diffusion model architecture to generate multi-modality data. Cross-modality epipolar condition modules (Fig. 3) are inserted between branches to enhance the cross-modality consistency.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the inter-view cross-attention in the red-boxed “Multi-view Image Block” trades off memory and consistency:\n\nChain-of-thought  \n1. In multi-camera setups each view only overlaps slightly with its neighbors.  \n2. A naïve “full” cross-attention over all views (as in MagicDrive) blows up the attention maps and needs ∼11 GB of GPU memory per scene.  \n3. X-Drive instead splits each view’s latent feature map into a left half and a right half.  \n4. It then only lets the left half of view i attend to the right half of view i–1, and the right half of view i attend to the left half of view i+1.  \n5. This “half-and-half” scheme slashes the attention matrix sizes (and thus GPU memory) down to ∼3 GB per scene.  \n6. At the same time, it still passes detailed, overlapping information between adjacent cameras, enforcing strong multi-view consistency in the shared regions.  \n\nAnswer  \nWithin the red-boxed multi-view block, X-Drive’s inter-view cross-attention splits each camera’s latent into two halves and only cross-attends overlapping halves between neighboring views. That focused, local attention reduces per-scene memory from about 11 GB to 3 GB, yet still stitches together consistent image content across the cameras."
    },
    {
        "question": "What potential limitations arise from Fourier embedding in PV box encoder under diverse object scales?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "PV box encoder",
            "Multi-view Image Block"
        ],
        "id": 932,
        "masked_question": "What potential limitations arise from Fourier embedding in [mask1] under diverse object scales?",
        "masked_number": 1,
        "masked_elements": [
            "PV box encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01123v1_figure_2.png",
        "paperid": "2411.01123v1",
        "paper_path": "./papers/2411.01123v1.json",
        "figure_id": "2411.01123v1_figure_2.png",
        "caption": "Figure 2: Overview of our proposed X-Drive framework. We design a dual-branch diffusion model architecture to generate multi-modality data. Cross-modality epipolar condition modules (Fig. 3) are inserted between branches to enhance the cross-modality consistency.",
        "qtype": "Others",
        "response": "In the multi‐view branch (the red‐boxed “perspective‐view box” encoder in Fig. 2b) every projected 3D‐corner coordinate is first lifted into a high‐dimensional space via a fixed, pre–specified Fourier feature map (i.e. a stack of sinusoids at prechosen frequencies) before being aggregated and fed into the cross–attention.  What that means in practice is:\n\n  1.  **Fixed frequency bands**  \n     Because the embedding uses a handful of sin/cos frequencies chosen once at design time, it tends to specialize to “medium”–sized spatial variations.  \n       •  Very small boxes (or small offsets between corners) live at the high end of the spectrum and can be poorly resolved unless you include very high frequencies.  \n       •  Very large boxes (or large corner displacements) wrap around the periodic embedding and become aliased unless you include very low frequencies.  \n\n  2.  **No intrinsic scale normalization**  \n     The same set of Fourier frequencies must handle centimeter‐level variations (e.g. fine edges of close objects) and meter‐level ones (e.g. far‐away buildings).  In the absence of an explicit multi‐scale or learned scaling factor, the network must somehow learn to “stretch” or “squeeze” those fixed sinusoids at inference time, which is awkward and often sub‐optimal.\n\n  3.  **Potential out‐of‐distribution coordinates**  \n     If at test time you try to render object corners outside the coordinate range seen during training, the periodic embedding will simply oscillate rather than extrapolate gracefully.  You can easily fall off the manifold that the model learned.\n\nIn short, by baking in a single–scale, fixed–bandwidth Fourier embedding of the box corners, you risk both aliasing large/deep‐scene boxes into the same code and under‐resolving tiny/close‐up boxes.  A truly scale‐adaptive positional encoding (e.g. a learned multi‐scale MLP or a sinusoid bank whose frequencies are conditioned on object size) would be required to cover the entire spectrum of real‐world object scales without drift or ambiguity."
    },
    {
        "question": "What privacy risks emerge from sharing mean embedding vectors when constructing cluster models across distributed robots?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "mean embedding vector",
            "cluster model"
        ],
        "id": 933,
        "masked_question": "What privacy risks emerge from sharing [mask1] when constructing cluster models across distributed robots?",
        "masked_number": 1,
        "masked_elements": [
            "mean embedding vector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04112v1_figure_1.png",
        "paperid": "2411.04112v1",
        "paper_path": "./papers/2411.04112v1.json",
        "figure_id": "2411.04112v1_figure_1.png",
        "caption": "Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use.",
        "qtype": "Others",
        "response": "When we say “share [mask1]” in the paper, the thing being shared is the mean‐embedding vector Vᵣ (highlighted in red in Fig. 1(c)).  Although it is much smaller than raw images or point clouds, these embeddings still carry information about the robot’s local visual scenery (lighting, textures, scene layout, even hints of private objects or locations).  By making these vectors available to the server (and, in a worst‐case compromise, to an eavesdropper), one opens up the following privacy risks:\n\n  • Reconstruction attacks/model inversion:  An adversary can train a decoder to turn embeddings back into approximate image patches or, at least, recover appearance and layout cues from them.  \n  • Membership inference:  Knowing a particular embedding came from a robot that’s repeatedly in a private facility (e.g. inside a factory or home) can let an attacker infer whether a given spot was in the training set.  \n  • Location and environment disclosure:  Embeddings cluster by scene type, so you can tell “this robot works indoors in a warehouse,” “that robot works on country roads,” etc.  Over time you can even track a robot’s migration from cluster to cluster (revealing its route, operating schedule, etc.).  \n\nIn short, even though Vᵣ is far lower‐bandwidth than raw images, it still encodes enough scene‐level detail that a determined adversary could extract sensitive or proprietary visual information."
    },
    {
        "question": "What scalability challenges emerge in robot clustering and cluster model aggregation as robot fleet scales?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "robot clustering",
            "cluster model"
        ],
        "id": 934,
        "masked_question": "What scalability challenges emerge in [mask1] and cluster model aggregation as robot fleet scales?",
        "masked_number": 1,
        "masked_elements": [
            "robot clustering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04112v1_figure_1.png",
        "paperid": "2411.04112v1",
        "paper_path": "./papers/2411.04112v1.json",
        "figure_id": "2411.04112v1_figure_1.png",
        "caption": "Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use.",
        "qtype": "Others",
        "response": "In our diagram, the red‐boxed region in (a) is exactly the “Robot Clustering and Model Aggregation” block on the server.  Everything that happens there—pulling in R mean–embedding vectors, running DBSCAN on them, averaging all of the local model weights in each one of the resulting clusters, and then shipping k different cluster models back out to the R robots—all of those things turn into scaling bottlenecks as R grows large:\n\n 1.  Clustering complexity.  Even though each robot only uploads a low–dimensional mean embedding, DBSCAN (or really any density–based or distance–based clustering) has to look at O(R) or worse, O(R²) pairwise distances to decide which robots belong to the same cluster.  With hundreds or thousands of robots that step alone can chew up CPU time and memory on the server.\n\n 2.  Model‐aggregation cost.  Once you’ve decided on k clusters, you have to average together the full model weights for each cluster.  That means k separate aggregation passes over potentially megabytes of parameters—again an O(R) or O(R · k) cost per round of communication.\n\n 3.  Communication load.  Every robot must send up its local weights (tens to hundreds of megabytes) each round, and the server in turn must send down the correct cluster model.  As R grows, both the aggregate upstream and downstream bandwidth requirements grow linearly (or worse, if k grows with R).\n\n 4.  Memory and bookkeeping.  The server has to store k distinct models and keep track of which robots live in which cluster at each round.  If the data distribution drifts over time or if brand‐new robots join in, that bookkeeping only gets more complicated and error‐prone.\n\nIn short, once you move past a few dozen robots, you begin to hit (a) rising compute cost for clustering, (b) rising compute cost for multiple model aggregations, (c) rapidly growing bandwidth usage, and (d) ever more complex state management on the server. Those are the scalability challenges lurking in the red box of Figure 1(a)."
    },
    {
        "question": "What motivates sharing a mean embedding vector along with local model weight?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Unlike previous methods where multiple rounds are needed [16 ###reference_b16###] or multiple models are communicated [17 ###reference_b17###], in each communication round the mean embedding vector which does not incur any additional communication cost is shared along with the local model.",
            "Using embeddings Fed-EC directly encodes the visual information of the local datasets. Similar embeddings represent robots deployed in similar regions or terrains. The mean embeddings are a single vector with small data sizes which are easy to upload."
        ],
        "final_answer": "Sharing the mean embedding alongside the local model enables the system to directly encode each robot’s local data distribution (as a compact visual embedding) so that robots in similar environments can be clustered together, while incurring minimal extra communication overhead due to the small size of the embedding vector.",
        "relevant_elements": [
            "Mean Embedding Vector",
            "Local Model Weight"
        ],
        "id": 935,
        "masked_question": "What motivates sharing a [mask1] along with [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Mean Embedding Vector",
            "Local Model Weight"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04112v1_figure_1.png",
        "paperid": "2411.04112v1",
        "paper_path": "./papers/2411.04112v1.json",
        "figure_id": "2411.04112v1_figure_1.png",
        "caption": "Figure 1: Workflow of Fed-EC. (a) Participating robots learn and communicates a mean embedding vector and local model weight to the server. The server clusters the robots using the mean embedding vector and aggregates local models in each cluster to learn a model which is shared with the robots based on their cluster identity. (b) The robots navigate to the a given GPS goal using the learnt model which takes as input RGB and depth images from the front facing camera. (c) If a new robot is deployed it computes a mean embedding and shares it with the server. The server assigns a cluster to the robot and sends the respective cluster model to the robot to use.",
        "qtype": "Design_Rationale",
        "response": "Let’s first identify the two masked items from the red and blue boxes in Fig. 1(a):\n\n  • Mask₁ (red box) is the per-robot mean embedding vector Vᵣ  \n  • Mask₂ (blue box) is the local model weights Wᵣ  \n\nNow, why do we send the mean embedding Vᵣ along with the model weights Wᵣ?\n\n1.  **Non-IID problem in federated learning**  \n    In a standard FL setup all robots share only model weights, but if each robot’s local data is drawn from a different distribution (urban streets vs. farm trails vs. forest paths), a single “global” model can be biased or converge slowly.\n\n2.  **Need for clustering**  \n    By grouping robots whose data look similar, we can aggregate only those local models whose training distributions are close, effectively creating multiple “personalized” global models—one per cluster. This avoids the pitfalls of averaging wildly different updates.\n\n3.  **Why embeddings?**  \n    –  Model weights or gradients only give an indirect signal of the underlying data, and comparing them to infer similarity is noisy.  \n    –  A mean embedding vector computed from each robot’s own images is a compact, direct summary of that robot’s visual environment.  \n\n4.  **Low extra cost**  \n    The mean embedding is just one small fixed-size vector per round, so we don’t blow up our bandwidth. We still only send one weight tensor (Wᵣ) and one embedding vector (Vᵣ).\n\nPutting it all together: sharing the mean embedding Vᵣ along with the local model weights Wᵣ lets the server cluster robots by actual data similarity—despite non-IID data—yet incurs almost zero extra communication cost."
    },
    {
        "question": "What motivates varying crossover rate and mutation rate in evolutionary algorithm for SNN privacy leak assessment?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Using the TennLab framework, we systematically varied parameters such as Population Size, Crossover Rate, Mutation Rate, Processor configurations, and Encoder settings. These variations were implemented to determine their impact on the privacy vulnerability of SNN models (Figure 1, green block)."
        ],
        "final_answer": "The crossover rate and mutation rate are varied in the evolutionary algorithm to determine how these genetic parameters affect the privacy vulnerability of SNN models under membership inference attacks.",
        "relevant_elements": [
            "Crossover Rate",
            "Mutation Rate",
            "Evolutionary Algorithm"
        ],
        "id": 937,
        "masked_question": "What motivates varying [mask1] and [mask2] in evolutionary algorithm for SNN privacy leak assessment?",
        "masked_number": 2,
        "masked_elements": [
            "Crossover Rate",
            "Mutation Rate"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06613v1_figure_1.png",
        "paperid": "2411.06613v1",
        "paper_path": "./papers/2411.06613v1.json",
        "figure_id": "2411.06613v1_figure_1.png",
        "caption": "Figure 1. Schematic representation of the experimental flow. The framework begins by comparing the privacy risks in ANNs and SNNs through MIA(yellow block). The SNN-specific exploration(green block) space evaluates privacy resilience using surrogate gradient algorithms and evolutionary algorithms with varying parameters. Finally, both ANNs and SNNs undergo DPSGD algorithm(blue block) to assess the privacy-utility tradeoff",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1. In the “SNN Exploration Space” (the green block in Fig. 1) the red‐outlined entry is the crossover rate and the blue‐outlined entry is the mutation rate.  \n2. Section 4.2 of the text tells us that, using an evolutionary algorithm, the authors “systematically varied parameters such as Population Size, Crossover Rate, Mutation Rate, Processor configurations, and Encoder settings.”  \n3. The stated goal of that phase is “to determine their impact on the privacy vulnerability of SNN models” under membership‐inference attacks.  \n4. Crossover and mutation are the two core operators in a genetic/evolutionary algorithm that govern exploration versus exploitation of the search space. By sweeping through different values for these hyperparameters, the authors can see whether—and how strongly—these choices affect how easily an attacker can infer membership (i.e. how much privacy leakage occurs).  \n\nAnswer  \nThey vary the crossover rate and mutation rate because those are the key knobs in the evolutionary algorithm that control how the SNN’s weights (and thus its behavior) evolve. By sweeping those two parameters they can measure how different evolutionary search dynamics influence the network’s susceptibility to membership‐inference attacks, and hence gauge which settings make the SNN more or less privacy‐vulnerable."
    },
    {
        "question": "What reasoning supports sampling minibatches of training data before applying clipped gradients and Gaussian noise in DPSGD?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Sampling minibatches of training data",
            "Clipped gradients",
            "Gaussian noise"
        ],
        "id": 938,
        "masked_question": "What reasoning supports [mask1] before applying clipped gradients and Gaussian noise in DPSGD?",
        "masked_number": 1,
        "masked_elements": [
            "Sampling minibatches of training data"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06613v1_figure_1.png",
        "paperid": "2411.06613v1",
        "paper_path": "./papers/2411.06613v1.json",
        "figure_id": "2411.06613v1_figure_1.png",
        "caption": "Figure 1. Schematic representation of the experimental flow. The framework begins by comparing the privacy risks in ANNs and SNNs through MIA(yellow block). The SNN-specific exploration(green block) space evaluates privacy resilience using surrogate gradient algorithms and evolutionary algorithms with varying parameters. Finally, both ANNs and SNNs undergo DPSGD algorithm(blue block) to assess the privacy-utility tradeoff",
        "qtype": "Design_Rationale",
        "response": "In DPSGD, the very first thing you do—pick a random minibatch of examples—is not just for computational convenience, but is in fact a key step in the privacy proof.  By sampling a fresh, uniformly random subset of the data at each step you\n\n  1. limit any one individual’s chance of affecting the gradient update (only those points in the minibatch can contribute at all),  \n  2. bound the sensitivity of your update (so that after you go on to clip each per-example gradient to a fixed ‖·‖≤C threshold, you know exactly how large the sum of those clipped gradients can be), and  \n  3. invoke the “privacy amplification by subsampling” lemma: because each record only has probability q (the batch size divided by the total dataset size) of appearing in any given batch, the overall ε you pay in the end is effectively reduced by roughly that factor.\n\nOnly after you have randomly sampled the minibatch and computed/​clipped the per-example gradients can you safely add Gaussian noise calibrated to that clipped‐gradient sensitivity and get a rigorous (ε,δ)‐DP guarantee."
    },
    {
        "question": "How does evolutionary algorithm use crossover rate and population size to optimize SNN encoder parameters?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Evolutionary Algorithm",
            "Crossover Rate",
            "Population"
        ],
        "id": 939,
        "masked_question": "How does [mask1] use crossover rate and population size to optimize SNN encoder parameters?",
        "masked_number": 1,
        "masked_elements": [
            "Evolutionary Algorithm"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06613v1_figure_1.png",
        "paperid": "2411.06613v1",
        "paper_path": "./papers/2411.06613v1.json",
        "figure_id": "2411.06613v1_figure_1.png",
        "caption": "Figure 1. Schematic representation of the experimental flow. The framework begins by comparing the privacy risks in ANNs and SNNs through MIA(yellow block). The SNN-specific exploration(green block) space evaluates privacy resilience using surrogate gradient algorithms and evolutionary algorithms with varying parameters. Finally, both ANNs and SNNs undergo DPSGD algorithm(blue block) to assess the privacy-utility tradeoff",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in Figure 1 is nothing mysterious – it is simply the evolutionary‐algorithm optimizer that lives inside the SNN exploration space.  In practice this is implemented (via EONS in the TENNLab framework) by “encoding” every SNN hyper-parameter—including the choice and internal settings of your spike encoder—into a genome.  The two knobs you ask about do exactly what they would in any genetic algorithm:\n\n  1. Population size  \n     •   This is just the number of distinct encoder-parameter sets (i.e. genomes) you carry forward at each generation.  \n     •   A larger population lets you explore more combinations of encoding-scheme parameters in parallel (better coverage), whereas a smaller one focuses you more narrowly on only the very best candidates (faster but riskier).  \n\n  2. Crossover rate  \n     •   Once you’ve scored (i.e. evaluated membership-inference vulnerability of) all individuals in your population, you pick parents by fitness.  \n     •   With probability equal to the crossover rate you then swap (“cross over”) chunks of their encoder-parameter genes to produce two new offspring genomes.  \n     •   By tuning this rate you control how aggressively encoder settings from different high-fitness parents get mixed into new candidate encoders.  \n\nOver many generations the combination of a suitably-sized population and a well-chosen crossover rate drives the optimizer toward encoder parameter settings that minimize MIA AUC."
    },
    {
        "question": "How are per example gradients norm-clipped and averaged prior to Gaussian noise addition in DPSGD?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Per example gradients",
            "Clipped gradients",
            "Gaussian noise"
        ],
        "id": 940,
        "masked_question": "How are [mask1] norm-clipped and averaged prior to [mask2] addition in DPSGD?",
        "masked_number": 2,
        "masked_elements": [
            "Per example gradients",
            "Gaussian noise"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.06613v1_figure_1.png",
        "paperid": "2411.06613v1",
        "paper_path": "./papers/2411.06613v1.json",
        "figure_id": "2411.06613v1_figure_1.png",
        "caption": "Figure 1. Schematic representation of the experimental flow. The framework begins by comparing the privacy risks in ANNs and SNNs through MIA(yellow block). The SNN-specific exploration(green block) space evaluates privacy resilience using surrogate gradient algorithms and evolutionary algorithms with varying parameters. Finally, both ANNs and SNNs undergo DPSGD algorithm(blue block) to assess the privacy-utility tradeoff",
        "qtype": "Implementation_Details",
        "response": "In DPSGD, the steps in the blue-boxed flow are as follows:  \n1. Compute a separate gradient for each example in the minibatch (the red-boxed “per-example gradients”).  \n2. Clip each of those per-example gradients to a fixed ℓ₂–norm bound C.  \n3. Average the clipped gradients over the minibatch.  \n4. Finally, add Gaussian noise (the blue-boxed noise) to that averaged gradient before applying the update."
    },
    {
        "question": "How are Predictions influenced by random edge masks in Masked Graphs within the GNN Classifier?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "Specifically, for each node, we randomly select a proportion ρ of its neighbors to mask through edge masking, and we conduct this random mask operation for M times with replacement. Through this strategy, we generate a set of masked graphs denoted as {G^t}, where G^t represents the masked graph generated in the t-th trial.",
            "Given a graph G^t with node features X and adjacency matrix A^t, the label probability for all nodes can be inferred using the backbone GNN, represented as follows: P^t = GNN(X, A^t), where P^t is the output matrix of the model under the t-th masked graph."
        ],
        "final_answer": "Each prediction is obtained by applying the GNN classifier to a differently masked version of the original graph—where a random subset of edges (neighbors) has been removed. As a result, the model produces a set of predictions (P^1, P^2, …, P^M), each reflecting a distinct neighbor context and thereby reducing the influence of any single (potentially noisy) neighbor.",
        "relevant_elements": [
            "Masked Graphs",
            "GNN Classifier",
            "Predictions"
        ],
        "id": 941,
        "masked_question": "How are [mask1] influenced by random edge masks in Masked Graphs within the GNN Classifier?",
        "masked_number": 1,
        "masked_elements": [
            "Predictions"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11020v1_figure_1.png",
        "paperid": "2411.11020v1",
        "paper_path": "./papers/2411.11020v1.json",
        "figure_id": "2411.11020v1_figure_1.png",
        "caption": "Figure 1: A comparison between our proposed label ensemble method and existing reliable labeling methods. The colors of the nodes represent their labels, while the symbols ”✓” and ”×” indicate whether the nodes are correctly or incorrectly labeled, respectively. Intuitively, our method avoids directly mistaking erroneous labels as correct by label ensemble. The experimental details of (b) are inline with Section V-E.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed region in the figure is simply the “Predictions” produced by the GNN when it is run on each of the masked graphs.  In LEGNN’s bootstrapping step we take the original graph, randomly drop (mask) a fraction of its edges to form K different adjacency matrices {A₁,…,A_K}, and then run our backbone GNN f_θ on each masked graph in turn.  Concretely, for the k-th trial we compute\n\n   Ŷ^(k) = f_θ(X, A_k).\n\nBecause each A_k has a different, randomly chosen subset of edges removed, the message‐passing neighborhood seen by each node is slightly different in each trial.  Those differences directly feed into the GNN’s aggregation and thus alter its output probabilities and final hard‐class predictions.  In other words, the “Predictions” (mask₁) are exactly the per-mask outputs Ŷ^(1), Ŷ^(2), … Ŷ^(K), and their diversity (which we then ensemble) comes from the randomness in the edge‐masking step."
    },
    {
        "question": "How does Gathered Label resolve conflicting Predictions to build the final label ensemble?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Subsequently, we ensemble these labels to construct the high‐probability multi‐labels. Concretely, we create a high‐probability multi‐label matrix Y⁺, where Y⁺_{i,c} indicates that the c‐th label is a high‐probability label for node i, otherwise, it indicates that it is not a high‐probability label. This can be formalized as follows: Y⁺ = ⋁_{t=1}ⁿ Y^{t⁺}. Consequently, the i‐th row Y⁺_i corresponds to the high‐probability multi‐labels for node i. This matrix retains predicted labels from different neighbor contexts, avoiding strong misguidance to the model where a single erroneous label is considered correct.",
            "Similarly, the low‐probability multi‐label matrix Y⁻ can be formally expressed as Y⁻ = ⋁_{t=1}ⁿ Y^{t⁻}. Here, Y⁻_{i,c} indicates that label c is a low‐probability label for node i. By ensembling the labeling results from multiple bootstrapped neighbor contexts, we generate both high‐probability and low‐probability multi‐labels for each node."
        ],
        "final_answer": "Instead of picking a single label when different masked graphs predict different classes, LEGNN takes the union (logical OR) of all high‐probability predictions (and separately all low‐probability predictions) across the T bootstrapped masks to form two multi‐label sets per node. In this way, any label predicted under any mask is included in the final ensemble, so conflicting predictions are resolved by retaining all of them as partial labels rather than forcing a single choice.",
        "relevant_elements": [
            "Predictions",
            "Gathered Label"
        ],
        "id": 942,
        "masked_question": "How does [mask1] resolve conflicting Predictions to build the final label ensemble?",
        "masked_number": 1,
        "masked_elements": [
            "Gathered Label"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11020v1_figure_1.png",
        "paperid": "2411.11020v1",
        "paper_path": "./papers/2411.11020v1.json",
        "figure_id": "2411.11020v1_figure_1.png",
        "caption": "Figure 1: A comparison between our proposed label ensemble method and existing reliable labeling methods. The colors of the nodes represent their labels, while the symbols ”✓” and ”×” indicate whether the nodes are correctly or incorrectly labeled, respectively. Intuitively, our method avoids directly mistaking erroneous labels as correct by label ensemble. The experimental details of (b) are inline with Section V-E.",
        "qtype": "Implementation_Details",
        "response": "In LEGNN the red‐boxed module is simply the “gathered label” step – i.e. the final label ensemble.  Concretely it works as follows:\n\n 1. LEGNN first runs the current GNN on K different masked versions of the graph.  Each run produces a per‐node soft prediction over all C classes.\n 2. For each node/class pair it collects two tally-lists across those K masks:  \n    • A “high‐probability” list of every mask in which that class score exceeded a preset threshold,  \n    • A “low‐probability” list of every mask in which it fell below another (lower) threshold.  \n 3. It then builds two C-dimensional multi-label vectors per node:  \n    – Y⁺ marks a class if it appeared sufficiently often in the high-prob lists.  \n    – Y⁻ marks a class if it appeared sufficiently often in the low-prob lists.  \n 4. Finally the “gathered label” for each node is the combination of Y⁺ and Y⁻: any class with strong positive support is taken as a candidate label, and any class with strong negative support is explicitly excluded.  \n\nBy turning each conflicting prediction into a simple vote over many random neighbourhoods – and by keeping both the “tops” (Y⁺) and the “flops” (Y⁻) – LEGNN automatically overrules spurious one-off mistakes and produces a much more robust multi-label ensemble."
    },
    {
        "question": "How does label ensemble's Gathered Label module differ from label refinement's Refined Label module in handling erroneous labels?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "Section 1: “When the noise level increases, simply encouraging the homophily could be problematic: a node might be similar to many nodes with inaccurate labels; directly using these incorrect signals actually introduces further noise, and eventually leads to unreliable labeling.”",
            "Section 4.2: “This matrix retains predicted labels from different neighbor contexts, avoiding strong misguidance to model where a single erroneous label is considered correct.”"
        ],
        "final_answer": "In contrast to the traditional Refined Label module—which collapses all neighbor information into one single label under the homophily assumption and thus can be driven astray when many neighbors are mislabeled—the Gathered Label module in label ensemble collects multiple label predictions across randomly masked neighbor contexts to form a set of high-probability (and low-probability) candidate labels. By keeping all these bootstrapped predictions rather than picking just one, it prevents any one erroneous neighbor label from dominating and instead dilutes its influence, thereby reducing the risk of propagating a single wrong label.",
        "relevant_elements": [
            "Gathered Label",
            "Refined Label"
        ],
        "id": 943,
        "masked_question": "How does label ensemble's [mask1] module differ from label refinement's [mask2] module in handling erroneous labels?",
        "masked_number": 2,
        "masked_elements": [
            "Gathered Label",
            "Refined Label"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11020v1_figure_1.png",
        "paperid": "2411.11020v1",
        "paper_path": "./papers/2411.11020v1.json",
        "figure_id": "2411.11020v1_figure_1.png",
        "caption": "Figure 1: A comparison between our proposed label ensemble method and existing reliable labeling methods. The colors of the nodes represent their labels, while the symbols ”✓” and ”×” indicate whether the nodes are correctly or incorrectly labeled, respectively. Intuitively, our method avoids directly mistaking erroneous labels as correct by label ensemble. The experimental details of (b) are inline with Section V-E.",
        "qtype": "Literature_Background",
        "response": "In the picture, the blue‐framed block ([mask2]) is the classic “refined‐label” module: you take your noisy labels, run one pass of the GNN over the full graph, then pick only the highest‐confidence predictions (or flip low‐confidence ones) to form a single new label for each node. Any mistake you make here gets locked in and then propagates through the next round of training.\n\nBy contrast, the red‐framed block ([mask1]) is LEGNN’s “gathered‐label” module. Instead of committing to one label, it repeatedly masks out random subsets of a node’s neighbors, runs the GNN on each masked version, and then collects two sets of candidates per node – the labels it saw most often (“high‐probability”) and those it saw least often (“low‐probability”).  It then trains against both sets via a partial‐label objective that (a) weights the high‐probability set so correct ones dominate, and (b) uses the low‐probability set as a kind of negative signal.  In this way erroneous labels are never treated as single ground truths, but are diluted across many bootstrapped views and down‐weighted rather than blindly accepted."
    },
    {
        "question": "What relationship exists between Predictions and Gathered Label in reducing label noise relative to confidence-based selection?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "Section IV-A (Bootstrapped Neighbor Context) – Discussion: \"Suppose a target node with k neighboring nodes, the ratio of neighbors with erroneous labels is r. Directly generating refined labels by averaging neighbors will result in an error rate of r; on the contrary, if we aggregate the bootstrapped neighbor context and then vote, the error rate is ∑_{i=⌈k/2⌉}^k (k choose i)·r^i·(1−r)^{k−i}, which is smaller than r when r<0.5.\"",
            "Section IV-B (Symmetric Label Ensemble): \"By ensembling the labeling results from multiple bootstrapped neighbor contexts, we generate a high-probability and a low-probability multi-labels for each node. This matrix retains predicted labels from different neighbor contexts, avoiding strong misguidance to model where a single erroneous label is considered correct.\""
        ],
        "final_answer": "LEGNN takes the individual Predictions made on multiple randomly masked graphs and aggregates them—via majority voting or symmetric high-/low-probability ensemble—into a single Gathered Label. This ensemble of Predictions lowers the overall error rate (noise) compared to selecting labels solely by single-shot confidence, because the probability that a majority of masked-graph predictions err is much smaller than the error rate of any one prediction when the base noise r<0.5.",
        "relevant_elements": [
            "Predictions",
            "Gathered Label"
        ],
        "id": 944,
        "masked_question": "What relationship exists between [mask1] and Gathered Label in reducing label noise relative to confidence-based selection?",
        "masked_number": 1,
        "masked_elements": [
            "Predictions"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.11020v1_figure_1.png",
        "paperid": "2411.11020v1",
        "paper_path": "./papers/2411.11020v1.json",
        "figure_id": "2411.11020v1_figure_1.png",
        "caption": "Figure 1: A comparison between our proposed label ensemble method and existing reliable labeling methods. The colors of the nodes represent their labels, while the symbols ”✓” and ”×” indicate whether the nodes are correctly or incorrectly labeled, respectively. Intuitively, our method avoids directly mistaking erroneous labels as correct by label ensemble. The experimental details of (b) are inline with Section V-E.",
        "qtype": "Literature_Background",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In the bottom row of the figure, the red‐boxed block is the “Predictions” output by the GNN when it is run on each of the masked graphs.  \n2.  An arrow then points from those “Predictions” into the final “Gathered Label” box.  \n3.  In the text (Sec. IV-B and IV-D) they explain that LEGNN takes all those per-mask predictions and ensembles them—symmetrically collecting both the high-probability and low-probability labels—into one multi-label “Gathered Label.”  \n4.  By doing so, they dilute the effect of any single noisy prediction (arising from one particular mask or one overconfident but wrong output) and thus attain much lower error growth under increasing noise than a straight confidence‐thresholding approach.  \n\nSo the answer is:  \n[mask1] is the set of per-mask “Predictions,” and these are precisely what get aggregated (voted/ensembled) into the “Gathered Label.”  That ensembling of multiple masked‐graph predictions is what drives the much stronger label-noise robustness, compared to simply picking the top-confidence label."
    },
    {
        "question": "How do Objective Planner and Workflow Planner extend hierarchical decomposition methods from classical task planning approaches?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Objective Planner",
            "Workflow Planner"
        ],
        "id": 945,
        "masked_question": "How do [mask1] and [mask2] extend hierarchical decomposition methods from classical task planning approaches?",
        "masked_number": 2,
        "masked_elements": [
            "Objective Planner",
            "Workflow Planner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.03743v1_figure_1.png",
        "paperid": "2411.03743v1",
        "paper_path": "./papers/2411.03743v1.json",
        "figure_id": "2411.03743v1_figure_1.png",
        "caption": "Figure 1: (a) The framework of the iterative refinement of PROTEUS . (b) A detailed illustration of the working process of PROTEUS .\nFirst, the Data Description module generates a precise description of the proteomics dataset, based on which the Objective Planner proposes a series of research objectives. Based on each objective, the Workflow Planner then plans a sequence of analytical workflows, such as analyzing expression differences or labeling cell types. These planned workflows are executed using specialized bioinformatics tools and follow a fixed sequence of steps. The Workflow Updater and Objective Updater analyze the system’s latest results, based on which they refine the subsequent workflows and objectives. PROTEUS produces numerous scientific hypotheses for each research objective, and continues its analysis until it reaches a pre-determined maximum number of objectives. These framework designs facilitate a robust, end-to-end proteomics research pipeline.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought:\n\n1.  In the figure, the red box highlights the “Objective Planner” module and the blue box highlights the “Workflow Planner” module.  \n2.  Classical hierarchical task planners (e.g. HTN planners) start from a high‐level goal and decompose it immediately into low‐level actions or operators in one shot.  \n3.  PROTEUS instead inserts an extra planning layer above the traditional “task” (tool) level.  The Objective Planner ([mask1]) first inspects the data description and dynamically decomposes the overall scientific goal (“discover hypotheses in this proteomics dataset”) into a tailored sequence of intermediate research objectives (e.g. “identify disease biomarkers,” “investigate disease mechanisms,” “propose drug targets”).  \n4.  Then the Workflow Planner ([mask2]) takes each of those objectives, plus descriptions of available analysis methods, and further decomposes them into concrete pipelines of bioinformatics workflows (e.g. clustering → cell‐type annotation → statistical testing).  \n5.  In this way PROTEUS builds a three-tiered hierarchy—research objectives, workflows, and individual tool‐calling steps—rather than a single goal-to-primitive decomposition.  This extension allows data‐driven, iterative refinement at each level, better reflecting the multi-stage nature of scientific inquiry.  \n\nAnswer:\nmask1 = Objective Planner  \nmask2 = Workflow Planner"
    },
    {
        "question": "How do Workflow Updater and Objective Updater adapt iterative refinement strategies from previous machine learning pipelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Workflow Updater",
            "Objective Updater"
        ],
        "id": 946,
        "masked_question": "How do [mask1] and [mask2] adapt iterative refinement strategies from previous machine learning pipelines?",
        "masked_number": 2,
        "masked_elements": [
            "Workflow Updater",
            "Objective Updater"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.03743v1_figure_1.png",
        "paperid": "2411.03743v1",
        "paper_path": "./papers/2411.03743v1.json",
        "figure_id": "2411.03743v1_figure_1.png",
        "caption": "Figure 1: (a) The framework of the iterative refinement of PROTEUS . (b) A detailed illustration of the working process of PROTEUS .\nFirst, the Data Description module generates a precise description of the proteomics dataset, based on which the Objective Planner proposes a series of research objectives. Based on each objective, the Workflow Planner then plans a sequence of analytical workflows, such as analyzing expression differences or labeling cell types. These planned workflows are executed using specialized bioinformatics tools and follow a fixed sequence of steps. The Workflow Updater and Objective Updater analyze the system’s latest results, based on which they refine the subsequent workflows and objectives. PROTEUS produces numerous scientific hypotheses for each research objective, and continues its analysis until it reaches a pre-determined maximum number of objectives. These framework designs facilitate a robust, end-to-end proteomics research pipeline.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the two masked regions to the figure and text:\n\n  1. The red‐boxed module in the bottom‐right of Figure 1(b) is labeled “Workflow Updater.”  \n  2. The blue‐boxed module in the middle‐right of Figure 1(b) is labeled “Objective Updater.”\n\nNow, how do these two updater modules borrow the idea of iterative refinement from prior machine-learning pipelines?\n\nChain of Thought\n\n1. In modern ML and AutoML systems you often see two nested feedback loops:  \n   - A low-level loop that tunes parameters or pipelines step by step (e.g. hyperparameter tuning, pipeline selection).  \n   - A high-level loop that adjusts overall objectives or search targets (e.g. meta-learning, objective reweighting).\n\n2. PROTEUS mirrors that design:  \n   - The Workflow Updater sits at the granularity of individual analysis steps (tool calls, parameter choices). After each step or mini‐pipeline completes, it examines the raw execution outputs (plots, tables, logs) and updates the *next* workflow plan—much as an AutoML system would adjust its next trial configuration based on the last trial’s validation error.  \n   - The Objective Updater sits one level up. Once a full workflow (a sequence of tools) has run to completion, it ingests those *workflow-level* summaries and revises the *research objectives* to pursue next—analogous to how a meta-learner or higher-level scheduler would shift its global objective (for example, changing which metric to optimize or which subtask to tackle) based on end-to-end performance.\n\n3. By explicitly separating these two loops—step-level (Workflow Updater) and objective-level (Objective Updater)—PROTEUS brings the proven benefits of hierarchical, iterative refinement from ML (faster convergence, error correction, deeper exploration) into the domain of proteomics analysis.\n\nAnswer\n\nThe red–boxed “Workflow Updater” module implements a low-level feedback loop very much like the hyperparameter­–or pipeline–tuning stage in ML: after each analysis step or tool execution, it inspects the raw outputs and adjusts the next workflow (tool choices and parameters). The blue–boxed “Objective Updater” module implements the higher-level loop, analogous to meta-learning or AutoML’s objective search: once a full workflow has run, it uses those aggregate results to revise which research objectives the system should tackle next. Together, these two nested updater loops adapt the iterative refinement pattern—fine-grained step tuning plus coarse-grained objective adjustment—borrowed from previous machine-learning and AutoML pipelines."
    },
    {
        "question": "How does Objective Updater interact with Workflow Planner to refine objectives after workflow-level results?",
        "relevant_section_ids": [
            "2.1.2"
        ],
        "relevant_context": [
            "For a certain research objective, we prompt the LLM with the objective, the data description, and a list of descriptions of all available data analysis workflows, then instruct it to plan a series of workflows.",
            "Following each workflow execution, the LLM refers to the newly obtained workflow results to update the original plan in preparation for subsequent workflow execution.",
            "It performs a similar step after analyzing each objective, using the latest results to refine future research objectives."
        ],
        "final_answer": "After the Workflow Planner executes the planned workflows and produces results, the Objective Updater module takes those workflow-level results and feeds them back into the LLM. The LLM then updates the original research objectives based on the new findings, and these refined objectives are passed back to the Workflow Planner to guide and generate the next sequence of workflows.",
        "relevant_elements": [
            "Objective Updater",
            "Workflow Planner"
        ],
        "id": 947,
        "masked_question": "How does [mask1] interact with [mask2] to refine objectives after workflow-level results?",
        "masked_number": 2,
        "masked_elements": [
            "Objective Updater",
            "Workflow Planner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.03743v1_figure_1.png",
        "paperid": "2411.03743v1",
        "paper_path": "./papers/2411.03743v1.json",
        "figure_id": "2411.03743v1_figure_1.png",
        "caption": "Figure 1: (a) The framework of the iterative refinement of PROTEUS . (b) A detailed illustration of the working process of PROTEUS .\nFirst, the Data Description module generates a precise description of the proteomics dataset, based on which the Objective Planner proposes a series of research objectives. Based on each objective, the Workflow Planner then plans a sequence of analytical workflows, such as analyzing expression differences or labeling cell types. These planned workflows are executed using specialized bioinformatics tools and follow a fixed sequence of steps. The Workflow Updater and Objective Updater analyze the system’s latest results, based on which they refine the subsequent workflows and objectives. PROTEUS produces numerous scientific hypotheses for each research objective, and continues its analysis until it reaches a pre-determined maximum number of objectives. These framework designs facilitate a robust, end-to-end proteomics research pipeline.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify our two modules in the figure:  \n• [mask1] (red box) is the “Objective Updater.”  \n• [mask2] (blue box) is the “Workflow Planner.”  \n\nChain‐of‐thought (how they talk to each other):\n\n1.  The Workflow Planner (blue) takes the current research objectives and the data description and lays out one or more bioinformatics workflows to execute.  \n2.  Those workflows run, producing workflow‐level results (e.g. clustering annotations, statistical tests, enrichment outputs).  \n3.  The Objective Updater (red) then ingests those workflow results, distills the key findings and biological insights, and uses them to refine the high‐level research objectives.  \n4.  The newly refined objectives are fed back as input into the Workflow Planner.  \n5.  The Workflow Planner, now armed with updated, more focused objectives, designs the next round of workflows that will probe the most promising directions revealed so far.  \n\nIn this way, the Objective Updater analyses workflow outputs to sharpen or pivot the research goals, and the Workflow Planner immediately consumes those updated goals to generate a fresh set of workflows. This closed‐loop enables PROTEUS to iteratively home in on the most informative hypotheses."
    },
    {
        "question": "How does Result Analyzer inform Workflow Updater to refine analysis steps after tool execution?",
        "relevant_section_ids": [
            "2.1.2"
        ],
        "relevant_context": [
            "For interpreting results, PROTEUS supports various formats of tool outputs, including text, data files, and visualization plots, and analyzes notable results within the context of the research objective.",
            "Hierarchical Iterative Refinement. Proteomics research is an iterative process in which results from preliminary analysis stages can be conducive to deeper and more detailed exploration. Therefore, we enable PROTEUS to refine its plans after each execution stage. Following each workflow execution, the LLM refers to the newly obtained workflow results to update the original plan in preparation for subsequent workflow execution. It performs a similar step after analyzing each objective, using the latest results to refine future research objectives. These additional steps assist PROTEUS in both handling errors and deepening scientific inquiry."
        ],
        "final_answer": "After a tool finishes executing, the Result Analyzer (an LLM) ingests the raw outputs—be they text summaries, data files, or plots—and distills the statistically or biologically notable findings. It then passes these step-level insights into the Workflow Updater, which re‐examines and adjusts the remaining analysis plan (tool choices, parameters, or sequence of steps) so as to handle any errors, follow up on unexpected but important trends, or pursue more detailed exploration in subsequent stages.",
        "relevant_elements": [
            "Result Analyzer",
            "Workflow Updater"
        ],
        "id": 948,
        "masked_question": "How does [mask1] inform [mask2] to refine analysis steps after tool execution?",
        "masked_number": 2,
        "masked_elements": [
            "Result Analyzer",
            "Workflow Updater"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.03743v1_figure_1.png",
        "paperid": "2411.03743v1",
        "paper_path": "./papers/2411.03743v1.json",
        "figure_id": "2411.03743v1_figure_1.png",
        "caption": "Figure 1: (a) The framework of the iterative refinement of PROTEUS . (b) A detailed illustration of the working process of PROTEUS .\nFirst, the Data Description module generates a precise description of the proteomics dataset, based on which the Objective Planner proposes a series of research objectives. Based on each objective, the Workflow Planner then plans a sequence of analytical workflows, such as analyzing expression differences or labeling cell types. These planned workflows are executed using specialized bioinformatics tools and follow a fixed sequence of steps. The Workflow Updater and Objective Updater analyze the system’s latest results, based on which they refine the subsequent workflows and objectives. PROTEUS produces numerous scientific hypotheses for each research objective, and continues its analysis until it reaches a pre-determined maximum number of objectives. These framework designs facilitate a robust, end-to-end proteomics research pipeline.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. In the bottom “Sequence of Steps” panel, the red‐boxed component is the Result Analyzer: after each tool runs (e.g. differential abundance, clustering), PROTEUS hands its raw output (tables, plots, metrics) to the Result Analyzer.  \n2. The Result Analyzer parses those outputs in the context of the current objective—it pulls out the statistically significant hits (e.g. cell types with p-values and log-fold changes), biological annotations, and key trends.  \n3. It then emits a concise, semantic summary of “what was found” (for example: “CD20 in TmDC is upregulated, logFC = 0.72, p = 0.003”).  \n4. That summary is handed to the blue‐boxed component, the Workflow Updater. The Workflow Updater cross-references those new findings against the remaining plan.  \n5. It uses the new insights to adjust the upcoming analysis steps—adding or reordering workflows, changing parameters (e.g. narrower subclustering on the newly highlighted cell type), or selecting alternative statistical tests to drill deeper into the observed signal.  \n6. In this way, the Result Analyzer’s interpreted outputs directly drive the Workflow Updater’s decision about how to refine and extend the analysis pipeline for the next iteration."
    },
    {
        "question": "How does the placement algorithm within Real-time Resource-aware Scheduling process frequency and model size inputs to optimize resource allocation?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "In line 1, we first investigate the frequency and data size of a request. If the current request frequency is larger than the frequency threshold and data size is smaller than the data size threshold, we deploy the request on the serverless platform.",
            "In line 1, we consider deploying the request with large data size on the Docker platform as the requests with large data sizes usually tolerate longer response time.",
            "In line 1, we deploy the requests using Flask (i.e., local web server) if the request has moderate data size and the current request frequency is low.",
            "In lines 1 and 1, requests with moderate data sizes are processed by Docker and serverless platforms only under specific conditions: when Flask is unavailable for more requests and the request frequency is moderate. Within this setup, Docker is given priority. Requests allocated to a Docker container will be run using RESTful APIs."
        ],
        "final_answer": "The algorithm takes each incoming request’s frequency and data (model) size, compares them to predefined thresholds, and then routes the request to the most suitable platform: high‐frequency, small‐size requests go to serverless; large‐size requests go to Docker; low‐frequency, moderate‐size requests run on the local Flask web server; and for moderate‐frequency, moderate‐size requests, Docker is tried first (falling back to serverless if Flask is saturated).",
        "relevant_elements": [
            "Placement Algorithm",
            "Real-time Resource-aware Scheduling"
        ],
        "id": 949,
        "masked_question": "How does the [mask1] within Real-time Resource-aware Scheduling process frequency and model size inputs to optimize resource allocation?",
        "masked_number": 1,
        "masked_elements": [
            "Placement Algorithm"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.18148v2_figure_1.png",
        "paperid": "2407.18148v2",
        "paper_path": "./papers/2407.18148v2.json",
        "figure_id": "2407.18148v2_figure_1.png",
        "caption": "Figure 1: The workflow of StraightLine.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed component in Layer 3 is simply the “Placement Algorithm.”  At runtime it continually watches two numbers for each incoming ML request—its instantaneous arrival rate (frequency) and the size of the model (or data) it must process—and then runs a tiny decision tree (line by line of Algorithm 1) to pick one of three execution targets:\n\n 1.  If frequency > fthreshold AND size < sthreshold  → send it to the serverless pool  \n 2.  Else if size > sthreshold  → launch it in a Docker container  \n 3.  Else (moderate size, low frequency) → dispatch to the local Flask web‐server  \n 4.  If Flask is already overloaded but the size/frequency are still “in between,” it will next try Docker, and only fall back to serverless if Docker is saturated.\n\nBy comparing (frequency, size) against those two empirically chosen thresholds, the placement algorithm automatically routes each request to the resource type that will give the best latency–cost trade-off."
    },
    {
        "question": "How does Layer 2's container customization adapt compressed models to heterogeneous infrastructures through RESTful APIs and AWS Lambda?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "StraightLine is designed for hybrid infrastructure so compressed models are implemented in three different ways: 1) local web server, 2) RESTful APIs, or 3) serverless computing. However, it is likely that the hybrid infrastructure cannot offer a compatible environment to many heterogeneous ML applications. Each computing unit in the hybrid infrastructure may run different operating systems, ML application runtime (e.g., TensorFlow [17], PyTorch [18], PyWren [19], etc.), and language environments (e.g., Python, Java, or R). It is imperative to consider the implementation difficulty resulting from software environment conflicts.",
            "We further offer the implementation of containerized ML applications. As shown in Figure 2, a containerized ML application only contains core information (e.g., model weights, and inference requirements) and the target environment (e.g., ML application runtime and language environment). Once a containerized ML application is triggered in the infrastructure, it can connect to the specified target environment and resources. When the task is finished, the provisioned resources will be released back to the infrastructure. Moreover, we can execute cross-platform ML implementation by specifying different target environments, such as different versions of Linux (e.g., Ubuntu), Windows or serverless environments.",
            "In practice, we use the Flask Python library to implement RESTful APIs for ML implementation since most machine learning libraries are built on Python. For serverless computing, we use AWS Lambda [15] to implement ML applications."
        ],
        "final_answer": "Layer 2 packages each compressed model into a lightweight Docker container that includes only its weights and inference code plus a descriptor of the desired runtime (e.g. OS, ML framework, language).  For on-premises or private servers it spins up a Flask-based RESTful API inside the container, and for serverless it wraps the same container as an AWS Lambda function.  In this way the identical compressed model can be deployed unchanged across heterogeneous infrastructures simply by selecting either the RESTful (Flask) endpoint or the AWS Lambda target.",
        "note": "",
        "relevant_elements": [
            "Container Customization",
            "RESTful APIs",
            "AWS Lambda"
        ],
        "id": 950,
        "masked_question": "How does Layer 2's [mask1] adapt compressed models to heterogeneous infrastructures through RESTful APIs and AWS Lambda?",
        "masked_number": 1,
        "masked_elements": [
            "Container Customization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.18148v2_figure_1.png",
        "paperid": "2407.18148v2",
        "paper_path": "./papers/2407.18148v2.json",
        "figure_id": "2407.18148v2_figure_1.png",
        "caption": "Figure 1: The workflow of StraightLine.",
        "qtype": "Experimental_Results",
        "response": "Layer 2 in StraightLine is literally what makes your compressed .h5 models “just work” everywhere—on Windows, Ubuntu, bare-metal, or serverless—by wrapping them up as either (a) a Flask-powered RESTful API in a Docker container or (b) an AWS Lambda function.   \n\nStep-by-step what happens in Layer 2 (“Container Customization”) is:  \n1. You take the compressed model (your .h5 file plus a tiny manifest of its Python/TensorFlow (or PyTorch) dependencies).  \n2. You generate two lightweight runtime artifacts:  \n   – A Docker image that bundles the model, the correct OS (e.g. Ubuntu or Windows Server core), the ML runtime, and a small Flask app exposing “/predict” (or whatever) over HTTP.  \n   – An AWS Lambda package that contains the same model and code, but in Lambda-compatible form.  \n3. At inference time you simply invoke one of these two endpoints—a local REST call against your container or an AWS Lambda remote call—depending on your SLA, data size, and request frequency.  \n4. Behind the scenes the container is spun up on your on-prem server or in a cloud VM, and the Lambda function is invoked in AWS. When the request completes, resources are torn down.  \n\nBecause every model is shipped in a self-contained container or function, it hides all OS/distribution/runtime differences.  You always talk to it via HTTP (“RESTful API”) or a Lambda invoke call, and you never have to worry about mismatched Python versions, missing libraries, or incompatible CUDA drivers."
    },
    {
        "question": "What limitations might arise from GPU Cluster Docker’s heavy workload requirement on model containerization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GPU Cluster Docker",
            "Model Containerization"
        ],
        "id": 951,
        "masked_question": "What limitations might arise from [mask1]’s heavy workload requirement on [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "GPU Cluster Docker",
            "Model Containerization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.18148v2_figure_1.png",
        "paperid": "2407.18148v2",
        "paper_path": "./papers/2407.18148v2.json",
        "figure_id": "2407.18148v2_figure_1.png",
        "caption": "Figure 1: The workflow of StraightLine.",
        "qtype": "Others",
        "response": "Here’s the intuition step by step:\n\n1.  In Layer 1 (“Model Containerization”) we see a bullet that says “Heavy workload” placed alongside the GPU–Docker cluster.  That tells us this tier is optimized for long‐running, resource-intensive tasks (e.g. batch training), not short, light inference calls.\n\n2.  The question asks: what limitations come from that “heavy workload” requirement when you try to use the same GPU–Docker cluster for other purposes?\n\n3.  If you insist on driving the GPU–Docker cluster only when you have truly heavy jobs, you end up with:\n    •  Slow start-up and tear-down (spinning up a GPU-backed container takes time).  \n    •  Poor support for bursty, low-latency inference traffic (it’s over-provisioned for small jobs).  \n    •  High operational cost and energy use when it is kept idle “just in case.”  \n    •  Potential queuing or contention whenever a sudden flood of smaller tasks arrives, because the cluster is tuned for fewer, larger jobs.  \n    •  A dependence on high‐speed storage and stable long‐running GPU allocations, which further limits elasticity in a hybrid (edge/serverless) deployment.\n\n4.  In short, by sizing and tuning that layer exclusively for heavy workloads, you lose the ability to elastically handle lightweight, high-frequency inference calls, and you pay a premium (both in $ and in latency) whenever you repurpose it."
    },
    {
        "question": "What alternative strategies could enhance real-time resource-aware scheduling beyond the placement algorithm’s current heuristics?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "In the future, we plan to explore the following directions: (1) evaluate StraightLine across diverse scenarios and workloads and compare its performance with alternative approaches; (2) enhance StraightLine’s empirical dynamic placing algorithm to consider additional parameters of the models and characteristics of the applications (e.g., SLOs); and (3) further refine StraightLine to dynamically allocate resources based on real-time demand fluctuations and workload patterns, ensuring optimal resource utilization and performance."
        ],
        "final_answer": "Beyond the current threshold‐based placement heuristics, StraightLine could be improved by: (1) incorporating additional model and application characteristics—such as service‐level objectives (SLOs)—into its placement decisions, and (2) moving from static thresholds to a fully dynamic allocation scheme that adapts in real time to demand spikes and evolving workload patterns.",
        "relevant_elements": [
            "Real-time Resource-aware Scheduling",
            "Placement Algorithm"
        ],
        "id": 952,
        "masked_question": "What alternative strategies could enhance [mask1] beyond the [mask2]'s current heuristics?",
        "masked_number": 2,
        "masked_elements": [
            "Real-time Resource-aware Scheduling",
            "Placement Algorithm"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.18148v2_figure_1.png",
        "paperid": "2407.18148v2",
        "paper_path": "./papers/2407.18148v2.json",
        "figure_id": "2407.18148v2_figure_1.png",
        "caption": "Figure 1: The workflow of StraightLine.",
        "qtype": "Others",
        "response": "Here’s one way you might think through what’s in the red box (Layer 3, the “real-time, resource-aware scheduling” or “placement algorithm” module) and what’s in the blue box (its current heuristic: “place each incoming ML request based only on request frequency and model size”), and then sketch some ideas for how you could go beyond that very simple 2-feature rule:\n\n1.   Identify the gap  \n     - Right now StraightLine looks only at two numbers per request:  \n       •  How often it comes (frequency)  \n       •  How big the model is (size in bytes)  \n     - It then picks container vs. VM vs. serverless, etc., purely by some empirical rule over those two.  \n     - That leaves out a lot of real-world signals that can make or break latency, cost, reliability, SLOs, etc.\n\n2.   What extra dimensions you could fold in  \n     a.  Performance targets or SLAs  \n         – End-to-end latency budgets (e.g. p99 ≤ 100 ms)  \n         – Jitter or tail-latency guarantees  \n         – Per-request priority levels (e.g. premium vs. best-effort)  \n     b.  Resource footprints beyond model size  \n         – Peak CPU, GPU or memory use (profiled offline or learned online)  \n         – Network I/O: transfer costs if data must travel far  \n         – Storage locality (is the data co-located with GPU‐cluster?)  \n     c.  Cost metrics  \n         – Real time spot vs. on-demand price signals (especially if using public clouds)  \n         – Energy or carbon costs if you care about sustainability  \n     d.  Temporal patterns  \n         – Diurnal or weekly workload forecasts (time-series prediction)  \n         – Burstiness metrics (variance, peak/mean ratios)  \n         – Feedback loops from recent misses or timeouts  \n     e.  Multi-tenant/resource contention  \n         – Current node utilization (CPU, GPU, memory, I/O)  \n         – Interference with co-located containers (cache / PCIe bus)  \n\n3.   Candidate scheduling strategies  \n     1.  Rule-plus-learning hybrid  \n         – Start with your existing 2-feature rule, but layer on a small reinforcement-learning agent that tweaks thresholds based on observed p95/p99 latencies or cost.  \n     2.  Multi-armed bandit or contextual bandit  \n         – Each placement option (container, VM, FaaS) is an “arm.” Context = vector of (freq, size, SLA, current load, cost–to–serve). Reward = negative latency or negative cost.  \n     3.  Model-based dynamic optimization  \n         – Build a lightweight queuing model or linear‐program formulation of your hybrid cluster, solve periodically for the lowest‐cost mix subject to latency constraints.  \n     4.  Predictive autoscaling  \n         – Use short-horizon time-series forecasts (e.g. Prophet, LSTM) of arrival rates + predictive performance profiles to pre-warm containers/VMs before bursts hit.  \n     5.  Control-theoretic approaches  \n         – A PID controller per resource type that tries to drive latency toward its setpoint by adjusting placement ratios in real time.  \n     6.  Genetic/evolutionary search  \n         – If you have a complex cost/latency tradeoff, evolve a population of placement plans over simulated workloads.  \n\n4.   Putting it all together  \n   Rather than “only look at request frequency & model size” your scheduler could, for example:  \n   – Continuously profile each model’s actual CPU/GPU/mem use and feed that into a small neural or regression model that predicts latency on each infra tier.  \n   – Combine that prediction with real-time price and utilization data via a bandit algorithm that learns which tier is cheapest while still meeting your p99 SLA.  \n   – Forecast tomorrow’s workload using historical traces and pre-warm the right number of containers or fire off spot-instance VMs in advance.  \n\nBy enriching the input space (SLA targets, resource‐usage profiles, prices, node load, arrival forecasts, etc.) and coupling it with one of the more powerful algorithmic paradigms above (RL, bandits, optimization, control theory), you can go well beyond the two‐feature empirical rules in the blue box and build a truly adaptive, cost- and performance-aware real-time scheduler."
    },
    {
        "question": "How could weighted Average Analyses components alter Compensation Index sensitivity to metric variations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Average Analyses",
            "Compensation Index"
        ],
        "id": 954,
        "masked_question": "How could weighted [mask1] components alter Compensation Index sensitivity to metric variations?",
        "masked_number": 1,
        "masked_elements": [
            "Average Analyses"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.05871v1_figure_2.png",
        "paperid": "2409.05871v1",
        "paper_path": "./papers/2409.05871v1.json",
        "figure_id": "2409.05871v1_figure_2.png",
        "caption": "Figure 2: The workflow of this project. A total of 1372 reaching data are collected from 7 subjects under the unbraced and braced condition, reaching the 49 targets horizontally or vertically. The reaching final pose (final joint locations and angles) and subjects  static anthropometry information are used to calculate four compensatory motion evaluation metrics: average joint location deviation, average joint angle difference, group separability score, and group clustering accuracy score. The four components are combined as the Compensation Index and presented in heatmap format.",
        "qtype": "Others",
        "response": "Here is a step‐by‐step reasoning of how introducing weights into the four (normalized) components of the Compensation Index would change its sensitivity to changes in those metrics:\n\n1.   Current form of the Compensation Index (CI)  \n     •  We have four normalized, equally weighted components:  \n       –  L = normalized average joint‐location deviation  \n       –  A = normalized average joint‐angle difference  \n       –  J = normalized group separability score  \n       –  H = normalized clustering‐accuracy score  \n     •  In the paper, CI is simply  \n         CI = (L + A + J + H) / 4  (Equation 14)  \n\n2.   Why weighting might help  \n     •  Not all metrics are equally reliable or equally clinically important.  \n     •  You might want CI to react more strongly when, say, angle differences are large (because those cost the user more discomfort), or less strongly when clustering accuracy fluctuates a lot (because it can be noisy).  \n\n3.   Introducing weights  \n     •  Replace the uniform 1/4 factors with a weight vector w = [wL, wA, wJ, wH], subject to wL + wA + wJ + wH = 1.  \n     •  New form becomes  \n         CIweighted = wL·L + wA·A + wJ·J + wH·H  \n\n4.   Effect on sensitivity  \n     •  The partial sensitivity of CI to each metric is now exactly its weight.  \n         ∂CIweighted/∂L = wL, ∂CIweighted/∂A = wA, and so on.  \n     •  If wA = 0.5 (and the others share the remaining 0.5), then a 10% change in A drives a 5% change in CI, whereas a 10% change in, say, J (if wJ = 0.1) only changes CI by 1%.  \n\n5.   Practical tuning  \n     •  Suppose angle measurements are noisier than location measurements. You could choose wA < wL to dampen CI’s sensitivity to angular noise.  \n     •  Conversely, if clustering accuracy (H) is felt to be the most clinically meaningful indicator of compensation, you’d boost wH—making CI much more reactive to small changes in H.  \n\n6.   Summary  \n     By moving from an unweighted average (all four metrics count equally) to a weighted sum, you gain a direct “knob” over how much each metric can swing the final Compensation Index up or down.  High weights amplify sensitivity to that metric’s fluctuations; low weights attenuate it."
    },
    {
        "question": "What rationale underlies concatenating anthropometry with final pose features before separability score analysis?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1"
        ],
        "relevant_context": [
            "The group performance analyses, utilising a feature vector, take the individual differences into account, reflecting the similarity between the features from the unbraced and the braced conditions.",
            "The feature vector v<sub>i,k</sub> combined the joint location, joint angle, and subjects’ static anthropometry information (height H<sub>i</sub> and arm length L<sub>i</sub>)."
        ],
        "final_answer": "By concatenating each subject’s static anthropometry (height and arm length) with their final‐pose joint locations and angles, the separability analysis can account for inter‐subject body‐size differences. This ensures that the computed separability score reflects genuine compensatory‐motion differences between braced and unbraced conditions rather than variations arising purely from differing anthropometry.",
        "relevant_elements": [
            "Anthropometry",
            "Separability Scores"
        ],
        "id": 955,
        "masked_question": "What rationale underlies concatenating [mask1] with final pose features before [mask2] analysis?",
        "masked_number": 2,
        "masked_elements": [
            "Anthropometry",
            "Separability Scores"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.05871v1_figure_2.png",
        "paperid": "2409.05871v1",
        "paper_path": "./papers/2409.05871v1.json",
        "figure_id": "2409.05871v1_figure_2.png",
        "caption": "Figure 2: The workflow of this project. A total of 1372 reaching data are collected from 7 subjects under the unbraced and braced condition, reaching the 49 targets horizontally or vertically. The reaching final pose (final joint locations and angles) and subjects  static anthropometry information are used to calculate four compensatory motion evaluation metrics: average joint location deviation, average joint angle difference, group separability score, and group clustering accuracy score. The four components are combined as the Compensation Index and presented in heatmap format.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1.  What are we doing in “Group Analyses” (the blue‐boxed step)?  \n    – We take all of the subjects’ data for a given target under both unbraced and braced conditions and run two multivariate analyses on it (separability and clustering).  The goal is to see how well the two conditions can be told apart as a group – i.e. whether the brace systematically shifts everyone’s final‐pose distribution.\n\n2.  What do those analyses operate on?  \n    – They operate on a feature vector for each subject-target pair.  By default that feature vector would be just the final joint locations and joint angles, but – crucially – it also includes each subject’s static anthropometry (height and arm length).\n\n3.  Why do we explicitly concatenate height & arm length before we run separability/clustering?  \n    – Because height and arm‐length impose a first‐order effect on where you end up when you reach.  If you compare two subjects of very different arm lengths, their joint-centre positions and joint angles at exactly the same target will differ even if neither of them is compensating more or less.  By tacking on each subject’s anthropometric measures, the group analyses “know” about those gross size differences and can discount them when deciding whether the brace itself has shifted people’s kinematic patterns.  \n\nIn short, we lump the two anthropometric numbers into our feature-vector so that our group-level separability and clustering scores truly reflect brace-induced changes in posture – not merely differences in subjects’ heights or arm lengths."
    },
    {
        "question": "Why apply empirical normalization scaling instead of standard min-max to prepare metrics for compensation index?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Regular normalisation methods, such as min-max normalisation, are not suitable. Such a normalisation method would set the maximum value in this research to one, but another experiment with different users/conditions might observe more significant compensatory motions and would exceed that bound of normalisation.",
            "Hence, to make the four components comparable and able to be adapted to other research, they were normalised to the same level using different approaches empirically: L was divided by 100 (equivalent to using ‘meter’ as the unit), A was divided by 10, J and H remained unchanged."
        ],
        "final_answer": "Because a standard min-max normalisation would lock the current study’s maximum to 1 and wouldn’t accommodate future studies with larger observed values, the authors instead chose an empirically scaled normalisation (dividing L by 100, A by 10, leaving J and H unchanged) so that the four metrics remain comparable and the approach remains adaptable to other datasets.",
        "relevant_elements": [
            "Normalise",
            "Compensation Index"
        ],
        "id": 956,
        "masked_question": "Why apply empirical normalization scaling instead of standard min-max to prepare metrics for [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Compensation Index"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.05871v1_figure_2.png",
        "paperid": "2409.05871v1",
        "paper_path": "./papers/2409.05871v1.json",
        "figure_id": "2409.05871v1_figure_2.png",
        "caption": "Figure 2: The workflow of this project. A total of 1372 reaching data are collected from 7 subjects under the unbraced and braced condition, reaching the 49 targets horizontally or vertically. The reaching final pose (final joint locations and angles) and subjects  static anthropometry information are used to calculate four compensatory motion evaluation metrics: average joint location deviation, average joint angle difference, group separability score, and group clustering accuracy score. The four components are combined as the Compensation Index and presented in heatmap format.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1.  What we’re normalizing  \n    •  We have four very different metrics –  \n       L: average joint‐location deviation (in mm),  \n       A: average joint‐angle difference (in °),  \n       J: separability score (unitless ratio),  \n       H: clustering accuracy (percentage).  \n    •  We want to combine them into one “Compensation Index,” so they all have roughly the same numerical range.\n\n2.  Why not use min–max?  \n    •  Min–max rescales each metric so that its minimum becomes 0 and its maximum becomes 1, but those min/max values are tied to *this* particular experiment.  \n    •  If a future study or a different subject set produced larger deviations or angles, some values would exceed the original “1.0” cap, breaking the scale.  \n    •  Min–max also over–emphasizes outliers and makes cross-study comparison very brittle.\n\n3.  Why empirical scaling instead?  \n    •  By choosing simple, interpretable divisors—100 mm for L (so 100 mm → 1), 10° for A (so 10° → 1), and leaving J and H as is—we place all four metrics on roughly the same order of magnitude.  \n    •  Those divisors are tied to physical units (100 mm, 10°) rather than the observed extrema, so if in a follow-on study you see a 150 mm deviation or a 15° angle difference, the scale still holds (1.5, 1.5).  \n    •  This makes the composite index both meaningful (it’s in “unit‐scaled” space) and extensible to new data without re-computing min/max bounds.\n\nIn short, empirical unit-based scaling gives each component a stable, interpretable range and lets future experiments plug into the same index without “max=1” breaking."
    },
    {
        "question": "Why apply vectorization prior to paired case retrieval in the contrastive strategy?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Specifically, for each erroneous case e, the vectorized features f_e are used to calculate and retrieve the m most similar correct cases:",
            "where f = vectorized features, d = distance measurement, m = number of pairs, and C^e is the paired set."
        ],
        "final_answer": "Vectorization is applied so that each case (both erroneous and correct) is represented as a feature vector f. This enables the use of a distance metric d(f_e, f_c) to identify and retrieve the m most similar correct cases for the contrastive learning step.",
        "relevant_elements": [
            "Vectorization",
            "paired cases"
        ],
        "id": 958,
        "masked_question": "Why apply [mask1] prior to [mask2] retrieval in the contrastive strategy?",
        "masked_number": 2,
        "masked_elements": [
            "Vectorization",
            "paired cases"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.00497v1_figure_2.png",
        "paperid": "2407.00497v1",
        "paper_path": "./papers/2407.00497v1.json",
        "figure_id": "2407.00497v1_figure_2.png",
        "caption": "Figure 2: Our LLMs-as-Instructors framework consists of four steps in each iteration cycle to improve the target model: 1. Data Selection (Section 2.2), where target data samples are selected to challenge and assess the capabilities we intend to enhance. 2. Result Collection (Section 2.2), involving the evaluation of the target model on these samples and collection of responses for analysis. 3. Instructor Analysis and Data Supply (Section 2.3), where the instructor conducts analysis and generates tailored training data. 4. Target Model Training and Evaluation (Section 2.4), having the target model learn from the errors and conducting the assessment of the improvements.",
        "qtype": "Design_Rationale",
        "response": "In the contrastive (LEC) strategy we are trying to “pair up” each mistake with its most similar correct examples.  To do that we need a way of measuring similarity, and that means we first have to turn all our text (questions + answers) into fixed‐length vectors.  Once everything lives in the same embedding space, we can compute distances and retrieve the top-k nearest (correct) neighbors.  \n\nSo in practice you:\n\n1. Run all your samples (both the error cases and the pool of correct cases) through your encoder to get vector representations (“vectorization,” the red-boxed step).  \n2. Then you do nearest-neighbour search in that vector space to pull out the k most similar correct examples (“get paired,” the blue-boxed step).  \n\nWithout first vectorizing, you have no numeric features to compare, so you cannot carry out the retrieval."
    },
    {
        "question": "How does Data Selection filter samples to challenge the Target Model's capabilities?",
        "relevant_section_ids": [
            "2",
            "2.2"
        ],
        "relevant_context": [
            "Data Selection (Section 2.2) is the initial step where we meticulously select target data samples designed to evaluate the capabilities we intend to enhance in subsequent iterations.",
            "To initiate the model enhancement process, we first engage in the critical task of measuring and identifying the errors of target model. As outlined in line 4 of Algorithm 1, this is achieved by carefully selecting a subset of the target dataset, denoted as S_t, from the base D."
        ],
        "final_answer": "Data Selection filters samples by choosing a tailored subset (S_t) from the full dataset D that is specifically designed to probe and evaluate the particular capabilities of the target model that we aim to improve.",
        "relevant_elements": [
            "Data Selection",
            "Target Model"
        ],
        "id": 959,
        "masked_question": "How does [mask1] filter samples to challenge the Target Model's capabilities?",
        "masked_number": 1,
        "masked_elements": [
            "Data Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.00497v1_figure_2.png",
        "paperid": "2407.00497v1",
        "paper_path": "./papers/2407.00497v1.json",
        "figure_id": "2407.00497v1_figure_2.png",
        "caption": "Figure 2: Our LLMs-as-Instructors framework consists of four steps in each iteration cycle to improve the target model: 1. Data Selection (Section 2.2), where target data samples are selected to challenge and assess the capabilities we intend to enhance. 2. Result Collection (Section 2.2), involving the evaluation of the target model on these samples and collection of responses for analysis. 3. Instructor Analysis and Data Supply (Section 2.3), where the instructor conducts analysis and generates tailored training data. 4. Target Model Training and Evaluation (Section 2.4), having the target model learn from the errors and conducting the assessment of the improvements.",
        "qtype": "Implementation_Details",
        "response": "Here’s the reasoning in steps:\n\n1.  Identify what “mask1” is pointing to.  \n    In the red‐boxed region of the diagram (Step 1: Data Selection), the system is taking the *entire* base corpus  and carving out a smaller set  that will stress‐test the very skills we care about (e.g. code, factual recall, mathematical reasoning).\n\n2.  Recall the textual description.  \n    Section 2.2 tells us that in Data Selection we “meticulously select target data samples designed to evaluate the capabilities we intend to enhance.”  In the pseudocode (Algorithm 1, line 4) this is simply  \n       S ← Select(D)  \n    where  S  is the held‐out subset used to probe the model’s current weaknesses.\n\n3.  How is the filtering actually done?  \n    •  We use metadata or hand-assigned tags on each example (e.g. “this is a math problem,” “this tests factual knowledge,” “this is a code snippet”).  \n    •  We pick only those examples whose tags correspond to the *particular* skill or capability we want to measure in this iteration.  \n    •  Optionally, we can also bias towards higher‐difficulty or edge‐case items to make sure the model really gets challenged.\n\n4.  Conclusion.  \n    Thus, “mask1” (the Data Selection step) applies a simple but focused selection predicate over the full dataset—using skill‐type labels (and possibly difficulty heuristics)—to assemble a probing subset  that *directly challenges* the target model on the very capabilities we want to train."
    },
    {
        "question": "How does vectorization compute distances to identify similar correct samples in Learning from Errors by Contrast?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Beyond the erroneous cases E, inspired by “Contrastive Learning” Hadsell et al. (2006); Chen et al. (2020), which highlights learning by comparing negative and positive samples, we incorporate correct cases C for contrast to enhance learning from errors.",
            "Specifically, for each erroneous case e_i, the vectorized features v(e_i) are used to calculate and retrieve the k most similar correct cases:",
            "where d(v(e_i), v(c)) measures the distance between the erroneous case’s vector and each correct case’s vector. These k retrieved paired cases, along with the incorrect case, form the contrast set."
        ],
        "final_answer": "Each question (correct or incorrect) is first mapped to a feature vector v(·). To find the k most similar correct samples for a given erroneous case e_i, the framework computes the distance d(v(e_i), v(c)) between the error’s vector and every correct sample’s vector, then selects the k correct cases with the smallest distances.",
        "relevant_elements": [
            "Learning from Errors by Contrast",
            "Vectorization"
        ],
        "id": 960,
        "masked_question": "How does [mask1] compute distances to identify similar correct samples in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Vectorization",
            "Learning from Errors by Contrast"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.00497v1_figure_2.png",
        "paperid": "2407.00497v1",
        "paper_path": "./papers/2407.00497v1.json",
        "figure_id": "2407.00497v1_figure_2.png",
        "caption": "Figure 2: Our LLMs-as-Instructors framework consists of four steps in each iteration cycle to improve the target model: 1. Data Selection (Section 2.2), where target data samples are selected to challenge and assess the capabilities we intend to enhance. 2. Result Collection (Section 2.2), involving the evaluation of the target model on these samples and collection of responses for analysis. 3. Instructor Analysis and Data Supply (Section 2.3), where the instructor conducts analysis and generates tailored training data. 4. Target Model Training and Evaluation (Section 2.4), having the target model learn from the errors and conducting the assessment of the improvements.",
        "qtype": "Implementation_Details",
        "response": "Let me walk through it:\n\n1. In the red‐boxed component (“Vectorization”), each question-answer pair—both the erroneous one and every correct one—is passed through an encoder to produce a fixed-length feature vector v.  \n\n2. In the blue‐boxed strategy (“Learning from Errors by Contrast”), for each error vector vᵉ the instructor computes  \n   \n   d(vᵉ, vᶜ)  \n   \n   for every correct case vector vᶜ, where d(·,·) is a standard distance in embedding space (e.g. Euclidean or cosine distance).  \n\n3. It then ranks all the correct cases by ascending d(vᵉ, vᶜ) and picks the top‐k closest ones as the “similar correct samples” to pair with that error.  \n\nIn short, the Vectorization step turns each sample into an embedding, and Learning-by-Contrast simply finds the k nearest correct embeddings to each error by measuring their vector‐space distances."
    },
    {
        "question": "How does f1_θ1 utilize classifier evaluations to adaptively select the optimal synthesis technique?",
        "relevant_section_ids": [
            "3.1",
            "3.1.2"
        ],
        "relevant_context": [
            "The data synthesis block aims to enhance the model’s ability to recognize minority classes in imbalanced datasets by increasing the number of minority class samples. Specifically, this module employs an adaptive algorithm based on the F1 score to evaluate various data synthesis techniques, including SMOTE and CTGAN. This enables the algorithm to automatically select the best-performing technique, ensuring that TriEnhance can choose the optimal synthesis strategy for various financial risk datasets.",
            "Meta-synthetic-data learning initially partitions the dataset D into a sub-training set and validation set, i.e., D_sub and D_val. For each predefined data synthesis technique, we utilize D_sub to generate synthetic data S_m, which is then merged with the sub-training set data to form an augmented dataset D_aug^m. Subsequently, we train a model g_m on this augmented dataset and evaluate it on the validation set D_val to obtain the F1 scores for each technique. After systematically evaluating all candidate techniques, we select the technique with the highest F1 score."
        ],
        "final_answer": "f1_θ1 implements a meta-synthetic-data learning loop: it partitions the data into sub-training and validation sets, applies each candidate synthesis method (e.g., SMOTE, CTGAN) to augment the sub-training set, trains a classifier on each augmented set, computes each classifier’s F1 score on the validation set, and then adaptively selects the synthesis technique that achieves the highest F1 score.",
        "relevant_elements": [
            "f1_θ1",
            "classifier"
        ],
        "id": 961,
        "masked_question": "How does [mask1] utilize classifier evaluations to adaptively select the optimal synthesis technique?",
        "masked_number": 1,
        "masked_elements": [
            "f1_θ1"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.09792v1_figure_2.png",
        "paperid": "2409.09792v1",
        "paper_path": "./papers/2409.09792v1.json",
        "figure_id": "2409.09792v1_figure_2.png",
        "caption": "Figure 2: Overview of the TriEnhance Architecture.",
        "qtype": "Implementation_Details",
        "response": "The little red box in the “Data‐synthesis” pane is exactly the meta–synthesizer **f¹₍θ₁₎**.  What it does is:\n\n1.  It takes your original minority/majority split and carves off a small validation fold.  \n2.  For each candidate generator (e.g. SMOTE, CTGAN, …) it  \n    a.  Synthesizes a batch of minority examples,  \n    b.  Merges them with the sub‐training fold,  \n    c.  Retrains your classifier on that augmented data,  \n    d.  Measures the F₁‐score on the held‐out validation fold.  \n3.  It compares all of those F₁‐scores and picks the synthesis method that achieved the highest one.  \n\nIn this way **f¹₍θ₁₎** “looks over the shoulder” of the classifier, uses its F₁ performance as a feedback signal, and adaptively selects the single best augmentation procedure for the task at hand."
    },
    {
        "question": "How does f2_θ2 reconcile D7 and D8 inputs to generate high-confidence pseudo-labels?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "D7(x_i,y_i)",
            "D8(x_i,y_i)",
            "f2_θ2"
        ],
        "id": 962,
        "masked_question": "How does [mask1] reconcile D7 and D8 inputs to generate high-confidence pseudo-labels?",
        "masked_number": 1,
        "masked_elements": [
            "f2_θ2"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.09792v1_figure_2.png",
        "paperid": "2409.09792v1",
        "paper_path": "./papers/2409.09792v1.json",
        "figure_id": "2409.09792v1_figure_2.png",
        "caption": "Figure 2: Overview of the TriEnhance Architecture.",
        "qtype": "Implementation_Details",
        "response": "The little green box in the red frame is in fact the self‐learning model \\(f^2_{\\theta_2}\\).  Its sole job is to take two “streams’’ of data –  \n  1.   \\(D_7\\), the small, *clean* set of real labels that just came out of the Data–Filtering block, and  \n  2.   \\(D_8\\), the large pool of as‐yet *unlabeled* (or weakly labeled) examples –  \ntrain on \\(D_7\\), run over \\(D_8\\), and then *only* hand back to the training pool those examples in \\(D_8\\) for which it is *extremely* confident.  \n\nConcretely, \\(f^2_{\\theta_2}\\) implements two tactics in tandem:  \n•  K–Fold Unknown‐Label Filtering (KFULF) – it splits \\(D_8\\) into \\(K\\) folds, temporarily “labels’’ \\(K-1\\) of them, retrains on that union plus \\(D_7\\), and then:\n\n   – predicts on the held‐out fold,  \n   – repeats for each fold,  \n   – and only keeps those points whose predicted label is *identical* in all \\(K\\) runs.  \n\n•  Delay Decision Strategy (DDS) – it looks at the remaining unchosen points, ranks them by their maximum soft‐max (or margin) score, and then *gradually* admits just the top–α most confident ones, postponing the rest to the next round until performance stops improving.  \n\nBy training on the truly labeled \\(D_7\\) and then *filtering* the outputs on \\(D_8\\) through a very strict “agree‐in‐all‐folds’’ test (and, if you like, a margin threshold), \\(f^2_{\\theta_2}\\) only ever appends *high‐confidence* pseudo‐labels to the pool."
    },
    {
        "question": "How does Data synthesis block integrate SMOTE's synthetic sampling principles into f1_theta1 mechanism?",
        "relevant_section_ids": [
            "3.1",
            "3.1.2"
        ],
        "relevant_context": [
            "Section 3.1: \"The data synthesis block aims to enhance the model’s ability to recognize minority classes in imbalanced datasets by increasing the number of minority class samples. Specifically, this module employs an adaptive algorithm based on the F1 score to evaluate various data synthesis techniques, including SMOTE and CTGAN. This enables the algorithm to automatically select the best‐performing technique, ensuring that TriEnhance can choose the optimal synthesis strategy for various financial risk datasets.\"",
            "Section 3.1.2: \"Meta-synthetic-data learning initially partitions the dataset D into a sub-training set and validation set. For each predefined data synthesis technique, we utilize D to generate synthetic data D_syn, which is then merged with the sub-training set data to form an augmented dataset. Subsequently, we train a model on this augmented dataset and evaluate it on the validation set to obtain the F1 scores for each technique. After systematically evaluating all candidate techniques, we select the technique with the highest F1 score. Subsequently, we integrate the samples correctly classified by the model on the validation set into the augmented dataset generated by this technique.\""
        ],
        "final_answer": "The Data synthesis block encapsulated in f1_θ1 treats SMOTE as one of its candidate synthesis methods. During the f1_θ1 routine, SMOTE is applied to the sub-training split to generate synthetic minority samples; the classifier is retrained on this augmented data, and its F1 score is measured on a held-out validation fold. If SMOTE achieves the highest F1 among all techniques (e.g., CTGAN), its synthetic samples—specifically those instances the model classifies correctly on validation—are merged back into the main training set. In this way, f1_θ1 integrates SMOTE’s interpolation-based synthetic sampling by adaptively generating, testing, and selecting SMOTE samples according to their F1 performance.",
        "relevant_elements": [
            "Data synthesis block",
            "f1_theta1"
        ],
        "id": 963,
        "masked_question": "How does [mask1] integrate SMOTE's synthetic sampling principles into f1_theta1 mechanism?",
        "masked_number": 1,
        "masked_elements": [
            "Data synthesis block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.09792v1_figure_2.png",
        "paperid": "2409.09792v1",
        "paper_path": "./papers/2409.09792v1.json",
        "figure_id": "2409.09792v1_figure_2.png",
        "caption": "Figure 2: Overview of the TriEnhance Architecture.",
        "qtype": "Literature_Background",
        "response": "The mystery “red‐boxed” module is nothing mystical – it is simply the Meta–Synthetic‐Data Learner (our f1θ₁ function) “standing on top of” SMOTE (and any other candidate synthesizer).  In practice what happens inside that red box is:\n\n 1.  We treat SMOTE as one of several off‐the‐shelf synthesis routines.  \n 2.  During each meta‐learning iteration we take our current minority set, feed it to SMOTE (with a chosen k–NN setting), and generate a batch of synthetic minority points by interpolating along the lines between each minority sample and its k nearest minority neighbours.  \n 3.  We then train a temporary classifier on the augmented training fold and measure its F1 on the held–out validation fold.  \n 4.  We repeat exactly the same process with CTGAN, borderline‐SMOTE, etc.  \n 5.  Whichever method (SMOTE or another) yields the highest F1 becomes our “winner” for that round.  f1θ₁ records that choice, permanently adds the correctly classified synthetic points back into D₂(xᵢ,yᵢ), and passes on any misclassified ones to the filtering block.  \n\nIn short, f1θ₁ “integrates SMOTE” by wrapping SMOTE’s k-nearest‐neighbour interpolation routine inside a small meta‐optimizer that (a) tries it, (b) measures its effect on F1, and (c) only keeps the synthetic points from SMOTE when they are demonstrably the best way to boost minority recall."
    },
    {
        "question": "How does self-learning block's f2_theta2 pseudo-label mechanism parallel classical pseudo-labeling methodologies?",
        "relevant_section_ids": [
            "2",
            "3.3"
        ],
        "relevant_context": [
            "Section 2, Pseudo-labeling Techniques: \"Pseudo-labeling[14], a semi-supervised learning approach, utilizes unlabeled data by assigning temporary labels based on the predictions of a trained model. The approach involves using the confident predictions of a model to generate labels for unlabeled data, which are then used to retrain the model, progressively improving its accuracy on both labeled and unlabeled datasets.\"",
            "Section 3.3, Self-learning block: \"K-Fold Unknown-label Filtering (KFULF). ... these artificially labeled datasets are combined to form the training data for retraining. Following this, the well-trained model predicts the unlabeled test set, identifying samples with explicit predictions as high-confidence samples. Upon completion of the K-Fold cycle, all high-confidence samples are incorporated into the training set.\"",
            "Section 3.3, Self-learning block: \"Delay-decision Strategy (DDS). ... iteratively assesses unlabeled samples and incorporates the top T highest confidence samples as high-confidence pseudo-labeled samples into the training set, those remaining samples are waiting for the next time precision.\""
        ],
        "final_answer": "The self-learning block’s f2_θ2 mechanism mirrors classical pseudo-labeling by taking a trained model’s high-confidence predictions on unlabeled data as temporary “pseudo-labels” and then retraining on these augmented samples (via K-Fold Unknown-label Filtering and Delay-decision Strategy), exactly as in standard pseudo-labeling schemes where confident model outputs on unlabeled examples are used to expand the labeled training set.",
        "relevant_elements": [
            "self-learning block",
            "f2_theta2"
        ],
        "id": 964,
        "masked_question": "How does [mask1]'s [mask2] pseudo-label mechanism parallel classical pseudo-labeling methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "self-learning block",
            "f2_theta2"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.09792v1_figure_2.png",
        "paperid": "2409.09792v1",
        "paper_path": "./papers/2409.09792v1.json",
        "figure_id": "2409.09792v1_figure_2.png",
        "caption": "Figure 2: Overview of the TriEnhance Architecture.",
        "qtype": "Literature_Background",
        "response": "Here, the two masks refer to\n\n  • [mask1] = the “Self-learning block”  \n  • [mask2] = the “f²₍θ₂₎ pseudo-label mechanism”  \n\nSo the question really asks:\n\n  “How does the Self-learning block’s f²₍θ₂₎ pseudo-label mechanism parallel classical pseudo-labeling methodologies?”\n\nAnswer (chain‐of-thought):\n\n  1. In classical pseudo-labeling (e.g. Lee, 2013), you  \n     – train a model on the small pool of genuine labeled data  \n     – use that model to predict labels on unlabeled data  \n     – pick only its high‐confidence predictions as “pseudo‐labels”  \n     – add those newly labeled points back into your training set  \n     – retrain and repeat  \n\n  2. TriEnhance’s self-learning block does exactly the same at its core.  Its f²₍θ₂₎ module  \n     – takes the current model f²₍θ₂₎ and applies it to the pool of unlabeled samples  \n     – uses two confidence‐based subroutines (K-Fold Unknown-label Filtering and Delay-Decision) to filter out only the most confident predictions  \n     – treats those high‐confidence predictions as pseudo-labeled examples  \n     – augments the original training set with them and retrains f²₍θ₂₎ in the next iteration  \n\n  3. Thus, the Self-learning block’s f²₍θ₂₎ exactly mirrors the classical pseudo-label cycle of  \n     predicting → confidence‐thresholding → augmenting the labeled set → retraining  \n\nIn short, TriEnhance’s f²₍θ₂₎ self-learning is a direct instantiation of traditional pseudo-labeling: it labels unlabeled data with the model’s own confident predictions and then iteratively refines itself on the enlarged pseudo-labeled set."
    },
    {
        "question": "How does HWPE Subsystem utilize multi-port TCDM Interconnect architecture to meet accelerator bandwidth demands?",
        "relevant_section_ids": [
            "3",
            "4.2"
        ],
        "relevant_context": [
            "Each core has one master port with decoupled request and response path connected to the TCDM interconnect, and the HWPE subsystem features a parametric number of master ports to allow the integration accelerators.",
            "As the four streamers are multiplexed in time, ITA requires of maximum bandwidth to fetch two input vectors per cycle; therefore, we use 16 master ports on the TCDM interconnect for the HWPE subsystem."
        ],
        "final_answer": "The HWPE subsystem hooks into the shared 32-bank L1 TCDM through a configurable number of master ports on the TCDM interconnect. In the case of the Integer Transformer Accelerator (ITA), it instantiates 16 master ports—multiplexed across its four HWPE streamers—to supply the peak bandwidth the accelerator requires.",
        "relevant_elements": [
            "HWPE Subsystem",
            "TCDM Interconnect"
        ],
        "id": 965,
        "masked_question": "How does [mask1] utilize multi-port [mask2] architecture to meet accelerator bandwidth demands?",
        "masked_number": 2,
        "masked_elements": [
            "HWPE Subsystem",
            "TCDM Interconnect"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.02473v1_figure_1.png",
        "paperid": "2408.02473v1",
        "paper_path": "./papers/2408.02473v1.json",
        "figure_id": "2408.02473v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed architecture featuring nine RISC-V cores (red), HWPE subsystem (orange), memory hierarchy with two levels of scratchpad memory (blue), an L1 TCDM interconnect and an AXI interconnect connecting the cluster to its host system (green), and peripherals such as the cluster DMA and on-chip peripherals like timers (purple). The ninth Snitch core is a dedicated data mover core coupled with a DMA to manage 1D and 2D data transfers within the cluster. The implemented architecture uses a DAXI,W=512subscript𝐷AXI,W512D_{\\textrm{AXI,W}}=512italic_D start_POSTSUBSCRIPT AXI,W end_POSTSUBSCRIPT = 512-bit wide AXI interconnect to accommodate high bandwidth requirement of Attention-based networks between SoC level data memory and L1 memory. We use a separate DAXI,N=64subscript𝐷AXI𝑁64D_{\\textrm{AXI},N}=64italic_D start_POSTSUBSCRIPT AXI , italic_N end_POSTSUBSCRIPT = 64-bit narrow AXI interconnect for peripherals and communication with the host system, which prevents deterioration in data bandwidth. NHWPE=16subscript𝑁HWPE16N_{\\textrm{HWPE}}=16italic_N start_POSTSUBSCRIPT HWPE end_POSTSUBSCRIPT = 16 ports from the HWPE subsystem are connected to the TCDM to fulfill high bandwidth requirement of Attention accelerator.",
        "qtype": "Literature_Background",
        "response": "mask1: the HWPE subsystem  \nmask2: the shared, multi-banked L1 TCDM (multi-port scratchpad)\n\nChain of thought:\n1. The HWPE subsystem is the orange block in the red outline. It houses the controller, multiple streamers (input and output), FIFO buffers, and the accelerator engine.  \n2. Each streamer must read/write on-chip data from/to the shared scratchpad (the L1 TCDM). To hit the very high sustained bandwidth the Integer Transformer Accelerator needs, a single port into the TCDM would quickly become the bottleneck.  \n3. Instead, the TCDM is architected as a 32-bank, multi-ported scratchpad with a fast combinational crossbar interconnect. In our ITA case, we parameterize it with N_HWPE = 16 master ports for the HWPE subsystem.  \n4. We then time-multiplex the three input streamers and one output streamer across those 16 ports—each streamer grabbing its slot on the TCDM crossbar in a fine-grained interleaved fashion.  \n5. The result is that the accelerator can fire a new data transaction every cycle on any of its ports, achieving the aggregate bandwidth (hundreds of bits per cycle) required for 8-bit GEMM, multi-head softmax, and so on, all with single-cycle L1 access latency in the absence of bank conflicts.\n\nAnswer:\nThe HWPE subsystem hooks its streamers into the shared L1 TCDM via a parameterized set of master ports (16 in the ITA implementation). By time-multiplexing its input/output streamers across these multi-banked, multi-ported TCDM interfaces, the accelerator sustains the high bandwidth it needs without adding private SRAMs or complex DMA paths."
    },
    {
        "question": "How does DMA core exploit Wide AXI interconnect width to optimize high-volume data transfers?",
        "relevant_section_ids": [
            "3",
            "4.2"
        ],
        "relevant_context": [
            "The cluster includes two parametrizable AXI interconnects: a wide crossbar with a bit data width and a narrow crossbar with a bit data width. The wide AXI interconnect is used to load instructions into the shared instruction cache and to transfer data from and to the SoC level memory system in conjunction with the DMA.",
            "Moreover, one Snitch core is coupled with a DMA to manage data movements within the cluster, facilitating double buffering to maintain high accelerator utilization.",
            "Consequently, we use a 512-bit wide data AXI interconnect to provide enough bandwidth for the instructions cache and ITA."
        ],
        "final_answer": "The DMA core drives bulk data movements over the wide AXI crossbar—configured as a 512-bit-wide interconnect—to ferry large tiles of weights, activations, and instructions between L2 (SoC) memory and the on-chip cluster. By issuing wide 512-bit bursts and double-buffering transfers, the DMA saturates the high-bandwidth link, minimizing transfer latency and keeping the accelerator continuously supplied with data.",
        "relevant_elements": [
            "DMA",
            "Wide AXI"
        ],
        "id": 966,
        "masked_question": "How does [mask1] core exploit Wide AXI interconnect width to optimize high-volume data transfers?",
        "masked_number": 1,
        "masked_elements": [
            "DMA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.02473v1_figure_1.png",
        "paperid": "2408.02473v1",
        "paper_path": "./papers/2408.02473v1.json",
        "figure_id": "2408.02473v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed architecture featuring nine RISC-V cores (red), HWPE subsystem (orange), memory hierarchy with two levels of scratchpad memory (blue), an L1 TCDM interconnect and an AXI interconnect connecting the cluster to its host system (green), and peripherals such as the cluster DMA and on-chip peripherals like timers (purple). The ninth Snitch core is a dedicated data mover core coupled with a DMA to manage 1D and 2D data transfers within the cluster. The implemented architecture uses a DAXI,W=512subscript𝐷AXI,W512D_{\\textrm{AXI,W}}=512italic_D start_POSTSUBSCRIPT AXI,W end_POSTSUBSCRIPT = 512-bit wide AXI interconnect to accommodate high bandwidth requirement of Attention-based networks between SoC level data memory and L1 memory. We use a separate DAXI,N=64subscript𝐷AXI𝑁64D_{\\textrm{AXI},N}=64italic_D start_POSTSUBSCRIPT AXI , italic_N end_POSTSUBSCRIPT = 64-bit narrow AXI interconnect for peripherals and communication with the host system, which prevents deterioration in data bandwidth. NHWPE=16subscript𝑁HWPE16N_{\\textrm{HWPE}}=16italic_N start_POSTSUBSCRIPT HWPE end_POSTSUBSCRIPT = 16 ports from the HWPE subsystem are connected to the TCDM to fulfill high bandwidth requirement of Attention accelerator.",
        "qtype": "Literature_Background",
        "response": "First, note that the red‐boxed block in the figure is the DMA unit that is grafted onto one of the Snitch cores (often called the “data mover” or “ninth Snitch core”).  That core does not execute application code—it drives the 512-bit wide AXI crossbar to stream large chunks of data in and out of the shared L1 (and the instruction cache) in single, sustained bursts.  By doing so it\n\n 1. issues full-width (512-bit) AXI transactions,  \n 2. always moves entire cache–line-sized blocks in one go,  \n 3. double-buffers data in TCDM so that while one tile is being consumed by the accelerator the next is already in flight,  \n\nwhich in concert completely saturates the wide AXI link and hides latency.  This is how the dedicated data-mover core exploits the 512-bit interconnect to optimize high-volume transfers."
    },
    {
        "question": "How does time-multiplexing sixteen HWPE ports to the TCDM interconnect improve data parallelism?",
        "relevant_section_ids": [
            "3.1",
            "4.2"
        ],
        "relevant_context": [
            "We time-multiplex multiple streamers to a multi-port interface with N ports and connect to the TCDM interconnect.",
            "As the four streamers are multiplexed in time, ITA requires of maximum bandwidth to fetch two input vectors per cycle; therefore, we use 16 master ports on the TCDM interconnect for the HWPE subsystem."
        ],
        "final_answer": "By time-multiplexing the HWPE streamers across sixteen physical TCDM ports, the four logical data channels (three inputs and one output) can share a wider multi-port interface.  Interleaving their accesses allows the accelerator to fetch up to two 8-bit input vectors per cycle and push results back without conflict, effectively boosting the number of simultaneous data transfers and maximizing the use of the multi-banked L1 memory.  This interleaved, cycle-by-cycle sharing of ports increases overall data‐parallel throughput to the HWPE.",
        "relevant_elements": [
            "HWPE Subsystem",
            "TCDM Interconnect"
        ],
        "id": 967,
        "masked_question": "How does time-multiplexing sixteen [mask1] ports to the TCDM interconnect improve data parallelism?",
        "masked_number": 1,
        "masked_elements": [
            "HWPE Subsystem"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.02473v1_figure_1.png",
        "paperid": "2408.02473v1",
        "paper_path": "./papers/2408.02473v1.json",
        "figure_id": "2408.02473v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed architecture featuring nine RISC-V cores (red), HWPE subsystem (orange), memory hierarchy with two levels of scratchpad memory (blue), an L1 TCDM interconnect and an AXI interconnect connecting the cluster to its host system (green), and peripherals such as the cluster DMA and on-chip peripherals like timers (purple). The ninth Snitch core is a dedicated data mover core coupled with a DMA to manage 1D and 2D data transfers within the cluster. The implemented architecture uses a DAXI,W=512subscript𝐷AXI,W512D_{\\textrm{AXI,W}}=512italic_D start_POSTSUBSCRIPT AXI,W end_POSTSUBSCRIPT = 512-bit wide AXI interconnect to accommodate high bandwidth requirement of Attention-based networks between SoC level data memory and L1 memory. We use a separate DAXI,N=64subscript𝐷AXI𝑁64D_{\\textrm{AXI},N}=64italic_D start_POSTSUBSCRIPT AXI , italic_N end_POSTSUBSCRIPT = 64-bit narrow AXI interconnect for peripherals and communication with the host system, which prevents deterioration in data bandwidth. NHWPE=16subscript𝑁HWPE16N_{\\textrm{HWPE}}=16italic_N start_POSTSUBSCRIPT HWPE end_POSTSUBSCRIPT = 16 ports from the HWPE subsystem are connected to the TCDM to fulfill high bandwidth requirement of Attention accelerator.",
        "qtype": "Experimental_Results",
        "response": "First, let us identify what the red-boxed “ports” really are.  If you look at the text in III-A you find:\n\n  •  “The HWPE template provides … streamers  \n  •  … We time-multiplex multiple streamers to a multi-port interface with Nₕwpe ports and connect to the TCDM interconnect.”  \n  •  In our concrete ITA example, Nₕwpe = 16.  \n\nSo the sixteen [mask1] ports are simply the sixteen HWPE “streamer” ports (i.e. the DMA-like channels that ferry data in and out of the accelerator).\n\n---\n\nChain of Thought:\n\n1.  The ITA accelerator inside the HWPE wants to process many 8‐bit elements in parallel (e.g. eight dot‐product units working each cycle).  \n2.  To keep those compute lanes busy it must fetch new input vectors, weights, biases and write back outputs without stalling.  \n3.  Each of those data flows is driven by a “streamer” which issues read/write requests into the shared L1 TCDM.  \n4.  In our design we have 4 logical streamers (3 inputs + 1 output), but at full accelerator bandwidth we need 16 simultaneous 64-bit requests per cycle to hide memory latency and feed all the dot-product units.  \n5.  Rather than wire up 16 separate physical streamer engines, we time-multiplex the four logical streamers across 16 master ports on the TCDM crossbar.  \n6.  The TCDM interconnect is a fully combinational, 32-bank crossbar that can accept up to one 64-bit request per port per cycle with no extra latency as long as there is no bank conflict.  \n7.  By giving the HWPE 16 logical ports into that crossbar and round-robin–scheduling the four streamers across them, we instantly multiply the number of in‐flight, outstanding transactions: we can now interleave up to 16 memory requests each cycle onto different banks.  \n8.  This interleaving (time-multiplexing) lets the accelerator pull in new data and push out results continuously, saturating the L1 bandwidth and keeping all its parallel compute units fed.\n\n---\n\nAnswer:\n\nBy time-multiplexing its four logical DMA “streamers” onto sixteen 64-bit master ports into the TCDM crossbar, the HWPE effectively presents sixteen independent memory channels each cycle.  These channels can be round-robin scheduled onto different L1 banks, which:\n\n  •  Increases the number of outstanding loads/stores that can be issued in a single cycle,  \n  •  Hides the memory access latency behind interleaved requests,  \n  •  Avoids bank conflicts by spreading accesses, and  \n  •  Keeps all of the accelerator’s parallel datapaths fully fed.\n\nIn short, the sixteen time-multiplexed streamer ports multiply the accelerator’s access concurrency and so directly boost data‐parallel throughput."
    },
    {
        "question": "How does LGM-based 3DGS reconstruction integrate with physics-grounded coarse object dynamics?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "3.2: Given a single image as input, our pipeline mainly consists of two parts: 1) we extract coarse 3DGS representation of foreground object, followed by applying a geometry-aware optimization. We generate physics-grounded dynamics of object using a MPM simulator; 2) we apply a diffusion-based video enhancement pipline to generate high-fidelity video with realistic object-background interaction.",
            "3.3: We use LGM [80] to obtain a coarse initial 3DGS reconstruction for the foreground object. … The output of the optimized Gaussians are send to a MPM simulator, resulting in a coarse object dynamics consisting of T frames. We adopt the open-sourced MPM solver [114]."
        ],
        "final_answer": "The pipeline uses LGM to produce a coarse 3D Gaussian Splatting (3DGS) reconstruction of the segmented foreground object, which is then refined via geometry-aware optimization (depth and color supervision). The optimized set of 3D Gaussians is passed directly into a Material Point Method (MPM) simulator to generate a physics-grounded coarse object dynamics sequence.",
        "relevant_elements": [
            "LGM",
            "Physics-grounded coarse object dynamics"
        ],
        "id": 969,
        "masked_question": "How does [mask1]-based 3DGS reconstruction integrate with physics-grounded coarse object dynamics?",
        "masked_number": 1,
        "masked_elements": [
            "LGM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17189v1_figure_2.png",
        "paperid": "2411.17189v1",
        "paper_path": "./papers/2411.17189v1.json",
        "figure_id": "2411.17189v1_figure_2.png",
        "caption": "Figure 2: \nOverview. Given a single image input 𝐈0superscript𝐈0\\mathbf{I}^{0}bold_I start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we introduce a novel pipeline to generate high-fidelity, physics-grounded video with 3D understanding. Our pipeline consists of two main stages: first, we perform a single-view 3DGS reconstruction of the segmented object from the input image, then synthesize a physics-grounded coarse object dynamics {ℐj}j=1Nsuperscriptsubscriptsubscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (Sec. 3.3); then, we apply diffusion-based video enhancement to produce the final enhanced video {ℐj∗}j=1Nsuperscriptsubscriptsubscriptsuperscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}^{*}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with backgrounds (Sec. 3.4), enabling users to create visually compelling, physics-driven video from a single image with an applied conditional force or torque.",
        "qtype": "Experimental_Results",
        "response": "Here is a step‐by‐step breakdown of how the LGM-based 3DGS reconstruction is hooked up to the physics-grounded coarse dynamics:\n\n1.  Identify “mask1.”  \n    – In the diagram the red box around “LGM” indicates that our single–view 3DGS reconstruction is initialized by LGM [80].  \n\n2.  From one image to a coarse 3DGS.  \n    – We take the segmented foreground from the input image and run LGM to produce a **coarse anisotropic Gaussian‐splatting** model of the object.  \n    – This gives us an initial set of Gaussians \\{(μ_i, Σ_i, α_i, c_i)\\}_i in world space.  \n\n3.  Geometry‐aware refinement.  \n    – We generate four synthetic views with MVDream and impose a hard‐depth loss (plus a color loss on the original view) to fine‐tune only the Gaussian **geometry** (μ_i, Σ_i).  \n    – After a few gradient‐steps, we end up with a spatially consistent 3DGS that faithfully matches the input depth and color.  \n\n4.  Turning Gaussians into MPM particles.  \n    – We treat each optimized Gaussian kernel as a little “material point” in an MPM simulator.  \n    – Its center μ_i is the particle position p_i, its covariance Σ_i encodes local shape, and its mass/opacity α_i encodes density.  \n\n5.  Physics‐grounded time evolution.  \n    – At each physics tick, MPM advances particle momenta via a forward Euler discretization of  \n         ρ ∂v/∂t = div σ + f_ext,  \n      interpolating between the Lagrangian particles and an Eulerian grid.  \n    – Once the grid velocity is updated, we push it back to the particles and update  \n       – positions: p_i ← p_i + Δt v_i  \n       – deformation gradients: F_i ← (I + Δt ∇v) F_i  \n    – Finally, we update each Gaussian’s parameters **in one shot** using the first‐order deformation map φ_t ≈ I + F and the spherical‐harmonic–based recipe from [92]:  \n         μ_i(t+1) = φ_t(μ_i(t)),  \n         Σ_i(t+1) = R_t Σ_i(t) R_tᵀ.  \n\n6.  Result: a coarse video {I_j} of the Gaussians marching under real physics.  \n    – Because the same Gaussians you got from LGM serve as MPM particles, you end up with a **physics‐grounded**, time‐dependent 3DGS.  \n    – This coarse sequence is then ready for the diffusion‐based enhancement stage.  \n\nIn short, LGM gives you a quick single‐view 3DGS; you refine it with geometry losses; then you hand it off to MPM so that each Gaussian “becomes” a particle whose position and covariance evolve according to continuum mechanics."
    },
    {
        "question": "How do coarse and enhanced video denoising stages collaborate to ensure temporal consistency?",
        "relevant_section_ids": [
            "3.4.2"
        ],
        "relevant_context": [
            "During DDIM+ sampling stage, we perform coarse and enhanced sampling processes simultaneously. Following [82], we switch the output of residual blocks and self-attention blocks in the enhanced sampling stage with corresponding outputs from the coarse sampling stage as … Feature injection is applied to all upsampling layers (i.e. the decoding stage) in the UNet. The timesteps for feature and attention injection is controlled by two hyperparameters, t_f and t_a.",
            "For latent key-frames during enhanced sampling, the self-attention features Q_e (queries), K_e (keys), V_e (values) are concatenated and shared to form the extended attention, with the queries and keys being replaced by the corresponding values in the coarse stage (for simplicity, we denote the V_c as V, where Q_c = K_c = V_c)…",
            "For latent non-key-frames, following [24], we propagate the enhanced key-frames to them, through the extracted Nearest-Neighbor correspondences from the coarse blended video v_coarse. For a spatial location p in the feature map of frame i, we replace its attention block output by a weighted average between neighboring key-frames …"
        ],
        "final_answer": "During DDIM+ sampling both a coarse denoising pass and an enhanced denoising pass run in parallel. At each UNet upsampling layer, the enhanced pass injects residual and self-attention outputs from the coarse pass (controlled by timesteps t_f and t_a). For key-frames, enhanced self-attention reuses the coarse pass’s attention values as its queries and keys, ensuring consistency. For non-key frames, their enhanced attention outputs are replaced by a distance-weighted blend of the nearest enhanced key-frame attention outputs, where the weights come from nearest-neighbor correspondences extracted on the coarse video. This cross-injection and propagation of features from the coarse to the enhanced stage enforces smooth, temporally consistent results.",
        "relevant_elements": [
            "Coarse Video Denoising",
            "Enhanced Video Denoising"
        ],
        "id": 970,
        "masked_question": "How do [mask1] and [mask2] stages collaborate to ensure temporal consistency?",
        "masked_number": 2,
        "masked_elements": [
            "Coarse Video Denoising",
            "Enhanced Video Denoising"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17189v1_figure_2.png",
        "paperid": "2411.17189v1",
        "paper_path": "./papers/2411.17189v1.json",
        "figure_id": "2411.17189v1_figure_2.png",
        "caption": "Figure 2: \nOverview. Given a single image input 𝐈0superscript𝐈0\\mathbf{I}^{0}bold_I start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we introduce a novel pipeline to generate high-fidelity, physics-grounded video with 3D understanding. Our pipeline consists of two main stages: first, we perform a single-view 3DGS reconstruction of the segmented object from the input image, then synthesize a physics-grounded coarse object dynamics {ℐj}j=1Nsuperscriptsubscriptsubscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (Sec. 3.3); then, we apply diffusion-based video enhancement to produce the final enhanced video {ℐj∗}j=1Nsuperscriptsubscriptsubscriptsuperscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}^{*}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with backgrounds (Sec. 3.4), enabling users to create visually compelling, physics-driven video from a single image with an applied conditional force or torque.",
        "qtype": "Experimental_Results",
        "response": "Here is the short answer followed by a more detailed, step-by-step “chain-of-thought”:\n\nAnswer  \nMask 1 (the red-boxed frames) is the physics-driven, coarse object video that comes straight out of the 3DGS+MPM simulation.  Mask 2 (the blue-boxed frames) is the final, TokenFlow-enhanced video after diffusion.  They collaborate as follows:\n\n• Mask 1 supplies a fully consistent, frame-by-frame latent “reference movie” that preserves object geometry, motion and depth.  \n• During the enhanced sampling stage (TokenFlow), we inject both the coarse UNet features (residuals) and the coarse self-attention outputs (the Q/K pairs) from mask 1 into the enhanced UNet at every timestep.  That locks the two branches together so that the diffusion model never drifts away from the original motion.  \n• For key-frames we further share and swap attention features across all key-frames (again using the coarse latent from mask 1 as the anchor).  \n• For non-key-frames we compute nearest-neighbor correspondences on the coarse video (mask 1) and use those to propagate the already-enhanced key-frame features forward and backward in time.  \n\nBy fusing the coarse stage’s features and attention maps into the enhanced stage, and by borrowing its frame-to-frame correspondences to “flow” enhanced detail onto non-key frames, the two stages guarantee a single, temporally coherent output video.  \n\n— — — — — — — — — — — — —  \nStep‐by‐step reasoning  \n1. Mask 1 (red) is introduced in Sec. 3.3: it is the output of the 3DGS reconstruction + MPM physics simulation.  You get N foreground-only frames that are perfectly consistent (same meshes of Gaussians moving under physics).  \n2. In Sec. 3.4.1 and 3.4.2 we blend these N frames with the background and feed them through a two-stage diffusion process.  \n3. In the “coarse video denoising” branch we simply denoise each blended frame, but in parallel we maintain an “enhanced” branch where we will inject features.  \n4. Every time the U-Net does an up-sampling or self-attention step in the enhanced branch, we copy over (inject) the corresponding residual and self-attention Q/K pairs from the coarse branch.  This ties the two denoisers together at every timestep, so the enhanced branch can add texture detail without losing track of the original motion.  \n5. For key-frames, we concatenate the self-attention features across all of them and do a Q/K swap so that each key-frame sees the others as context—again using the coarse branch as the key.  \n6. For non-key-frames we find nearest-neighbor patches in the coarse video and use them to interpolate (warp) the enhanced key-frame features onto the non-key frames.  \n7. By the end of sampling you have an enhanced video whose fine details come from the diffusion model but whose motion, geometry and inter-frame correspondence are “locked in” by the coarse, red-boxed branch.  "
    },
    {
        "question": "What limitations arise from LGM during single-view 3DGS reconstruction under heavy occlusions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "LGM",
            "single-view 3DGS reconstruction"
        ],
        "id": 971,
        "masked_question": "What limitations arise from [mask1] during single-view 3DGS reconstruction under heavy occlusions?",
        "masked_number": 1,
        "masked_elements": [
            "LGM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17189v1_figure_2.png",
        "paperid": "2411.17189v1",
        "paper_path": "./papers/2411.17189v1.json",
        "figure_id": "2411.17189v1_figure_2.png",
        "caption": "Figure 2: \nOverview. Given a single image input 𝐈0superscript𝐈0\\mathbf{I}^{0}bold_I start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we introduce a novel pipeline to generate high-fidelity, physics-grounded video with 3D understanding. Our pipeline consists of two main stages: first, we perform a single-view 3DGS reconstruction of the segmented object from the input image, then synthesize a physics-grounded coarse object dynamics {ℐj}j=1Nsuperscriptsubscriptsubscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (Sec. 3.3); then, we apply diffusion-based video enhancement to produce the final enhanced video {ℐj∗}j=1Nsuperscriptsubscriptsubscriptsuperscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}^{*}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with backgrounds (Sec. 3.4), enabling users to create visually compelling, physics-driven video from a single image with an applied conditional force or torque.",
        "qtype": "Others",
        "response": "The little red‐boxed module in Figure 2 is exactly the off‐the‐shelf LGM single‐view 3DGS reconstructor, and what the paper points out (and what you see in the ablations) is that as soon as you start hiding big chunks of your object behind occluders, LGM simply can’t hallucinate what’s behind.  Concretely, under heavy occlusion it will:\n\n  • Collapse or “flatten out” the occluded geometry  \n  • Leave large holes or spurious “floating” Gaussians in masked‐out regions  \n  • Produce overly smooth, blurry shapes with wrong surface normals  \n  • Mis–texture the unseen parts of the object  \n\nIn short, LGM by itself gives you a very coarse, holey, and often artifact-ridden 3DGS whenever the single input view hides significant portions of the object, which is why the authors have to add depth and color‐based supervision (and their hard‐depth loss) to pull the geometry back into agreement with the one true view."
    },
    {
        "question": "What ethical risks accompany GPT-4o use for extracting textural priors in DDIM+ Inversion?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "GPT-4o",
            "DDIM+ Inversion"
        ],
        "id": 972,
        "masked_question": "What ethical risks accompany [mask1] use for extracting textural priors in DDIM+ Inversion?",
        "masked_number": 1,
        "masked_elements": [
            "GPT-4o"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17189v1_figure_2.png",
        "paperid": "2411.17189v1",
        "paper_path": "./papers/2411.17189v1.json",
        "figure_id": "2411.17189v1_figure_2.png",
        "caption": "Figure 2: \nOverview. Given a single image input 𝐈0superscript𝐈0\\mathbf{I}^{0}bold_I start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT, we introduce a novel pipeline to generate high-fidelity, physics-grounded video with 3D understanding. Our pipeline consists of two main stages: first, we perform a single-view 3DGS reconstruction of the segmented object from the input image, then synthesize a physics-grounded coarse object dynamics {ℐj}j=1Nsuperscriptsubscriptsubscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT (Sec. 3.3); then, we apply diffusion-based video enhancement to produce the final enhanced video {ℐj∗}j=1Nsuperscriptsubscriptsubscriptsuperscriptℐ𝑗𝑗1𝑁\\{\\mathcal{I}^{*}_{j}\\}_{j=1}^{N}{ caligraphic_I start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT with backgrounds (Sec. 3.4), enabling users to create visually compelling, physics-driven video from a single image with an applied conditional force or torque.",
        "qtype": "Others",
        "response": "Chain-of-Thought:\n\n1. In §3.4.1 (“Inversion Stage”) the text tells us that “[m]oreover, textural priors are extracted by GPT-4o [2] to further augment the DDIM+ inversion process,” and in Fig. 2 the only component drawn in a red box is labeled “GPT-4o.”  \n2. Hence the [mask1] in the question refers to GPT-4o.  \n3. GPT-4o is a large, proprietary multimodal model trained on massive crawled data, so using it to produce “textural priors” raises several well-known ethical concerns:  \n   • Hallucinations or factual errors – the model may invent textures or mis‐describe appearance, leading to misleading or unusable priors.  \n   • Training‐data bias – any cultural, racial, or gender stereotypes baked into its training set can be propagated into the generated textures.  \n   • Copyright and ownership – GPT-4o may reproduce or paraphrase textural descriptions that trace back to copyrighted works, potentially infringing on IP.  \n   • Privacy leaks – if the model was trained on private or sensitive images or descriptions, the extracted priors may unintentionally reveal or reconstruct proprietary content.  \n   • Transparency and accountability – reliance on a closed, third-party system makes it difficult to audit, correct or attribute mistakes, and locks the pipeline into proprietary-only tools.  \n   • Dual-use / misuse potential – high-fidelity textural priors can be leveraged for deepfakes or other deceptive media generation.  \n4. All of these factors constitute the ethical risks of using GPT-4o to extract textural priors in the DDIM+ Inversion pipeline.\n\nAnswer:\n\nThe red-boxed component is GPT-4o, and outsourcing texture‐prior extraction to it carries risks of hallucinated or biased descriptions, inadvertent copyright or privacy leaks, lack of transparency (since it’s a closed‐source model), and dual‐use concerns (e.g. enabling more convincing deep-fake content)."
    },
    {
        "question": "What limitations arise from relying on one-shot offline calibration for variable-length grouped Anda activations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "one-shot offline calibration",
            "variable-length grouped Anda data type"
        ],
        "id": 973,
        "masked_question": "What limitations arise from relying on [mask1] for variable-length grouped Anda activations?",
        "masked_number": 1,
        "masked_elements": [
            "one-shot offline calibration"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15982v1_figure_1.png",
        "paperid": "2411.15982v1",
        "paper_path": "./papers/2411.15982v1.json",
        "figure_id": "2411.15982v1_figure_1.png",
        "caption": "Figure 1: Overview of the drop-in replacement for FP activations using the variable-length grouped Anda data type via a one-shot offline calibration process.\nThis enables online variable-precision LLM inference,\nsignificantly improving speed and energy efficiency through the adaptive precision combination search algorithm and the Anda-aware architecture.",
        "qtype": "Others",
        "response": "The red–boxed region in Fig. 1 is nothing more mysterious than the per-module, variable-length mantissa assignments that you get when you run Anda’s one-shot, off-line calibration (“optimal Anda precision per LLM module”).  By itself, that block is simply a look-up table of “use 11 bits here, 8 bits there, 13 bits over in the feed-forward,” etc.  The drawbacks of leaning on that precomputed table are:\n\n  1.  It’s completely static.  Once you pick your 4-tuple of mantissa widths for each major tensor type, those widths never change at run time.  If your real-world inputs wander outside the narrow range covered by your small calibration set, you can suddenly blow past your accuracy target or be left sitting on bits you don’t really need.\n\n  2.  It only works at the granularity of the four coarse “tensor types” (Q/K/V, attention-output, up-projection, down-projection).  Anything finer (per-layer, per-token, per-batch) was explicitly thrown away to keep the search fast.  You lose any potential gains from exploiting intra-module variance in sensitivity.\n\n  3.  Because you now have half a dozen different mantissa widths to juggle, your memory layout and your activation buffers grow more complicated (multiple depths in the bit-plane format, more complicated address generators, etc.).  In the absence of the full Anda hardware stack, a naïve implementation can easily negate the footprint and access–cost savings you were hoping for.\n\nIn short, “just use the precomputed, per-module mantissa widths” is cheap and one-shot, but it forces you into a rigid, coarse-grained scheme that (a) may not generalize beyond your calibration data, (b) cannot adapt at inference time, and (c) adds complexity to any hardware or software that has to load and unpack half-lulled bit-planes of wildly different depths."
    },
    {
        "question": "What justifies using optimal Anda precision per LLM module to achieve accuracy-efficiency balance?",
        "relevant_section_ids": [
            "2.3",
            "3.1"
        ],
        "relevant_context": [
            "From section II-C (2.3): “activations from different modules have varying impacts on model accuracy across all three models. Q consistently shows the most significant influence, while K demonstrates low sensitivity in OPT-6.7B but has a more pronounced effect in the LLaMA series models... These observations motivate us to introduce the new variable-length grouped data format for FP activations, along with a methodology for post-training quantization (PTQ) and rapid selection of tolerable reduced mantissa lengths for any LLM.”",
            "From section III-A (3.1): “Its key feature is the ability to dynamically select mantissa lengths for different tensors based on their precision sensitivity, maintaining consistency within each tensor while optimizing the accuracy-efficiency trade-off.”"
        ],
        "final_answer": "Because different LLM modules exhibit different sensitivities to mantissa‐bit reduction, selecting an optimal Anda mantissa length per module (based on each module’s precision sensitivity) preserves overall model accuracy while allowing more aggressive compression—and thus greater computational and storage efficiency—in less sensitive parts of the network.",
        "relevant_elements": [
            "optimal Anda precision per LLM module",
            "accuracy-efficiency balance"
        ],
        "id": 975,
        "masked_question": "What justifies using [mask1] to achieve [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "optimal Anda precision per LLM module",
            "accuracy-efficiency balance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15982v1_figure_1.png",
        "paperid": "2411.15982v1",
        "paper_path": "./papers/2411.15982v1.json",
        "figure_id": "2411.15982v1_figure_1.png",
        "caption": "Figure 1: Overview of the drop-in replacement for FP activations using the variable-length grouped Anda data type via a one-shot offline calibration process.\nThis enables online variable-precision LLM inference,\nsignificantly improving speed and energy efficiency through the adaptive precision combination search algorithm and the Anda-aware architecture.",
        "qtype": "Design_Rationale",
        "response": "The reason we pick “optimal Anda precision per LLM module” is that, as our sensitivity study (Sect. II-C, Figs. 6–7) shows, different transformer sub-modules tolerate mantissa‐bit cuts to very different degrees.  By measuring each module’s accuracy loss as we shrink its mantissa, we can then assign exactly the minimum number of bits each one needs to stay within our 1 % PPL budget.  In practice that module-wise, variable-length mantissa (the “optimal Anda precision per LLM module”) is the only way to squeeze out the maximum compute / storage savings while still hitting our accuracy-efficiency target—i.e. achieving the “accuracy–efficiency balance.”"
    },
    {
        "question": "What reasons underpin reducing storage & access cost while improving computation efficiency with Anda format?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As presented in Fig. 8(d), our proposed Anda format computation scheme offers some unique advantages in contrast with the previous approaches.",
            "Firstly, the activations are no longer stored in memory in FP16 format, but directly in the Anda data format, reducing storage overhead and data access overhead while avoiding frequent data conversion.",
            "Secondly, the shared exponent enables INT dot-product operations within a group, followed by FP32 accumulation across groups, reducing the computational overhead of FP-INT GeMMs.",
            "Thirdly, the variable-length mantissa considerably decreases dot-product operations and memory accesses use the minimal necessary word length.",
            "Finally, converting only the final FP32 results back to Anda format before writing to memory minimizes the storage requirement and the additional overhead from switching data format."
        ],
        "final_answer": "Anda reduces storage and access cost by storing activations directly in its compressed format (avoiding repeated FP16↔Anda conversions) and only reconverting the final FP32 outputs. It improves computation efficiency by sharing exponents for group-wise INT dot-products (cutting FP-INT overhead) and by using variable-length mantissas to minimize both dot-product operations and memory accesses.",
        "relevant_elements": [
            "reduced storage & access cost",
            "improved computation efficiency"
        ],
        "id": 976,
        "masked_question": "What reasons underpin [mask1] while [mask2] with Anda format?",
        "masked_number": 2,
        "masked_elements": [
            "reduced storage & access cost",
            "improved computation efficiency"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15982v1_figure_1.png",
        "paperid": "2411.15982v1",
        "paper_path": "./papers/2411.15982v1.json",
        "figure_id": "2411.15982v1_figure_1.png",
        "caption": "Figure 1: Overview of the drop-in replacement for FP activations using the variable-length grouped Anda data type via a one-shot offline calibration process.\nThis enables online variable-precision LLM inference,\nsignificantly improving speed and energy efficiency through the adaptive precision combination search algorithm and the Anda-aware architecture.",
        "qtype": "Design_Rationale",
        "response": "Step 1: Identify what [MASK1] and [MASK2] refer to in the illustration  \n– The red‐outlined label in the figure is “(+) reduced storage & access cost.”  \n– The blue‐outlined label is “(+) improved computation efficiency.”  \n\nStep 2: Locate the text passages that explain those two benefits  \nIn Sec. III-B (“Efficient FP-INT GeMM Using Anda Format”) the authors list four bullet-style advantages of moving to the Anda data layout.  Two of those map exactly onto our masks:  \n\n  1. “The activations are no longer stored in memory in FP16 format, but directly in the Anda data format, reducing storage overhead and data access overhead while avoiding frequent data conversion.”  \n  2. “The shared exponent enables INT dot-product operations within a group, followed by FP32 accumulation across groups, reducing the computational overhead of FP-INT GeMMs.  Thirdly, the variable-length mantissa considerably decreases dot-product operations and memory accesses by using the minimal necessary word length.”  \n\nStep 3: Synthesize a direct answer  \nWhat reasons underpin “reduced storage & access cost” while “improved computation efficiency” with the Anda format?  \n\nAnswer:  \n  • Reduced storage & access cost arises because Anda writes activations directly in its grouped BFP layout (sign + shared exponent + short mantissa) rather than full FP16.  This slashes the bits per element, removes repeated FP16⇄Anda conversions, and hence cuts both memory footprint and data‐movement overhead.  \n  • Improved computation efficiency follows from two Anda design choices:  \n     – A shared exponent lets each group of mantissas be multiplied as pure INT4×INTM dot‐products, with only a cheap FP32 accumulation step, instead of many full FP16 multiplies.  \n     – The variable‐length mantissa picks just as many bits as each tensor really needs, further shrinking the per-element bit-width of every multiply–accumulate and the number of memory accesses."
    },
    {
        "question": "What rationale supports initial Camera Keyframes Selection prior to Camera Keyframes Parameters Design?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "As shown in Figure1, in the actual process of creating dance camera movements, animators first select keyframes on the timeline, then determine the camera parameters of keyframes, and finally modify the tween curves which are used to control the changing speed of the camera parameters from one keyframe to the next.",
            "In the animation community’s dance camera-making procedure, the animators first select keyframes on the timeline when browsing the dance and music. Thus, we imitate this procedure to design a Camera Keyframe Detection stage and solve this problem in a classification manner."
        ],
        "final_answer": "The model follows the animator’s established workflow: animators first pick keyframes to mark important shot boundaries and rhythmic changes in the dance, and then they set the precise camera parameters at those key moments. By selecting keyframes first, the system can segment the performance into coherent shots—capturing both smooth continuous movements and abrupt switches—and then focus on designing the detailed camera settings for each shot.",
        "relevant_elements": [
            "Camera Keyframes Selection",
            "Camera Keyframes Parameters Design"
        ],
        "id": 977,
        "masked_question": "What rationale supports initial [mask1] prior to Camera Keyframes Parameters Design?",
        "masked_number": 1,
        "masked_elements": [
            "Camera Keyframes Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.14925v1_figure_1.png",
        "paperid": "2409.14925v1",
        "paper_path": "./papers/2409.14925v1.json",
        "figure_id": "2409.14925v1_figure_1.png",
        "caption": "Figure 1. Hierarchical dance-camera-making procedure by animators. According to the given music and dance, animators first select keyframes on the timeline. Next, animators set the camera parameters at each keyframe to capture the dance details or highlights. Then, for the non-keyframes between keyframes, animators produce the camera movements by editing tween curves that control the camera moving speed from one keyframe to the next. Finally, the 3D engine can render results with camera movements and dance.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed stage is simply “Camera Keyframe Selection” – that is, the automatic picking of the time‐points at which the camera should switch or pause.  We do this up front for exactly the same reason that professional animators do: dance camera work naturally falls into variable‐length “shots” punctuated by discrete, often abrupt changes in angle, distance or framing.  By first detecting those shot boundaries (the keyframes) we can  \n1. isolate where the camera needs to jump or reset (so we don’t smear together smooth pans with sudden cuts),  \n2. focus the downstream parameter‐synthesis step on only a handful of critical poses, and  \n3. recover fine‐grained control over editing and interpolation (the tween curves) between those anchors.  \nIn short, selecting keyframes first mirrors the human animator’s workflow, prevents unwanted jitter or over‐smoothing, and gives us a clean, shot‐aware scaffold on which to build the actual camera parameters."
    },
    {
        "question": "What motivates using a monotonically increasing tween function for non-keyframe interpolation?",
        "relevant_section_ids": [
            "1",
            "4.3"
        ],
        "relevant_context": [
            "After observation, we find that the tween curves are monotonically increasing so that the smoothness of complete shots can be guaranteed.",
            "To overcome the jittering of the camera, we generate tween function values instead of camera parameters in the Tween Function Prediction model so that the camera will move from one keyframe to the next at different speeds without moving in other directions.",
            "…we process the intermediate increments for non-negativization…, calculate the cumulative sum…, and conduct normalization to produce ρ which are monotonically increasing value from 0 to 1."
        ],
        "final_answer": "Using a monotonically increasing tween function ensures that camera motion between keyframes proceeds smoothly and without unintended reversals or jitter. By enforcing monotonic increases from 0 to 1, the method replicates animator practice with Bezier curves—guaranteeing smooth complete shots—and prevents the camera from moving in unwanted directions between keyframes.",
        "relevant_elements": [
            "Camera Tween Function Design",
            "Computing Non-keyframes"
        ],
        "id": 978,
        "masked_question": "What motivates using a monotonically increasing [mask1] for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Camera Tween Function Design",
            "Computing Non-keyframes"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.14925v1_figure_1.png",
        "paperid": "2409.14925v1",
        "paper_path": "./papers/2409.14925v1.json",
        "figure_id": "2409.14925v1_figure_1.png",
        "caption": "Figure 1. Hierarchical dance-camera-making procedure by animators. According to the given music and dance, animators first select keyframes on the timeline. Next, animators set the camera parameters at each keyframe to capture the dance details or highlights. Then, for the non-keyframes between keyframes, animators produce the camera movements by editing tween curves that control the camera moving speed from one keyframe to the next. Finally, the 3D engine can render results with camera movements and dance.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. In traditional 3D animation (e.g. MikuMikuDance), after placing keyframes animators edit “tween” or easing curves to drive the camera smoothly from one key pose to the next.  \n2. Empirically, those Bezier-style tween curves are always monotonically increasing in t so that the interpolation parameter ρ(t) goes from 0 to 1 without ever dipping or reversing.  \n3. By enforcing monotonicity on ρ(t), the camera parameters  \n   Cₜ = Cₜ₁ + ρ(t)·(Cₜ₂ – Cₜ₁)  \n   move steadily along the straight path between keyframes, purely changing speed but never “backtracking” or oscillating.  \n4. This guarantees a smooth, jitter-free shot between keyframes—exactly the property animators rely on to capture dance details without unwanted camera shakes.  \n\nAnswer  \nThey borrow the animator’s practice of using strictly increasing easing curves so that the interpolation weight ρ(t) marches cleanly from 0 to 1.  This monotonic tween ensures that, when computing the in-between camera parameters, the camera moves only forward along the intended trajectory—varying speed but never reversing—thus producing smooth, stable shots."
    },
    {
        "question": "How does Camera Keyframes Selection utilize music and dance embeddings to classify frames as keyframes?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Given input music and dance poses, we first extract the acoustic features f_t^m from the music following FACT (Li et al., 2021) to use Librosa (McFee et al., 2015) and represent the dance poses with positions of 60 joints as f_t^p. Then we exploit a sliding window to select music–dance context as M_t and P_t and use encoders to encode the above input as \\tilde F_t^a, \\tilde F_t^p, and \\tilde K_t. Using these embeddings, we employ a transformer decoder and a linear layer to obtain the probability sequence of being a keyframe as:\np_t = W^d Dec(\\tilde F^k, \\tilde F^p, \\tilde F^a)_t + b_d\nFollowing this, we can predict whether there is a keyframe at time t by comparing the probabilities as:\ny_t = 1 if p_t > 0.5 else 0"
        ],
        "final_answer": "Camera Keyframe Selection first extracts acoustic features from music and joint‐position features from dance, then uses a sliding window to build music–dance context. These features are encoded into embeddings (for music, dance, and prior keyframe history) and fed into a transformer decoder followed by a linear layer that outputs a per‐frame probability p_t of being a keyframe. Finally, frames with p_t > 0.5 are classified as keyframes.",
        "relevant_elements": [
            "Camera Keyframes Selection",
            "Music",
            "Dance"
        ],
        "id": 979,
        "masked_question": "How does [mask1] utilize [mask2] and dance embeddings to classify frames as keyframes?",
        "masked_number": 2,
        "masked_elements": [
            "Camera Keyframes Selection",
            "Music"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.14925v1_figure_1.png",
        "paperid": "2409.14925v1",
        "paper_path": "./papers/2409.14925v1.json",
        "figure_id": "2409.14925v1_figure_1.png",
        "caption": "Figure 1. Hierarchical dance-camera-making procedure by animators. According to the given music and dance, animators first select keyframes on the timeline. Next, animators set the camera parameters at each keyframe to capture the dance details or highlights. Then, for the non-keyframes between keyframes, animators produce the camera movements by editing tween curves that control the camera moving speed from one keyframe to the next. Finally, the 3D engine can render results with camera movements and dance.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In the figure the red-boxed module is the “Camera Keyframe Detection” stage: its job is to look at a short chunk of audio (the blue-boxed music signal) together with the corresponding dance poses, and decide for each frame “keyframe or not.”  \n2. Concretely, at time t we gather  \n   - Mt, a sliding‐window of recent acoustic features extracted from the raw waveform (blue box → Librosa/FACT features)  \n   - Pt, the 60-joint pose vectors over the same window  \n   - Ht, the history of past keyframe tags (zero-padded at start)  \n3. Each of these three pieces (Mt, Pt, Ht) is passed through its own encoder to yield embeddings em, ep, ek.  \n4. We feed the concatenated sequence of em, ep, ek into a transformer decoder and then a small linear head that spits out, for every time step in the window, a probability pi of “this frame is a keyframe.”  \n5. Finally we threshold pi (or take the larger of “keyframe” vs “non-keyframe” logits) to produce a binary tag yt∈{0,1}.  \n6. During training we supervise these tags with a weighted binary cross-entropy so that the model learns to fire only on true keyframe instants."
    },
    {
        "question": "How do Camera Keyframes Parameters Design and Camera Tween Function Design jointly enforce smooth inter-keyframe transitions?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "In the Camera Keyframe Synthesis Stage (Sec. 4.2), we infer the camera parameters at each detected keyframe by decoding music–dance context and the past camera‐movement history: “With these conditions, we use a transformer decoder to generate keyframe camera poses at T₁ and T₂.”",
            "In the Tween Function Prediction Stage (Sec. 4.3), we predict a per‐frame interpolation curve ρ(t) that is forced to be monotonically increasing from 0 to 1.  We do so by predicting non‐negative increments, taking their cumulative sum, and normalizing: “…we first process 𝚫 for non-negativization to obtain Δ denoting the increment of the tween function. Then, we calculate the cumulative sum of Δ as S and conduct normalization to produce ρ which are monotonically increasing value from 0 to 1…”.  Finally, we compute the in-between camera parameters as Cₜ = C_{T₁} + ρ(t)·(C_{T₂} − C_{T₁})."
        ],
        "final_answer": "By first generating precise camera parameters at each keyframe (Sec. 4.2) and then interpolating between those endpoints using a learned, strictly increasing tween function ρ(t) (Sec. 4.3), the method guarantees that camera poses move smoothly from one keyframe to the next without abrupt jumps or jitter.",
        "relevant_elements": [
            "Camera Keyframes Parameters Design",
            "Camera Tween Function Design"
        ],
        "id": 980,
        "masked_question": "How do [mask1] and [mask2] jointly enforce smooth inter-keyframe transitions?",
        "masked_number": 2,
        "masked_elements": [
            "Camera Keyframes Parameters Design",
            "Camera Tween Function Design"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.14925v1_figure_1.png",
        "paperid": "2409.14925v1",
        "paper_path": "./papers/2409.14925v1.json",
        "figure_id": "2409.14925v1_figure_1.png",
        "caption": "Figure 1. Hierarchical dance-camera-making procedure by animators. According to the given music and dance, animators first select keyframes on the timeline. Next, animators set the camera parameters at each keyframe to capture the dance details or highlights. Then, for the non-keyframes between keyframes, animators produce the camera movements by editing tween curves that control the camera moving speed from one keyframe to the next. Finally, the 3D engine can render results with camera movements and dance.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the two work together to guarantee a smooth, musically‐synchronized blend between keyframes:\n\n1. Mask 1 (the red‐boxed “Music” waveform) is converted into frame‐wise acoustic embeddings that carry the beat, energy and rhythmic structure of the song. These embeddings are passed, along with the dance poses and the two bracketing keyframe cameras (at T₁ and T₂), into the Tween Function Prediction network.\n\n2. Mask 2 (the blue‐boxed graph of ρ(t)) is the *tween function* that the network actually predicts. It is forced to be\n   - non-negative at every step (so that it never reverses),  \n   - cumulatively summed and normalized to run monotonically from 0 up to 1 over the interval [T₁, T₂].\n\n3. At runtime we simply interpolate every intermediate camera parameter Ct as  \n      Ct = C_T₁ + ρ(t) · (C_T₂ – C_T₁).  \n   Because ρ(t) is a smooth, strictly increasing curve whose shape was conditioned on the music, the resulting camera path eases in and out in time with the beat, producing seamless transitions between the two keyframe poses."
    },
    {
        "question": "How are pseudo point clouds generated from 2D RGB images without paired depth data?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Specifically, the entire pipeline consists of two flows: (1) Image → Pseudo PC, by leveraging a large-scale 2D images training set, our method begins by converting images to pseudo point clouds through monocular depth estimation and approximate camera parameter.",
            "Utilizing 2D datasets for 3D detection presents difficulties due to the absence of metric depth images and camera parameters. To overcome these obstacles, we use a metric depth estimation model to obtain single-view depth images. Additionally, we employ fixed camera intrinsics, with the focal length calculated based on a 55-degree field of view and the image dimensions.\n\nHowever, the absence of camera extrinsics (where R is the rotation matrix and t is the translation vector set to 0) results in the arbitrary orientation of point clouds. To correct this, we use a rotation correction module to ensure the ground plane is horizontal. After obtaining the camera intrinsics matrix K and the camera extrinsics matrix [R|t] through the previous steps, depth images are converted into point clouds."
        ],
        "final_answer": "Pseudo point clouds are generated by first running a monocular depth estimation model on the 2D RGB images to produce per-pixel depth maps.  Those depth maps are then lifted into 3D space using fixed camera intrinsics (computed from a 55° field of view) and an estimated camera extrinsics correction (aligning the ground plane via a normal-based rotation module), yielding pseudo point clouds.",
        "relevant_elements": [
            "RGB Images",
            "Pseudo Point Clouds"
        ],
        "id": 981,
        "masked_question": "How are [mask1] generated from 2D [mask2] without paired depth data?",
        "masked_number": 2,
        "masked_elements": [
            "Pseudo Point Clouds",
            "RGB Images"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.24001v1_figure_1.png",
        "paperid": "2410.24001v1",
        "paper_path": "./papers/2410.24001v1.json",
        "figure_id": "2410.24001v1_figure_1.png",
        "caption": "Figure 1: Left: Traditional methods require paired RGB-D data for training and use single-modality point clouds as input during inference. Right: ImOV3D involves using a vast amount of 2D images to generate pseudo point clouds during the training phase, which are then rendered back into images. In the inference phase, with only point clouds as input, we still construct a pseudo-multimodal representation to enhance detection performance.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought (step by step)  \n1. In the diagram, the blue box ([mask2]) clearly labels “RGB Images” as the only input at training time.  \n2. The red box ([mask1]) labels “Pseudo Point Clouds,” which are produced from those RGB Images despite having no paired depth.  \n3. The text describes exactly how this is done:  \n   a. A pretrained monocular depth estimator (ZoeDepth) predicts a metric depth map D for each RGB image.  \n   b. Fixed camera intrinsics K (focal length set by a 55° FOV) are assumed, so each pixel (u,v,D(u,v)) can be back‐projected into 3D space via X = K⁻¹·[u,v,1]ᵀ·D(u,v).  \n   c. Because the scene orientation is initially unknown, they run a lightweight rotation‐correction: estimate per‐pixel normals, find the dominant “ground” normal, and compute a corrective rotation so that the ground plane becomes horizontal.  \n   d. Applying that rotation to all back‐projected points yields the final pseudo point cloud.  \n\nAnswer  \nThey run a monocular depth model on each RGB image to get a depth map, back‐project every pixel through assumed intrinsics into 3D, and then apply a learned rotation‐correction (via surface‐normal estimation) so that the resulting point cloud has its ground plane level."
    },
    {
        "question": "How does the multimodal detector fuse GT point clouds with pseudo images during inference?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "By leveraging $P^{pc}$ and $P^{img}$, a 3D backbone is trained to obtain seed points $S^{pc}$, where $|S^{pc}|$ represents the number of seeds, along with 3D feature representations $F^{pc}$, with $d$ denoting the feature dimension. Then, seed points are projected back into 2D space via the camera matrix.",
            "These seeds that fall within the 2D bounding boxes $B^{img}$ retrieve the corresponding 2D cues associated with these boxes and bring them back into 3D space. These lifted 2D cues features are represented as $F^{2D}$, where $c$ represents the feature dimension.",
            "Finally, the point cloud features $F^{pc}$ and image features $F^{2D}$ are concatenated, forming the joint representation $F^{joint}$."
        ],
        "final_answer": "During inference, the detector takes the ground-truth point cloud as $P^{pc}$ and the corresponding pseudo image as $P^{img}$. It first runs the point cloud through a 3D backbone to produce seed points and their 3D features $F^{pc}$. Those seed points are projected into the pseudo image via the known camera matrix. For any seed that lies inside a 2D box, the detector samples the associated 2D feature (the “pseudo” image cue) and lifts it back into 3D as $F^{2D}$. Finally, it concatenates the 3D point‐cloud features $F^{pc}$ with the lifted 2D image features $F^{2D}$ to form a joint multimodal representation, which is then used for detection.",
        "relevant_elements": [
            "GT Point Clouds",
            "Pseudo Image",
            "Multimodal Detector"
        ],
        "id": 982,
        "masked_question": "How does the multimodal detector fuse [mask1] with pseudo images during inference?",
        "masked_number": 1,
        "masked_elements": [
            "GT Point Clouds"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.24001v1_figure_1.png",
        "paperid": "2410.24001v1",
        "paper_path": "./papers/2410.24001v1.json",
        "figure_id": "2410.24001v1_figure_1.png",
        "caption": "Figure 1: Left: Traditional methods require paired RGB-D data for training and use single-modality point clouds as input during inference. Right: ImOV3D involves using a vast amount of 2D images to generate pseudo point clouds during the training phase, which are then rendered back into images. In the inference phase, with only point clouds as input, we still construct a pseudo-multimodal representation to enhance detection performance.",
        "qtype": "Implementation_Details",
        "response": "First, note that the little red‐boxed “mystery” block on the inference side is simply the ground‐truth point cloud itself (i.e. the real 3D scan that we feed in at test time).  During inference, ImOV3D never sees any real RGB images – instead it “hallucinates” a companion image by running that GT point cloud through its learned point‐cloud renderer + ControlNet to get a pseudo-image.  \n\nHow these two streams then get fused is exactly the same trick they used in training:  \n\n 1. The GT point cloud is passed through the 3D backbone to produce N “seed” points, each with a D-dimensional feature vector fᵖᶜ.  \n 2. Using the known camera matrix, each seed is projected into the pseudo-image plane.  Wherever a seed lands, you sample the corresponding 2D feature fⁱᵐᵍ from the pseudo-image branch.  \n 3. You lift those 2D features back into 3D space and simply concatenate them with the original 3D features:  [fᶜᶜ; fⁱᵐᵍ] → joint feature.  \n 4. The detector’s heads then operate on that joint feature to predict box proposals and scores.  \n\nIn short, at inference time ImOV3D fuses the GT point cloud with its own rendered pseudo-image by projecting each 3D seed into the 2D pseudo image, sampling the image features there, and concatenating them with the 3D features before the final detection heads."
    },
    {
        "question": "How does the pseudo image complement pseudo point clouds in multimodal detector training compared to paired RGB-D methods?",
        "relevant_section_ids": [
            "1",
            "3.4"
        ],
        "relevant_context": [
            "Existing methods [30, 29, 28, 10, 49] seek help from powerful open-vocabulary 2D detectors. A common method leverages paired RGB-D data together with 2D detectors to generate 3D pseudo labels to address the label scarcity issue ... But they are still restricted by the small scale of existing paired RGB-D data. Moreover, the from scratch trained 3D detector can hardly inherit from powerful open-vocabulary 2D detector models directly due to the modality difference.",
            "Observing that the modality gap prevents a direct knowledge transfer, we propose to leverage a pseudo multi-modal representation to close the gap. On one hand, we can lift a 2D image into a pseudo-3D representation through estimating the depth and camera matrix. On the other hand, we can convert a 3D point cloud into a pseudo-2D representation through rendering. The pseudo RGB image–PC multimodal representation could serve as a common ground for better transferring knowledge from 2D to 3D.",
            "Point cloud data has inherent limitations, such as the inability of sparse point clouds to capture detailed textures. 2D images can enrich 3D data by providing additional texture information that point clouds lack. ... we develop a point cloud renderer to convert point clouds into detailed pseudo images."
        ],
        "final_answer": "Unlike paired RGB-D methods—which only use real RGB and depth to generate 3D pseudo labels for a pure point-cloud detector and still suffer from a modality gap—ImOV3D produces “pseudo images” by rendering its pseudo point clouds, providing exactly the kind of 2D texture and semantic cues that would normally come from an RGB camera. These pseudo images are paired with the pseudo point clouds to form a unified multimodal training input. By jointly learning from both modalities (geometry from the pseudo point cloud and texture/semantic information from the pseudo image), the detector can more effectively transfer powerful 2D detection knowledge into 3D, all without ever needing true RGB-D pairs.",
        "relevant_elements": [
            "pseudo image",
            "pseudo point clouds",
            "multimodal detector"
        ],
        "id": 983,
        "masked_question": "How does the [mask1] complement pseudo point clouds in multimodal detector training compared to paired RGB-D methods?",
        "masked_number": 1,
        "masked_elements": [
            "pseudo image"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.24001v1_figure_1.png",
        "paperid": "2410.24001v1",
        "paper_path": "./papers/2410.24001v1.json",
        "figure_id": "2410.24001v1_figure_1.png",
        "caption": "Figure 1: Left: Traditional methods require paired RGB-D data for training and use single-modality point clouds as input during inference. Right: ImOV3D involves using a vast amount of 2D images to generate pseudo point clouds during the training phase, which are then rendered back into images. In the inference phase, with only point clouds as input, we still construct a pseudo-multimodal representation to enhance detection performance.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed element is in fact the “pseudo image” stream – i.e. the network’s synthetic 2D view of the scene, rendered (and then colourized) from the pseudo point cloud.  During training we feed both the lifted 3D point cloud and this rendered pseudo–RGB image into a unified detector; the point cloud carries the metric, geometric cues, while the pseudo image carries back all of the rich 2D texture and semantics so that the detector can learn to fuse them.  \n\nIn contrast, classical paired RGB–D methods  \n • train a 3D detector from scratch on a small set of real RGB–D pairs,  \n • generate their 3D supervision by projecting the real RGB through a fixed 2D detector into the depth charge,  \n • but at the end of the day only the point-cloud stream survives at inference and the two modalities never really learn to co-adapt.  \n\nBy “closing the loop” – lifting 2D images into pseudo point clouds, rendering them back into pseudo images, and then training a single multimodal network with shared weights – ImOV3D both massively expands the training corpus (all of internet‐scale 2D data) and forces the model to learn a true 2D⇄3D feature fusion.  That is precisely how the pseudo image branch complements the pseudo point clouds, where paired RGB–D systems cannot."
    },
    {
        "question": "How do shared weights facilitate cross-modal learning between point cloud and multimodal detectors relative to point cloud-only inference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "point cloud detector",
            "multimodal detector",
            "shared weights"
        ],
        "id": 984,
        "masked_question": "How do [mask1] facilitate cross-modal learning between point cloud and [mask2] relative to point cloud-only inference?",
        "masked_number": 2,
        "masked_elements": [
            "shared weights",
            "multimodal detector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.24001v1_figure_1.png",
        "paperid": "2410.24001v1",
        "paper_path": "./papers/2410.24001v1.json",
        "figure_id": "2410.24001v1_figure_1.png",
        "caption": "Figure 1: Left: Traditional methods require paired RGB-D data for training and use single-modality point clouds as input during inference. Right: ImOV3D involves using a vast amount of 2D images to generate pseudo point clouds during the training phase, which are then rendered back into images. In the inference phase, with only point clouds as input, we still construct a pseudo-multimodal representation to enhance detection performance.",
        "qtype": "Literature_Background",
        "response": "Here is a step‐by‐step reasoning (chain‐of‐thought) and the final answer with the masked terms filled in:\n\n1.  In the left half of Figure 1 (“Previous Methods”), you see a vanilla 3D detector trained on paired RGB-D data but at test time it sees only point clouds.  \n2.  In the right half (“ImOV3D”), during training we convert 2D web images into:\n    - pseudo-3D point clouds (via monocular depth)  \n    - and pseudo-2D renderings of those point clouds (via our renderer)  \n   and feed both into a single detection network.  \n3.  Crucially, the pink “Multimodal Detector” used in training and the one used at test time share exactly the same parameters (that is what the red box around the arrow labeled “Shared Weights” denotes).  \n4.  Because the network has to ingest both rendered images and point clouds through the same set of weights, it is forced to align features from the 2D modality with features from the 3D modality.  \n5.  At inference, even though we only input real point clouds (plus on-the-fly renderings), the detector still uses those shared weights—and so it “remembers” the 2D semantic signals it saw at train time and injects them into its 3D reasoning.  \n6.  A classical point-cloud-only model never saw 2D signals, so it cannot benefit from that cross-modal transfer.  \n\nAnswer:  \n“How do Shared Weights facilitate cross‐modal learning between point cloud and Multimodal Detector relative to point cloud‐only inference?”\n\nBy forcing the same network parameters to process both rendered images and 3D point clouds during training, the shared weights align the two modalities in a common embedding space.  This means the rich 2D semantic knowledge (learned from pseudo-images) is baked directly into the 3D branch—so at test time, even when only point clouds (and their renderings) are available, the detector still applies the image‐derived concepts, yielding much stronger open-vocabulary performance than a model trained on point clouds alone."
    },
    {
        "question": "How does bi-attention enhance shared R_MP extraction compared to independent self-attention branches for PET and MRI?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "Section 3.1: “Our proposed framework DiaMond consists of three branches based on pure ViTs to process each of the data space: … receives both MRI and PET, then captures their shared information in data space R_MP, and finally maps those to the latent encoding Z_MP of length L.”",
            "Section 3.3: “A novel bi-attention mechanism Θ is introduced … uniquely designed to focus on capturing their similarities in the high-dimensional feature space … the bi-attention blocks aim to produce features for each modality conditioned on the other, targeting on their potential disease-specific similarities.”",
            "Section 3.3: “Distinct from the conventional cross-attention mechanism, which captures both similarities and dissimilarities between patches in the correlation matrices, our proposed bi-attention selectively preserves only the pronounced similarities, resulting in a sparse representation. This sparsity is achieved by applying a constant threshold τ to the correlation matrix between the query and key matrices from two modalities, filtering out negligible values.”"
        ],
        "final_answer": "Independent self-attention branches each extract unique modality information (R_P from PET, R_M from MRI) by operating within one modality only. In contrast, bi-attention interleaves queries and keys across PET and MRI, thresholds their correlation matrix to keep only the strongest cross-modal correlations, and thus explicitly and sparsely captures the shared feature space R_MP (disease-specific dependencies) that self-attention alone cannot isolate.",
        "relevant_elements": [
            "Bi-Attention",
            "Self-Attention"
        ],
        "id": 985,
        "masked_question": "How does [mask1] enhance shared R_MP extraction compared to independent self-attention branches for PET and MRI?",
        "masked_number": 1,
        "masked_elements": [
            "Bi-Attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23219v1_figure_1.png",
        "paperid": "2410.23219v1",
        "paper_path": "./papers/2410.23219v1.json",
        "figure_id": "2410.23219v1_figure_1.png",
        "caption": "Figure 1: MRI and PET are two modalities with disease-specific dependency. We introduce a novel framework including self-attention mechanism with multi-modal normalization to capture distinct features from single modalities, and a novel bi-attention mechanism to exclusively extract their similarities.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the red‐boxed module in the figure with the text.  The red box surrounds the “Bi-Attention” block that sits between the PET and MRI self-attention streams.  Now, step by step:\n\n1.  Self-attention on PET (branch ) and MRI (branch ) only ever looks “within” one modality, so each of those heads learns modality-specific structure ( and ) but never the true joint patterns in .  \n2.  The bi-attention block takes the embeddings from both modalities and computes a cross‐modal correlation matrix (queries from PET vs. keys from MRI, and vice versa).  \n3.  It then applies a hard threshold to that correlation matrix, zeroing out all but the strongest (i.e. most similar) inter-modal patch correspondences.  This yields a sparse attention map that highlights only the pronounced PET↔MRI matches.  \n4.  By re–weighting each modality’s values with this thresholded cross‐correlation, bi-attention explicitly mines the overlapping region  and suppresses irrelevant or modality-unique information.  \n5.  In contrast, two independent self-attention branches cannot “talk to” each other and so have no mechanism to isolate or enhance exactly those features that live in .  \n\nIn short, the bi‐attention module in [mask1] conditions each modality’s features on the other via a thresholded cross–correlation, producing a sparse, focused representation of the shared disease-specific subspace  that simple single‐modality self‐attention cannot extract."
    },
    {
        "question": "How does RegBN integrate with bi-attention to mitigate redundant dependencies in shared PET-MRI features?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "3.1: “We extract regions R_P using self-attention mechanisms, together with a recently-developed normalization technique RegBN [10] to ensure feature independence. A novel bi-attention mechanism is introduced to explore the similarities between the two modalities in region R_MP.”",
            "3.4: “Since the proposed bi-attention block focuses explicitly on the underlying similarities between input modalities via the self-attention modules, it is essential to eliminate redundant shared information between them. Otherwise, the neural network may primarily optimize within the overlapped region R_MP, increasing the risk of getting trapped in local minima. Thus, we use RegBN to separate the latent encoding Z_M from Z_P.”",
            "3.4: “RegBN represents one latent encoding in terms of another using a linear regression model: Z_i = P Z_j + R_i, in which P is a projection matrix, and R_i denotes the difference between the input latent encodings, so-called residual. The residual segment contains a portion of Z_i that is independent from Z_j.”"
        ],
        "final_answer": "RegBN is applied to the latent PET and MRI encodings before they enter the bi-attention branch.  By regressing one modality’s latent features on the other and using only the residual (the portion independent of the other modality), RegBN removes redundant overlapping dependencies.  This ensures that the subsequent bi-attention mechanism focuses exclusively on the disease-specific shared information (R_MP) rather than re-encoding already-shared signal, thereby mitigating the risk of overfitting to redundant PET-MRI correlations.",
        "relevant_elements": [
            "RegBN",
            "Bi-Attention"
        ],
        "id": 986,
        "masked_question": "How does [mask1] integrate with bi-attention to mitigate redundant dependencies in shared PET-MRI features?",
        "masked_number": 1,
        "masked_elements": [
            "RegBN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.23219v1_figure_1.png",
        "paperid": "2410.23219v1",
        "paper_path": "./papers/2410.23219v1.json",
        "figure_id": "2410.23219v1_figure_1.png",
        "caption": "Figure 1: MRI and PET are two modalities with disease-specific dependency. We introduce a novel framework including self-attention mechanism with multi-modal normalization to capture distinct features from single modalities, and a novel bi-attention mechanism to exclusively extract their similarities.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought:  \n1. The red-boxed module in the figure is labeled “RegBN,” which in the text is described as a multi-modal batch-normalization with regularization.  \n2. RegBN sits immediately after the single-modality self-attention branches and before the features enter the bi-attention block.  \n3. Its purpose is to remove the partially redundant dependencies (the overlap Rp∩Rm in Fig. 1) that are carried over from acquisition into the latent embeddings of PET and MRI.  \n4. Concretely, RegBN fits a linear regression from one modality’s embedding to the other and takes the residual as the “independent” part, enforcing this via a Frobenius-norm penalty over the mini-batch.  \n5. By feeding these decorrelated, residual features into the bi-attention module, the bi-attention no longer “wastes” its capacity on trivial shared signals but can focus exclusively on the disease-specific similarities Rmp.  \n\nAnswer:  \nRegBN is interposed between the single-modality self-attention outputs and the bi-attention block.  It regresses one modality’s latent vectors onto the other, uses the residual as the independent feature component (minimizing the regression error with a Frobenius-norm regularizer), and only passes these decorrelated residuals into the bi-attention.  In this way, bi-attention operates on features from which redundant PET–MRI correlations have already been stripped out, ensuring that it concentrates purely on the disease-relevant shared information."
    },
    {
        "question": "How does RegBN improve feature independence across self-attention branches?",
        "relevant_section_ids": [
            "3.2",
            "3.4",
            "6.2"
        ],
        "relevant_context": [
            "Each self-attention branch aims to independently extract unique features from one input modality. To ensure that each branch efficiently identifies distinct modality-dependent features, a normalization technique RegBN [10] is later applied to the latent space, aiming to reduce redundant partial dependency between the self-attention branches.",
            "RegBN represents one latent encoding in terms of another using a linear regression model: in which P is a projection matrix, and r denotes the difference between the input latent encodings, so-called residual. The residual segment contains a portion of M that is independent from P.",
            "RegBN is incorporated into DiaMond as a normalization technique to make self-attention branches independent, aiming to reduce the redundant partial dependency between the input modalities."
        ],
        "final_answer": "RegBN improves feature independence by regressing one modality’s latent features onto another and using the residual as the new encoding. This removes shared (redundant) information between branches, thereby reducing their partial dependency and ensuring each self-attention branch learns distinct, independent features.",
        "relevant_elements": [
            "RegBN",
            "self-attention"
        ],
        "id": 987,
        "masked_question": "How does [mask1] improve feature independence across self-attention branches?",
        "masked_number": 1,
        "masked_elements": [
            "RegBN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.23219v1_figure_1.png",
        "paperid": "2410.23219v1",
        "paper_path": "./papers/2410.23219v1.json",
        "figure_id": "2410.23219v1_figure_1.png",
        "caption": "Figure 1: MRI and PET are two modalities with disease-specific dependency. We introduce a novel framework including self-attention mechanism with multi-modal normalization to capture distinct features from single modalities, and a novel bi-attention mechanism to exclusively extract their similarities.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the RegBN module (the “multimodal batch-norm with regularization”) that sits on top of each self-attention branch.  Concretely, RegBN takes the two latent encodings (one from the PET branch, one from the MRI branch), fits a linear regression from one to the other, and then subtracts the fitted (i.e. shared) part, leaving only the residual.  By penalizing the regression weights (via a Frobenius‐norm regularizer over mini-batches), RegBN forces those residuals to be as decorrelated as possible.  The net effect is that any feature common to both branches is regressed out, so each self-attention branch is left with only its modality-specific information, greatly reducing redundant cross-dependencies."
    },
    {
        "question": "How does bi-attention selectively extract shared information while avoiding redundant modality features?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Distinct from the conventional cross-attention mechanism, which captures both similarities and dissimilarities between patches in the correlation matrices, our proposed bi-attention selectively preserves only the pronounced similarities, resulting in a sparse representation.",
            "This sparsity is achieved by applying a constant threshold τ to the correlation matrix between the query and key matrices from two modalities, filtering out negligible values.",
            "A novel bi-attention mechanism Aᵦ is introduced in 3.3 to compute the interweaved attention between two modalities M and P, uniquely designed to focus on capturing their similarities in the high-dimensional feature space: Aᵦ = I(Qᴹ (Kᴾ)ᵀ ≥ τ)"
        ],
        "final_answer": "Bi-attention first computes the cross‐modal correlation matrix between the query features of one modality and the key features of the other. It then applies a hard threshold τ (via an indicator function) to that matrix, zeroing out all correlations below τ. By keeping only the strong, above‐threshold correlations, the mechanism yields a sparse attention map that captures pronounced similarities (shared disease‐specific features) while discarding weak or redundant cross‐modal relationships.",
        "relevant_elements": [
            "bi-attention"
        ],
        "id": 988,
        "masked_question": "How does [mask1] selectively extract shared information while avoiding redundant modality features?",
        "masked_number": 1,
        "masked_elements": [
            "bi-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.23219v1_figure_1.png",
        "paperid": "2410.23219v1",
        "paper_path": "./papers/2410.23219v1.json",
        "figure_id": "2410.23219v1_figure_1.png",
        "caption": "Figure 1: MRI and PET are two modalities with disease-specific dependency. We introduce a novel framework including self-attention mechanism with multi-modal normalization to capture distinct features from single modalities, and a novel bi-attention mechanism to exclusively extract their similarities.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐box around “mask1” in Fig. 2 is exactly the bi‐attention module (the block labeled “Bi-Attention”).  Its sole job is to mine the intersection region Rₘₚ (the shared MRI⧸PET features) and to throw away everything else.  It does that in two ways:\n\n 1.  **Decouple each modality beforehand**  \n     – Both the MRI self-attention branch and the PET self-attention branch are passed through RegBN, which fits and removes any linear projection of one branch onto the other.  In practice that means each branch only carries its *own* unique information, with overlapping (redundant) bits subtracted off as a “residual.”  \n\n 2.  **Sparse, thresholded cross-attention**  \n     – The bi-attention block now takes the de-correlated queries and keys from MRI and PET and computes their pairwise correlation matrix.  \n     – Rather than keeping *all* correlations (as in ordinary cross-attention), it applies a hard threshold τ via an indicator function: any cell whose correlation is below τ is zeroed out.  \n     – Only the *strongest*, disease-relevant alignments survive, producing a very sparse attention map that by design represents only the shared subspace Rₘₚ.  \n\nBy first stripping away partial dependencies with RegBN and then only “lighting up” the highest inter-modal correlations, the bi-attention block zeroes in on true multimodal *common* structure and avoids reintroducing any modality-specific or redundant features."
    },
    {
        "question": "How does Multi-GraphConv Layer's multi-head convolution affect inter-entity representation aggregation?",
        "relevant_section_ids": [
            "4.2.2"
        ],
        "relevant_context": [
            "Initially, the linear transformation of the adjacency matrix A on the input features H is computed for each head k. Subsequently, the impact of neighboring nodes' features on the present node is determined for each layer l, and the current node's features are consolidated with the previous layer's output.",
            "The feature representation resulting from the output of each head is combined to form the final output of this layer: H^{l+1} = Concat_{k=1 to K}(h^{l+1}_{:,k}) W^{l}, where W^{l} is the weight parameter of the linear transformation applied to the ultimate output."
        ],
        "final_answer": "By deploying multiple graph‐convolution heads, the Multi-GraphConv layer lets each head learn its own adjacency weighting and aggregate neighbor (entity) features independently. The head-wise outputs are then concatenated and linearly transformed, yielding a fused node representation that integrates diverse structural cues and thus produces richer inter-entity representations.",
        "relevant_elements": [
            "Multi-GraphConv Layer"
        ],
        "id": 989,
        "masked_question": "How does [mask1]'s multi-head convolution affect inter-entity representation aggregation?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-GraphConv Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.21384v2_figure_2.png",
        "paperid": "2407.21384v2",
        "paper_path": "./papers/2407.21384v2.json",
        "figure_id": "2407.21384v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of our method. The gray circles with different depths belong to different sentences, and the color depth of the square is the basis to distinguish the attention weight score.",
        "qtype": "Experimental_Results",
        "response": "Let me first align what the red‐boxed module is.  In Figure 2 the red rectangle surrounds the “Multi-GraphConv Layer” inside the GEGA module.  Textually (Section 4.2.2) this is where, for each of N_heads, you take the (soft) dependency‐based adjacency matrix A^h, apply a linear transform to your current entity features, perform a graph convolution to pull in neighbors’ features, and then fuse all of the heads (and, via dense connections, all of the preceding layers) into a single updated embedding for each entity.\n\nNow:  \n1. Because there are multiple heads, each head learns its own adjacency weighting (A^1, A^2, …, A^H) and its own projection W^(h,l).  Each head therefore “views” the entity graph through a slightly different lens of edge strengths.  \n2. Within each head you perform the usual GCN update, so a node’s new feature is a weighted sum of its neighbors’ (and its own) features under that head’s adjacency.  \n3. By concatenating (or summing) the outputs of all H heads—then also densely connecting across layers—you aggregate H different “messages” from each neighbor set.  \n\nIn plain English, the multi-head convolution lets the model simultaneously collect and blend multiple, complementary relational views of the same entity graph.  Rather than aggregating neighbors once under a single edge–weight pattern, you do it H times under H learned patterns and fuse the results.  This multiplicity of graph‐convolution heads enriches each entity’s representation by capturing diverse inter‐entity interactions, yielding a more robust, richly aggregated embedding for downstream relation extraction."
    },
    {
        "question": "What limitations arise from dense connectivity in the Multi-GraphConv Layer regarding spurious attention distributions?",
        "relevant_section_ids": [
            "8"
        ],
        "relevant_context": [
            "Firstly, when utilizing Multi-GraphConv Layers to induce multiple fully connected attention distribution matrices, there is a possibility of generating one matrix that differs significantly from others in terms of weight distribution. This could lead to significant deviations in prediction results."
        ],
        "final_answer": "Because the Multi-GraphConv Layer densely connects all nodes to produce multiple attention distribution matrices, it can sometimes generate an individual matrix whose weight distribution is wildly different (i.e., spurious) from the others, leading to substantial deviations in the model’s predictions.",
        "relevant_elements": [
            "Multi-GraphConv Layer"
        ],
        "id": 991,
        "masked_question": "What limitations arise from dense connectivity in the [mask1] regarding spurious attention distributions?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-GraphConv Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.21384v2_figure_2.png",
        "paperid": "2407.21384v2",
        "paper_path": "./papers/2407.21384v2.json",
        "figure_id": "2407.21384v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of our method. The gray circles with different depths belong to different sentences, and the color depth of the square is the basis to distinguish the attention weight score.",
        "qtype": "Others",
        "response": "Step 1: Image–text alignment  \nIf you look at the red‐boxed region in Figure 2, it is exactly the “Multi‐GraphConv Layer” where the model builds multiple (fully)-connected attention matrices (one per head) over all entity nodes.  \n\nStep 2: Chain-of-thought  \n1. In the Multi-GraphConv Layer every entity pair is connected (i.e. “dense connectivity”), so each attention head learns a full N×N weight matrix over entities.  \n2. The paper points out that, with so many free weights, one head can easily learn an attention pattern that is wildly different from the others.  \n3. When one attention matrix becomes an outlier, it can inject “spurious” (i.e. misleading or noisy) scores into the downstream encoder.  \n4. Those spurious attention weights then skew the final relation predictions and can cause large swings in performance.  \n\nAnswer  \nBecause the Multi-GraphConv Layer is fully dense, it can produce one or more attention heads whose weight‐distributions are outliers—i.e. spurious attention maps that deviate greatly from the rest—leading to unstable or incorrect relation predictions."
    },
    {
        "question": "What ethical implications emerge from using retrieved evidence in the Collaborative Prediction Layer for relation extraction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Collaborative Prediction Layer"
        ],
        "id": 992,
        "masked_question": "What ethical implications emerge from using retrieved evidence in the [mask1] for relation extraction?",
        "masked_number": 1,
        "masked_elements": [
            "Collaborative Prediction Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.21384v2_figure_2.png",
        "paperid": "2407.21384v2",
        "paper_path": "./papers/2407.21384v2.json",
        "figure_id": "2407.21384v2_figure_2.png",
        "caption": "Figure 2: The overall architecture of our method. The gray circles with different depths belong to different sentences, and the color depth of the square is the basis to distinguish the attention weight score.",
        "qtype": "Others",
        "response": "Mask1 corresponds to the “Co-prediction layer” – the red-boxed module in Figure 2 that fuses the Relation-Extraction (RE) and Evidence-Retrieval (ER) sub-networks.\n\nChain of thought:\n1. In GEGA’s architecture, the Co-prediction layer takes two inputs: a distribution over candidate relations (from the RE head) and a distribution over evidence sentences (from the ER head), then combines them (⊕) to form the final relation score.  \n2. By routing the model’s attention through retrieved evidence, any errors or biases in the retrieval stage will be carried directly into the relation prediction.  \n3. If the evidence sentences come from sources that are incomplete, biased, or contain sensitive personal data, the RE head can end up 1) propagating factual errors or unfair stereotypes, 2) infringing on privacy by exposing private information as “evidence,” or 3) making spurious inferences based on misleading context.  \n4. Moreover, because the Co-prediction layer tightly couples evidence and prediction, it becomes harder to audit which piece of text actually drove a decision, raising transparency and accountability issues.  \n5. Finally, reliance on automatically retrieved evidence risks over-trusting the model’s outputs (“automation bias”), potentially leading users to accept incorrect or ethically problematic relations without human verification.\n\nAnswer:\nBy plugging retrieved sentences directly into the Co-prediction layer, we risk propagating biases and errors from poorly sourced or sensitive evidence into our relation predictions, jeopardizing fairness, privacy, transparency, and accountability."
    },
    {
        "question": "What limitations might local prompts face when regional differences are minimal across classes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "local prompts"
        ],
        "id": 993,
        "masked_question": "What limitations might [mask1] face when regional differences are minimal across classes?",
        "masked_number": 1,
        "masked_elements": [
            "local prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.04796v1_figure_1.png",
        "paperid": "2409.04796v1",
        "paper_path": "./papers/2409.04796v1.json",
        "figure_id": "2409.04796v1_figure_1.png",
        "caption": "Figure 1: Comparison of prompt learning for OOD detection task. Prompts with global optimization may fail in challenging OOD samples as they are overall similar to ID samples and only have subtle regional differences. For example, cat and tiger are generally similar (blue boxes) and only differ in forehead (red box). Our approach with local outlier knowledge tackles the issue to some extent.",
        "qtype": "Others",
        "response": "Chain-of-thought  \n1. In our framework (Fig. 1, right), the red box highlights the “local prompts” – i.e. the small, per-region textual embeddings we learn to pick up subtle outlier cues.  \n2. Local prompts derive their power from detecting fine-grained differences in specific image patches (e.g. the tiger’s forehead versus a cat’s forehead).  \n3. If two classes have almost identical local appearance everywhere – that is, there simply aren’t any meaningful regional differences to exploit – then the very signal that local prompts are designed to pick up disappears.  \n4. In that regime:  \n   – the local prompt embeddings receive no useful gradient signal to specialize (all regions look the same across classes),  \n   – they may collapse to redundant or noisy defaults,  \n   – and the overall OOD detector loses the extra boost provided by region-aware scoring.  \n5. Thus, when regional differences are minimal, local prompts have little discriminative information to learn from and their benefit over a purely global prompt goes away.  \n\nAnswer  \nLocal prompts depend on finding distinctive regional cues. If classes share virtually identical local appearance, there’s no fine-grained signal for them to latch onto, so they cannot specialize or improve OOD separation and may collapse to noisy or redundant embeddings."
    },
    {
        "question": "What potential conflicts could arise when optimizing global prompts and local prompts simultaneously?",
        "relevant_section_ids": [
            "1",
            "2.3"
        ],
        "relevant_context": [
            "Others use the same prompts to match both global and local image features [17, 18], so the gap between them may lead to inaccurate local outlier identification.",
            "However, all existing approaches take same prompts for all features. By contrast, our method directly enhances OOD detection with ID-related areas and refines local prompts to leverage local outlier knowledge."
        ],
        "final_answer": "If one tries to optimize a single set of prompts to serve both global (whole‐image) and local (region‐level) representations at once, the two goals can compete. Global prompts are tuned to capture overall semantics, while local prompts must focus on fine‐grained regional cues. Using the same prompts for both can therefore create a mismatch—a ‘gap’—that interferes with the model’s ability to correctly identify subtle local outliers.",
        "relevant_elements": [
            "global prompts",
            "local prompts"
        ],
        "id": 994,
        "masked_question": "What potential conflicts could arise when optimizing [mask1] and local prompts simultaneously?",
        "masked_number": 1,
        "masked_elements": [
            "global prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.04796v1_figure_1.png",
        "paperid": "2409.04796v1",
        "paper_path": "./papers/2409.04796v1.json",
        "figure_id": "2409.04796v1_figure_1.png",
        "caption": "Figure 1: Comparison of prompt learning for OOD detection task. Prompts with global optimization may fail in challenging OOD samples as they are overall similar to ID samples and only have subtle regional differences. For example, cat and tiger are generally similar (blue boxes) and only differ in forehead (red box). Our approach with local outlier knowledge tackles the issue to some extent.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed items in the right‐hand “Optimization Space” are the global prompts (i.e. the hand‐crafted or learned tokens that are meant to steer the model’s _overall_ understanding of an image).  Our method instead freezes those and only learns local prompts, but the question asks:\n\n  “What potential conflicts could arise when optimizing global prompts _and_ local prompts simultaneously?”\n\nHere is a quick chain of thought:\n\n1.  **Divergent Objectives**  \n    – Global prompts are tuned to separate in‐distribution classes at a coarse, holistic level (e.g. “a photo of a {class}”).  \n    – Local prompts are trying to magnify tiny regional differences that signal out‐of‐distribution (e.g. that little patch on the cat’s forehead).  \n    – If you update them in one monolithic loop, the gradient coming from the global objective (maximize overall class likelihood) can directly conflict with the gradient coming from the local objective (maximize regional discriminability).\n\n2.  **Gradient Interference**  \n    – In practice those two losses live in the same parameter space and will compete:  \n       • A global‐scale update might “wash out” the fine‐tuned adjustments that the local prompts have just made.  \n       • Likewise, emphasizing local anomalies too heavily can steer the shared backbone into a corner where it no longer represents global class identities cleanly.\n\n3.  **Capacity Trade‐off**  \n    – The model’s “prompt budget” is limited. If both prompt sets are changing, they eat up each other’s representational capacity, so neither can specialize fully.  \n    – You can end up with prompts that are mediocre at _both_ global classification and _local_ anomaly detection, instead of excellent at one.\n\n4.  **Stability and Convergence Issues**  \n    – Jointly optimizing two orthogonal signal streams often leads to unstable training: oscillations, very slow convergence, or getting stuck in poor local minima that satisfy neither objective well.\n\nFor precisely these reasons the paper advocates _freezing_ the global prompts (so they remain a stable anchor for class‐level separation) and _only_ fine‐tuning the local prompts (so they can learn to spot subtle regional OOD cues without being constantly overridden by global‐scale updates)."
    },
    {
        "question": "Why choose regional enhancement alongside global optimization for OOD detection?",
        "relevant_section_ids": [
            "1",
            "2.3"
        ],
        "relevant_context": [
            "The most challenging scene for OOD detection is that one hard OOD sample is similar to a known class on the whole and only has subtle differences locally, which naturally requires the detector to identify outliers through local outlier regions.",
            "Some methods merely focus on utilizing global features only, which ignores local features and inevitably brings about coarse description.",
            "Consequently, it is straightforward that enhancing regional information to empower the model with local outlier knowledge could be significant to OOD detection.",
            "By contrast, our method directly enhances OOD detection with ID-related areas and refines local prompts to leverage local outlier knowledge."
        ],
        "final_answer": "Because challenging OOD samples can look very similar to in-distribution images at a global level but differ in only small regions, relying solely on global prompts misses these subtle local differences. Regional enhancement explicitly captures and leverages local outlier cues, complementing global optimization to more accurately detect such hard OOD cases.",
        "relevant_elements": [
            "regional enhancement",
            "global optimization"
        ],
        "id": 995,
        "masked_question": "Why choose [mask1] alongside global optimization for OOD detection?",
        "masked_number": 1,
        "masked_elements": [
            "regional enhancement"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.04796v1_figure_1.png",
        "paperid": "2409.04796v1",
        "paper_path": "./papers/2409.04796v1.json",
        "figure_id": "2409.04796v1_figure_1.png",
        "caption": "Figure 1: Comparison of prompt learning for OOD detection task. Prompts with global optimization may fail in challenging OOD samples as they are overall similar to ID samples and only have subtle regional differences. For example, cat and tiger are generally similar (blue boxes) and only differ in forehead (red box). Our approach with local outlier knowledge tackles the issue to some extent.",
        "qtype": "Design_Rationale",
        "response": "Here “\\[mask1\\]” is the red‐boxed module in the right‐hand “Our method” panel—namely, the Regional Enhancement step.  We add Regional Enhancement alongside global prompt optimization because:  \n1.  A purely global prompt learns to separate ID from OOD at the image–level, but will often miss those OODs whose only clue is a tiny, localized discrepancy (e.g. the forehead stripe on a tiger vs. a cat).  \n2.  By explicitly tuning a set of *local* prompts in the Regional Enhancement block, we force the model to attend to and regularize those same small regions during fine‐tuning.  \n3.  This regional branch is orthogonal to any global‐prompt method, so it can sit “on top” of your favorite global optimization without interfering with it.  \n4.  In practice, combining the coarse global separation with this fine local discrimination greatly reduces false positives on hard OOD samples and boosts overall detection performance."
    },
    {
        "question": "What motivates freezing global prompts when fine-tuning local prompts?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Global prompts can be viewed as coarse guidance for negative augmentation standing for overall representation and are frozen in our framework.",
            "It is worth emphasizing that our approach is orthogonal to all existing global prompt optimization strategies, i.e., global prompts are built without tuning in our structure … Note that our main purpose is to decompose global and local prompts and showcase the effectiveness of local outlier enhancement for OOD detection."
        ],
        "final_answer": "Freezing the global prompts preserves their role as fixed, coarse guidance for both negative‐augmentation selection and OOD scoring. This decouples the global, overall representation (which is already effective) from the fine, local prompt tuning, enabling the model to focus exclusively on learning regional outlier knowledge without disturbing the reliable global semantics.",
        "relevant_elements": [
            "global prompts",
            "local prompts"
        ],
        "id": 996,
        "masked_question": "What motivates freezing [mask1] when fine-tuning local prompts?",
        "masked_number": 1,
        "masked_elements": [
            "global prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.04796v1_figure_1.png",
        "paperid": "2409.04796v1",
        "paper_path": "./papers/2409.04796v1.json",
        "figure_id": "2409.04796v1_figure_1.png",
        "caption": "Figure 1: Comparison of prompt learning for OOD detection task. Prompts with global optimization may fail in challenging OOD samples as they are overall similar to ID samples and only have subtle regional differences. For example, cat and tiger are generally similar (blue boxes) and only differ in forehead (red box). Our approach with local outlier knowledge tackles the issue to some extent.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed block in the right hand panel is simply the set of “global prompts.”  In our pipeline we never update those – we “freeze” them – while we fine-tune the local prompts.  The reason is twofold:  \n\n1.  Prior work (e.g. [16,17]) has shown that even hand‐crafted or pre‐trained global prompts already give very strong, stable global features for tasks like negative‐sample selection and overall OOD scoring.  \n2.  By holding the global prompts fixed, we decouple the coarse, image-level guidance (the globals) from our new, region-aware local prompt tuning.  This lets the model focus all of its learning capacity on discovering and exploiting subtle local differences (i.e. the red-boxed “local regions”) without disturbing a proven, high-quality global representation."
    },
    {
        "question": "What motivates combining self-supervision and multimodal supervision objectives?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "While both classes of representation learning approaches, i.e., SS and MM, have shown promising results, neither one fully leverages diverse sources of information present in IMU time series.",
            "L_SS ensures that φ remains invariant to noise, similar to those that are introduced by slight changes in sensor position or type (§II-A).",
            "L_MM pushes IMU representations towards aligned text and video representations, allowing φ to learn the rich semantic information present in other modalities (§II-B)."
        ],
        "final_answer": "The two objectives are combined because self-supervision provides invariance to sensor noise and perturbations, while multimodal supervision injects rich semantic signals from aligned video and text; individually they each only capture part of the information in IMU data, but together they leverage complementary strengths to learn more transferable representations.",
        "relevant_elements": [
            "Self-Supervision",
            "Multi-Modal Supervision"
        ],
        "id": 997,
        "masked_question": "What motivates combining [mask1] and multimodal supervision objectives?",
        "masked_number": 1,
        "masked_elements": [
            "Self-Supervision"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15127v1_figure_1.png",
        "paperid": "2411.15127v1",
        "paper_path": "./papers/2411.15127v1.json",
        "figure_id": "2411.15127v1_figure_1.png",
        "caption": "Figure 1: PRIMUS Overview. We use a multi-objective pretraining including three terms, ℒS⁢S,ℒM⁢M,subscriptℒ𝑆𝑆subscriptℒ𝑀𝑀\\mathcal{L}_{SS},\\mathcal{L}_{MM},caligraphic_L start_POSTSUBSCRIPT italic_S italic_S end_POSTSUBSCRIPT , caligraphic_L start_POSTSUBSCRIPT italic_M italic_M end_POSTSUBSCRIPT , and ℒN⁢Nsubscriptℒ𝑁𝑁\\mathcal{L}_{NN}caligraphic_L start_POSTSUBSCRIPT italic_N italic_N end_POSTSUBSCRIPT. Self-supervised losses encourage the IMU encoder to be augmentation invariant, while multimodal and nearest neighbor losses align the IMU data to co-occurring video and/or text data. We use open-source pretrained models developed by others for both text and video encoders.",
        "qtype": "Design_Rationale",
        "response": "Here, “[mask1]” is simply the self-supervision objective (ℒSS) shown in the red box.  The reason we train with both ℒSS and the multimodal objective ℒMM is that they supply complementary signals:\n\n  1. ℒSS (self-supervision) forces the IMU encoder to become invariant to the kinds of noise and perturbations you get in wearable sensors (e.g. small shifts in position, random scaling, time-reversal).  \n  2. ℒMM (multimodal supervision) simultaneously pulls those IMU embeddings toward the rich, high-level semantics that live in text and video encoders.  \n\nNeither by itself fully exploits all the information in the raw IMU streams.  By combining them we (a) learn a representation that is robust to sensor-specific noise and (b) ground that representation in the same semantic space as natural language and vision.  Empirically, this joint training produces embeddings that transfer much better to downstream IMU tasks than using either self-supervision or multimodal alignment alone."
    },
    {
        "question": "What motivates integrating a feature queue into nearest neighbor supervision?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The loss terms introduced so far, L_SS and L_MM, both derive supervision from within the same triplet segment.",
            "To increase the diversity of supervision and go beyond a single instance, we leverage nearest-neighbor supervision [23, 30] (shown in the rightmost block in orange in Fig. 1 and in detail in Fig. 3).",
            "We leverage the video representations for identifying the closest pairs because the video encoder is pretrained on a large dataset, and therefore produces stable representations. Also, videos capture much finer details about human activities compared to text descriptions."
        ],
        "final_answer": "Integrating a feature queue is motivated by the desire to diversify the supervision signal beyond each isolated triplet. By caching and querying nearest-neighbor embeddings—particularly stable, high-fidelity video features—the model can contrast an IMU example not just with its own augmented versions or its text/video pair, but also with semantically similar instances across the dataset.",
        "relevant_elements": [
            "Nearest Neighbor Supervision",
            "Feature Queue"
        ],
        "id": 998,
        "masked_question": "What motivates integrating a [mask1] into [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Feature Queue",
            "Nearest Neighbor Supervision"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15127v1_figure_1.png",
        "paperid": "2411.15127v1",
        "paper_path": "./papers/2411.15127v1.json",
        "figure_id": "2411.15127v1_figure_1.png",
        "caption": "Figure 1: PRIMUS Overview. We use a multi-objective pretraining including three terms, ℒS⁢S,ℒM⁢M,subscriptℒ𝑆𝑆subscriptℒ𝑀𝑀\\mathcal{L}_{SS},\\mathcal{L}_{MM},caligraphic_L start_POSTSUBSCRIPT italic_S italic_S end_POSTSUBSCRIPT , caligraphic_L start_POSTSUBSCRIPT italic_M italic_M end_POSTSUBSCRIPT , and ℒN⁢Nsubscriptℒ𝑁𝑁\\mathcal{L}_{NN}caligraphic_L start_POSTSUBSCRIPT italic_N italic_N end_POSTSUBSCRIPT. Self-supervised losses encourage the IMU encoder to be augmentation invariant, while multimodal and nearest neighbor losses align the IMU data to co-occurring video and/or text data. We use open-source pretrained models developed by others for both text and video encoders.",
        "qtype": "Design_Rationale",
        "response": "Here, the red‐boxed module (“feature‐queue”) is literally the memory bank of past IMU/video/text representations, and the blue‐boxed one (“NN‐emb.”) is the operation that pulls the single nearest‐neighbour entry out of that bank to form a new positive pair.  \n\nWhy do we hook the queue into the NN‐embedding step?  Because by caching a large set of recent embeddings (via the feature‐queue) and then using the NN‐emb. operator to pick the closest one, we can go beyond the one‐to‐one, within‐triplet positives.  In other words, the feature‐queue lets us mine “harder,” more diverse positives from across the dataset—using stable, rich video features—to give the IMU encoder richer, more adaptive contrastive signals than self‐ or pairwise multimodal losses alone."
    },
    {
        "question": "How does nearest neighbor supervision retrieve NN Emb. from the Feature Queue to refine IMU encoder outputs?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "During training, we maintain a feature queue Q, where Z^I, Z^V, and Z^T are cached representations of IMU, video, and text produced from their respective encoders.",
            "For every given instance x_i in a batch B, we define k*(i) = argmax_j Q^V_j ⋅ z^I_i which identifies the index k*(i) in Q corresponding to the video embedding that is the most similar to z^I_i.",
            "We then push z^I_i close to z^I_{k*(i)} by L_NN, which consists of a unimodal and multimodal loss similar to L_SS and L_MM."
        ],
        "final_answer": "Nearest neighbor supervision maintains a queue of past IMU, video, and text embeddings. For each new IMU embedding z^I_i, it computes similarities with all video embeddings in the queue and selects the index k*(i) of the most similar video embedding. It then retrieves the corresponding IMU embedding z^I_{k*(i)} from the queue and uses it as the positive example in an additional contrastive loss (L_NN) to refine the IMU encoder’s output.",
        "relevant_elements": [
            "Nearest Neighbor Supervision",
            "Feature Queue"
        ],
        "id": 1000,
        "masked_question": "How does nearest neighbor supervision retrieve NN Emb. from the [mask1] to refine IMU encoder outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Queue"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15127v1_figure_1.png",
        "paperid": "2411.15127v1",
        "paper_path": "./papers/2411.15127v1.json",
        "figure_id": "2411.15127v1_figure_1.png",
        "caption": "Figure 1: PRIMUS Overview. We use a multi-objective pretraining including three terms, ℒS⁢S,ℒM⁢M,subscriptℒ𝑆𝑆subscriptℒ𝑀𝑀\\mathcal{L}_{SS},\\mathcal{L}_{MM},caligraphic_L start_POSTSUBSCRIPT italic_S italic_S end_POSTSUBSCRIPT , caligraphic_L start_POSTSUBSCRIPT italic_M italic_M end_POSTSUBSCRIPT , and ℒN⁢Nsubscriptℒ𝑁𝑁\\mathcal{L}_{NN}caligraphic_L start_POSTSUBSCRIPT italic_N italic_N end_POSTSUBSCRIPT. Self-supervised losses encourage the IMU encoder to be augmentation invariant, while multimodal and nearest neighbor losses align the IMU data to co-occurring video and/or text data. We use open-source pretrained models developed by others for both text and video encoders.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the nearest‐neighbor block (the orange “NN” module) works, step by step:\n\n1.  You keep a rolling “feature queue” (the red‐boxed buffer in the diagram) that at any time holds three queues of past embeddings:  \n    – Q_IMU : IMU embeddings  \n    – Q_vid : video embeddings  \n    – Q_txt : text embeddings  \n\n2.  At each training step you take your current IMU encoder’s output, h_i (for sample i), and use it to search Q_vid:  \n    • You compute cosine‐similarities (or dot‐products) between h_i and every entry in Q_vid.  \n    • You pick the index k that maximizes similarity:  \n       k = arg max_j  ⟨h_i, Q_vid[j]⟩  \n\n3.  Having found that most‐similar video embedding in Q_vid at position k, you then pull out from Q_IMU the IMU embedding stored at that very same index k.  That retrieved vector is your NN Emb.  \n\n4.  Finally, you treat that NN Emb. as a “positive” in a contrastive term (together with h_i), pushing the current IMU output closer to its nearest neighbor in past data.  This adds the extra supervision signal ℒ_NN that helps refine the encoder."
    },
    {
        "question": "How does the Similarity-First, Diversity-Aware strategy sample and deduplicate open-source instructions?",
        "relevant_section_ids": [
            "3.4.2",
            "3.4.3"
        ],
        "relevant_context": [
            "We calculate the cross-dataset similarity between $\\mathcal{S}$ and $\\mathcal{C}$ to select a subset $\\mathcal{C}'$ that resembles $\\mathcal{S}$ the most.",
            "Then, we pinpoint top $m$ samples in $\\mathcal{C}$ that share the most similarity with $\\mathcal{S}$ by maximizing along the rows of $D$: $\\mathcal{C}'=\\arg\\max_j D_{ij}$.",
            "We remove duplicates in $\\mathcal{C}'$ to improve its overall diversity for the selected dataset.",
            "We follow the SemDeDup to perform semantic deduplication by thresholding with $\\theta$. If the similarity between any two instructions exceeds $\\theta$, we discard the one whose similarity with $\\mathcal{S}$ is lower."
        ],
        "final_answer": "First, all open-source instructions are encoded into embeddings and a cosine-distance matrix $D$ is computed against the $k$-shot examples. The top $m$ instructions with the highest similarity scores (i.e. those maximizing each row of $D$) form the candidate set. Second, to avoid semantic redundancy, a pairwise similarity matrix is built over this candidate set, and using a threshold $\\theta$ (as in SemDeDup), whenever two instructions exceed $\\theta$ in similarity, the one less similar to the $k$-shot examples is discarded. This yields a final, diverse yet task-relevant augmentation set.",
        "relevant_elements": [
            "Similarity-First",
            "Diversity-Aware"
        ],
        "id": 1002,
        "masked_question": "How does the [mask1], [mask2] strategy sample and deduplicate open-source instructions?",
        "masked_number": 2,
        "masked_elements": [
            "Similarity-First",
            "Diversity-Aware"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.15915v2_figure_1.png",
        "paperid": "2408.15915v2",
        "paper_path": "./papers/2408.15915v2.json",
        "figure_id": "2408.15915v2_figure_1.png",
        "caption": "Figure 1: \nGiven few annotated data from any task of interest (K𝐾Kitalic_K-shot), we aim to advance LLMs in task expertise by leveraging open-source models and datasets.\nWe propose an efficient and scalable pipeline to fully exploit the steering role of K𝐾Kitalic_K-shot throughout model and data selection.\nHighly promising experts are first selected from the model bank by comprehensive consideration of their perplexity and performance on the K𝐾Kitalic_K-shot and intra-group diversity.\nThese experts are initialized as one MoE system.\nSubsequently,\nwe perform data augmentation by selecting diverse open instructions that resemble K𝐾Kitalic_K-shot the most.\nFinally,\nwe fine-tune the MoE system with both K𝐾Kitalic_K-shot and the augmented data,\nwhich not only improves token-wise cooperation between experts but also integrates broad knowledge into the system.\nThe ultimate task expert benefits from the complementary skills and knowledge of constituting experts.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the “Similarity‐First, Diversity‐Aware” two‐stage selection works in §3.4:\n\n1. Similarity‐First  \n   - We encode each K-shot example and each candidate open‐source instruction into a fixed-length vector via a pre-trained encoder (BGE).  \n   - We then compute the cosine‐distance (or equivalently cosine‐similarity) between every open-source instruction embedding and every K-shot embedding, building a cross-dataset similarity matrix.  \n   - For each K-shot example, we pick the top-𝑁 most similar open-source instructions.  This yields a much smaller candidate pool 𝐶 that is highly relevant to our task of interest.  \n\n2. Diversity‐Aware  \n   - Within this reduced pool 𝐶, we compute pairwise cosine similarities among all instruction embeddings to form an intra-dataset similarity matrix.  \n   - We apply a threshold τ: whenever two candidates exceed τ in similarity, we drop the one that has the lower similarity back to the original K-shot set.  \n   - This “semantic deduplication” step (inspired by SemDeDup) ensures that the final augmented instruction set 𝐴 is both close to the task (high relevance) and free of near-duplicate entries (high diversity).  \n\nBy first pulling in only those open‐source instructions most like our few K-shot exemplars, and then pruning away any that are overly redundant, this pipeline gives us a small, highly relevant—but also diverse—auxiliary set for fine-tuning our MoE."
    },
    {
        "question": "How does integrating perplexity-performance-diversity ranking inform expert selection relative to ensemble pruning approaches?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Perplexity",
            "Performance",
            "Diversity"
        ],
        "id": 1003,
        "masked_question": "How does integrating [mask1]-performance-diversity ranking inform expert selection relative to ensemble pruning approaches?",
        "masked_number": 1,
        "masked_elements": [
            "Perplexity"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.15915v2_figure_1.png",
        "paperid": "2408.15915v2",
        "paper_path": "./papers/2408.15915v2.json",
        "figure_id": "2408.15915v2_figure_1.png",
        "caption": "Figure 1: \nGiven few annotated data from any task of interest (K𝐾Kitalic_K-shot), we aim to advance LLMs in task expertise by leveraging open-source models and datasets.\nWe propose an efficient and scalable pipeline to fully exploit the steering role of K𝐾Kitalic_K-shot throughout model and data selection.\nHighly promising experts are first selected from the model bank by comprehensive consideration of their perplexity and performance on the K𝐾Kitalic_K-shot and intra-group diversity.\nThese experts are initialized as one MoE system.\nSubsequently,\nwe perform data augmentation by selecting diverse open instructions that resemble K𝐾Kitalic_K-shot the most.\nFinally,\nwe fine-tune the MoE system with both K𝐾Kitalic_K-shot and the augmented data,\nwhich not only improves token-wise cooperation between experts but also integrates broad knowledge into the system.\nThe ultimate task expert benefits from the complementary skills and knowledge of constituting experts.",
        "qtype": "Literature_Background",
        "response": "In our pipeline (see the red‐boxed module in Fig. 1), “mask₁–performance–diversity” simply expands to “perplexity–performance–diversity.”  In other words, when we pick LoRA experts to build our Mixture‐of‐Experts we don’t just look at “who gets the most few‐shot examples right,” nor do we blindly merge a giant pool and then prune it down.  Instead we:\n\n  1. Score every candidate on reasoning “confidence” (we run it in an auto-regressive way over our K-shot prompts and compute its perplexity, lower is better).  \n  2. Score every candidate on end-to-end few-shot accuracy (classic exact‐match performance).  \n  3. Measure pairwise model heterogeneity (parameter-space diversity) so that we avoid picking a bunch of nearly identical experts.\n\nWe then rank models jointly by these three criteria, eliminate the weak/confidently wrong ones first, and only from the remaining pool do we search for the highest‐diversity subset.  \n\nBy contrast, typical ensemble‐pruning or post-hoc weight‐pruning schemes (e.g. magnitude‐based pruning, or methods that only consider validation‐set accuracy) tend to\n\n  • ignore model “confidence” (they never look at per‐token perplexity),  \n  • never explicitly enforce diversity (they may end up keeping many clones of the same skill), and  \n  • only remove experts after a full mixture has already been trained (so you’ve already paid the full training cost).\n\nOur perplexity–performance–diversity ranking thus “front-loads” both uncertainty and complementariness into the selection step, yielding a small but high-capacity expert set that is more efficient to fine-tune and less prone to redundancy than a naïvely pruned ensemble."
    },
    {
        "question": "How does similarity-first diversity-aware data selection affect token-wise cooperation in MoE fine-tuning methodologies?",
        "relevant_section_ids": [
            "3.4",
            "3.4.3"
        ],
        "relevant_context": [
            "It has three advantages including: 1) high cost-efficiency of utilizing the massive and free open-source datasets, 2) prevention of overfitting by introducing diverse and beneficial instructions, and 3) improvement of token-wise collaboration between experts via acquiring novel knowledge.",
            "A greater level of diversity not only improves the token-wise cooperation between experts on broader domains and topics but also reduces the overfitting of the MoE system on -shot datapoints."
        ],
        "final_answer": "By first retrieving the most task-relevant examples (similarity-first) and then removing semantic duplicates to maximize diversity, the selected data introduce novel and varied instruction contexts. This broader, more diverse training set enables the MoE router to assign individual tokens to the most appropriate experts more effectively, thereby improving token-wise cooperation among experts during fine-tuning.",
        "relevant_elements": [
            "Similarity-First Diversity-Aware Data Selection",
            "Mixture-of-Expert Fine-Tuning"
        ],
        "id": 1004,
        "masked_question": "How does [mask1] affect token-wise cooperation in MoE fine-tuning methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "Similarity-First Diversity-Aware Data Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.15915v2_figure_1.png",
        "paperid": "2408.15915v2",
        "paper_path": "./papers/2408.15915v2.json",
        "figure_id": "2408.15915v2_figure_1.png",
        "caption": "Figure 1: \nGiven few annotated data from any task of interest (K𝐾Kitalic_K-shot), we aim to advance LLMs in task expertise by leveraging open-source models and datasets.\nWe propose an efficient and scalable pipeline to fully exploit the steering role of K𝐾Kitalic_K-shot throughout model and data selection.\nHighly promising experts are first selected from the model bank by comprehensive consideration of their perplexity and performance on the K𝐾Kitalic_K-shot and intra-group diversity.\nThese experts are initialized as one MoE system.\nSubsequently,\nwe perform data augmentation by selecting diverse open instructions that resemble K𝐾Kitalic_K-shot the most.\nFinally,\nwe fine-tune the MoE system with both K𝐾Kitalic_K-shot and the augmented data,\nwhich not only improves token-wise cooperation between experts but also integrates broad knowledge into the system.\nThe ultimate task expert benefits from the complementary skills and knowledge of constituting experts.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought:\n\n1.  In the diagram the red box is labeled “Similarity-First / Diversity-Aware” – that is the [mask1] term.  \n2.  This refers to the procedure in §3.4.2–3.4.3 where, when augmenting a few-shot set with open-source data, we (a) first pick examples most similar to the K-shot inputs, then (b) deduplicate to keep the set semantically diverse.  \n3.  The goal of that two-stage selection is not merely to avoid overfitting but to expose the MoE’s router and its experts to a broad yet relevant variety of token patterns.  \n4.  By feeding in new but task-aligned tokens (thanks to “similarity-first”) that nonetheless span different semantic niches (thanks to “diversity-aware”), the router is forced to activate different experts on different token types.  \n5.  In effect, this mixed, curated data makes the router learn finer-grained, token-wise allocations—so that multiple experts truly cooperate at the sub-token level rather than one expert dominating.  \n\nAnswer:\n\n[mask1] (“Similarity-First / Diversity-Aware” data selection) improves token-wise cooperation by constructing an augmented dataset that is both closely aligned to the target few-shot examples and richly varied.  This balanced mix of relevance and diversity forces the MoE router to dispatch different tokens to different experts, thereby strengthening fine-grained, token-level collaboration among the experts."
    },
    {
        "question": "How does DDIM Inversion support Mask Propagation for reliable frame mask extension?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Inspired from DIFT [32], the three upsampling blocks in the UNet are replete with rich semantic information. Therefore, we can retain the feature map F from upsampling block-2 at a given step t during DDIM inversion in the source video, and use them to calculate the pixel correspondences between frames.",
            "Specifically, to pinpoint the pixel in the (i+1)-th frame that most closely resembles a given point p in the i-th frame, we calculate the cosine similarity between the corresponding point in the feature map F_i and all points in the feature map F_{i+1}.",
            "Given an initial object mask for the first frame from the user, we use k-NN [33] to propagate this mask to subsequent frames. For each point in the (i+1)-th frame, we find the k most similar points in the i-th frame using point matching. If the majority of these k points are in the foreground, the corresponding point in the (i+1)-th frame is also classified as foreground; otherwise, it is background."
        ],
        "final_answer": "During DDIM Inversion the model produces intermediate feature maps (from block-2 of the UNet) that carry rich semantic information. By retaining these feature maps for each frame, the method computes cosine similarities between corresponding features in adjacent frames to establish point correspondences. A k-NN vote on these matched points then propagates the initial frame’s mask reliably to subsequent frames.",
        "relevant_elements": [
            "DDIM Inversion",
            "Mask Propagation"
        ],
        "id": 1005,
        "masked_question": "How does [mask1] support Mask Propagation for reliable frame mask extension?",
        "masked_number": 1,
        "masked_elements": [
            "DDIM Inversion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.20084v3_figure_2.png",
        "paperid": "2410.20084v3",
        "paper_path": "./papers/2410.20084v3.json",
        "figure_id": "2410.20084v3_figure_2.png",
        "caption": "Figure 2: Overall framework. It is structured around three main components:\n(1) Point-Matching Mask Propagation, (2) AdaIN-Guided Video Style Transfer(Attention-shift and latent-shift) and (3) Sliding-Window Consistent Smoothing.",
        "qtype": "Literature_Background",
        "response": "The mystery red‐boxed module is nothing exotic – it is simply the DDIM inversion pass on your input video frames.  What that means in practice is:\n\n  1.  As you run each video frame backwards through the DDIM denoiser (the green UNet), you “peek out” an intermediate feature map (in particular the output of the second up-sampling block) that is very rich in semantic information.  \n  2.  At propagation time you take that feature map from frame i and frame j and compute a cosine similarity between every pixel in frame i and every pixel in frame j.  \n  3.  You then do a k-NN majority-vote on those similarities – if most of the k nearest neighbors of a pixel were marked foreground in the seed mask (frame 1, or one of a small set of anchor frames), you paint that pixel foreground in the new frame, otherwise background.  \n  4.  By keeping a handful of “anchor” frames (including the very first frame) in that voting loop and randomly down‐sampling them, you stop errors from drifting too far and keep the propagation both accurate and efficient.  \n\nIn short, the DDIM inversion gives you stable, high-level feature embeddings for each frame; these embeddings are the basis for your point-matching and k-NN voting, which is exactly how you extend a single user-drawn mask reliably to every frame."
    },
    {
        "question": "How does point-matching mask propagation use feature-map correspondences and K-NN voting to propagate masks across frames?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Inspired from DIFT [32], the three upsampling blocks in the UNet are replete with rich semantic information. Therefore, we can retain the feature map Fi from upsampling block-2 at a given step t during DDIM inversion in the source video, and use them to calculate the pixel correspondences between frames.",
            "Given an initial object mask for the first frame from the user, we use K-NN [33] to propagate this mask to subsequent frames. For each point in the j-th frame, we find the K most similar points in the i-th frame using point matching. If the majority of these K points are in the foreground, the corresponding point in the j-th frame is also classified as foreground; otherwise, it is background."
        ],
        "final_answer": "During inversion, the model saves intermediate feature maps (from upsampling block-2) for each frame. To transfer the mask from a source frame to a target frame, it computes cosine similarities between every pixel’s feature vector in the target and all feature vectors in the source, thereby establishing point correspondences. Then, for each target pixel, it picks its K most similar source pixels and performs a majority vote: if more than half of those K neighbors lie inside the source’s mask (foreground), the target pixel is labeled foreground; otherwise it is labeled background.",
        "relevant_elements": [
            "Point-Matching Mask Propagation",
            "Feature Maps",
            "K-NN"
        ],
        "id": 1007,
        "masked_question": "How does [mask1] use [mask2] and K-NN voting to propagate masks across frames?",
        "masked_number": 2,
        "masked_elements": [
            "Point-Matching Mask Propagation",
            "Feature Maps"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.20084v3_figure_2.png",
        "paperid": "2410.20084v3",
        "paper_path": "./papers/2410.20084v3.json",
        "figure_id": "2410.20084v3_figure_2.png",
        "caption": "Figure 2: Overall framework. It is structured around three main components:\n(1) Point-Matching Mask Propagation, (2) AdaIN-Guided Video Style Transfer(Attention-shift and latent-shift) and (3) Sliding-Window Consistent Smoothing.",
        "qtype": "Experimental_Results",
        "response": "The little red box in the figure is nothing more than the “Point–Matching Mask Propagation’’ module; the little blue box inside it is the pair of UNet feature–maps (at time�i and time�i+1) that we are going to match.  Here is exactly what happens, step by step:\n\n1. During DDIM‐inversion on the source video we tap out the intermediate feature maps from the second up‐sampling block of the U-Net.  Call those two 2D feature–maps Fᵢ (the blue box on the left) and Fᵢ₊₁ (the blue box on the right).  \n2. We take the user’s binary mask on frame�1 (the only frame they have drawn) as our “ground truth’’ foreground/background labels on F₁.  \n3. Now, to get a mask for frame�2,�3,�… we do the following for each pixel p in frame�i+1:  \n   a. We pull out its feature vector f_p from Fᵢ₊₁.  \n   b. We compute cosine similarity between f_p and every feature vector in Fᵢ (or in a small set of *anchor* frames plus Fᵢ if we are using the anchor‐frame extension).  \n   c. We select the top-K most similar feature vectors in Fᵢ.  \n   d. We look up the known foreground/background labels of those K neighbors (from our already-propagated mask on frame�i).  \n   e. We do a majority vote: if more than ⌊K/2⌋ of those neighbors were labeled “foreground,” we label p “foreground”; otherwise we label it “background.”  \n4. That new label for p becomes part of our estimated mask on frame�i+1.  We repeat the process in a sliding fashion (possibly re-referencing the first frame plus the last T–1 frames to avoid drift), and whenever we fetch labels we sub-sample the foreground and background points to keep the cost down.\n\nIn short, the red-boxed module takes the blue-boxed feature maps, does a K-NN lookup of each new pixel’s feature against the old frame’s features, and uses a simple majority vote over those K nearest neighbors’ known labels to decide whether the new pixel is foreground or background."
    },
    {
        "question": "How might reliance on KL divergence hinder Augmented Alignment robustness under class imbalance?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Augmented Alignment",
            "KL divergence"
        ],
        "id": 1011,
        "masked_question": "How might reliance on [mask1] hinder Augmented Alignment robustness under class imbalance?",
        "masked_number": 1,
        "masked_elements": [
            "KL divergence"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.14336v1_figure_1.png",
        "paperid": "2409.14336v1",
        "paper_path": "./papers/2409.14336v1.json",
        "figure_id": "2409.14336v1_figure_1.png",
        "caption": "Figure 1: (a) Traditional skeleton-based zero-shot action recognition methods use a single alignment model to align the two modalities.\n(b) Our DVTA approach employs a dual alignment strategy, where Direct Alignment is used for initial alignment to strengthen the association between modalities, and distribution alignment is further applied to enhance generalization to unseen classes. KL divergence is employed to generate more positive examples, facilitating the joint optimization of the two spaces.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed module in Figure 1(b) is the KL‐divergence loss—that is our [mask1].  In our Augmented Alignment (AA) we train a small deep network to predict a “soft” similarity score between every skeleton–text pair, and we drive that network with a KL‐divergence loss against the “ground‐truth” similarity matrix.  \n\nHere is why leaning too heavily on KL‐divergence can make AA brittle under class imbalance:\n\n1.  In KL‐based contrastive losses, every class in a batch contributes some “positive” mass in the target distribution.  When a class is well represented in the batch this target is estimated stably, but if a class only has one or two samples (i.e. is under-sampled), its positive mass is very noisy.  \n2.  KL divergence is asymmetric and heavily penalizes “missing” probability mass in low-density regions.  So if a rare class’s predicted similarities fail to match the tiny “spike” of the true distribution, the KL loss shoots way up and delivers very large, unstable gradients for that class.  \n3.  Meanwhile, head (majority) classes, by virtue of having many more samples, dominate the overall divergence and gradient‐flow.  The model ends up specializing on those abundant classes and overfitting their similarity structure, at the expense of under-represented ones.  \n\nIn short, under severe class imbalance the KL term can (a) become extremely noisy for minority classes and (b) shift the model’s capacity toward the majority classes.  That double-whammy undermines the robustness of the Augmented Alignment module when some classes are rare."
    },
    {
        "question": "What limitations might Octree-based geometry compression introduce in preserving fine-grained spatial details?",
        "relevant_section_ids": [
            "2.5"
        ],
        "relevant_context": [
            "After geometry compression, multiple points within a voxel in Octree are merged into a single point. To maintain color consistency, the reconstructed points are recolored by applying the attributes of the nearest Gaussian from the original 3D GS."
        ],
        "final_answer": "By merging all Gaussians within the same voxel into a single representative point, the Octree discretization loses any spatial variations among those points and therefore cannot preserve fine-grained structure inside each voxel.",
        "relevant_elements": [
            "Octree"
        ],
        "id": 1013,
        "masked_question": "What limitations might [mask1]-based geometry compression introduce in preserving fine-grained spatial details?",
        "masked_number": 1,
        "masked_elements": [
            "Octree"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06976v1_figure_1.png",
        "paperid": "2411.06976v1",
        "paper_path": "./papers/2411.06976v1.json",
        "figure_id": "2411.06976v1_figure_1.png",
        "caption": "Figure 1: Framework of HGSC. Q𝑄Qitalic_Q and Q−1superscript𝑄1Q^{-1}italic_Q start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT denote the processes of quantization and dequantization, respectively.",
        "qtype": "Others",
        "response": "First, note that the little “blue‐and‐grey cube” in the red box is nothing more exotic than an octree index over the 3D point centers—the text in II-D even says “we compress the 3D positions with Octree.”   An octree is just a recursive subdivision of space into voxels, and each point gets snapped to the center (or occupancy bit) of the finest leaf voxel.  \n\nChain of thought:  \n1. Mask₁ is pointing at the octree block in the Geometry Compression stage.  \n2. By construction, an octree imposes a fixed maximum depth (i.e. a fixed smallest voxel size).  \n3. Any geometric feature or variation that is finer than that voxel size simply cannot be represented – it is quantized away to the nearest voxel center or merged with its neighbors.  \n4. Moreover, all points within one voxel are collapsed into a single occupancy bit (and later merged into one Gaussian), which further smooths out any intra‐voxel detail.  \n5. In practice this leads to blockiness or stair-step artifacts along sharp edges, loss of small bumps or cavities, and an overall oversmoothing of finely varying structure whenever you stop subdividing the octree (either to save bits or to bound decoding cost).  \n\nSo, in short, octree-based compression trades away sub‐voxel detail for compactness. Fine‐scale geometry smaller than the leaf‐voxel size is irretrievably lost, and merging multiple points per voxel further erases subtle shape cues."
    },
    {
        "question": "What alternative partitioning strategies could enhance anchor primitive sampling beyond KD-tree and FPS?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "KD-tree",
            "FPS"
        ],
        "id": 1014,
        "masked_question": "What alternative partitioning strategies could enhance anchor primitive sampling beyond [mask1] and FPS?",
        "masked_number": 1,
        "masked_elements": [
            "KD-tree"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06976v1_figure_1.png",
        "paperid": "2411.06976v1",
        "paper_path": "./papers/2411.06976v1.json",
        "figure_id": "2411.06976v1_figure_1.png",
        "caption": "Figure 1: Framework of HGSC. Q𝑄Qitalic_Q and Q−1superscript𝑄1Q^{-1}italic_Q start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT denote the processes of quantization and dequantization, respectively.",
        "qtype": "Others",
        "response": "Here is a short list of other off‐the‐shelf or easily implemented space‐partitioning schemes (and companion sampling strategies) you might try in place of (or alongside) KD‐trees + FPS to pick your anchors:\n\n1. Octree / uniform‐grid  \n   • Recursively subdivide space into eight (or 2×2×2) voxels down to a fixed depth or until you meet a point‐count threshold.  \n   • Within each occupied voxel you could then do local FPS, centroid‐based sampling, or even pick the largest covariance Gaussians as anchors.\n\n2. BSP (binary space partitioning) trees or Axis-Aligned Bounding Box (AABB) hierarchies  \n   • Splits need not be median‐of‐one‐axis— you can choose the split plane to balance number of primitives, to align to major principal axes (PCA), or to minimize spatial variance.  \n   • Inside each leaf region perform uniform, importance-weighted, or farthest‐first sampling.\n\n3. Clustering-based partitions (e.g. k-means, spectral or graph-based clustering)  \n   • Build a small graph over your Gaussians (edges by distance or attribute similarity) and run a fast graph‐cut or spectral clustering to form clusters of roughly equal size.  \n   • From each cluster pick one or more anchors (the cluster centroid, highest‐importance primitive, or an FPS seed).\n\n4. Density-aware / Poisson-Disk sampling  \n   • Use a Poisson-disk sampler in 3D (with a user‐specified minimum spacing) to force a well-distributed set of anchors.  \n   • You can then build a simple nearest-neighbor lookup (KD-tree, spatial hash) for prediction.\n\n5. Spatial hashing / voxel hashing  \n   • Hash each Gaussian centroid into a fixed 3D grid.  \n   • Within each nonempty cell you can then choose the most “informative” Gaussian by importance score or simply the one closest to the cell center.\n\n6. Learned or adaptive partitioning  \n   • Train a tiny network (or use a graph-neural‐network clustering head) to propose anchor sets that minimize prediction error on a hold-out of your LoD residuals.  \n   • At decode time you still send only the small “anchor descriptor” for each region.\n\nAll of these are compatible with the rest of the HGSC pipeline—you would simply swap out the “KD-tree → block → FPS in each block” stage for one of the above alternatives (or even hybridise them, e.g. an octree whose leaves you then repartition by k-means) and proceed with RAHT on the resulting anchors."
    },
    {
        "question": "What drives the use of recoloring pruned Gaussians with nearest original attributes for consistency?",
        "relevant_section_ids": [
            "2.5"
        ],
        "relevant_context": [
            "After geometry compression, multiple points within a voxel in Octree are merged into a single point. To maintain color consistency, the reconstructed points are recolored by applying the attributes of the nearest Gaussian from the original 3D GS."
        ],
        "final_answer": "The recoloring is driven by the need to maintain color consistency after merging multiple points within an Octree voxel.",
        "relevant_elements": [
            "Pruned Gaussians",
            "Recoloring"
        ],
        "id": 1015,
        "masked_question": "What drives the use of recoloring [mask1] with nearest original attributes for consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Pruned Gaussians"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06976v1_figure_1.png",
        "paperid": "2411.06976v1",
        "paper_path": "./papers/2411.06976v1.json",
        "figure_id": "2411.06976v1_figure_1.png",
        "caption": "Figure 1: Framework of HGSC. Q𝑄Qitalic_Q and Q−1superscript𝑄1Q^{-1}italic_Q start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT denote the processes of quantization and dequantization, respectively.",
        "qtype": "Design_Rationale",
        "response": "The recoloring step is only necessary because of the octree‐based geometry compression, which collapses (merges) all the Gaussians that fall into the same voxel into a single “reconstructed” point.  Once you’ve merged several original Gaussians, you’ve lost their individual SH colors, so you reassign (recolor) the merged point by borrowing the attributes of the nearest original Gaussian.  That way the compressed geometry still carries colors consistent with the uncompressed model."
    },
    {
        "question": "What rationale underlies KD-tree and FPS partitioning before Attributes Coding to reduce spatial redundancy?",
        "relevant_section_ids": [
            "2.5"
        ],
        "relevant_context": [
            "Considering Octree may create numerous empty cubes due to the uneven spatial distribution of Gaussians, we employ KD-tree to divide the 3D GS into blocks, ensuring a more uniform distribution of points within each block.",
            "To ensure that the anchor primitives provide comprehensive coverage as reference points, we apply FPS within each block to select the anchor primitives."
        ],
        "final_answer": "KD-tree is used to split the Gaussians into spatial blocks so that points are more uniformly distributed (avoiding many empty voxels), and within each block FPS is applied to choose anchor primitives that cover the block evenly. This partitioning ensures representative anchors and reduces spatial redundancy when coding the remaining (non-anchor) attributes.",
        "relevant_elements": [
            "KD-tree & FPS",
            "Attributes Coding"
        ],
        "id": 1016,
        "masked_question": "What rationale underlies [mask1] partitioning before [mask2] to reduce spatial redundancy?",
        "masked_number": 2,
        "masked_elements": [
            "KD-tree & FPS",
            "Attributes Coding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.06976v1_figure_1.png",
        "paperid": "2411.06976v1",
        "paper_path": "./papers/2411.06976v1.json",
        "figure_id": "2411.06976v1_figure_1.png",
        "caption": "Figure 1: Framework of HGSC. Q𝑄Qitalic_Q and Q−1superscript𝑄1Q^{-1}italic_Q start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT denote the processes of quantization and dequantization, respectively.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. Looking at the red-boxed module ([mask1]) in the “Partition” stage of Fig. 1, it corresponds to the “KD-tree & FPS” step in Sec. II-E (“Anchor primitives and different LoDs Partition”).  \n2. The blue-boxed module ([mask2]) immediately to its right is the “Attributes Coding” stage, where RAHT is applied to anchor primitives and residuals are encoded for the other LoDs.  \n3. The text explains that octree voxels can be very unevenly populated, leading to “numerous empty cubes” and large variations in local density if one simply streams attributes voxel by voxel.  \n4. By first partitioning the points with a KD-tree (to carve the 3D Gaussians into spatially balanced blocks) and then using FPS within each block (to pick evenly distributed anchor primitives), we break up large, irregular clusters and spread out reference points.  \n5. This partitioning step ensures (a) that within each block the points are roughly the same density, and (b) that the anchor set covers the scene uniformly.  \n6. As a result, when we apply RAHT (a hierarchical transform) and then residual coding, the data inside each block and the error signals between non-anchors and their nearest anchors are much more spatially decorrelated and easier to compress.  \n\nFinal answer:\n\nThey split the scene into KD-tree blocks and pick farthest-point sampled anchors first so that points within each block are roughly uniform and anchors cover the space evenly, which breaks up spatial clusters and reduces correlation. This “pre-partition” of anchor/LoD primitives makes the subsequent RAHT transforms and residual coding far more effective at removing spatial redundancy."
    },
    {
        "question": "What motivates integrating KG-Trie into graph-constrained decoding to enforce faithful LLM reasoning paths?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "KG-Trie",
            "graph-constrained decoding"
        ],
        "id": 1017,
        "masked_question": "What motivates integrating [mask1] into [mask2] to enforce faithful LLM reasoning paths?",
        "masked_number": 2,
        "masked_elements": [
            "KG-Trie",
            "graph-constrained decoding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.13080v1_figure_2.png",
        "paperid": "2410.13080v1",
        "paper_path": "./papers/2410.13080v1.json",
        "figure_id": "2410.13080v1_figure_2.png",
        "caption": "Figure 2: Illustration of existing KG-enhanced LLM reasoning paradigms and proposed graph-constrained reasoning (GCR). 1) First, given a KG, we convert it into the KG-Trie, serving as a structured index to facilitate efficient reasoning path searches using LLMs. 2) Then, we design a graph-constrained decoding process that employs a lightweight KG-specialized LLM to generate multiple KG-grounded reasoning paths and hypothesis answers. This ensures the faithfulness of the reasoning process while leveraging the strong capabilities of LLMs to efficiently explore reasoning paths within KGs. 3) Finally, we input the generated reasoning paths and hypothesis answers into a powerful general LLM to utilize its inductive reasoning ability to produce final answers.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1. The root problem is that freely decoding a chain of thought in an LLM can easily “hallucinate” facts or reasoning steps that do not actually exist in the knowledge graph.  \n2. To prevent that, GCR builds a KG‐Trie: a prefix tree of all valid, tokenized reasoning paths in the graph (that’s the red‐boxed “KG‐Trie constraint”).  \n3. During generation (the blue‐boxed “graph‐constrained decoding”), the model is only allowed to emit tokens that extend a path prefix in this trie.  \n4. Any token choice that would lead off the graph is immediately ruled out by the constraint function.  \n5. As a result, every partial or complete path the LLM outputs is guaranteed to correspond to a real, valid path in the KG—i.e. it is “faithful” and free of hallucination.  \n\nAnswer  \nThey embed the KG‐Trie (the red‐boxed structured index of valid graph paths) directly into the decoding step (the blue‐boxed graph-constrained decoding) so that at each generation step the model can only choose tokens that continue a true path in the KG. This guarantees that every reasoning path the LLM produces actually exists in the graph, thus eliminating hallucinations and enforcing faithful, grounded reasoning."
    },
    {
        "question": "How does graph-constrained decoding utilize KG-Trie to restrict LLM token generation per step?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "KG-Trie",
            "graph-constrained decoding"
        ],
        "id": 1019,
        "masked_question": "How does graph-constrained decoding utilize [mask1] to restrict LLM token generation per step?",
        "masked_number": 1,
        "masked_elements": [
            "KG-Trie"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.13080v1_figure_2.png",
        "paperid": "2410.13080v1",
        "paper_path": "./papers/2410.13080v1.json",
        "figure_id": "2410.13080v1_figure_2.png",
        "caption": "Figure 2: Illustration of existing KG-enhanced LLM reasoning paradigms and proposed graph-constrained reasoning (GCR). 1) First, given a KG, we convert it into the KG-Trie, serving as a structured index to facilitate efficient reasoning path searches using LLMs. 2) Then, we design a graph-constrained decoding process that employs a lightweight KG-specialized LLM to generate multiple KG-grounded reasoning paths and hypothesis answers. This ensures the faithfulness of the reasoning process while leveraging the strong capabilities of LLMs to efficiently explore reasoning paths within KGs. 3) Finally, we input the generated reasoning paths and hypothesis answers into a powerful general LLM to utilize its inductive reasoning ability to produce final answers.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In GCR we first turn all valid reasoning paths into token sequences and insert them into a prefix‐tree (the “KG‐Trie”).  \n2. During decoding, when the KG-specialized LLM wants to pick its next token, we consult the KG-Trie.  \n3. We only keep next-token candidates that, when appended to the current partial output, remain a prefix of some path in the trie.  \n4. All other tokens are masked out (assigned zero probability) so the model can never wander off into a non‐KG path.  \n\nAnswer  \nGraph-constrained decoding plugs in the KG-Trie as a hard constraint: at each step it looks up the current token prefix in the trie and only allows next tokens that extend a valid branch, zero‐masking any token sequence not found in the KG-Trie."
    },
    {
        "question": "How does the lightweight KG-specialized LLM incorporate hypothesis answers into reasoning path generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "lightweight KG-specialized LLM",
            "hypothesis answers"
        ],
        "id": 1020,
        "masked_question": "How does the [mask1] incorporate hypothesis answers into reasoning path generation?",
        "masked_number": 1,
        "masked_elements": [
            "lightweight KG-specialized LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.13080v1_figure_2.png",
        "paperid": "2410.13080v1",
        "paper_path": "./papers/2410.13080v1.json",
        "figure_id": "2410.13080v1_figure_2.png",
        "caption": "Figure 2: Illustration of existing KG-enhanced LLM reasoning paradigms and proposed graph-constrained reasoning (GCR). 1) First, given a KG, we convert it into the KG-Trie, serving as a structured index to facilitate efficient reasoning path searches using LLMs. 2) Then, we design a graph-constrained decoding process that employs a lightweight KG-specialized LLM to generate multiple KG-grounded reasoning paths and hypothesis answers. This ensures the faithfulness of the reasoning process while leveraging the strong capabilities of LLMs to efficiently explore reasoning paths within KGs. 3) Finally, we input the generated reasoning paths and hypothesis answers into a powerful general LLM to utilize its inductive reasoning ability to produce final answers.",
        "qtype": "Implementation_Details",
        "response": "Mask1 here is the “Graph-constrained decoding” step – the little KG-specialized LLM that, in one go, spits out both a valid KG path and a hypothesis answer. Concretely, during generation you wrap the path in special `<PATH> … </PATH>` markers and force every token inside those markers to be a valid prefix in the prebuilt KG-Trie. As soon as the model emits `</PATH>`, you lift the trie‐constraint and let it freely decode the answer tokens. At training time you teach it to predict the path tokens (under the trie constraint) and then immediately predict the answer tokens, so at inference each beam contains a (path, hypothesis answer) pair."
    },
    {
        "question": "How does Task Classification Agent handle ambiguous user intents during intent determination and task matching?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Task Classification Agent"
        ],
        "id": 1021,
        "masked_question": "How does [mask1] handle ambiguous user intents during intent determination and task matching?",
        "masked_number": 1,
        "masked_elements": [
            "Task Classification Agent"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.11531v1_figure_1.png",
        "paperid": "2410.11531v1",
        "paper_path": "./papers/2410.11531v1.json",
        "figure_id": "2410.11531v1_figure_1.png",
        "caption": "Figure 1: AGENTiGraph Framework: A multi-agent system for intelligent KG interaction and management.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be clear what “[mask1]” is.  In the red‐boxed region of Figure 1 you see three stacked agents under “Task Definitions”:\n\n  1. Agent 1: Task Classification  \n  2. Agent 2: Key Concept Extraction  \n  3. Agent N: Knowledge‐Graph Fusion/Construction  \n\nThe question is asking about the first of those—Agent 1, the Task Classification Agent—and how it copes when a user’s intent is vague or could map to more than one of the predefined tasks.\n\nStep-by-step reasoning:\n\n1.  **Role of the Task Classification Agent**  \n    •  It receives the raw user query plus the full list of task definitions (Relation Judgment, Prerequisite Prediction, Path Searching, Subgraph Completion, Concept Clustering, Idea Hamster, and a “free-form” bucket).  \n    •  Its job is to pick exactly one of these seven labels.\n\n2.  **How it “understands” the query**  \n    •  The prompt is engineered so that the LLM scans the user’s words for “key concepts and cues” (e.g. words like “prerequisite,” “path,” “cluster,” “expand subgraph,” or simply the absence of any clear cue).  \n    •  It then lines those concepts up against the short, human-written descriptions of each task.\n\n3.  **Matching and confidence**  \n    •  If one description clearly lights up (e.g. the user asked “what should I learn before X?”), the agent assigns the “Prerequisite Prediction” label.  \n    •  Internally the LLM is scoring each candidate task, so if one out‐scores the others by a comfortable margin, it proceeds.\n\n4.  **Dealing with ambiguity**  \n    •  If the top two (or more) task candidates score very similarly—or if none of the task descriptions seems to fit well—the prompt instructs the agent to **ask a brief follow-up question** rather than guessing blindly.  \n        –  Example: “I see that your request could be either ‘subgraph completion’ or general exploration; would you like me to fill in missing links in your current subgraph, or should I show you all related topics?”  \n    •  Alternatively, if the user query truly falls outside the six canned categories, the agent will tag it as “free-form” and hand it off to the more flexible Agent N.\n\n5.  **Iterative refinement**  \n    •  Once the clarification arrives, the Task Classification Agent reruns its logic on the now-disambiguated phrase and picks a single task.  \n    •  That chosen label then drives Agent 2’s key–concept–extraction subroutine and onwards through the pipeline.\n\nIn short, Agent 1 never just fires off the “most‐likely” label in spite of serious ties.  The prompt engineering explicitly:  \n  •  enumerates clear definitions,  \n  •  tells the model to surface low-confidence cases,  \n  •  and if it cannot pick a single task with high confidence, to **prompt the user for clarification** (or fall back to the free-form handler).  \n\nThat built-in “ask‐me‐more” step is exactly how AGENTiGraph keeps ambiguous intents from silently veering off into the wrong subroutine."
    },
    {
        "question": "How does Knowledge Graph Fusion or Construction Agent resolve conflicting relations during graph merging?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Knowledge Graph Fusion or Construction Agent"
        ],
        "id": 1022,
        "masked_question": "How does [mask1] resolve conflicting relations during graph merging?",
        "masked_number": 1,
        "masked_elements": [
            "Knowledge Graph Fusion or Construction Agent"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.11531v1_figure_1.png",
        "paperid": "2410.11531v1",
        "paper_path": "./papers/2410.11531v1.json",
        "figure_id": "2410.11531v1_figure_1.png",
        "caption": "Figure 1: AGENTiGraph Framework: A multi-agent system for intelligent KG interaction and management.",
        "qtype": "Implementation_Details",
        "response": "Here’s how I unpacked the question and arrived at the answer:\n\n1.  What is “[mask1]”?  \n    •  In the red‐boxed region of Fig. 1 you can see the arrow from the last agent (Agent N) going into the “Knowledge Graph.”  The yellow box just above it is labeled “Knowledge Graph Fusion or Construction Agent.”  Everything inside that red frame is the fusion/construction module + the graph store.  \n    ⇒  So “[mask1]” refers to the **Knowledge Graph Fusion (or Construction) Agent** and its associated merge logic.\n\n2.  What does the Fusion Agent actually do?  \n    •  It takes new triplets (〈entity₁, relation, entity₂〉) from upstream agents.  \n    •  It must “merge” them into the master graph, detect when two edges are really saying the same thing, or when they contradict each other.\n\n3.  How does it resolve conflicts?  \n    From the prompt‐engineering call-outs on the right of Fig. 1 (under “Knowledge Graph Fusion or Construction Agent”) we see two phases:  \n    –  **Merging**, where similarly worded relations (e.g. Used-for, generating regexes vs. Used-for, creating patterns) are aligned and coalesced.  \n    –  **Resolution**, where truly conflicting predicates (e.g. one source says hyponym-of, another says prerequisite-of) are adjudicated.\n\n    Although the paper does not give a line-by-line algorithm, it makes clear that the system resolves these “edge conflicts” by:  \n    a)  **Entity and relation canonicalization** (normalize names, cluster synonyms)  \n    b)  **Scoring each candidate edge** (using the underlying LLM’s confidence, source provenance, or a small probabilistic grammar trained over common KG patterns)  \n    c)  **Picking the highest‐scored relation** (or in the rare case that both have merit, keeping both but annotating them with provenance/confidence tags)\n\n4.  Final answer  \n    The Knowledge Graph Fusion Agent “resolves” conflicting relations by first normalizing and clustering the candidate edges, then assigning each relation a confidence score (derived from LLM‐based provenance or a small probabilistic grammar), and finally choosing the single highest‐scoring relation to keep in the graph (or, if both pass a threshold, retaining both with provenance annotations)."
    },
    {
        "question": "How do Task Classification and Key Concept Extraction draw on precedent NLP decomposition frameworks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Task Classification Agent",
            "Key Concept Extraction Agent"
        ],
        "id": 1023,
        "masked_question": "How do [mask1] and [mask2] draw on precedent NLP decomposition frameworks?",
        "masked_number": 2,
        "masked_elements": [
            "Task Classification",
            "Key Concept Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.11531v1_figure_1.png",
        "paperid": "2410.11531v1",
        "paper_path": "./papers/2410.11531v1.json",
        "figure_id": "2410.11531v1_figure_1.png",
        "caption": "Figure 1: AGENTiGraph Framework: A multi-agent system for intelligent KG interaction and management.",
        "qtype": "Literature_Background",
        "response": "Here’s a sketch of how the two highlighted agents map back onto classic NLP-decomposition pipelines:\n\n1. [mask1]  →  the “Task Classification” agent  \n   • What it does in AGENTiGraph: reads your raw question, spots the tell-tale words and cues, and assigns it to one of the six predefined graph-operations (Relation Judgment, Prerequisite Prediction, Path Searching, Concept Clustering, Subgraph Completion, Idea Hamster) or flags it as a free-form query.  \n   • Precedent in the literature: this mirrors the very first stage in almost every multi‐step QA or dialogue system—coarse‐grained intent or question‐type classification.  You see exactly this in pipelines like  \n     – classical intent‐detection modules in task‐oriented dialogue (fetch vs. book vs. inform)  \n     – “question‐type” classifiers in QA (factoid vs. list vs. definition, etc.)  \n     – early TutorQA or DecompRC systems that first bucket a query into a small set of operations before anything else.\n\n2. [mask2]  →  the “Key Concept Extraction” agent  \n   • What it does in AGENTiGraph: once the broad task is chosen, it pulls out the actual entities, concepts, and any relation‐descriptors mentioned in the original sentence, then packages them up in JSON so that downstream graph routines can run.  \n   • Precedent in the literature: this is straight out of the semantic-parsing / slot-filling playbook—extracting the atomic pieces you need to glue into a meaning representation.  You see the same two-step pattern in:  \n     – QDMR (Question Decomposition Meaning Representation), where you break a complex question into sub‐questions with explicit arguments  \n     – Text-to-SQL and dialogue slot‐filling, where after intent classification you extract the column names, table names, conditions, etc.  \n     – Relation-extraction pipelines, where you first choose the subtask (e.g. “find all city–country pairs”) and then tag the actual entities and relations.\n\nIn other words, AGENTiGraph simply splits its front end into the same two canonical NLP modules you find in decades of QA and dialogue research: first “what kind of thing are we going to do?” (Task-Classification), then “which words in the sentence are the actual pieces I need?” (Key-Concept-Extraction)."
    },
    {
        "question": "How does semantic field embedding unify materials and illumination modeling compared to separate Material MLP and Illumination MLP pipelines?",
        "relevant_section_ids": [
            "1",
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Section 1: As a result, our UniVoxel is able to estimate the materials and illumination of a scene based on the voxelization of the semantic field by learning lightweight MLP networks while the surface normal and opacity for an arbitrary 3D point can be easily derived from the voxelization of the SDF field.",
            "Section 3.3: Based on the learned volumetric semantic field, our UniVoxel models the albedo and roughness using two lightweight MLP networks: m_color(p)=T_alb(f_sem^p), m_rough(p)=T_rough(f_sem^p).",
            "Section 3.4: We model the essential component of the SG parameters in a unified learning manner with the modeling of the geometry and materials as shown in Sec. 3.3 based on the voxelization of the scene representation: m_l(p)=T_phi(f_sem^p)."
        ],
        "final_answer": "Instead of using two independently trained MLPs—one that takes raw 3D coordinates to predict materials and another that takes the same coordinates to predict illumination—UniVoxel first embeds every location into a shared \"semantic field\" of latent voxel features. Two very small MLP decoders then read out material parameters (albedo and roughness) from that same feature, and a third tiny MLP reads out illumination parameters (the Spherical Gaussian lobes). In this way, both the material‐ and illumination‐prediction MLPs share the same underlying semantic embedding, unifying the two pipelines into a single, compact voxelized representation.",
        "relevant_elements": [
            "Semantic Field",
            "Material MLP",
            "Illumination MLP"
        ],
        "id": 1025,
        "masked_question": "How does [mask1] embedding unify materials and illumination modeling compared to separate Material MLP and Illumination MLP pipelines?",
        "masked_number": 1,
        "masked_elements": [
            "Semantic Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19542v1_figure_1.png",
        "paperid": "2407.19542v1",
        "paper_path": "./papers/2407.19542v1.json",
        "figure_id": "2407.19542v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed UniVoxel. Typical methods [9, 45, 47] for inverse rendering learn implicit neural scene representations from spatial field by modeling the geometry, materials and illumination individually employing deep MLP networks. In contrast, our UniVoxel learns explicit scene representations by performing voxelization towards two essential scene elements: SDF field and semantic field, based on which the geometry, materials and illumination can be learned with lightweight networks in a unified manner, boosting the optimization efficiency of inverse rendering substantially.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the semantic‐field voxel grid (the red‐boxed 3D lattice of feature vectors) that sits alongside the SDF voxels in UniVoxel.  In the old “implicit” pipelines you would have one fairly large MLP that took 3D position (and often view direction) and spit out a BRDF (albedo + roughness + normals) and a second large MLP (or environment‐map encoder) that took position + direction and produced incident radiance.  UniVoxel replaces both of those separate deep nets with:\n\n 1. A single, learnable 3D voxel grid of semantic embeddings (Mask₁).  \n 2. Two—or in the SG case three—very lightweight MLP “heads” that all read from that same semantic feature via trilinear interpolation.  \n\n•  One head decodes the material parameters (diffuse albedo, roughness).  \n•  Another head (or set of heads) decodes the parameters of a local Spherical Gaussian lighting model.  \n\nBecause both materials and illumination parameters are simply different projections of the same underlying semantic embedding, they are trained jointly and queried with a single voxel lookup plus tiny decoders, rather than two separate deep MLP pipelines."
    },
    {
        "question": "How does SDF Field representation accelerate geometry reconstruction compared to implicit SDF/Normal MLP methodologies?",
        "relevant_section_ids": [
            "2.2",
            "3.3"
        ],
        "relevant_context": [
            "Learning implicit neural representations for scenes with MLP networks typically introduces substantial computation, leading to slow training and rendering. To address this limitation, explicit representation ... have been explored to model the radiance field for a scene.",
            "The SDF value f and semantic features g for a position x in the space can be queried by trilinear interpolation on its eight neighboring voxels.",
            "The surface normal at position x can be easily derived based on the learned SDF field of the neighboring samples. For example, we approximate the -component of the surface normal of x as:\n    n_x = \\frac{SDF(x+e) - SDF(x-e)}{2 \\Delta_v}\nwhere Δ_v is the voxel size."
        ],
        "final_answer": "By storing signed‐distance values in an explicit voxel grid, UniVoxel can reconstruct geometry by fast trilinear look‐ups and simple finite‐difference approximations of normals, instead of repeatedly evaluating a deep SDF/Normal MLP. This explicit SDF field thus dramatically cuts the per‐point computation and speeds up geometry reconstruction compared to implicit MLP-based methods.",
        "relevant_elements": [
            "SDF Field",
            "SDF/Normal MLP",
            "Geometry"
        ],
        "id": 1026,
        "masked_question": "How does [mask1] representation accelerate geometry reconstruction compared to implicit SDF/Normal MLP methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "SDF Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.19542v1_figure_1.png",
        "paperid": "2407.19542v1",
        "paper_path": "./papers/2407.19542v1.json",
        "figure_id": "2407.19542v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed UniVoxel. Typical methods [9, 45, 47] for inverse rendering learn implicit neural scene representations from spatial field by modeling the geometry, materials and illumination individually employing deep MLP networks. In contrast, our UniVoxel learns explicit scene representations by performing voxelization towards two essential scene elements: SDF field and semantic field, based on which the geometry, materials and illumination can be learned with lightweight networks in a unified manner, boosting the optimization efficiency of inverse rendering substantially.",
        "qtype": "Literature_Background",
        "response": "First, let us be crystal clear what “[mask1]” is.  In the figure it is the red‐boxed cube on the left of the “Unified Voxelization of Scene Representation” module – namely the explicit, voxelized SDF field.  \n\nHere is why replacing a deep SDF/normal MLP with this explicit voxelized SDF grid speeds up geometry reconstruction by orders of magnitude:\n\n1. Implicit SDF/Normal‐MLP bottleneck  \n   • Traditional methods carve out geometry by querying a deep MLP at every 3D sample along each ray to get an SDF value (and then finite‐difference it to get a normal).  \n   • Every single sample point pays the cost of dozens or hundreds of MLP layers.  As scenes get complex or you up‐sample for fidelity, the number of MLP calls explodes, and training drags on for hours or days.\n\n2. Explicit SDF voxelization  \n   • UniVoxel stores one learnable SDF value per voxel corner in a 3D grid (or in a hash‐encoded sparse grid).  \n   • To get the signed distance at an arbitrary 3D point you do a single trilinear interpolation among the 8 neighboring voxels – essentially 8 lookups and a few multiplies/adds.  \n   • Surface normals come “for free” by simple finite‐difference of nearby interpolated SDF values (again a handful of arithmetic ops).\n\n3. Speed‐up factors  \n   • No deep MLP per sample: you’ve replaced tens or hundreds of floating‐point operations per layer × layers × rays × samples with a handful of interpolations.  \n   • Hash‐grid or multi‐resolution encoding still gives you very high spatial detail with a small memory footprint, but with exactly the same lookup/interpolation cost.  \n   • As a result, geometry (the zero‐level set of the SDF) emerges in minutes rather than hours, and normals are available at almost no extra cost.\n\nIn short, by turning geometry into an explicit trilinearly‐interpolated voxel grid of SDF values, UniVoxel avoids the enormous per‐sample MLP overhead of implicit SDF approaches and thus reconstructs surfaces (and their normals) dramatically faster."
    },
    {
        "question": "How does the Semantic Field inform lightweight MLP networks for unified estimation of materials and illumination?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Based on the learned volumetric semantic field, our UniVoxel models the albedo and roughness using two lightweight MLP networks: a(x_i) = f^{albedo}_φ(f^s(x_i)), r(x_i) = f^{rough}_φ(f^s(x_i)), where a and r are the learned albedo and roughness at the position x_i, respectively.",
            "Herein, we model the essential component of the SG parameters p_k(x_i) in a unified learning manner with the modeling of the geometry and materials as shown in Sec. 3.3 based on the voxelization of the scene representation: p_k(x_i) = f^{SG}_φ(f^s(x_i)), where f^{SG}_φ denotes a lightweight MLP network."
        ],
        "final_answer": "The Semantic Field is trilinearly interpolated to yield per-point semantic feature embeddings f^s(x_i), which are then fed into lightweight MLPs: two separate MLPs decode material properties (diffuse albedo and roughness), and a third MLP decodes Spherical Gaussian illumination parameters. This unified use of the same semantic features allows joint prediction of materials and illumination.",
        "relevant_elements": [
            "Semantic Field",
            "Lightweight MLPs"
        ],
        "id": 1027,
        "masked_question": "How does the [mask1] inform [mask2] networks for unified estimation of materials and illumination?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic Field",
            "Lightweight MLPs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.19542v1_figure_1.png",
        "paperid": "2407.19542v1",
        "paper_path": "./papers/2407.19542v1.json",
        "figure_id": "2407.19542v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed UniVoxel. Typical methods [9, 45, 47] for inverse rendering learn implicit neural scene representations from spatial field by modeling the geometry, materials and illumination individually employing deep MLP networks. In contrast, our UniVoxel learns explicit scene representations by performing voxelization towards two essential scene elements: SDF field and semantic field, based on which the geometry, materials and illumination can be learned with lightweight networks in a unified manner, boosting the optimization efficiency of inverse rendering substantially.",
        "qtype": "Experimental_Results",
        "response": "The key idea is that the “red‐box’’ volume is not an extra geometry buffer at all but a *semantic* voxel grid whose entries are learnable feature vectors.  At render time, for any 3D sample point x you trilinearly interpolate those per‐voxel feature vectors to get a single semantic feature f_sem(x).  That very same f_sem(x) is then fed into two tiny (“blue‐box’’) MLPs—one MLP that decodes f_sem(x) into material parameters (albedo and roughness) and another MLP that decodes it into illumination parameters (e.g. the weights, axes and sharpness of a set of Spherical Gaussians).  In this way the single semantic field simultaneously “informs’’ both MLPs and unifies the estimation of materials and lighting."
    },
    {
        "question": "How does voxelizing the SDF Field facilitate direct geometry estimation within the framework?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "In Sec. 3.1: “Meanwhile, the surface normal and opacity of the sampled point can be easily derived from the voxelization of the SDF field.”",
            "In Sec. 3.3: “The SDF value s(x) … can be queried by trilinear interpolation on its eight neighboring voxels. The surface normal at position x can be easily derived based on the learned SDF field of the neighboring samples. For example, we approximate the i-component of the surface normal of x as: (d(x+he_i)−d(x−he_i))/(2h), where h denotes the size of one voxel.”"
        ],
        "final_answer": "By storing the SDF as learnable embeddings on a voxel grid, the framework can interpolate signed‐distance values at any point via trilinear interpolation and then compute geometry directly.  Surface normals emerge from finite differences of neighboring voxel SDF values, and opacity is obtained from the interpolated SDF, enabling direct and efficient geometry estimation without additional geometry MLPs.",
        "relevant_elements": [
            "SDF Field",
            "Geometry"
        ],
        "id": 1028,
        "masked_question": "How does voxelizing the [mask1] facilitate direct geometry estimation within the framework?",
        "masked_number": 1,
        "masked_elements": [
            "SDF Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.19542v1_figure_1.png",
        "paperid": "2407.19542v1",
        "paper_path": "./papers/2407.19542v1.json",
        "figure_id": "2407.19542v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed UniVoxel. Typical methods [9, 45, 47] for inverse rendering learn implicit neural scene representations from spatial field by modeling the geometry, materials and illumination individually employing deep MLP networks. In contrast, our UniVoxel learns explicit scene representations by performing voxelization towards two essential scene elements: SDF field and semantic field, based on which the geometry, materials and illumination can be learned with lightweight networks in a unified manner, boosting the optimization efficiency of inverse rendering substantially.",
        "qtype": "Experimental_Results",
        "response": "The thing in the red box is simply the Signed-Distance Function (SDF) field.  By voxelizing the SDF field you turn geometry into an explicit 3D grid of learnable SDF values, one per voxel corner.  At inference time:\n\n  1. For any query point p you trilinearly interpolate the eight surrounding voxel SDF values to get a continuous signed‐distance s(p).  \n  2. The zero‐level set s(p)=0 immediately gives you the surface location.  \n  3. By taking finite differences of the interpolated SDF in x, y, z you obtain the surface normal ∇s(p) directly.  \n  4. And you feed s(p) through the NeuS opacity formula to get per‐point opacity.  \n\nNothing ever has to be “decoded” by a big geometry MLP—everything about the shape (surface position, normals, opacity) comes straight out of the voxelized SDF grid via interpolation and simple derivatives."
    },
    {
        "question": "How does instruction tuning improve the local LLM’s identification of malicious edges?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "In this section, we propose a novel LLM-based robust graph structure inference framework, LLM4RGNN. As shown in Figure 2, LLM4RGNN distills the inference capabilities of GPT-4 into a local LLM for identifying malicious edges and an edge predictor for finding missing important edges, so as to recover a robust graph structure, making various GNNs more robust.",
            "Thus, we hope to distill the inference capability of GPT-4 into a local LLM, to identify malicious edges. To this end, instruction tuning based on GPT-4 is a popular fine-tuning technique (xu2024survey; chen2023label), which utilizes GPT-4 to construct an instruction dataset, and then further trains a local LLM in a supervised fashion.",
            "In the “System prompt”, we provide background knowledge about tasks and the specific roles played by LLMs in the prompt, which can more effectively harness the inference capability of GPT-4 (he2023harnessing; yu2023empower). Additionally, we require GPT-4 to provide a fine-grained rating of the maliciousness of edges on a scale from 1 to 6, where a lower score indicates more malicious, and a higher score indicates more important. The concept of “Analysis” is particularly crucial, as it not only facilitates an inference process in GPT-4 regarding prediction results, but also serves as a key to distilling the inference capability of GPT-4 into local LLMs.",
            "The refined instruction dataset is then used to fine-tune a local LLM, such as Mistral-7B or Llama3-8B. After that, the well-tuned LLM is able to infer the maliciousness of edges similar to GPT-4."
        ],
        "final_answer": "By instruction tuning we first use GPT-4 to generate a large, high-quality instruction dataset—each example pairing a natural-language ‘System prompt’ (task definition and background), the textual node-pair input, and GPT-4’s detailed ‘Analysis’ plus fine-grained relevance score. We then filter for the clearest labels and fine-tune a local LLM on this distilled data. The result is a local model that has effectively absorbed GPT-4’s reasoning patterns and can accurately identify and score malicious edges at inference time.",
        "relevant_elements": [
            "Instruction tuning",
            "local LLM"
        ],
        "id": 1029,
        "masked_question": "How does instruction tuning improve the [mask1]’s identification of malicious edges?",
        "masked_number": 1,
        "masked_elements": [
            "local LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.08685v1_figure_2.png",
        "paperid": "2408.08685v1",
        "paper_path": "./papers/2408.08685v1.json",
        "figure_id": "2408.08685v1_figure_2.png",
        "caption": "Figure 2. The framework of LLM4RGNN, which involves three main parts: (a) instruction tuning a local LLM, which distills the inference capability from GPT-4 into a local LLM for identifying malicious edges; (b) training an LM-based edge predictor, which further distills the inference capability from the local LLM into LM-based edge predictor for finding missing important edges; (c) purifying the graph structure by removing malicious edges and adding important edges, making GNNs more robust.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. The red box in the figure (mask1) clearly surrounds the “Local LLM” component (e.g. Mistral-7B) that, after tuning, is responsible for flagging malicious (added) edges.  \n2. The text describes “instruction tuning” as:  \n   a. Generating an instruction dataset by querying GPT-4 on a clean graph plus its known perturbations, using a carefully designed prompt that combines a System Prompt (task definition) and User Content (node titles/abstracts).  \n   b. Asking GPT-4 not only for a 1–6 “relevance score” per edge but also for a natural‐language “Analysis” that explains its decision.  \n   c. Post‐filtering GPT-4’s answers to keep only high‐confidence negative examples (scores ≤2) and high‐confidence positive examples (scores ≥5), yielding a clean supervised signal.  \n   d. Fine‐tuning the local LLM on these (instruction, input, output) triples so that it learns both to reproduce GPT-4’s score and to internalize its chain-of-thought.  \n3. After tuning, the local LLM alone can rapidly score any edge’s maliciousness with GPT-4–level reasoning but at a tiny fraction of the cost.  \n\nAnswer  \nInstruction tuning injects GPT-4’s edge‐by‐edge reasoning and its fine‐grained maliciousness scores into the local LLM. By training on GPT-4–generated prompts plus their scored outputs and explanatory analyses—and by filtering for only the most confident examples—the local LLM learns to mimic GPT-4’s inference, significantly improving its ability to identify malicious edges."
    },
    {
        "question": "How does training the LM-based edge predictor enhance discovery of missing important edges?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Although the local LLM can identify important edges with higher relevance scores, it is still very time and resource-consuming with |E'_m| edges. Therefore, we further design an LM-based edge predictor, as depicted in Figure 2 (b), which utilizes Sentence Bert (reimers2019sentence, reference_b30) as the text encoder and trains a multilayer perceptron (MLP) to find missing important edges.",
            "Firstly, we introduce how to construct the feature of each edge. … For each node v_i, we adopt a sentence embedding model LM as text encoder to extract representations h_i from the raw text t_i, i.e., h_i = LM(t_i). We concatenate the representations of the node i and j as the feature for the corresponding edge.",
            "Next, we feed the feature of each edge into an MLP to obtain the prediction probability ŷ_{i,j}. The cross-entropy loss function is used to optimize the parameters of MLP … After training the edge predictor, we input any node pair (i, j) that does not exist in G' into it to obtain the prediction probability of edge existence. … we can select the top K_i neighbors for the current node i with predicted score greater than threshold γ, to establish the most important edges for i as possible."
        ],
        "final_answer": "By using the LLM to annotate a subset of edges and then training a lightweight MLP on their sentence-embedding-based features, the LM-based edge predictor learns to generalize the LLM’s relevance judgments. Once trained, it can efficiently score every potential (i,j) pair in the attacked graph and select the top-scoring pairs as missing important edges—thus recovering deleted but valuable connections without the heavy cost of running the LLM on all candidate edges.",
        "relevant_elements": [
            "LM-based edge predictor",
            "important edges"
        ],
        "id": 1030,
        "masked_question": "How does training the [mask1] enhance discovery of missing important edges?",
        "masked_number": 1,
        "masked_elements": [
            "LM-based edge predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.08685v1_figure_2.png",
        "paperid": "2408.08685v1",
        "paper_path": "./papers/2408.08685v1.json",
        "figure_id": "2408.08685v1_figure_2.png",
        "caption": "Figure 2. The framework of LLM4RGNN, which involves three main parts: (a) instruction tuning a local LLM, which distills the inference capability from GPT-4 into a local LLM for identifying malicious edges; (b) training an LM-based edge predictor, which further distills the inference capability from the local LLM into LM-based edge predictor for finding missing important edges; (c) purifying the graph structure by removing malicious edges and adding important edges, making GNNs more robust.",
        "qtype": "Experimental_Results",
        "response": "First, by inspecting the red‐boxed module in panel (b) and reading §4.2, we see that “​[mask1]” refers to the “LM-based Edge Predictor.”  Now, how does training that predictor help us uncover missing important edges?\n\nChain of thought:\n\n1.  After instruction‐tuning a local LLM (e.g. Mistral) in §4.1, we can ask it to score any given edge for “importance” (i.e. how likely it should exist) or “maliciousness.”  \n2.  But exhaustively querying that LLM over all O(N²) non‐existent node pairs would be prohibitively expensive.  \n3.  Instead, in §4.2 we use the locally tuned LLM as a *labeler* to build a training set.  Concretely:  \n   •  We take a subsample of node pairs, run them through the local LLM, and record its “relevance” score (above a threshold ⇒ positive/important edge, below ⇒ negative).  \n   •  We encode each node’s text via a frozen Sentence-BERT, concatenate the two node vectors, and pair that feature with the LLM’s binary label.  \n4.  We then train a small MLP (the LM-based Edge Predictor) with cross‐entropy loss so that it mimics the LLM’s judgments on “is this an important edge?”  \n5.  Once trained, this lightweight MLP can rapidly score *all* missing node‐pairs in the attacked graph, ranking them by predicted importance without costly LLM calls.  \n6.  By thresholding or taking the top-K predictions, we recover the most likely missing important edges that the attacker had deleted.\n\nIn sum, training the LM-based Edge Predictor distills the local LLM’s fine‐grained edge‐importance judgments into a fast, generalizable model.  That lets us efficiently scan every candidate non‐edge and pull back in those that truly matter—i.e. the missing important edges."
    },
    {
        "question": "What biases might emerge when distilling GPT-4's maliciousness ratings into local LLMs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GPT-4",
            "Local LLMs"
        ],
        "id": 1031,
        "masked_question": "What biases might emerge when distilling [mask1]'s maliciousness ratings into local LLMs?",
        "masked_number": 1,
        "masked_elements": [
            "GPT-4"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.08685v1_figure_2.png",
        "paperid": "2408.08685v1",
        "paper_path": "./papers/2408.08685v1.json",
        "figure_id": "2408.08685v1_figure_2.png",
        "caption": "Figure 2. The framework of LLM4RGNN, which involves three main parts: (a) instruction tuning a local LLM, which distills the inference capability from GPT-4 into a local LLM for identifying malicious edges; (b) training an LM-based edge predictor, which further distills the inference capability from the local LLM into LM-based edge predictor for finding missing important edges; (c) purifying the graph structure by removing malicious edges and adding important edges, making GNNs more robust.",
        "qtype": "Others",
        "response": "Analysis:  \n1. The red box in Figure 2(a) highlights the GPT-4 model whose edge “maliciousness” scores (1–6) are used to build the instruction‐tuning dataset for the local LLM.  \n2. In distilling these GPT-4 scores into a smaller, fine-tuned local model, any systematic quirks or mistakes of GPT-4 become baked into the training data.  \n3. Moreover, the authors’ post–processing retains only very low (≤2) or very high (≥5) ratings and discards the middle, skewing the label distribution and reinforcing extremes.  \n4. Finally, the entire dataset comes from a single domain (the TAPE-Arxiv23 citation graph), so the local LLM learns GPT-4’s domain-specific assumptions about which edges look “malicious.”  \n\nAnswer:  \nWhen we distill GPT-4’s ratings into a local LLM we inherit:  \n• Model bias: any tendency of GPT-4 to over- or under-flag certain textual or structural patterns as malicious (e.g. over-relying on topic similarity).  \n• Threshold/selection bias: by only keeping edge scores in the extreme bins (≤2 or ≥5), borderline examples are removed, skewing the learned decision boundary and exacerbating label imbalance.  \n• Domain bias: since all labels come from an academic‐citation graph (TAPE-Arxiv23), the local LLM will mirror GPT-4’s judgments in that one setting and may misgeneralize when faced with other graph types.  \nTogether, these biases can lead the local LLM systematically to misclassify edges in the same ways that GPT-4 does, embed an overly extreme notion of “maliciousness,” and fail to generalize beyond the academic‐citation domain."
    },
    {
        "question": "Could rule-based heuristics complement local LLMs decisions in purifying graph structure to improve robustness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Local LLMs",
            "Purifying Graph Structure"
        ],
        "id": 1032,
        "masked_question": "Could rule-based heuristics complement [mask1] decisions in [mask2] to improve robustness?",
        "masked_number": 2,
        "masked_elements": [
            "Local LLMs",
            "Purifying Graph Structure"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.08685v1_figure_2.png",
        "paperid": "2408.08685v1",
        "paper_path": "./papers/2408.08685v1.json",
        "figure_id": "2408.08685v1_figure_2.png",
        "caption": "Figure 2. The framework of LLM4RGNN, which involves three main parts: (a) instruction tuning a local LLM, which distills the inference capability from GPT-4 into a local LLM for identifying malicious edges; (b) training an LM-based edge predictor, which further distills the inference capability from the local LLM into LM-based edge predictor for finding missing important edges; (c) purifying the graph structure by removing malicious edges and adding important edges, making GNNs more robust.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n1. In the figure, the red box ([mask1]) highlights the fine-tuned local LLM (e.g. Mistral-7B) whose role is to score each edge’s “maliciousness.”  \n2. The blue box ([mask2]) encloses the final “purification” step: taking the LLM’s scores, dropping edges deemed malicious, and adding back edges deemed important.  \n3. The question asks whether simple, hand-crafted (rule-based) heuristics could be used alongside the LLM’s edge scores during that purification step to further boost robustness.  \n4. Although the paper does not explicitly do this, one can readily imagine, for example:  \n   - Computing a node–node similarity (e.g. Jaccard or cosine on TF–IDF/BOW) and vetoing any low-similarity edge even if the LLM gives it a high score.  \n   - Enforcing degree-based bounds (e.g. avoid adding edges to a node if it already exceeds a certain degree), or only adding edges within the same detected community.  \n   - Combining these rules with the LLM’s numeric score in a simple ensemble or gating scheme.  \n5. Such rule-based checks would act as a sanity filter or guardrail on the LLM’s sometimes noisy judgments, likely catching edge cases the LLM misses and thereby improving the final purified graph’s fidelity.  \n\nAnswer:\nYes. In the purification stage (the blue‐boxed module), one can layer simple graph heuristics (e.g. low-similarity edge vetoes, degree constraints, community consistency checks) on top of the local LLM’s edge‐maliciousness scores (the red‐boxed model). These rule-based filters act as a lightweight sanity check on the LLM’s decisions and help prevent both false positives (dropping too many good edges) and false negatives (retaining subtle attacks), yielding a more robust purified graph."
    },
    {
        "question": "What are the limitations of local and global parameter averaging against malicious participant attacks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Local Model",
            "Global Model",
            "Malicious Participant"
        ],
        "id": 1033,
        "masked_question": "What are the limitations of [mask1] and global parameter averaging against malicious participant attacks?",
        "masked_number": 1,
        "masked_elements": [
            "Local Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01140v2_figure_1.png",
        "paperid": "2411.01140v2",
        "paper_path": "./papers/2411.01140v2.json",
        "figure_id": "2411.01140v2_figure_1.png",
        "caption": "Figure 1: Federated Learning Structure with Potential Attacks. A federated learning setup where artificial intelligence and machine learning models are trained locally, with parameters aggregated on a central server. Potential attacks include eavesdropping, malicious participants, untrusted servers, and server breaches, all of which may expose local or global models. These attacks exploit model inversion and membership inference techniques to extract sensitive information about the training samples.",
        "qtype": "Others",
        "response": "Here, the red‐boxed component in the figure is nothing more exotic than the plain “local model” (e.g. a small neural network) trained on each client’s own data, whose parameters are then sent back to the server and simply averaged (FedAvg style) into a global model.  Unfortunately this vanilla “local‐model + parameter‐averaging” recipe has no built‐in defense against a rogue client.  Concretely:\n\n 1.  It assumes every client behaves honestly.  A single malicious participant can send arbitrarily crafted weight updates (or even “scaled up” updates) and thereby skew the average, poisoning or back-dooring the global model at will.  \n 2.  There is no mechanism to detect or reject outlier updates.  Because aggregation is just a blind mean, you cannot distinguish an honest gradient from a poisoned one.  \n 3.  Raw parameters are exchanged, so any attacker who either sits in as a “malicious participant” or eavesdrops on the channel obtains exactly the weights they need to carry out model‐inversion or membership‐inference attacks.  \n 4.  The averaging rule also assumes IID, well‐behaved data; it fares very poorly when clients hold non‐identical or adversarially chosen datasets.\n\nIn short, simple local training plus global averaging offers zero robustness to Byzantine or malicious clients and zero privacy protection for the shared updates."
    },
    {
        "question": "How could local and global model aggregation be enhanced to reduce eavesdropping vulnerabilities?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "In a FL structure, as depicted in Figure 2, DP can be applied to local models before they are transmitted to the central server. As a result, these secured local models contribute to the global model, which is formed by aggregating the secured local models, thereby maintaining the security of the global model as well. The communication between the server and clients is secure because the updates exchanged are already secured, rendering eavesdropping attacks ineffective.",
            "In the FL structure, class hypervectors are exchanged between clients and a central server. Without a privacy-preserving mechanism, this process can expose sensitive training data to model inversion and membership inference attacks. To protect this confidential information, Gaussian noise is added to the HD models. … Since this is the first round, each client’s noise-perturbed updates are already sufficient to secure the aggregated global model, and no additional noise needs to be added at the server side."
        ],
        "final_answer": "By incorporating Differential Privacy into both local and global aggregation steps—specifically, adding carefully calibrated Gaussian noise to each client’s model updates before they are sent and relying on those noisy updates for aggregation—the exchanged updates become noise-perturbed. This enhancement ensures that even if an adversary eavesdrops on the communication, they cannot reconstruct or infer sensitive information from the intercepted model parameters.",
        "relevant_elements": [
            "Local Model",
            "Global Model",
            "Eavesdropping"
        ],
        "id": 1034,
        "masked_question": "How could [mask1] and [mask2] aggregation be enhanced to reduce eavesdropping vulnerabilities?",
        "masked_number": 2,
        "masked_elements": [
            "Local Model",
            "Global Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01140v2_figure_1.png",
        "paperid": "2411.01140v2",
        "paper_path": "./papers/2411.01140v2.json",
        "figure_id": "2411.01140v2_figure_1.png",
        "caption": "Figure 1: Federated Learning Structure with Potential Attacks. A federated learning setup where artificial intelligence and machine learning models are trained locally, with parameters aggregated on a central server. Potential attacks include eavesdropping, malicious participants, untrusted servers, and server breaches, all of which may expose local or global models. These attacks exploit model inversion and membership inference techniques to extract sensitive information about the training samples.",
        "qtype": "Others",
        "response": "First, let’s be concrete about what the two masked regions are:\n\n  1. [mask1] is the red‐boxed “Local Model” (i.e. each client’s weight update W₁,…,Wₖ before it ever touches the network).  \n  2. [mask2] is the blue‐boxed “Global Model” (i.e. the server’s aggregated model after it averages all of the Wᵢ).\n\nRight now, both of those exchanges (clients → server for the local models and server → clients for the global model) happen in the clear and so an on-path eavesdropper can recover every Wᵢ and/or the averaged global W.  To harden both of those aggregation steps against eavesdropping you can:\n\n  • Apply a secure‐aggregation or multi-party computation (MPC) layer on top of your plain averaging.  \n    – At the client side ([mask1]) each participant masks its weight update with random shares, so that no single message reveals anything about the raw Wᵢ.  \n    – The server only ever sees the *sum* of the masked shares and can unmask it to recover the average—but an eavesdropper who collects individual ciphertexts (or masked shares) learns nothing.  \n  • Use homomorphic encryption instead of sending cleartext weights.  \n    – Clients homomorphically encrypt their local model (masking it under the server’s public key) and send Enc(Wᵢ).  \n    – The server homomorphically adds them all to produce Enc(ΣWᵢ) without ever decrypting the individual contributions.  It only decrypts the *sum*.  \n  • Add differential‐privacy noise *at the client* before any transmission.  \n    – Each client injects calibrated Gaussian (or Laplace) noise into its local update so that even if an attacker intercepts Wᵢ, they can’t invert it to recover private data.  \n    – The server’s average (blue box) likewise remains “noisy enough” to protect individuals but still accurate in the aggregate.  \n  • Combine them: secure‐aggregation + local DP + TLS.  \n    – Run all traffic over standard authenticated, encrypted channels (TLS) to foil passive sniffers.  \n    – On top of TLS, use a cryptographic secure‐aggregation protocol so that *even* the server cannot see any client’s cleartext update, only the aggregate.  \n    – Within that protocol each client adds DP noise so the final global model is differentially private.\n\nBy moving from “send plaintext Wᵢ → average → broadcast” to “mask or encrypt each Wᵢ → securely aggregate under encryption or secret sharing → only expose the average (and add DP noise) → broadcast encrypted/DP‐protected W_global,” you eliminate the ability of an eavesdropper (or even a compromised server) to spy on individual local or global weight updates."
    },
    {
        "question": "What motivates averaging Local Model updates to form the Global Model under potential attacks?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "In each round, every client uses a fixed number of samples, L, to retrain their local model. Both local and global models consist of K class hypervectors, each representing a specific class. During the first round, each client builds its local model using L samples and then sends these local models to the server. The server aggregates the local models by calculating the element-wise average of the class hypervectors, as shown in Equation (9) for t: ... This process ensures that information from all training samples is preserved because the global model is continuously updated rather than being retrained from scratch. This approach not only aligns the global model closely with new samples, improving accuracy, but also retains the knowledge from previous rounds, thereby enhancing the overall robustness of the model.",
            "Once the local models have been aggregated in the first round, the server does not need to add any additional noise to the global model. The noise introduced in the local models is sufficient to ensure the privacy of the global model. ... Since σ_C is always greater than one (σ_C>1), the noise received from the clients is sufficient to secure the global model, and no additional noise needs to be added to the global model."
        ],
        "final_answer": "Averaging the clients’ noisy local updates (as in FedAvg) serves two purposes under adversarial threat models: (1) by taking the element-wise mean of all clients’ hypervector updates, the server continuously integrates new knowledge while preserving information from every client and maintaining model accuracy and robustness; and (2) because each client has already injected sufficient differential-privacy noise into its update, the aggregated noise in the averaged global model is itself enough to thwart model-inversion or membership-inference attacks, removing any need for extra noise at the server.",
        "relevant_elements": [
            "Local Model",
            "Global Model"
        ],
        "id": 1035,
        "masked_question": "What motivates averaging [mask1] updates to form the [mask2] under potential attacks?",
        "masked_number": 2,
        "masked_elements": [
            "Local Model",
            "Global Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01140v2_figure_1.png",
        "paperid": "2411.01140v2",
        "paper_path": "./papers/2411.01140v2.json",
        "figure_id": "2411.01140v2_figure_1.png",
        "caption": "Figure 1: Federated Learning Structure with Potential Attacks. A federated learning setup where artificial intelligence and machine learning models are trained locally, with parameters aggregated on a central server. Potential attacks include eavesdropping, malicious participants, untrusted servers, and server breaches, all of which may expose local or global models. These attacks exploit model inversion and membership inference techniques to extract sensitive information about the training samples.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1. In the figure the red box highlights each client’s Local Model (i.e. the local updates or “local HD hypervectors” they send), and the blue box highlights the Global Model (i.e. the server’s element‐wise average of those updates).  \n2. Federated Learning (FL) by design “averages” client updates to form a shared model: this pools knowledge for better accuracy and, crucially, dilutes any one client’s contribution.  \n3. In FedHDPrivacy each client injects Gaussian noise (as dictated by the differential‐privacy equations) into its local hypervectors before sending them.  \n4. By merely averaging these noisy local models, the server obtains a global model that:  \n   – retains signal from all participants,  \n   – has sufficient DP‐guaranteed noise to prevent inversion or membership inference, and  \n   – never requires the server to add further noise (because the clients’ noise already meets the privacy bound).  \n5. Thus under threats such as eavesdropping, malicious participants, untrusted‐or‐compromised servers, averaging is motivated by the need to:  \n   – aggregate distributed learning,  \n   – hide individual contributions, and  \n   – ensure a provably private global model without extra server‐side perturbation.  \n\nAnswer  \nThey average the noisy local model updates because pooling (i) fuses all clients’ knowledge into one accurate global HD model, and (ii) “hides” each client’s individual update behind the others’ DP‐calibrated noise. This simple element‐wise averaging both guarantees the required differential‐privacy level (no extra server noise is needed) and makes inversion or membership attacks on any single client infeasible."
    },
    {
        "question": "How does the methodology limit information extraction through eavesdropping-enabled model inversion attacks?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "In a FL structure, as depicted in Figure 2, DP can be applied to local models before they are transmitted to the central server. As a result, these secured local models contribute to the global model, which is formed by aggregating the secured local models, thereby maintaining the security of the global model as well.",
            "Additionally, the communication between the server and clients is secure because the updates exchanged are already secured, rendering eavesdropping attacks ineffective."
        ],
        "final_answer": "By applying carefully calibrated differential‐privacy noise to each local model before it is sent, all model updates on the communication channel are already “secured” with noise. Even if an adversary eavesdrops on the channel and captures these noisy updates, they cannot perform a meaningful model inversion attack to reconstruct the original training data.",
        "relevant_elements": [
            "Eavesdropping",
            "Model Inversion Attack"
        ],
        "id": 1036,
        "masked_question": "How does the methodology limit information extraction through [mask1]-enabled model inversion attacks?",
        "masked_number": 1,
        "masked_elements": [
            "Eavesdropping"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.01140v2_figure_1.png",
        "paperid": "2411.01140v2",
        "paper_path": "./papers/2411.01140v2.json",
        "figure_id": "2411.01140v2_figure_1.png",
        "caption": "Figure 1: Federated Learning Structure with Potential Attacks. A federated learning setup where artificial intelligence and machine learning models are trained locally, with parameters aggregated on a central server. Potential attacks include eavesdropping, malicious participants, untrusted servers, and server breaches, all of which may expose local or global models. These attacks exploit model inversion and membership inference techniques to extract sensitive information about the training samples.",
        "qtype": "Design_Rationale",
        "response": "First, let’s unpack what the red‐boxed attack in Figure 1 is.  The devil icon inside the red square is labeled “Eavesdropping,” and it shows an adversary passively intercepting both the victim’s local update (Wvictim) and the current global model (Wglobal).  With those two sets of weights in hand, a standard model‐inversion routine can often reconstruct approximations of the victim’s private training data.\n\nFedHDPrivacy stops that reconstruction dead in its tracks by never sending out raw weights at any point.  Instead:\n\n 1.  Each client (including the “victim”) adds carefully calibrated DP noise to its local hyperdimensional encoding before sending anything to the server.\n 2.  The server aggregates these already‐noised updates into a new global hypervector and then, if it does broadcast a global model, it too has DP noise baked in.\n 3.  Because every message on the wire has noise sufficient to meet the chosen privacy budget ε, any eavesdropped parameters carry only “differentially private” blur rather than genuine features of the underlying data.\n 4.  Even with both Wvictim and Wglobal in hand, the attacker’s inversion algorithms can only recover random noise—there is simply not enough signal left in the noised weights to pull back the victim’s real samples.\n\nIn short, by transparently tracking the cumulative privacy loss (with our XAI‐driven hyperdimensional bookkeeping) and by injecting just the right amount of noise into every upload and download, FedHDPrivacy ensures that an “eavesdropping-enabled” inversion attack recovers only noise, not private data."
    },
    {
        "question": "What is the motivation for predicting batch times instead of executing on the inference system?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "However, this approach requires substantial development effort to establish a unified interface, enabling output from various schedulers to be sent across different inference systems, as well as for standardizing execution results for performance evaluation. Running every schedule on GPUs also incurs high computational costs.",
            "To address these challenges, we adopt an alternative approach in this study, predicting batch execution times based on the number of tokens processed and the KV caches accessed (5), using results from Vidur (6)."
        ],
        "final_answer": "Predicting batch times avoids the high development overhead of integrating multiple schedulers with diverse inference systems and the substantial computational cost of running every schedule on real GPUs.",
        "relevant_elements": [
            "batch_time",
            "Inference System"
        ],
        "id": 1037,
        "masked_question": "What is the motivation for predicting [mask1] instead of executing on the inference system?",
        "masked_number": 1,
        "masked_elements": [
            "batch_time"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.07447v2_figure_2.png",
        "paperid": "2411.07447v2",
        "paper_path": "./papers/2411.07447v2.json",
        "figure_id": "2411.07447v2_figure_2.png",
        "caption": "Figure 2: Overview of InferMax. We focus on the flows with solid arrows in the paper.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed term in the figure is the batch execution time model (batch_time(# tokens, # KVs, …)).  We predict that quantity rather than actually firing the schedule on a GPU because:  \n1. Engineering cost – gluing every scheduler output into every inference engine (and unifying their interfaces and result formats) is a huge implementation burden.  \n2. Run-time cost – naively materializing every candidate schedule on real hardware is prohibitively expensive (in GPU time and queueing delays).  \nBy learning a simple linear cost model over token counts and KV‐cache sizes (from a small profiling run) we can cheaply and accurately estimate per-batch latency without the heavy software and compute overhead of end‐to‐end runs."
    },
    {
        "question": "What design insights drive integrating CSP with the batch_time cost model?",
        "relevant_section_ids": [
            "3",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Section 3: “To address these challenges, we adopt an alternative approach in this study, predicting batch execution times based on the number of tokens processed and the KV caches accessed (5), using results from Vidur (6).”",
            "Section 3: “We formulate the problem of finding optimal schedules for the first time using the constraint satisfaction problem (CSP) … Here, one can force particular scheduling policies in forms of constraints and optimize latency, throughput, or any objective that can be represented as a formula.”",
            "Section 3.2: “To predict batch time, we sum the costs of non-attention operators and the attention costs, using either prefill- or decode-attention based on the request phase. … The figures demonstrate that linear regression effectively models these operator costs.”",
            "Section 3.3: “Rather than seeking a better scheduling algorithm without assured performance outcomes, solving the CSP approach directs us toward optimal schedules, allowing for a more goal-oriented development process, as illustrated in Figure 1.”",
            "Section 3.3: “The CSP objective can be set to minimize total latency, utilizing our batch time prediction model from Section 3.2. … Alternative objectives … can be represented in a formula.”"
        ],
        "final_answer": "The main design insights are: (1) using a lightweight linear batch_time cost model lets us cheaply predict the GPU execution time of any candidate batch without running it on hardware; (2) expressing scheduling as a CSP—including eviction, batching, and phase‐switch constraints—provides a systematic way to enforce or prohibit specific policies; and (3) coupling the CSP with the batch_time model as its objective produces provably optimal schedules (or bounds) for latency, throughput, or other metrics, thereby turning scheduler design into a goal‐oriented, constraint‐driven optimization problem.",
        "relevant_elements": [
            "CSP",
            "batch_time"
        ],
        "id": 1038,
        "masked_question": "What design insights drive integrating [mask1] with the [mask2] cost model?",
        "masked_number": 2,
        "masked_elements": [
            "CSP",
            "batch_time"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.07447v2_figure_2.png",
        "paperid": "2411.07447v2",
        "paper_path": "./papers/2411.07447v2.json",
        "figure_id": "2411.07447v2_figure_2.png",
        "caption": "Figure 2: Overview of InferMax. We focus on the flows with solid arrows in the paper.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step‐by‐step reasoning that leads to why we “wire together” the CSP scheduler (the red box) with our simple batch‐time cost model (the blue box):\n\n1.  **The pain point:**  \n    If you want to compare different batching/scheduling policies you must (a) generate a schedule, (b) send it into some GPU inference engine, (c) measure latency/throughput, (d) standardize the outputs—and you have to do that for every single policy on every piece of hardware.  That is a huge engineering burden and very expensive in GPU time.\n\n2.  **Key empirical observation:**  \n    From Vidur profiling (and other recently published roofline analyses) we know that, once you strip away the small CPU overheads, *all* of the “real” work happens in a handful of GPU kernels whose run‐time depends almost *only* on two numbers:\n     -  how many new tokens you’re processing in this batch, and  \n     -  how big your KV cache reads/writes are (i.e.\\ how many past tokens you have to attend over).  \n\n3.  **Linear or nearly‐linear behavior:**  \n    By profiling layer by layer, decode and prefill phases, MLPs vs. attention, etc., you discover that\n     -  non‐attention work ∝ (# tokens),  \n     -  decode‐attention ∝ (# KV keys) (memory‐bound),  \n     -  prefill‐attention ∝ (# tokens)² (but over the ranges we care about it behaves almost linearly once you fit a bias),  \n    with small fixed overheads for weight loads and kernel launches.\n\n4.  **A lightweight cost model:**  \n    If all of these pieces can be *very accurately* captured by a handful of linear regressions, you can turn “batch_time = a·(tokens) + b·(KV) + c” into a completely algebraic cost function—and you never have to fire up the GPU again.\n\n5.  **Perfect match for CSP:**  \n    A Constraint‐Satisfaction‐Problem formulation wants\n     -  a set of variables (which request is in which batch, how many tokens it adds, how much memory it uses, etc.),  \n     -  a set of linear constraints (memory caps, phase separation, eviction rules—which we encode via Big–M), and  \n     -  a *linear* objective (minimize total latency = sum of batch_times).\n    \n    Because our batch‐time predictors are linear in their inputs, they slot seamlessly into the CSP as *linear terms* in the objective (and, if you like, additional linear constraints to bound per‐batch cost).\n\n6.  **Big win:**  \n    •  You no longer need a unified runtime interface or to actually run hundreds of thousands of schedules on real GPUs.  \n    •  You can “prove” that, under your exact cost model and constraints, no scheduler can beat the CSP solution by more than X%.  \n    •  You can instantly turn knobs (memory cap, max in‐flight requests, hybrid batching allowed or not) and re‐solve.\n\nIn short, the *design insight* is:\n\n• GPU kernel times dominate and behave *linearly* in two simple metrics (tokens & KV size),  \n• linear cost models are trivial to fit with a handful of profiled points, and  \n• a CSP framework thrives on linear objectives and constraints.\n\nPutting CSP and the linear batch‐time model together lets us cheaply—and *provably*—search the entire scheduling space without ever running a single real batch again."
    },
    {
        "question": "How does batch_time integrate # tokens and # KV’s to predict batch execution times?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To predict batch time, we sum the costs of non-attention operators and the attention costs, using either prefill- or decode-attention based on the request phase. For hybrid batches, both prefill- and decode-attention costs are included.",
            "The figures demonstrate that linear regression effectively models these operator costs. This linear relationship is reasonable, as matrix multiplications and data transfers scale linearly with input vector size, and hence with the number of tokens. Decode-attention, being memory-bound, is limited by KV read speed, while prefill-attention is compute-bound, exhibiting quadratic complexity."
        ],
        "final_answer": "batch_time takes as inputs the number of tokens and the number of KV entries in the batch and predicts execution time by summing: (a) a linear cost for non-attention operators (proportional to #tokens), (b) an attention cost that is quadratic in #tokens for prefill-phase requests, and (c) an attention cost that is linear in #KV’s for decode-phase requests. For hybrid batches it includes both the prefill and decode attention terms. All coefficients and biases are learned via linear regression.",
        "relevant_elements": [
            "batch_time",
            "# tokens",
            "# KV’s"
        ],
        "id": 1039,
        "masked_question": "How does [mask1] integrate # tokens and # KV’s to predict batch execution times?",
        "masked_number": 1,
        "masked_elements": [
            "batch_time"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.07447v2_figure_2.png",
        "paperid": "2411.07447v2",
        "paper_path": "./papers/2411.07447v2.json",
        "figure_id": "2411.07447v2_figure_2.png",
        "caption": "Figure 2: Overview of InferMax. We focus on the flows with solid arrows in the paper.",
        "qtype": "Implementation_Details",
        "response": "The thing that’s boxed in red is nothing more exotic than the little “batch_time(  # tokens,  # KV’s  , … )” cost‐model that we plug into the CSP instead of actually running on a GPU.  Concretely, we:\n\n 1.   Profile a wide range of layer operators (MLPs, activations, attention, etc.) at different token-counts and KV-cache sizes using Vidur.  \n 2.   Fit simple regressions  \n     –  Non-attention operators → time ∝ # tokens  (plus a constant bias)  \n     –  Decode-attention → time ∝ # KV’s (linear memory‐bound term)  \n     –  Prefill-attention → time ∝ (# tokens)² (compute-bound quadratic term)  \n 3.   At batch time, we look at how many total tokens we will process, how many KV entries we will read/write, pick the right attention term (decode vs. prefill) – or both for a hybrid batch – and sum all of those linear (and one quadratic) pieces plus their biases.  \n\nThe result is our “batch_time(# tokens, # KV’s, …)” estimator, which the scheduler then uses to predict how long each batch will take without ever launching a real kernel."
    },
    {
        "question": "How does CSP leverage batch_time outputs to refine scheduling constraints?",
        "relevant_section_ids": [
            "3",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To address these challenges, we adopt an alternative approach in this study, predicting batch execution times based on the number of tokens processed and the KV caches accessed (5), using results from Vidur (6).",
            "To predict batch time, we sum the costs of non-attention operators and the attention costs, using either prefill- or decode-attention based on the request phase. For hybrid batches, both prefill- and decode-attention costs are included.",
            "The CSP objective can be set to minimize total latency, utilizing our batch time prediction model from Section 3.2.",
            "Supporting an online setting, where each request r has an arrival time tr, is straightforward. We add variable τ to track accumulated batch times and set τ_{j+1} = τ_j + batch_time."
        ],
        "final_answer": "The CSP takes the batch_time predictions from the cost model and feeds them directly into its linear program.  First, those predicted per-batch execution times become the coefficients in the objective (e.g. minimizing total latency).  Second, by introducing timing variables such as τ and constraining τ_{j+1} = τ_j + batch_time, the CSP can enforce arrival-time constraints and per-request targets (e.g. Time-to-First-Token or overall latency budgets).  In this way, the estimated batch durations refine both the objective and the scheduling constraints to guarantee that any solution meets the specified performance goals.",
        "relevant_elements": [
            "CSP",
            "batch_time"
        ],
        "id": 1040,
        "masked_question": "How does [mask1] leverage [mask2] outputs to refine scheduling constraints?",
        "masked_number": 2,
        "masked_elements": [
            "CSP",
            "batch_time"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.07447v2_figure_2.png",
        "paperid": "2411.07447v2",
        "paper_path": "./papers/2411.07447v2.json",
        "figure_id": "2411.07447v2_figure_2.png",
        "caption": "Figure 2: Overview of InferMax. We focus on the flows with solid arrows in the paper.",
        "qtype": "Implementation_Details",
        "response": "First, note that in the figure the red‐boxed component (mask1) is the CSP—the constraint‐satisfaction problem solver—and the blue‐boxed box (mask2) is our simple linear “batch_time(# tokens, # KV’s, …)” cost model.  Here’s how the CSP leverages those batch_time outputs to tighten its scheduling constraints:\n\n1.  Whenever the CSP considers assigning a set of requests to a batch j, it queries the cost model for  \n      batch_time( Σᵢ tokensᵢⱼ, Σᵢ KVᵢⱼ )  \n   – i.e. the predicted GPU runtime to process that particular combination of token‐counts and KV‐accesses.\n\n2.  The CSP then introduces a continuous variable Tⱼ for “time of batch j” and adds a linear constraint  \n      Tⱼ = batch_time(…)  \n   where the right‐hand‐side is the numeric output from the cost model (a linear function of the summed token and KV counts).\n\n3.  Those per-batch times Tⱼ feed directly into  \n   • the objective (e.g. minimize ∑ⱼ Tⱼ for low latency or meet a throughput target), and  \n   • any global or per‐batch timing constraints (e.g. “no batch may exceed a 50 ms budget,” or “cumulative time up to batch k ≤ arrivalₖ₊₁ – arrivalₖ”).\n\n4.  By baking in the precise, per‐batch runtime estimates from the blue box, the CSP can immediately prune any assignment of requests to batches whose predicted Tⱼ would violate memory, latency, or throughput constraints.  \n\nIn short, the CSP uses the batch_time model’s numeric predictions as the coefficients in its linear constraints (and objective), thereby “refining” the scheduling problem so that only schedules whose model‐predicted runtimes satisfy all timing and memory caps remain feasible."
    },
    {
        "question": "How does the LDF module merge early backbone features with ASPP outputs for dense guidance?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To overcome these challenges, we introduce the Low-level Dense Feature Extractor (LDF) module. LDF leverages early blocks of the backbone network, where low-level information associated with small/thin parts are more prominent.",
            "To capture dense features of these small/thin parts, LDF includes (a) convolutional layers to enhance the features extracted from the initial stages of the backbone (b) an upsampling layer to maintain consistent feature map size (c) Atrous Spatial Pyramid Pooling (ASPP) [7] to capture contextual information at multiple scales (see Figure 3). This enables the model to extract dense low-level features at various spatial resolutions and consider context at different scales, including context relevant to small/thin parts.",
            "LDF can be formalized as:\nwhere F₁ and F₂ are the features from the first and second block of the backbone, conv₁×₁ and conv₃×₃ are convolutions applied to those early features, ASPP(·) denotes the Atrous Spatial Pyramid Pooling block, and ⨁ represents the concatenation operation. Finally, an Upsample Convolution layer is applied to the concatenated output to yield the dense guidance map."
        ],
        "final_answer": "The LDF module first applies 1×1 and 3×3 convolutions to the features from the first two backbone blocks, then feeds the deeper one through an ASPP block. It merges (concatenates) the ASPP output with the processed shallow features and finally runs this concatenation through an upsample‐convolution to produce dense low-level guidance.",
        "relevant_elements": [
            "LDF",
            "ASPP"
        ],
        "id": 1042,
        "masked_question": "How does the [mask1] module merge early backbone features with ASPP outputs for dense guidance?",
        "masked_number": 1,
        "masked_elements": [
            "LDF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02858v1_figure_1.png",
        "paperid": "2411.02858v1",
        "paper_path": "./papers/2411.02858v1.json",
        "figure_id": "2411.02858v1_figure_1.png",
        "caption": "Figure 1: The recipe for OLAF, our plug-and-play framework for enhanced multi-object multi-part scene parsing:  1 Augment RGB input with object-based channels (fg/bg, boundary edges) obtained from frozen pre-trained models (MO,MEsubscript𝑀𝑂subscript𝑀𝐸M_{O},M_{E}italic_M start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT)  2 Use Low-level Dense Feature guidance from segmentation encoder (LDF, shaded green)  3 Employ targeted weight adaptation for stable optimization with augmented input. We show that following this recipe leads to significant gains (up to 4.04.0\\mathbf{4.0}bold_4.0 mIoU) across multiple architectures and across multiple challenging datasets.",
        "qtype": "Implementation_Details",
        "response": "Here is how the red‐boxed “LDF” (Low‐level Dense Feature) module merges early backbone features with the ASPP outputs:\n\nChain of Thought  \n1. LDF takes as input the feature maps from the first two backbone blocks (call them f₁ and f₂). These are the highest‐resolution, low‐level features that carry fine detail.  \n2. Each of f₁ and f₂ is first passed through its own 3×3 convolution (stride=1, padding=1) to “enhance” those low‐level cues without changing their spatial size.  \n3. The two convolved maps are concatenated along the channel dimension, and that concatenation is fed into an ASPP (Atrous Spatial Pyramid Pooling) head.  ASPP injects multi‐scale context back into these early features.  \n4. The output of ASPP is then concatenated again with the original convolved f₁/f₂ streams, fusing pure low‐level detail and multi‐scale context.  \n5. Finally, this big concatenated tensor is run through an upsample‐conv (upsample → 3×3 conv → BN → ReLU) so that the resulting feature map matches the spatial resolution expected by the decoder.  \n\nBy (a) convolving the very early backbone features, (b) passing them through ASPP for context at multiple atrous scales, and then (c) merging (via simple concatenation) and upsampling, LDF delivers a dense, high‐resolution guidance signal into the later decoder stages—especially sharpening the prediction of small and thin parts."
    },
    {
        "question": "How does feeding M_O and M_E outputs as input channels compare to auxiliary loss–based guidance methods?",
        "relevant_section_ids": [
            "2",
            "3.1"
        ],
        "relevant_context": [
            "In contrast, our work OLAF adds object segmentation and edge information directly as additional channels to the input which is observed to be more beneficial.",
            "Conventional segmentation approaches typically include auxiliary tasks to learn foreground/background [44] and edges during training [74]. However, directly including foreground/background and edges as part of the input can be thought of as a structural inductive bias for the task. These masks provide strong boundary cues throughout the optimization process. In addition, they eliminate the issue of irregular gradient flow arising from ad-hoc scaling of task-related losses [14] in existing (RGB input only) approaches."
        ],
        "final_answer": "Feeding the M_O (foreground) and M_E (edge) masks as extra input channels acts as a structural inductive bias that provides strong boundary cues throughout training, avoids the irregular gradient‐scaling issues of auxiliary‐loss methods, and is empirically more beneficial than using those cues via auxiliary losses.",
        "relevant_elements": [
            "M_O",
            "M_E"
        ],
        "id": 1043,
        "masked_question": "How does feeding [mask1] and [mask2] outputs as input channels compare to auxiliary loss–based guidance methods?",
        "masked_number": 2,
        "masked_elements": [
            "M_O",
            "M_E"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02858v1_figure_1.png",
        "paperid": "2411.02858v1",
        "paper_path": "./papers/2411.02858v1.json",
        "figure_id": "2411.02858v1_figure_1.png",
        "caption": "Figure 1: The recipe for OLAF, our plug-and-play framework for enhanced multi-object multi-part scene parsing:  1 Augment RGB input with object-based channels (fg/bg, boundary edges) obtained from frozen pre-trained models (MO,MEsubscript𝑀𝑂subscript𝑀𝐸M_{O},M_{E}italic_M start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT)  2 Use Low-level Dense Feature guidance from segmentation encoder (LDF, shaded green)  3 Employ targeted weight adaptation for stable optimization with augmented input. We show that following this recipe leads to significant gains (up to 4.04.0\\mathbf{4.0}bold_4.0 mIoU) across multiple architectures and across multiple challenging datasets.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the frozen object‐segmentation net’s foreground/background mask (M_O), and Mask₂ is the frozen edge‐detection net’s boundary map (M_E).  Instead of tacking on an extra “object” or “edge” head and backpropagating a separate loss term, OLAF simply concatenates those two masks as two more input channels.  In practice that  \n• gives a constant, strong boundary/foreground prior throughout training,  \n• avoids fiddly loss‐weight tuning or irregular gradient flows from auxiliary objectives,  \n• yields better part‐segmentation accuracy than the usual auxiliary‐loss schemes."
    },
    {
        "question": "How does LDF's use of ASPP relate to existing multi-scale context extraction techniques?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To overcome these challenges, we introduce the Low-level Dense Feature Extractor (LDF) module. LDF leverages early blocks of the backbone network, where low-level information associated with small/thin parts are more prominent. To capture dense features of these small/thin parts, LDF includes (a) convolutional layers to enhance the features extracted from the initial stages of the backbone (b) an upsampling layer to maintain consistent feature map size (c) Atrous Spatial Pyramid Pooling (ASPP) [7] to capture contextual information at multiple scales (see Figure 3).",
            "This enables the model to extract dense low-level features at various spatial resolutions and consider context at different scales, including context relevant to small/thin parts."
        ],
        "final_answer": "LDF incorporates Atrous Spatial Pyramid Pooling (ASPP)—a well-known multi-scale context extraction module—directly into its low-level feature extractor. By applying ASPP to early backbone features, LDF captures context at multiple dilation rates (i.e. multiple spatial scales), much as prior approaches have done for high-level features, but here specifically to enrich the representation of small and thin parts.",
        "relevant_elements": [
            "LDF",
            "ASPP"
        ],
        "id": 1044,
        "masked_question": "How does [mask1]'s use of [mask2] relate to existing multi-scale context extraction techniques?",
        "masked_number": 2,
        "masked_elements": [
            "LDF",
            "ASPP"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02858v1_figure_1.png",
        "paperid": "2411.02858v1",
        "paper_path": "./papers/2411.02858v1.json",
        "figure_id": "2411.02858v1_figure_1.png",
        "caption": "Figure 1: The recipe for OLAF, our plug-and-play framework for enhanced multi-object multi-part scene parsing:  1 Augment RGB input with object-based channels (fg/bg, boundary edges) obtained from frozen pre-trained models (MO,MEsubscript𝑀𝑂subscript𝑀𝐸M_{O},M_{E}italic_M start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT)  2 Use Low-level Dense Feature guidance from segmentation encoder (LDF, shaded green)  3 Employ targeted weight adaptation for stable optimization with augmented input. We show that following this recipe leads to significant gains (up to 4.04.0\\mathbf{4.0}bold_4.0 mIoU) across multiple architectures and across multiple challenging datasets.",
        "qtype": "Literature_Background",
        "response": "Here’s the step-by-step reasoning:  \n1. In the figure, the red-boxed module ([mask1]) is the Low-level Dense Feature extractor (LDF).  \n2. The blue-boxed component ([mask2]) is an Atrous Spatial Pyramid Pooling (ASPP) block.  \n3. ASPP is a well-known “multi-scale context” mechanism (it applies parallel atrous/dilated convolutions at different rates to gather contextual cues over varying fields of view).  \n4. By embedding an ASPP block inside LDF, OLAF is effectively re-using this classic multi-scale context extractor—but on low-level (early‐stage) features instead of only at the final decoder.  \n5. In other words, OLAF’s LDF module “borrows” the same multi-scale aggregation idea as DeepLab-style ASPP, but applies it to preserve fine details and small/thin part cues coming out of the backbone’s first layers.  \n\nAnswer: OLAF’s LDF simply incorporates an ASPP block (i.e. [mask2]) so that it can pull in multi-scale context from low-level feature maps—much like existing architectures that use ASPP late in the decoder, but here applied early to boost small-part segmentation."
    },
    {
        "question": "How does the HE Adapter reconcile histogram equalization with transformer-based image encoder adaptation methodologies?",
        "relevant_section_ids": [
            "3.1",
            "3.1.1"
        ],
        "relevant_context": [
            "To facilitate the image encoder adaptation, we design an histogram equalization adapter laterally connected with the image encoder.",
            "The internal structure of the enhanced-image adapter module is presented in Fig. 3 (a), which mainly consists of a histogram equalization, a high-frequency filter and MLP blocks.",
            "Given that the features of water are not pronounced in most challenging scenarios, we first conduct histogram equalization operation to highlight the contrast and texture of input image.",
            "The enhanced image is then passed through a high-frequency filter to extract high-frequency information beneficial for segmentation, and converted into frequency patch embedding.",
            "The patch embedding of original input image is reduced in dimension by fully-connected layer (FC) and added to the frequency patch embedding.",
            "This fused feature is mapped by individual MLP blocks and one parameter-shared MLP, and then merged with the original features of each transformer block in the SAM image encoder."
        ],
        "final_answer": "The HE-Adapt module sits alongside the SAM transformer backbone. It first applies histogram equalization to boost contrast and texture, then uses a high-frequency filter to extract frequency-domain information. This frequency patch embedding is fused (via a small FC) with the standard patch embeddings of the input image. The combined embedding is run through MLP adapter blocks and then injected into each transformer block of the frozen SAM image encoder, effectively adapting the transformer to better capture water-related features.",
        "relevant_elements": [
            "HE Adapter Module",
            "Image Encoder"
        ],
        "id": 1045,
        "masked_question": "How does the [mask1] reconcile histogram equalization with transformer-based image encoder adaptation methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "HE Adapter Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.08109v1_figure_2.png",
        "paperid": "2407.08109v1",
        "paper_path": "./papers/2407.08109v1.json",
        "figure_id": "2407.08109v1_figure_2.png",
        "caption": "Figure 2: The proposed Large-Small Model Co-adapter Paradigm, which include a histogram equalization adapter, a triple-S prompt adapter and a dynamic prompt combiner. All components except the image encoder of SAM are trained for prompt generation, learning and adaptation, toward adverse waterlogging detection.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is nothing mystical or external to the transformer at all, but simply the “HE-Adapt” lateral adapter that sits alongside each ViT block of the frozen SAM image encoder.  In practice it works like this:\n\n 1.   Take the raw input image, run it through a standard histogram‐equalization step to boost local contrast, then apply a high-frequency filter so that edges and fine textures stand out.  \n 2.   Turn that enhanced, high-pass‐filtered image into patch embeddings exactly as the ViT would (“frequency patch embeddings”).  \n 3.   Meanwhile, take the ordinary patch embeddings from the unmodified image, run them through a small fully-connected layer to reduce their dimensionality, and add them to the frequency patch embeddings.  \n 4.   Pass that sum through a series of lightweight MLP “adapter” blocks (one per transformer layer plus one shared block) whose outputs are then *added back* into the corresponding transformer block’s main stream.  \n\nBy doing so, HE-Adapt “reconciles” histogram equalization (and high-frequency emphasis) with the frozen transformer: it never touches any of the ViT weights but quietly injects contrast- and texture-enhanced signals into each layer via tiny learnable MLP adapters."
    },
    {
        "question": "How does the Dynamic Prompt Combiner's adaptive embedding mechanism align with prior weighted prompt fusion methodologies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Dynamic Prompt Combiner",
            "Adaptive Embedding"
        ],
        "id": 1046,
        "masked_question": "How does the [mask1]'s adaptive embedding mechanism align with prior weighted prompt fusion methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "Dynamic Prompt Combiner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.08109v1_figure_2.png",
        "paperid": "2407.08109v1",
        "paper_path": "./papers/2407.08109v1.json",
        "figure_id": "2407.08109v1_figure_2.png",
        "caption": "Figure 2: The proposed Large-Small Model Co-adapter Paradigm, which include a histogram equalization adapter, a triple-S prompt adapter and a dynamic prompt combiner. All components except the image encoder of SAM are trained for prompt generation, learning and adaptation, toward adverse waterlogging detection.",
        "qtype": "Literature_Background",
        "response": "The little “mystery block” in the red‐boxed region of Fig. 2 is what the paper calls the adaptive embedding in the Dynamic Prompt Combiner (DPC).  If you unpack what the text says and look back at that diagram, you see that DPC does two things very familiar from earlier prompt‐fusion work:\n\n • It learns a separate scalar weight for each type of prompt (spatial, semantic, style)—exactly as many prior methods do when they “fuse” e.g. a text prompt and an image prompt by learning a weight for each channel.  \n • It then takes those three weighted prompt embeddings and concatenates them with one extra vector (“adaptive embedding”) that is itself learned end-to-end.  \n\nIn other words, just like earlier prompt‐fusion schemes taught the network to place more or less emphasis on different inputs by learning per-prompt weights, here we also learn w₁, w₂, w₃.  On top of that, the added adaptive embedding serves as a learnable bias (or “catch‐all” vector) so that the decoder can pick up any higher-order or non-linear interactions that a purely weighted sum would miss.  In that sense the mechanism is a direct generalization of classic weighted prompt fusion: we still learn scalar multipliers for each prompt, but we also give the model an extra trainable vector so it can flexibly capture any residual signal."
    },
    {
        "question": "How does the HE Adapter Module fuse frequency patch embeddings with original features across transformer blocks?",
        "relevant_section_ids": [
            "3.1.1"
        ],
        "relevant_context": [
            "The enhanced image is then passed through a high-frequency filter to extract high-frequency information beneficial for segmentation, and converted into frequency patch embedding. The patch embedding of original input image is reduced in dimension by fully-connected layer (FC) and added to the frequency patch embedding. This fused feature is mapped by individual MLP blocks and one parameter-shared MLP, and then merged with the original features of each transformer block in the SAM image encoder."
        ],
        "final_answer": "The HE Adapter first extracts a frequency patch embedding from the histogram‐equalized image, then reduces the dimensionality of the original patch embedding via a fully connected layer and adds it to the frequency embedding. The sum is passed through per-block MLPs (plus one shared MLP) and finally merged laterally into every transformer block’s features in the SAM image encoder.",
        "relevant_elements": [
            "HE Adapter Module",
            "Transformer Block"
        ],
        "id": 1047,
        "masked_question": "How does the [mask1] fuse frequency patch embeddings with original features across transformer blocks?",
        "masked_number": 1,
        "masked_elements": [
            "HE Adapter Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.08109v1_figure_2.png",
        "paperid": "2407.08109v1",
        "paper_path": "./papers/2407.08109v1.json",
        "figure_id": "2407.08109v1_figure_2.png",
        "caption": "Figure 2: The proposed Large-Small Model Co-adapter Paradigm, which include a histogram equalization adapter, a triple-S prompt adapter and a dynamic prompt combiner. All components except the image encoder of SAM are trained for prompt generation, learning and adaptation, toward adverse waterlogging detection.",
        "qtype": "Experimental_Results",
        "response": "Here is the reasoning step by step:\n\n1.  Identify the red‐boxed module.  \n    • In the diagram the red box surrounds what the text calls the “Histogram Equalization Adapter Module” (HE-Adapt).  \n\n2.  Locate its description in the text (Sec. 3.1.1).  \n    • It consists of three sub-steps:  \n       – histogram equalization on the input image,  \n       – a high-frequency filter to pull out detailed (edge/texture) information,  \n       – conversion of that high-frequency output into a “frequency patch embedding.”  \n\n3.  Understand how it aligns dimensions before fusion.  \n    • The original SAM patch embedding (i.e. the frozen image encoder’s patch output) is passed through a small fully-connected layer to reduce its channels so that it matches the channel dimension of the new frequency patch embedding.  \n\n4.  See how the two embeddings are fused.  \n    • Those two same-sized patch embeddings (the reduced original one and the new high-frequency one) are simply added together element-wise.  \n\n5.  Trace what happens to the sum.  \n    • The summed patch embedding then flows through a stack of lightweight MLP “adapter” blocks (one block dedicated to each transformer stage plus one shared MLP) to re-project it into the right feature space.  \n\n6.  Observe how it is injected back into the SAM encoder.  \n    • Finally, at every transformer block in the frozen image encoder, that re-projected, fused patch embedding is added in laterally (i.e. as a residual) to the block’s own feature map.  \n\nPutting that all together, the HE-Adapter fuses its high-frequency patch embeddings with the original SAM features by (a) bringing them to the same channel size via an FC, (b) summing them, (c) running the result through per-block adapter MLPs, and (d) injecting the adapted sum back into each transformer block as an additive residual."
    },
    {
        "question": "How does the dynamic prompt combiner adjust weights among spatial, semantic, and style prompt embeddings during training?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "The dynamic prompt combiner (DPC) is designed to find the optimal combination of the above three types of prompts. DPC comprises three sets of dynamic weights w₁, w₂, w₃ assigned to spatial, semantic and style prompt, respectively, and an adaptive embedding Eₗₑₐᵣₙₐbₗₑ to improve potential bias.",
            "During training, the weights are dynamically updated to encourage well-performing prompts while diminishing less-effective prompts."
        ],
        "final_answer": "The DPC maintains three learnable weights (w₁, w₂, w₃) over the spatial, semantic and style embeddings, and during training it dynamically updates these weights so that prompts that boost segmentation performance receive higher weights, while those that are less effective are down-weighted. The weighted embeddings (along with an added adaptive bias embedding) are then concatenated to form the final prompt.",
        "relevant_elements": [
            "Dynamic Prompt Combiner"
        ],
        "id": 1048,
        "masked_question": "How does the [mask1] adjust weights among spatial, semantic, and style prompt embeddings during training?",
        "masked_number": 1,
        "masked_elements": [
            "Dynamic Prompt Combiner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.08109v1_figure_2.png",
        "paperid": "2407.08109v1",
        "paper_path": "./papers/2407.08109v1.json",
        "figure_id": "2407.08109v1_figure_2.png",
        "caption": "Figure 2: The proposed Large-Small Model Co-adapter Paradigm, which include a histogram equalization adapter, a triple-S prompt adapter and a dynamic prompt combiner. All components except the image encoder of SAM are trained for prompt generation, learning and adaptation, toward adverse waterlogging detection.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in the figure is exactly what the paper calls the “Dynamic Prompt Combiner.”  What it does is very simple:\n\n 1. It maintains three separate, trainable weight vectors (or scalars) w₁, w₂ and w₃ — one for the spatial prompt, one for the semantic prompt and one for the style prompt.  \n 2. On each forward pass it performs an element-wise multiplication of each prompt embedding by its own weight:  \n      – spatial_embedding ← w₁ ⊙ spatial_embedding  \n      – semantic_embedding ← w₂ ⊙ semantic_embedding  \n      – style_embedding ← w₃ ⊙ style_embedding  \n 3. It also has a small “adaptive embedding” e_adp that it concatenates alongside the three weighted embeddings to allow the model to learn any residual or non-linear combination.  \n 4. All of these weights (w₁, w₂, w₃, and e_adp) are learned end-to-end by back-propagation, using the same segmentation losses that train the rest of the system.  In other words, if increasing the semantic prompt helps reduce the loss, then w₂ will grow; if the style prompt is unhelpful, w₃ will shrink.  \n\nIn this way, during training the combiner “turns up” the volume on whichever prompt type is most beneficial and “turns down” the others, automatically discovering the optimal mix of spatial, semantic and style cues."
    },
    {
        "question": "How does domain embedding via AdaIN in Denoising Network D_t encourage learning multiple diffusion trajectories?",
        "relevant_section_ids": [
            "3.1",
            "3.2",
            "4.4"
        ],
        "relevant_context": [
            "In Section 3.1: “We introduced a denoising diffusion model based multi-domain harmonization framework (Figure 1) which includes a domain invariant condition extractor and a denoising network. Both modules were controlled by the domain embedding which determines the diffusion trajectory.”",
            "In Section 3.2: “To correlate the domain embedding with the denoising network and the condition extractor, we utilized Adaptive Instance Normalization (AdaIN) to normalize each layer in the target layers of the modules.”",
            "In Section 4.4: “Domain embedding controls diffusion model to learn multiple diffusion trajectories at once which enables multi-domain harmonization with single model.”"
        ],
        "final_answer": "By injecting the domain embedding into each layer of the denoising U-Net via AdaIN, the network’s normalization statistics are modulated differently for each domain. This layer-wise modulation steers the denoising process along distinct, domain-specific pathways, thereby enabling the model to learn and execute multiple diffusion trajectories in a single network.",
        "relevant_elements": [
            "domain embedding",
            "AdaIN",
            "Denoising Network D_t"
        ],
        "id": 1049,
        "masked_question": "How does [mask1] via AdaIN in Denoising Network D_t encourage learning multiple diffusion trajectories?",
        "masked_number": 1,
        "masked_elements": [
            "domain embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.00807v1_figure_1.png",
        "paperid": "2409.00807v1",
        "paper_path": "./papers/2409.00807v1.json",
        "figure_id": "2409.00807v1_figure_1.png",
        "caption": "Figure 1: The overall framework of the proposed method with training and sampling details.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed “mystery block” in your diagram is nothing exotic—it is simply the domain embedding Z (sometimes written Zₛₒᵤᵣcₑ) that we pull off the frozen classifier and then inject into Dₜ via AdaIN.  By using AdaIN at every controlled layer of the U-Net we are effectively giving Dₜ a different set of affine (scale-and‐shift) parameters γ(d), β(d) for each domain d.  In other words, as you run the diffusion backward from t = T down to 0, the network’s normalization statistics get modulated in a domain‐specific way.  That modulation steers the network onto a slightly different denoising path—or “diffusion trajectory”—for each domain embedding.  Because those γ/β parameters are learned jointly, the model ends up with one U-Net that simultaneously encodes multiple noise‐removal schedules, simply by swapping in a different Z via AdaIN."
    },
    {
        "question": "What limitations arise from using edge maps as learned domain invariant conditions?",
        "relevant_section_ids": [
            "5.1"
        ],
        "relevant_context": [
            "For our experiment, the learned domain condition edge map for the diffusion model has been only tested for the images acquired under the same field strength and harmonization focused on the imaging texture heterogeneities.",
            "In this study, we did not perform the harmonization tasks on images acquired under different field strengths or incorporating the super-resolution, nor the reliability of learned domain condition edge map has been tested in those cases.",
            "Also, proposed method was only tested on the neuroimaging data, which is the focus of this study."
        ],
        "final_answer": "Using edge maps as the domain invariant condition was only validated on images acquired at a single field strength and focusing on texture heterogeneity. It has not been tested on data from different field strengths or with super-resolution, and its reliability in those scenarios remains unverified. Moreover, this approach has only been evaluated on neuroimaging data.",
        "relevant_elements": [
            "Domain invariant condition extractor C",
            "Edge map"
        ],
        "id": 1051,
        "masked_question": "What limitations arise from using [mask1] as learned [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Edge map",
            "Domain invariant condition extractor C"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.00807v1_figure_1.png",
        "paperid": "2409.00807v1",
        "paper_path": "./papers/2409.00807v1.json",
        "figure_id": "2409.00807v1_figure_1.png",
        "caption": "Figure 1: The overall framework of the proposed method with training and sampling details.",
        "qtype": "Others",
        "response": "Here’s the short answer to the masked question:\n\n  “Because the model’s learned, domain-invariant condition is simply an edge map, it has only been shown to work on images with the same field strength and only corrects for texture-type discrepancies.  It has never been validated for images from different field strengths, for super-resolution, or on non–neuroimaging data.  In other words, by using an edge map as the sole learned domain-invariant condition, the method is restricted to harmonizing within the narrow regime of same-field-strength, texture-based heterogeneity and may not generalize to other contrasts, resolutions, or modalities.”"
    },
    {
        "question": "How might alternative cross-attention mechanisms mitigate limitations of concat. in joint latent space?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "cross-attention",
            "concat."
        ],
        "id": 1053,
        "masked_question": "How might alternative [mask1] mechanisms mitigate limitations of concat. in joint latent space?",
        "masked_number": 1,
        "masked_elements": [
            "cross-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.08196v1_figure_1.png",
        "paperid": "2411.08196v1",
        "paper_path": "./papers/2411.08196v1.json",
        "figure_id": "2411.08196v1_figure_1.png",
        "caption": "Figure 1: (a) UNet-based models align text embeddings with image embeddings via cross-attention layers. In contrast, DiT creates a joint latent space by combining the text embedding and image embedding, then feeds them into the denoising block. (b) DiT’s has semantic disentangled latent space, where intensities of image semantics in generated images are controlled by separate directions, which can be easily identified.",
        "qtype": "Others",
        "response": "In DiT the simplest “fusion” of image- and text-latents is just to stick them side by side and let the self-attention layers figure out how to mix them.  In practice however this pure-concat strategy can leave certain attributes under- or over-conditioned and in our ablations we see that naïve concatenation alone doesn’t always faithfully preserve all the original semantics.  \n\nOne of the quickest remedies is to borrow the very cross-attention blocks that standard U-Net-based models use to inject text into vision.  Rather than merely concatenate, one can:  \n• insert cross-attention heads that use the text latents as “keys” and “values” and the image latents as “queries,” so that each image token only attends to exactly the bits of text it needs;  \n• interleave “self-attend–cross-attend” stacks so that the model learns a modular, layerwise gating of language information instead of a monolithic joint space;  \n• or even introduce small FiLM-style adapters or gating MLPs after each multihead block to dynamically weight how much text influences each spatial location.  \n\nAll of those fall under the broad umbrella of “cross-modal attention or modulation” as opposed to pure concatenation.  Empirically they tend to:  \n  – sharpen which semantics are passed from text to image (stronger conditioning on the target attribute),  \n  – reduce unintended drift in untouched attributes (better disentanglement),  \n  – and recover finer-grained control when you push beyond very light edits (mitigating the so-called semantic-loss phenomenon).  \n\nIn short, replacing or augmenting a big concat-and-self-attend block with explicit cross-attention (or gated FiLM‐style layers) restores much of the fine‐grained, disentangled control that pure concatenation alone struggles to guarantee."
    },
    {
        "question": "What ethical risks emerge when manipulating facial attributes via editing direction n?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "editing direction n"
        ],
        "id": 1054,
        "masked_question": "What ethical risks emerge when manipulating facial attributes via [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "editing direction n"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.08196v1_figure_1.png",
        "paperid": "2411.08196v1",
        "paper_path": "./papers/2411.08196v1.json",
        "figure_id": "2411.08196v1_figure_1.png",
        "caption": "Figure 1: (a) UNet-based models align text embeddings with image embeddings via cross-attention layers. In contrast, DiT creates a joint latent space by combining the text embedding and image embedding, then feeds them into the denoising block. (b) DiT’s has semantic disentangled latent space, where intensities of image semantics in generated images are controlled by separate directions, which can be easily identified.",
        "qtype": "Others",
        "response": "Here’s the core issue step by step:\n\n1.  In the right‐hand panel of Figure 1(b), the red box highlights a single “editing direction” n in the joint latent space—essentially a vector along which the model will shift facial attributes (e.g. smile intensity, apparent age, gender presentation, skin tone).\n\n2.  Because you can dial any person’s photo up or down along that one vector, you can very quickly produce highly realistic but manipulated faces—turning a neutral face into a big grin, making someone look older or younger, or even subtly shifting perceived race or gender cues.\n\n3.  As with any tool that can so effortlessly change protected or identity‐bearing traits, this opens a host of ethical risks:\n   •  Non-consensual deepfakes: creating videos or images of someone “smiling” or “angry” when they never did, for harassment, defamation, or political rumor.\n   •  Identity theft & fraud: altering a legitimate user’s face to fool biometrics or impersonate them online.\n   •  Bias amplification: if the editing vector itself encodes stereotyped associations (e.g. darker skin linked to certain expressions), you could reinforce harmful social biases.\n   •  Loss of authenticity & trust: widespread, undetectable face edits undermine confidence in any photo or video as evidence.\n\nIn short, giving anyone access to that single “n” vector makes it trivial to manufacture extremely convincing—but potentially deceptive—alterations of people’s faces, with all the attendant privacy, fairness and misinformation dangers."
    },
    {
        "question": "What advantage motivates concatenating image and text embeddings before self-attention in diffusion transformers?",
        "relevant_section_ids": [
            "1",
            "2.1"
        ],
        "relevant_context": [
            "In contrast, DiT’s self-attention architecture projects image and text inputs into a joint latent space, potentially enabling direct link between image semantics and text prompts.",
            "Before the de-noising loops, z_t and c are combined into a joint latent embedding and input into the de-noising transformer. Here, the image and text latent embedding dimension are chosen to be the same in order to project both modalities into the joint latent space."
        ],
        "final_answer": "By concatenating the image and text embeddings up‐front, diffusion transformers build a single joint latent space where visual and linguistic features attend to each other. This unified representation enables a direct link between image semantics and text prompts and underpins the improved controllability and semantic disentanglement observed in text–to–image generation.",
        "relevant_elements": [
            "concat.",
            "Self-Attn Blocks"
        ],
        "id": 1055,
        "masked_question": "What advantage motivates [mask1] image and text embeddings before [mask2] in diffusion transformers?",
        "masked_number": 2,
        "masked_elements": [
            "concat.",
            "Self-Attn Blocks"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.08196v1_figure_1.png",
        "paperid": "2411.08196v1",
        "paper_path": "./papers/2411.08196v1.json",
        "figure_id": "2411.08196v1_figure_1.png",
        "caption": "Figure 1: (a) UNet-based models align text embeddings with image embeddings via cross-attention layers. In contrast, DiT creates a joint latent space by combining the text embedding and image embedding, then feeds them into the denoising block. (b) DiT’s has semantic disentangled latent space, where intensities of image semantics in generated images are controlled by separate directions, which can be easily identified.",
        "qtype": "Design_Rationale",
        "response": "Here’s the high-level answer, mapping the red and blue boxes to their roles and spelling out why we do the red‐box step before the blue‐box step:\n\n• [mask1] is simply “concatenating (i.e. merging) the image and text token sequences into one joint sequence.”  \n• [mask2] is the stack of “self‐attention layers” (the de-noising transformer).  \n\nWhy fuse (concatenate) them before self‐attention? Because by doing so you get a single multimodal latent space over which every token—whether coming from the image or the text—can freely attend to every other token. This early fusion\n\n  - lets the model learn cross‐modal interactions directly,  \n  - yields a more disentangled semantic embedding (each attribute lives in its own “direction” in that joint space), and  \n  - gives you fine‐grained, controllable edits without extra cross‐attention modules or costly per‐attribute optimizations."
    },
    {
        "question": "What is the rationale for introducing a joint latent space instead of separate text and image embeddings?",
        "relevant_section_ids": [
            "1",
            "2.1"
        ],
        "relevant_context": [
            "Recently, Diffusion Transformers (DiT) introduced a new architecture that combines input image and text embeddings into a joint latent space and processes them through stacked self-attention layers.",
            "In contrast, DiT’s self-attention architecture projects image and text inputs into a joint latent space, potentially enabling direct link between image semantics and text prompts.",
            "Here, the image and text latent embedding dimension are chosen to be the same in order to project both modalities into the joint latent space. They are concatenated to obtain z, creating a joint latent space."
        ],
        "final_answer": "By embedding both modalities into the same space and processing them jointly with self-attention, the model can directly link image semantics and text prompts, uncover disentangled semantic subspaces, and thereby enable more precise, controllable editing than treating text and image embeddings separately.",
        "relevant_elements": [
            "joint latent space",
            "text embedding",
            "image embedding"
        ],
        "id": 1056,
        "masked_question": "What is the rationale for introducing a [mask1] instead of separate text and [mask2] embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "joint latent space",
            "image embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.08196v1_figure_1.png",
        "paperid": "2411.08196v1",
        "paper_path": "./papers/2411.08196v1.json",
        "figure_id": "2411.08196v1_figure_1.png",
        "caption": "Figure 1: (a) UNet-based models align text embeddings with image embeddings via cross-attention layers. In contrast, DiT creates a joint latent space by combining the text embedding and image embedding, then feeds them into the denoising block. (b) DiT’s has semantic disentangled latent space, where intensities of image semantics in generated images are controlled by separate directions, which can be easily identified.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In the UNet-based design, image embeddings zₜ and text embeddings c are kept in separate streams and only interact via cross-attention at each denoising step.  \n2. In DiT, by contrast, we “concatenate” the image and text codes into one combined representation—the [mask1]—before feeding them through the same self-attention layers.  \n3. Empirically, we see two failure modes when one modality is left isolated:  \n   – Editing only zₜ (“pure image pathway”) suffers from the “semantic loss” problem (race, glasses, etc. get randomized).  \n   – Editing only c (“pure text pathway”) can’t introduce certain details (e.g. adding a colored coat).  \n4. By merging both into a unified space, DiT preserves all semantics from the forward noising of zₜ (avoiding semantic washout) and still lets text-derived directions (computed from differences of c vectors) exert precise control.  \n5. Moreover, in this joint latent space the different attributes naturally occupy nearly orthogonal subspaces, so you can disentangle and edit each attribute with a simple linear shift along a text-determined direction.  \n\nAnswer:\n\nThey introduce a joint latent space instead of keeping text and image embeddings separate because only in a shared, unified representation can (a) the image’s semantics survive the noisy forward pass, (b) text-driven editing directions be faithfully applied, and (c) different attributes emerge as nearly orthogonal subspaces that can be independently manipulated."
    },
    {
        "question": "What is the rationale for clustering diverse plans into representative plans using embedding?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "After brainstorming through multiple sampling, we select m representative plans as candidates. Specifically, we first divide all k samples into m clusters using a text embedding model and the classical k-means++ algorithm (Arthur and Vassilvitskii, 2007), and then select the plan closest to the cluster centroid from each cluster (Line 3).",
            "Intuitively, the Navigator groups similar plans together and selects representative ones, ensuring a diverse set of high-level solution plans."
        ],
        "final_answer": "By embedding and clustering the sampled plans, the Navigator groups similar solutions together and picks the centroid plan from each cluster. This process ensures that the remaining candidate plans are both representative and diverse, avoiding near-duplicate strategies while covering different high-level solution approaches.",
        "relevant_elements": [
            "Diverse plans",
            "Embedding",
            "Clustering"
        ],
        "id": 1057,
        "masked_question": "What is the rationale for clustering [mask1] into representative plans using embedding?",
        "masked_number": 1,
        "masked_elements": [
            "Diverse plans"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.05001v1_figure_2.png",
        "paperid": "2409.05001v1",
        "paper_path": "./papers/2409.05001v1.json",
        "figure_id": "2409.05001v1_figure_2.png",
        "caption": "Figure 2. Overview of our PairCoder, in which a Navigator agent and a Driver agent collaborate on code generation.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1. In Step 1 the Navigator “brainstorms” many candidate solution plans by sampling the LLM at a non‐zero temperature.  This produces a large pool of plans, some of which may be near‐duplicates or slight variants of one another.  \n2. We only want to carry forward a small set (say k) of truly distinct, high‐level strategies (e.g. “brute‐force,” “greedy,” “two‐pointer,” “DP,” etc.), not dozens of nearly identical plans.  \n3. To automatically identify which plans are semantically similar, we embed each plan text into a vector space.  In that space, plans that “mean” the same thing end up close together.  \n4. We then run k-means++ on those embeddings, forming k clusters of similar plans.  \n5. From each cluster we pick the single plan closest to the centroid.  Those k centroids become our representative plans.  \n\nBy clustering in embedding space we therefore (a) collapse redundant variants, (b) ensure broad coverage of different solution ideas, and (c) limit the downstream search to a small, diverse set of high‐level plans."
    },
    {
        "question": "What motivates leveraging historical memory and execution feedback to decide plan changes?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We observe that code refinement tends to get stuck in a dead-end loop if the generated code or execution feedback has already occurred in the past.",
            "Therefore, we introduce a long-term memory module to systematically store and maintain the coding and execution history under the current solution plan.",
            "Given the buggy code and its execution feedback, the current solution plan will be considered unpromising if any of them has already occurred in the historical memory, leading to a re-selection of the optimal plan."
        ],
        "final_answer": "Because code refinement can become trapped in repetitive dead-end loops when the same buggy code or feedback recurs, the framework uses historical memory and execution feedback to detect unpromising plans and trigger plan changes.",
        "relevant_elements": [
            "historical memory",
            "execution feedback"
        ],
        "id": 1058,
        "masked_question": "What motivates leveraging [mask1] and execution feedback to decide plan changes?",
        "masked_number": 1,
        "masked_elements": [
            "historical memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.05001v1_figure_2.png",
        "paperid": "2409.05001v1",
        "paper_path": "./papers/2409.05001v1.json",
        "figure_id": "2409.05001v1_figure_2.png",
        "caption": "Figure 2. Overview of our PairCoder, in which a Navigator agent and a Driver agent collaborate on code generation.",
        "qtype": "Design_Rationale",
        "response": "Mask1 refers to the “historical memory” box in step 5.  The paper explains that, in practice, LLM‐driven code refinement will often cycle through the same buggy programs and error messages if you stick to one plan.  By recording every (program, feedback) pair in a long-term memory and checking, at each iteration, whether the new buggy code or error has already occurred, the Navigator can detect when it’s simply going in circles.  As soon as it sees a repeat in the historical memory, it treats the current plan as unpromising and switches to a fresh plan, thereby avoiding dead-end loops and wasted iterations."
    },
    {
        "question": "How does clustering group plan embeddings to ensure diversity in representative plan selection?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "After brainstorming through multiple sampling, we select k representative plans as candidates. Specifically, we first divide all m samples into k clusters using a text embedding model and the classical k-means++ algorithm (Arthur and Vassilvitskii, 2007), and then select the plan closest to the cluster centroid from each cluster (Line 3).",
            "Intuitively, the Navigator groups similar plans together and selects representative ones, ensuring a diverse set of high-level solution plans."
        ],
        "final_answer": "The Navigator embeds each sampled plan into a vector space, runs k-means++ to partition the m embedded plans into k clusters, and then picks the single plan whose embedding is closest to each cluster’s centroid. By choosing one plan per cluster, it ensures that the final set of k plans covers diverse strategies rather than many similar ones.",
        "relevant_elements": [
            "Embedding",
            "Clustering"
        ],
        "id": 1059,
        "masked_question": "How does [mask1] group plan [mask2] to ensure diversity in representative plan selection?",
        "masked_number": 2,
        "masked_elements": [
            "Clustering",
            "Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.05001v1_figure_2.png",
        "paperid": "2409.05001v1",
        "paper_path": "./papers/2409.05001v1.json",
        "figure_id": "2409.05001v1_figure_2.png",
        "caption": "Figure 2. Overview of our PairCoder, in which a Navigator agent and a Driver agent collaborate on code generation.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the clustering module and Mask2 is the plan embeddings.  Concretely, after sampling N plans we first turn each plan into a fixed-length vector via a text-embedding model.  We then run k-means++ clustering on those embedding vectors to form M clusters of “similar” plans.  Finally, we pick the plan whose embedding lies nearest to each cluster centroid.  By carving the embedding space into distinct regions and choosing one plan per region, we guarantee that the M representatives are as diverse as possible."
    },
    {
        "question": "How are test outcomes from Perform code testing used by Direct next iteration to update plan or repair strategy?",
        "relevant_section_ids": [
            "3.3",
            "3.2"
        ],
        "relevant_context": [
            "In contrast to the high-level planning of the Navigator, the Driver agent focuses all its attention on specific code tasks, including generating initial code guided by a new plan (Step 3), testing code on public test cases (Step 4), and repairing the buggy code (Step 6). … If the execution feedback is Pass, we will terminate the iterative process and consider P as the final output (Line 16); Otherwise, the Driver will deliver the current program P and execution feedback r to the Navigator, which are used to direct the next iteration in Step 5.",
            "Once the generated code P in the last iteration does not pass all the public test cases T, it is the Navigator’s turn to direct the next iteration. Instead of stubbornly persisting in a single solving path to repair the incorrect code … the Navigator can timely adjust the solution plan to seek a turnaround. … We apply a simple but effective heuristic strategy to determine whether to change the solution plan. Given the buggy code and its execution feedback, the current solution plan will be considered unpromising if any of them has already occurred in the historical memory … leading to a re-selection of the optimal plan. … Another potential iteration direction is to repair the buggy code, which pursues gradual progress without abandoning a promising solution plan. Based on the execution feedback, the Navigator leverages the reasoning ability of LLMs to propose a directive repair strategy."
        ],
        "final_answer": "After the Driver runs the code on the public tests, it classifies the outcome (Pass, Runtime Error, Wrong Answer or Time Limit Exceeded) and hands the buggy program plus the execution feedback back to the Navigator. The Navigator then consults its long-term memory to see if the same bug or feedback has already occurred under the current plan. If it has, the plan is deemed unpromising, discarded, and a new plan is selected; otherwise the Navigator uses the specific type of feedback to prompt the LLM to generate a tailored repair strategy (e.g. syntax/exception fixes for runtime errors, logic corrections for wrong answers, performance optimizations for time-outs).",
        "relevant_elements": [
            "Perform code testing",
            "Direct next iteration"
        ],
        "id": 1060,
        "masked_question": "How are test outcomes from [mask1] used by Direct next iteration to update plan or repair strategy?",
        "masked_number": 1,
        "masked_elements": [
            "Perform code testing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.05001v1_figure_2.png",
        "paperid": "2409.05001v1",
        "paper_path": "./papers/2409.05001v1.json",
        "figure_id": "2409.05001v1_figure_2.png",
        "caption": "Figure 2. Overview of our PairCoder, in which a Navigator agent and a Driver agent collaborate on code generation.",
        "qtype": "Implementation_Details",
        "response": "Here’s what happens at the red-boxed “Perform code testing” stage and how its outcomes drive Step 5 (“Direct next iteration”):\n\n1.  After the Driver generates a candidate solution, it runs it on the public test suite and classifies the result as one of:\n    – Pass  \n    – Runtime Error  \n    – Wrong Answer  \n    – Time Limit Exceeded  \n\n2.  Those labels (the “execution feedback”) are handed back to the Navigator along with the attempted code and a log of all prior (code, feedback) pairs.\n\n3.  In Step 5 the Navigator does two things with that information:\n\n    a.  **Check historical memory.**  \n        – If the *exact same* failure (e.g. “Wrong Answer on test X” or “TLE on input Y”) has already occurred under the *same* high-level plan, the Navigator marks that plan as “stuck,” discards it, resets the memory, and goes back to Step 2 to pick a *new* plan.  \n\n    b.  **Otherwise, stay on the current plan and repair.**  \n        – The Navigator prompts the LLM to produce a *targeted repair strategy* tailored to the failure type:  \n          • For Runtime Errors → fix syntax/exception issues (e.g. index bounds, null checks)  \n          • For Wrong Answers → adjust logic branches or data‐flow  \n          • For TLE → optimize loops or data structures  \n        – That repair strategy is then fed to the Driver in Step 6 to patch the code.\n\n4.  If at testing the outcome had been “Pass,” the Navigator simply halts and returns that solution.\n\nIn short, the pass/error verdicts from the red-boxed test step are the *sole signal* the Navigator uses to decide: “Have we tried this plan + failure before? If yes → switch plans. If no → issue a targeted repair strategy.”"
    },
    {
        "question": "How does preprocessing use padding and deduplication to standardize contour sketches and remove redundancies?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Generated contour sketches are not directly suitable for subsequent processes. By padding, we ensure all sketches are presented in appropriate proportions.",
            "Given that most mechanical components exhibit symmetry, the same sketch may be rendered from different perspectives. We utilize ImageHash technology for deduplication."
        ],
        "final_answer": "Preprocessing first pads each contour sketch to a consistent size and aspect ratio so that all sketches are standardized in proportion. Then it applies ImageHash–based deduplication to detect and remove sketches that are effectively duplicates (e.g., arising from symmetric viewpoints), leaving only unique, informative contours for downstream processing.",
        "relevant_elements": [
            "Padding",
            "Deduplication"
        ],
        "id": 1061,
        "masked_question": "How does preprocessing use [mask1] and deduplication to standardize contour sketches and remove redundancies?",
        "masked_number": 1,
        "masked_elements": [
            "Padding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.05966v2_figure_2.png",
        "paperid": "2408.05966v2",
        "paper_path": "./papers/2408.05966v2.json",
        "figure_id": "2408.05966v2_figure_2.png",
        "caption": "Figure 2. An overview of our method. (1) Stage-One: we generate contour sketches based on 26 viewpoints (represented by colorful points) of a cube (grey) . After that, Preprocessing and View Selection export appropriate contour sketches.\n(2) Stage-Two: By receiving initial strokes and features captured by our encoder from regular contour sketch, the stroke generator produces a set of strokes, which are next fed to a differentiable rasterizer to create a vector freehand sketch.",
        "qtype": "Implementation_Details",
        "response": "In our Stage-One pipeline, the little red‐boxed step in the “Preprocessing” module is simply padding, and it works in tandem with a very lightweight deduplication pass to (1) bring every generated contour into the same size and framing, and (2) throw away any repeats.  Concretely:  \n1. Padding: each raw render is “padded” (i.e. letter-boxed or zero-padded) to a fixed canvas size and aspect ratio so that every contour sketch sits in exactly the same coordinate frame and resolution.  This guarantees that downstream stages always see sketches with identical dimensions and centering.  \n2. Deduplication: once all 26 views have been rendered and padded, we compute a perceptual hash (via ImageHash) on each image.  Any two sketches whose hashes collide (or lie beneath a small Hamming‐distance threshold) are treated as duplicates and one is discarded.  \n\nTogether, these two steps enforce a uniform input format and strip away redundant (symmetry‐induced) sketches before we even run our learned viewpoint selector."
    },
    {
        "question": "How does stroke generator leverage encoder outputs and initial strokes to progressively generate freehand sketches?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "As illustrated in Figure 2, freehand sketch generator consists of four components: an encoder, a stroke generator, a guidance sketch generator, and a differentiable rasterizer.",
            "Our encoder utilizes CLIP ViT-B/32 (Radford et al., 2021) and an adapter to extract essential vision and semantic information from input.",
            "Our stroke generator consists of eight transformer decoder layers and two MLP decoder layers. During training, to guarantee the stroke generator learns features better, process sketches (K=8 in this paper) extracted from each intermediate layer are guided by guidance sketches generated at the corresponding intermediate step of the optimization process in the guidance sketch generator. In the inference phase, the stroke generator optimizes initial strokes generated from trainable parameters into a set of n Bézier curves. These strokes are then fed into the differentiable rasterizer to produce a vector sketch."
        ],
        "final_answer": "The stroke generator first receives the encoded contour‐sketch features from the CLIP ViT-B/32 + adapter encoder together with a small set of learned initial stroke vectors. These are fed into a sequence of eight transformer decoder layers (with two final MLP decoders) that cross-attend to the encoder outputs and iteratively refine the stroke representations. At each decoder layer, intermediate “process sketches” are extracted and compared against corresponding guidance sketches to guide learning. In inference, the same decoder stack progressively transforms the initial strokes into final Bézier‐curve control points, which are then rasterized to produce the freehand sketch.",
        "relevant_elements": [
            "Encoder",
            "Stroke Generator"
        ],
        "id": 1062,
        "masked_question": "How does [mask1] leverage [mask2] outputs and initial strokes to progressively generate freehand sketches?",
        "masked_number": 2,
        "masked_elements": [
            "Stroke Generator",
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.05966v2_figure_2.png",
        "paperid": "2408.05966v2",
        "paper_path": "./papers/2408.05966v2.json",
        "figure_id": "2408.05966v2_figure_2.png",
        "caption": "Figure 2. An overview of our method. (1) Stage-One: we generate contour sketches based on 26 viewpoints (represented by colorful points) of a cube (grey) . After that, Preprocessing and View Selection export appropriate contour sketches.\n(2) Stage-Two: By receiving initial strokes and features captured by our encoder from regular contour sketch, the stroke generator produces a set of strokes, which are next fed to a differentiable rasterizer to create a vector freehand sketch.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the stroke‐generator block (the red‐boxed module in Fig. 2) and Mask2 is the ViT-based encoder (the blue box).  At a high level the stroke generator works exactly like a transformer decoder that is seeded with a small set of initial Bézier strokes and then “talks to’’ the encoder’s global feature vector at every layer to gradually refine them:\n\n 1.  Encoder (Mask2) takes the selected contour sketch and, via CLIP’s ViT-B/32 plus a tiny adapter, produces a fixed-length feature embedding that carries all of the sketch’s geometric and semantic information.  \n 2.  Meanwhile we learn a handful of initial Bézier curves (their control-point parameters are trainable) and feed those as the decoder’s input tokens.  \n 3.  Each of the eight transformer-decoder layers in the stroke generator (Mask1) performs self-attention over the current stroke tokens and cross-attention against the encoder’s feature embedding.  In this way every layer “sees’’ both what the sketch looks like and where its strokes already lie, and it emits a small update to each curve’s parameters.  \n 4.  After each decoder layer we collect the set of Bézier curves (the so-called process sketch) and, during training only, compare it to a guidance sketch via a matching‐plus-Hausdorff loss to force it toward the expert‐generated shape.  \n 5.  Finally, the last layer’s hidden states are passed through two MLP heads which output the final n×8 control‐point vectors.  A differentiable rasterizer then turns those Béziers into the completed freehand sketch.  \n\nIn short, Mask1 progressively refines a few trainable initial strokes by repeatedly attending to and updating them with the global contour features provided by Mask2, yielding a stable, geometrically faithful freehand sketch after eight rounds of decoding."
    },
    {
        "question": "How can the view selection module adapt traditional occluding contour methods for selecting informative viewpoints?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As shown in Figure 2 Stage-One, we can imagine placing a mechanical component within a cube and selecting centers of the six faces, midpoints of the twelve edges, and eight vertices of the cube as 26 viewpoints. Subsequently, we use PythonOCC (Paviot, 2018 ###reference_b47###), a Python wrapper for the CAD-Kernel OpenCASCADE, to infer engineering modeling information and render regular contour sketches of the model from these 26 viewpoints.",
            "Generated contour sketches are not directly suitable for subsequent processes. … Therefore, we design a viewpoint selector based on ICNet (Zhao et al., 2018 ###reference_b70###), which is trained by excellent viewpoint sketches picked out by modeling experts, to simulate the viewpoint selection task engineers face during sketching."
        ],
        "final_answer": "The module first generates occluding‐contour sketches from 26 canonical views (faces, edges, and corners of a bounding cube) via the OpenCASCADE kernel. It then applies an ICNet‐based selector—trained on expert‐labeled “good” sketches—to pick the most informative contour views for downstream sketch generation.",
        "relevant_elements": [
            "Contour Sketch Generator",
            "View Selection"
        ],
        "id": 1063,
        "masked_question": "How can the [mask1] module adapt traditional occluding contour methods for selecting informative viewpoints?",
        "masked_number": 1,
        "masked_elements": [
            "View Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.05966v2_figure_2.png",
        "paperid": "2408.05966v2",
        "paper_path": "./papers/2408.05966v2.json",
        "figure_id": "2408.05966v2_figure_2.png",
        "caption": "Figure 2. An overview of our method. (1) Stage-One: we generate contour sketches based on 26 viewpoints (represented by colorful points) of a cube (grey) . After that, Preprocessing and View Selection export appropriate contour sketches.\n(2) Stage-Two: By receiving initial strokes and features captured by our encoder from regular contour sketch, the stroke generator produces a set of strokes, which are next fed to a differentiable rasterizer to create a vector freehand sketch.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module in Figure 2 is nothing mystical – it is simply our learned “view-selector” that turns the old idea of “occluding‐contour extraction” into a data-driven ranking over camera poses.  Concretely:\n\n 1.  We first place the CAD model in a virtual cube and render pure contour maps (silhouettes + feature edges) at the 26 canonical viewing positions (face centers, edge‐midpoints and corners).\n\n 2.  After normalizing (padding) and throwing away exact duplicates (via an ImageHash test), we feed each of those 26 contour images into a lightweight ICNet‐based network.\n\n 3.  That network has been trained on a small “ground-truth” set of expert‐chosen sketches, so it learns to score each view by how “informative” its occluding contours are.\n\n 4.  Finally we simply take the top K views by score as our output.  \n\nIn this way the module “adapts” the classic occluding‐contour idea – instead of hand-picking front/side/top, it exhaustively samples 26 silhouettes and then learns, via ICNet, which ones carry the most geometric information for freehand sketching."
    },
    {
        "question": "How does the Distribution Selection Module incorporate Softmax outputs into Gumbel-Softmax sampling?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "The output of the optimal distribution selection network is formulated as follows: a = softmax(Wᵀh + b).",
            "However, this discrete selection is non-differentiable due to the argmax operation. To tackle this, we employ the straight-through Gumbel-softmax (Jang et al., 2016): ŷ = softmax((log α + g)/τ), where τ is the temperature parameter, which controls the approximation degree between the Gumbel-softmax distribution and the discrete distribution."
        ],
        "final_answer": "The module first computes a softmax probability vector α from the MLP outputs. It then draws Gumbel noise g, adds it to log α, divides by a temperature τ, and applies softmax again. This straight-through Gumbel-Softmax step produces a differentiable, approximately one-hot selection mask from the original softmax probabilities.",
        "relevant_elements": [
            "Distribution Selection Module",
            "Gumbel-Softmax"
        ],
        "id": 1065,
        "masked_question": "How does the [mask1] incorporate Softmax outputs into Gumbel-Softmax sampling?",
        "masked_number": 1,
        "masked_elements": [
            "Distribution Selection Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.08585v1_figure_2.png",
        "paperid": "2408.08585v1",
        "paper_path": "./papers/2408.08585v1.json",
        "figure_id": "2408.08585v1_figure_2.png",
        "caption": "Figure 2. The overall framework of our proposed OptDist.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the little MLP‐and‐Softmax block in the red box – our Distribution Selection Module.  Its job is to turn the shared representation h into a probability vector α = softmax(MLP(h)), and then use exactly those α values as the “logits” in a straight‐through Gumbel–Softmax.  Concretely:\n\n 1. Compute α₁…α_L = softmax(MLP(h))  \n 2. Draw Gumbel noise g_i ∼ Gumbel(0,1) for each i   \n 3. Form the “noisy logits” (log α_i + g_i)/τ and apply a softmax over i   \n 4. Use that softmax both as a differentiable approximation during back‐prop and (via a straight‐through argmax) as a one‐hot mask y_p  \n\nIn this way the raw Softmax outputs α_i become the base probabilities that get “Gumbelized” (noisy and temperature‐scaled) to produce a hard, differentiable selection of exactly one sub-distribution for each user."
    },
    {
        "question": "How does the Alignment mechanism balance hard and soft pseudo labels when optimizing DSM?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "By normalizing the loss values generated by different SDNs for each user, the alignment mechanism can generate pseudo labels to guide the training of the DSM, reducing the difficulty of searching for the optimal sub-distribution for DSM. As Fig. 3 illustrated, when a set of loss values on possible distribution L is given, we can obtain the hard pseudo labels y^p_u from these loss values. First, the hard label y^p_u can construct a cross-entropy loss. In addition, considering that in practical applications, the classification of CLTV is imbalanced, which may result in the cross-entropy of high-value users being overlooked, making it challenging for DSM to distinguish them. Therefore, to mitigate this issue, we have introduced a variant of focal weight (Lin et al., 2017) in the cross-entropy loss.",
            "Then, we generate soft labels y^ω_u based on the losses for each sub-distribution: the larger the ℓ_{u,i}, the more suitable the i-th sub-distribution is for user u according to DLM. Then, we adopt Kullback–Leibler (KL) divergence (Bishop, [n.d.]) between DLM and DSM. The advantage of considering both hard and soft labels here lies in that a hard label can make DSM focus on DLM information while ignoring other label information, which is complemented by soft labels."
        ],
        "final_answer": "The alignment mechanism first derives a hard pseudo label (the best sub-distribution according to normalized losses) and uses it in a focal-weighted cross-entropy loss to train DSM. It also forms a soft pseudo-label distribution by normalizing the per-sub-distribution losses and applies a KL-divergence loss to align DSM’s output to this soft distribution. By summing both the focal-weighted cross-entropy on the hard labels and the KL-divergence on the soft labels, DSM is guided by both crisp and smooth supervisory signals.",
        "relevant_elements": [
            "Alignment",
            "DSM"
        ],
        "id": 1066,
        "masked_question": "How does the [mask1] mechanism balance hard and soft pseudo labels when optimizing DSM?",
        "masked_number": 1,
        "masked_elements": [
            "Alignment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.08585v1_figure_2.png",
        "paperid": "2408.08585v1",
        "paper_path": "./papers/2408.08585v1.json",
        "figure_id": "2408.08585v1_figure_2.png",
        "caption": "Figure 2. The overall framework of our proposed OptDist.",
        "qtype": "Literature_Background",
        "response": "The red–boxed “alignment” block is nothing mystical – it is simply a two‐part pseudo-labeling loss that simultaneously “pins” the DSM onto a single best sub-distribution (the hard label) and “nudges” it to respect the relative quality of all of them (the soft label).  Concretely, at every training step:\n\n 1.  We take the vector of losses \\{ℓ_{u1},…,ℓ_{uL}\\} that the DLM’s L sub-networks just incurred on user u and  \n    a)  pick the index of the minimum loss to form a one-hot (hard) pseudo-label y^p_u  \n    b)  run a softmax (or temperature-controlled softmax) over the *negative* losses to form a soft pseudo-label y^ω_u  \n\n 2.  We feed the same user representation into the DSM and get its predicted weight distribution α_u (via Gumbel-softmax during training).  \n\n 3.  We then penalize α_u in two ways:  \n    •  A focal-weighted cross-entropy against the one-hot y^p_u, forcing DSM to *commit* to the single best network, and  \n    •  A KL-divergence against the soft vector y^ω_u, making sure DSM also “remembers” how the other candidates compared.  \n\nBy summing those two losses, the DSM learns to make sharp, discrete selections (hard labels) without completely discarding the “second-best” signals (soft labels)."
    },
    {
        "question": "How does alignment mechanism leverage hard and soft pseudo labels to optimize DSM?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "By normalizing the loss values generated by different SDNs for each user, the alignment mechanism can generate pseudo labels to guide the training of the DSM, reducing the difficulty of searching for the optimal sub-distribution for DSM.",
            "First, the hard label y^p can construct a cross-entropy loss. In addition, considering that in practical applications, the classification of CLTV is imbalanced, which may result in the cross-entropy of high-value users being overlooked, making it challenging for DSM to distinguish them. Therefore, to mitigate this issue, we have introduced a variant of focal weight (Lin et al., 2017). The loss function can be defined as:",
            "Then, we generate soft labels based on the losses for each sub-distribution: The larger the y^ω_u, the more suitable the j-th sub-distribution is for user u according to DLM. Then, we adopt Kullback-Leibler (KL) divergence (Bishop, [n.d.]) between DLM and DSM:",
            "The advantage of considering both hard and soft labels here lies in that a hard label can make DSM focus on DLM information while ignoring other label information, which is complemented by soft labels."
        ],
        "final_answer": "The alignment mechanism first normalizes the per‐SDN losses for each user and selects the sub‐distribution with the lowest loss to form a one-hot (hard) pseudo label y^p_u, which is used to supervise the DSM via a focal-weighted cross-entropy loss. It then converts the same normalized losses into a soft label distribution y^ω_u and aligns the DSM’s output to this soft distribution by minimizing their KL divergence. In this way the hard labels force DSM to focus on the best sub‐distribution while the soft labels provide additional gradient signal from all candidates.",
        "relevant_elements": [
            "Alignment Mechanism",
            "Distribution Selection Module"
        ],
        "id": 1068,
        "masked_question": "How does [mask1] leverage hard and soft pseudo labels to optimize DSM?",
        "masked_number": 1,
        "masked_elements": [
            "Alignment Mechanism"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.08585v1_figure_2.png",
        "paperid": "2408.08585v1",
        "paper_path": "./papers/2408.08585v1.json",
        "figure_id": "2408.08585v1_figure_2.png",
        "caption": "Figure 2. The overall framework of our proposed OptDist.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed component is nothing more exotic than the “alignment mechanism” between the DLM and the DSM.  In that box you can see that, for each user, we take the vector of losses {ℓᵤ₁, …, ℓᵤL} produced by the L sub‐distribution networks and turn it into two kinds of pseudo‐labels:\n\n 1.  A hard one‐hot label yᵤᵖ : we pick the index k with minimal normalized loss and set yᵤᵖₖ=1, the rest zero.  \n 2.  A soft label yᵤʷ : we apply a (temperature-controlled) softmax over –ℓᵤ to get a probability distribution over all L choices.  \n\nWe then train the DSM’s softmax output αᵤ by  \n •  A focal-weighted cross-entropy against the one-hot hard label yᵤᵖ  \n •  A KL-divergence against the soft label yᵤʷ  \n\nSumming those two terms gives the alignment loss Lᵤ that is back-propagated into the DSM.  In this way the DSM is explicitly nudged both to pick the single best sub‐distribution (via the hard label) and to match the full performance profile of all sub‐distributions (via the soft label)."
    },
    {
        "question": "How does the optical-flow model interact with the temporal module to stabilize video predictions?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "“we design a novel training strategy (Fig. 4) that employs two different types of losses: A regularization loss that forces the model to produce results aligned with the image model, and an optical flow based stabilization loss as described in Section 3.2.”",
            "“The overall training loss is: L = λ L_regularize + L_stable, where λ is the weight for per-frame regularization with pretrained single-view depth or normal predictors, and L_stable is the optical flow based temporal stabilization loss defined in Sec. 3.2.”",
            "“During training, a fixed pre-trained image model and an optical flow model are also deployed aside from the trained video model. We calculate the single frame prediction and the optical flow maps in a just-in-time manner.”",
            "“We apply a pre-trained optical flow estimator to calculate the correspondence between adjacent frames for the temporal consistency stabilization. Specifically, given the predicted optical flow maps between two adjacent frames F_{i→i+1} and F_{i→i−1}, a stabilization loss between the two frames can be defined as: L_stable = …”",
            "“To prevent that [inaccurate flows] from harming the effectiveness of the loss, we add two filtering methods to curate the correctly corresponded pixels across the frames.”"
        ],
        "final_answer": "The video model contains lightweight temporal blocks inserted between the frozen image‐model layers; these temporal blocks are the only parts of the network receiving gradients during training. A pre-trained optical-flow estimator runs alongside the video model to predict pixel correspondences between adjacent frames. Those flow maps are used to warp one frame’s depth (or normal) prediction into the coordinate frame of its neighbor, and the difference between the warped prediction and the neighbor’s own prediction defines an optical-flow stabilization loss. This loss is back-propagated through the video model—specifically through the temporal blocks—thereby forcing the temporal module to learn to produce outputs that remain consistent across time.",
        "relevant_elements": [
            "Pre-trained Video Optical-Flow Model",
            "Temporal Module"
        ],
        "id": 1069,
        "masked_question": "How does the optical-flow model interact with the [mask1] to stabilize video predictions?",
        "masked_number": 1,
        "masked_elements": [
            "Temporal Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17249v1_figure_2.png",
        "paperid": "2411.17249v1",
        "paper_path": "./papers/2411.17249v1.json",
        "figure_id": "2411.17249v1_figure_2.png",
        "caption": "Figure 2: Visualization of Our Training Pipeline. Our pipeline consists of three branches: an optical flow network that extracts optical flow from input video to guide temporal smoothness; a fixed single-frame image model for regularization, and the trained video model that integrates a fine-tuned image backbone with temporal layers.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the flow network “hooks into” the video‐model branch (the red-boxed module) to enforce temporal smoothness:\n\n1. At each training step you feed two adjacent RGB frames into the fixed, pre-trained optical-flow network.  It spits out a flow field F that tells you, for every pixel in frame t, where that pixel moved in frame t+1.  \n2. In parallel you run your video model (the green backbone plus its red-boxed temporal module) on both frames, producing two predicted depth (or normal) maps Dt and Dt+1.  \n3. You then use F to warp Dt forward into the coordinate frame of Dt+1.  Call that warped map W(Dt).  \n4. The “stabilization loss” Lstable is simply the difference between the warped prediction W(Dt) and the direct prediction Dt+1, over only those pixels whose flow you trust.  Minimizing Lstable forces the video model’s outputs to agree across time.  \n5. To avoid penalizing on bad flows you (a) keep only pixels that survive a forward–backward (cycle) consistency check and (b) drop pixels too close to depth-edges (using a Canny filter).  \n\nBy back-propagating Lstable through the red-boxed temporal module (but not through the frozen optical-flow net), you teach the video model to produce geometries that are not only individually plausible (via the single-frame regularizer) but also geometrically coherent from frame to frame."
    },
    {
        "question": "How does using randomly sampled frames influence the fixed image model's regularization consistency?",
        "relevant_section_ids": [
            "3.1",
            "4.3"
        ],
        "relevant_context": [
            "To speed up training, we randomly select one frame from the video in each iteration and calculate the regularization loss on this frame only.",
            "Interestingly, Ours all frames shows similar performance to our standard model, suggesting that single-frame regularization sufficiently maintains alignment with the image prior."
        ],
        "final_answer": "By randomly sampling a single frame per iteration for the fixed image model’s regularization loss, the training is accelerated without sacrificing consistency: single-frame regularization is sufficient to keep the video model aligned with the fixed image model’s prior.",
        "relevant_elements": [
            "Randomly Sampled Frame",
            "Fixed Image Model"
        ],
        "id": 1070,
        "masked_question": "How does using [mask1] influence the fixed image model's regularization consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Randomly Sampled Frame"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17249v1_figure_2.png",
        "paperid": "2411.17249v1",
        "paper_path": "./papers/2411.17249v1.json",
        "figure_id": "2411.17249v1_figure_2.png",
        "caption": "Figure 2: Visualization of Our Training Pipeline. Our pipeline consists of three branches: an optical flow network that extracts optical flow from input video to guide temporal smoothness; a fixed single-frame image model for regularization, and the trained video model that integrates a fine-tuned image backbone with temporal layers.",
        "qtype": "Experimental_Results",
        "response": "Let’s first identify what “[mask1]” is. In the red‐boxed area on the left branch of the diagram we see a “Randomly Sampled Frame” being sent into the fixed image model. The text then tells us:\n\n 1.  At each training iteration, rather than running every frame through the fixed image model, they pick exactly one frame at random.  \n 2.  They compute the regularization loss (the affine‐invariant depth/ℓ₁ normal loss) only on that single frame, with weight λ₁ = 1.  \n 3.  Over the course of training, every frame gets its turn to be that randomly sampled one, so in expectation the video model stays well aligned with the single‐frame image prior.  \n\nBecause of this design,\n\n •  It slashes both compute and memory use (you only back-propagate the regularization loss through one frame instead of all N).  \n •  Yet it still enforces consistency with the pretrained image model “on average” across the sequence, so the video model never strays far from the strong geometric prior.  \n\nIn short, using the randomly sampled frame for the regularization branch ensures that you get nearly the same per-frame alignment to the fixed image model as if you’d used every frame—but at a fraction of the computational cost—and thus preserves regularization consistency across the video."
    },
    {
        "question": "What limitations emerge when only using Pre-trained Video Optical-Flow Model for temporal consistency?",
        "relevant_section_ids": [
            "3.2",
            "5"
        ],
        "relevant_context": [
            "In practice, however, the optical flow prediction can be inaccurate or wrong due to the limitations of the pretrained model, harming the effectiveness of the loss as Fig. 3 shows.",
            "Second, while optical flow provides smoothness and temporal consistency between adjacent frames, it only account for correlations across continuous frames. It may fail to, for instance, capture consistent depth information for objects that temporarily leave and re-enter the scene."
        ],
        "final_answer": "Relying solely on a pre-trained optical-flow model can be problematic because its flow estimates are sometimes inaccurate or outright wrong—undermining the stabilization loss—and it only enforces consistency between adjacent frames, failing to capture longer-range or re-entry motions when objects leave and return to the scene.",
        "relevant_elements": [
            "Pre-trained Video Optical-Flow Model"
        ],
        "id": 1071,
        "masked_question": "What limitations emerge when only using [mask1] for temporal consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Pre-trained Video Optical-Flow Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17249v1_figure_2.png",
        "paperid": "2411.17249v1",
        "paper_path": "./papers/2411.17249v1.json",
        "figure_id": "2411.17249v1_figure_2.png",
        "caption": "Figure 2: Visualization of Our Training Pipeline. Our pipeline consists of three branches: an optical flow network that extracts optical flow from input video to guide temporal smoothness; a fixed single-frame image model for regularization, and the trained video model that integrates a fine-tuned image backbone with temporal layers.",
        "qtype": "Others",
        "response": "First, let us clarify what “[mask1]” refers to.  In the red‐boxed area in the top right of Figure 2, the model is only using the pre–trained video optical‐flow estimator and the corresponding stabilization loss  \nLₛₜₐbₗₑ  \nto enforce temporal smoothness.  \n\nWhat goes wrong if you rely solely on that optical‐flow‐based term?\n\n1.  No absolute anchoring  \n    •  The stabilization loss only says “make depth at x in frame t look like depth at its mapped location in frame t+1, up to whatever the flow says.”  \n    •  If you don’t also pull each frame towards a single‐image depth prior, the network can drift in scale and shift arbitrarily—trivial or degenerate solutions (e.g. nearly constant depths plus a tiny wiggle to satisfy the flow) can minimize the flow‐consistency term.  \n\n2.  Flow errors propagate bad supervision  \n    •  Off-the-shelf flow models are far from perfect.  They often fail around occlusions, near motion boundaries, in textureless regions or under large displacements.  \n    •  When the flow is wrong, the stabilization loss will force the network to hallucinate incorrect depth changes, actually increasing temporal jitter rather than reducing it.  \n\n3.  Only local, adjacent‐frame coupling  \n    •  Optical flow links only consecutive frames.  If an object disappears (occludes) and re-appears later, there is no mechanism to keep its depth consistent across that gap.  \n    •  You get short‐term smoothness but no long‐range coherence, so slow scale drift or “rubber-band” effects can still emerge over many frames.  \n\nFor these reasons the authors combine Lₛₜₐbₗₑ with a fixed, single‐frame image prior (to lock down scale and per‐frame correctness) and add learned temporal modules plus outlier filters (cycle‐consistency and edge‐masking) to fence off bad flow correspondences."
    },
    {
        "question": "What alternatives could supplement Single Frame Image Model Output for guiding Video Model Output regularization?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "In order to satisfy both per-frame accuracy (aligning with an image model) and temporal consistency, we design a novel training strategy that employs two different types of losses: a regularization loss that forces the model to produce results aligned with the image model, and an optical flow based stabilization loss as described in Section 3.2.",
            "Specifically, given the predicted optical flow maps between two adjacent frames, we define a stabilization loss between the two frames.  To make this loss robust, we apply cycle-validation to select correctly matched pixels and filter out regions near depth edges (via a Canny detector) before computing the flow-alignment term."
        ],
        "final_answer": "Beyond the single-frame image-model regularization loss, the authors introduce an optical-flow-based temporal stabilization loss—computed using a fixed, pre-trained video optical-flow model and further refined by cycle-validation and Canny-edge filtering—to guide and regularize the video model’s output.",
        "relevant_elements": [
            "Single Frame Image Model Output",
            "Video Model Output"
        ],
        "id": 1072,
        "masked_question": "What alternatives could supplement [mask1] for guiding [mask2] regularization?",
        "masked_number": 2,
        "masked_elements": [
            "Single Frame Image Model Output",
            "Video Model Output"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17249v1_figure_2.png",
        "paperid": "2411.17249v1",
        "paper_path": "./papers/2411.17249v1.json",
        "figure_id": "2411.17249v1_figure_2.png",
        "caption": "Figure 2: Visualization of Our Training Pipeline. Our pipeline consists of three branches: an optical flow network that extracts optical flow from input video to guide temporal smoothness; a fixed single-frame image model for regularization, and the trained video model that integrates a fine-tuned image backbone with temporal layers.",
        "qtype": "Others",
        "response": "Here’s the key idea: in our pipeline the red‐boxed branch (“mask 1”) is simply a frozen, single‐image geometry predictor whose output we use to “anchor” every frame of the video model (“mask 2”) back to a known per‐frame prior.  There is nothing sacred about the particular model we chose (Depth-Anything-V2 or Marigold-E2E-FT) – you can swap in or add any off-the-shelf single‐view geometry network to play exactly the same role.  \n\nConcretely, you could supplement (or replace) the fixed image model with any of the following for regularization:  \n • A different monocular depth predictor (e.g. MiDaS/DPT, AdaBins, DORN, NeWCRFs, VNLNet, DenseDepth, LeReS, etc.)  \n • A standalone surface-normal estimator (e.g. MarrNet, GeoNet) alongside or instead of the depth model  \n • A multi-task per-frame network that jointly predicts depth + normals + semantics (so you could regularize on segmentation or normal maps in addition to depth)  \n • Even purely geometric priors such as SfM‐based sparse depths or fits from a multi-view stereo run (whenever available)  \n\nAll of these will serve exactly the same purpose as our red-boxed branch: they provide a stable, high-quality per-frame prediction that the video model can be gently steered toward at each time step, while the optical–flow loss enforces temporal coherence."
    },
    {
        "question": "What limitations stem from relying on Search Logs for LLM generator to create annotation guidelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Search Logs",
            "LLM generator"
        ],
        "id": 1073,
        "masked_question": "What limitations stem from relying on [mask1] for LLM generator to create annotation guidelines?",
        "masked_number": 1,
        "masked_elements": [
            "Search Logs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.11860v1_figure_1.png",
        "paperid": "2409.11860v1",
        "paper_path": "./papers/2409.11860v1.json",
        "figure_id": "2409.11860v1_figure_1.png",
        "caption": "Figure 1: \nOur proposed framework works by extracting a query-product pair from our search query-click logs (1). The query (e.g. black sneakers) is then passed on to the LLM generator (2). The LLM generator creates specific annotation instructions for the given query. The query-specific annotation guidelines and the query-product pair (e.g. black sneakers and the retrieved product) are provided as input to the LLM annotator (3). Lastly, the annotated query-product pair is forwarded to the search engine evaluation module (4).",
        "qtype": "Others",
        "response": "Mask1 corresponds to the “Search Logs” block.  By leaning on raw search‐click logs to prompt the LLM generator, you inherit all of the logs’ shortcomings:\n\n1. Ambiguity and under‐specification  \n   • Users often type very terse or imprecise queries (“shoes”).  An LLM can only invent guidelines for the attributes it can infer, so it may miss critical intent (e.g. men’s vs. women’s, sport vs. casual).  \n\n2. Noise and errors  \n   • Logs contain typos, partial reformulations and even nonsensical queries.  Any noise in the logs gets baked into the generated requirement lists and instructions.  \n\n3. Sampling bias and coverage gaps  \n   • The logs reflect the walk of past traffic—over‐represented popular terms, under‐represented long‐tails and new trends.  As a result, the LLM never sees instructions for unseen or rare query patterns.  \n\n4. Lack of context  \n   • A search log entry often omits context like geographic locale, active filters, price range, user history or seasonality.  Guidelines built from the bare query string can therefore be incomplete or misleading.  \n\n5. Propagation of bias  \n   • Any systemic bias (brand preferences, demographic skews, promotion‐driven clicks) in the logs will be mirrored in the generated annotation criteria, potentially miscalibrating relevance judgments.  \n\nIn short, the LLM generator’s query‐level guidelines can be no better than the raw, noisy, biased, context‐poor queries found in the search logs."
    },
    {
        "question": "How might annotation errors from the LLM annotator propagate through Search engine evaluation and affect fairness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM annotator",
            "Search engine evaluation"
        ],
        "id": 1074,
        "masked_question": "How might annotation errors from the [mask1] propagate through Search engine evaluation and affect fairness?",
        "masked_number": 1,
        "masked_elements": [
            "LLM annotator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.11860v1_figure_1.png",
        "paperid": "2409.11860v1",
        "paper_path": "./papers/2409.11860v1.json",
        "figure_id": "2409.11860v1_figure_1.png",
        "caption": "Figure 1: \nOur proposed framework works by extracting a query-product pair from our search query-click logs (1). The query (e.g. black sneakers) is then passed on to the LLM generator (2). The LLM generator creates specific annotation instructions for the given query. The query-specific annotation guidelines and the query-product pair (e.g. black sneakers and the retrieved product) are provided as input to the LLM annotator (3). Lastly, the annotated query-product pair is forwarded to the search engine evaluation module (4).",
        "qtype": "Others",
        "response": "Here’s a step-by-step “chain of thought” showing how mistakes in the red-boxed component (the LLM annotator) can bleed into the search-engine evaluation and ultimately skew fairness:\n\n1. Identify the red box (​[mask1]​)  \n   – In Fig. 1, the red box sits around component (3), labeled “LLM annotator.”  \n   – This is the module that, given (a) the query, (b) the query-specific guideline, and (c) the product (text + image), spits out a relevance label.\n\n2. Where do its outputs go?  \n   – Its labels become the “ground truth” for the search engine evaluation in step (4).  \n   – All downstream metrics—precision@k, recall, NDCG, hard-disagreement counts, etc.—are computed against those labels.\n\n3. What kinds of annotation errors can occur at that stage?  \n   – False negatives (marking a legitimately relevant product as “irrelevant”)  \n   – False positives (marking an irrelevant product as “highly relevant”)  \n   – Systematic biases (e.g. consistently misunderstanding certain brands, languages, or styles)\n\n4. How those errors propagate:  \n   a. Noisy metrics  \n      – Every mislabeled query-product pair corrupts the counts used to compute accuracy or agreement.  \n      – A single systematic bias (e.g. all “dark academia” queries get under-scored) will drag down the measured performance on that slice.  \n   b. Mis-ranking of models  \n      – Two search-engine variants A and B may actually be identical on real relevance, but if A happens to retrieve more instances of a class that the LLM annotator mislabels, A will look worse on paper.  \n   c. Unfair trade-offs  \n      – Engineers optimize to please the flawed labels. They may over-tune for queries where the LLM annotator happens to be easy (or lenient), and neglect hard-cases that are actually important to under-served user groups.\n\n5. Fairness consequences  \n   – Subgroups of queries (by language, brand, style, or demographics) that the LLM annotator misjudges will consistently see either over- or under-ranking in production.  \n   – Those users then face a degraded experience, and the “evaluated” model never corrects for it because the evaluation was itself biased.  \n   – Over time, this feedback loop entrenches inequality: some product categories or languages get systematically worse treatment, and developers aren’t even aware because their evaluation bank was polluted by LLM-annotation errors.\n\nSummary answer  \nAnnotation errors from the LLM annotator become “ground-truth” labels in step (4), corrupting all downstream metrics. Systematic mislabels skew model comparisons, misdirect engineering effort, and mean that entire subsets of queries or user groups are unfairly penalized or advantaged in both offline evaluation and the live system."
    },
    {
        "question": "What advantages arise from separating the LLM generator for query-specific annotation guidelines?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "Additionally, our pipeline’s modular design allows for caching and parallel processing, which is crucial for scaling up to larger systems.",
            "As illustrated with dashed lines in Fig. 2, all outputs and intermediate steps are stored in a database. This caching serves two key functions in our pipeline. Firstly, it facilitates efficient retrieval and reuse. When evaluating a new search engine configuration (or a variation of existing ones), the database is queried to retrieve relevant pieces of information, including the query requirement list, query-specific annotation guidance, textual and visual product descriptions, and relevance scores. We only compute the missing pieces of information. Secondly, it ensures consistent evaluation across different search engines, as intermediate steps (such as query-specific annotation guidelines) are computed only once and then used to evaluate various search engines."
        ],
        "final_answer": "By separating the LLM generator for query-specific guidelines into its own module, the system becomes cacheable and parallelizable. All intermediate outputs (e.g. guidelines and requirements lists) are stored once and then efficiently retrieved and reused. This avoids redundant computations when evaluating variants of the search engine and guarantees consistency across evaluations, enabling the framework to scale up to large deployments.",
        "relevant_elements": [
            "LLM generator",
            "query-specific annotation guidelines"
        ],
        "id": 1075,
        "masked_question": "What advantages arise from separating the [mask1] for query-specific annotation guidelines?",
        "masked_number": 1,
        "masked_elements": [
            "LLM generator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.11860v1_figure_1.png",
        "paperid": "2409.11860v1",
        "paper_path": "./papers/2409.11860v1.json",
        "figure_id": "2409.11860v1_figure_1.png",
        "caption": "Figure 1: \nOur proposed framework works by extracting a query-product pair from our search query-click logs (1). The query (e.g. black sneakers) is then passed on to the LLM generator (2). The LLM generator creates specific annotation instructions for the given query. The query-specific annotation guidelines and the query-product pair (e.g. black sneakers and the retrieved product) are provided as input to the LLM annotator (3). Lastly, the annotated query-product pair is forwarded to the search engine evaluation module (4).",
        "qtype": "Design_Rationale",
        "response": "Here’s a step-by-step walk-through of how we identify the [mask1] and why isolating it brings benefits:\n\n1.  Image–text alignment  \n    - In Figure 1, the red box highlights component (2), labeled “LLM generator.”  \n    - From the caption and Section 3.1, we know this module takes in the raw query (e.g. “black sneakers”) and spits out two things:  \n      a) the query requirement list (brand, color, category, etc.)  \n      b) the query-specific annotation guideline (rules on what counts as “irrelevant,” “acceptable substitute,” or “highly relevant” for that exact query).\n\n2.  What it means to “separate” that module  \n    - Instead of hard-coding a single, global annotation spec for every query, we deliberately carve out a **generator** subroutine whose sole job is to produce per-query guidelines.  \n    - The downstream annotator (component 3) then simply consumes those guidelines along with the product data to decide relevance.\n\n3.  Key advantages of that separation  \n    a) **Modularity & clarity**  \n       • You keep “guideline creation” logic in one place and “label assignment” logic in another.  \n       • Each component can be developed, tested and debugged in isolation.  \n    b) **Caching & reuse**  \n       • Once you generate guidelines for a given query, you store them in your database.  \n       • If you re-run your evaluation on a new ranking algorithm, you don’t have to regenerate those specs – you just pull them out of cache.  \n    c) **Consistency across runs**  \n       • All search engines under test see exactly the same query guidelines.  \n       • Eliminates variance from regenerating slightly different “wordings” each time.  \n    d) **Efficiency & scalability**  \n       • Generation is relatively expensive (LLM calls), annotation is cheap. By decoupling, you only pay that cost once per query rather than once per query–product pair.  \n       • Enables parallelizing guideline creation offline and feeding many annotators in parallel.  \n    e) **Flexibility for future extensions**  \n       • If you want to tweak the generator prompt or add new rule types, you only touch component (2), without having to overhaul the entire annotation pipeline.\n\nIn short, by isolating the **LLM generator** that emits query-specific annotation guidelines, the system becomes far more modular, cache-friendly, consistent and scalable."
    },
    {
        "question": "Why is annotation performed by an LLM annotator before search engine evaluation?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "A prerequisite for this evaluation is the availability of a large enough pool of query-product relevance labels Voorhees (2001 ###reference_b24###); Halvey et al. (2015 ###reference_b10###), which indicate whether a retrieved product is semantically relevant to the query.",
            "The LLM assigns a relevance score to the query-product pair using a set of predefined labels. In its simplest form, the output is a database with one row for each (query, product, relevance score)."
        ],
        "final_answer": "Annotation is performed first in order to generate the query–product relevance labels (the “ground truth” scores) that are required to evaluate and compare search engine performance.",
        "relevant_elements": [
            "LLM annotator",
            "Search engine evaluation"
        ],
        "id": 1076,
        "masked_question": "Why is annotation performed by a [mask1] before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "LLM annotator",
            "Search engine evaluation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.11860v1_figure_1.png",
        "paperid": "2409.11860v1",
        "paper_path": "./papers/2409.11860v1.json",
        "figure_id": "2409.11860v1_figure_1.png",
        "caption": "Figure 1: \nOur proposed framework works by extracting a query-product pair from our search query-click logs (1). The query (e.g. black sneakers) is then passed on to the LLM generator (2). The LLM generator creates specific annotation instructions for the given query. The query-specific annotation guidelines and the query-product pair (e.g. black sneakers and the retrieved product) are provided as input to the LLM annotator (3). Lastly, the annotated query-product pair is forwarded to the search engine evaluation module (4).",
        "qtype": "Design_Rationale",
        "response": "First, let’s align the diagram to the text:\n\n– The red‐outlined box is step (3) “LLM annotator,” which takes the query, the query‐specific guidelines (produced in step 2), and the multimodal product description and spits out a relevance label for that query–product pair.  \n– The blue‐outlined box is step (4) “Search engine evaluation,” which ingests those labeled pairs and computes retrieval metrics (precision, recall, etc.) for one or more search engines.\n\nWhy must we do (3) before (4)?  \n\nChain of thought:\n\n1. The evaluation module (blue box) needs *ground‐truth relevance judgments* for each query–product pair in order to calculate any quality metrics.  \n2. Those judgments don’t exist a priori—they have to be generated, and in our pipeline we automate that via the “LLM annotator” (red box).  \n3. By first running the LLM annotator, we attach a label (“irrelevant,” “acceptable substitute,” or “highly relevant”) to each pair.  \n4. Only once every pair is labeled can we feed them into the evaluation stage to compare and score different search configurations.\n\nAnswer: Because the search evaluation step depends on having relevance labels in the first place—those labels are produced by the LLM annotator—so annotation (step 3) must precede evaluation (step 4)."
    },
    {
        "question": "What reasoning underlies distilling GPT-4 outputs into SEA-S using Mistral-7B?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We empirically observe that Mistral-7B and GPT-3.5 tend to simply concatenate the original contents. In contrast, GPT-4 leads them by integrating reviews in an unified format and providing detailed evidence for each argument.",
            "However, the API for GPT-4 is costly and inflexible. Inspired by Alpaca, we distill GPT-4’s excellent data standardization capabilities into open-source models."
        ],
        "final_answer": "The authors found that GPT-4 substantially outperforms open-source models (like Mistral-7B and GPT-3.5) at integrating multiple peer reviews into a unified format with detailed evidence, but using GPT-4 via its API is expensive and inflexible. Therefore, they distilled GPT-4’s superior standardization capabilities into Mistral-7B (forming SEA-S) so as to obtain high-quality, unified review outputs in a more cost-effective and flexible open-source model.",
        "relevant_elements": [
            "GPT-4",
            "SEA-S",
            "Mistral-7B"
        ],
        "id": 1077,
        "masked_question": "What reasoning underlies distilling [mask1] outputs into SEA-S using Mistral-7B?",
        "masked_number": 1,
        "masked_elements": [
            "GPT-4"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.12857v2_figure_2.png",
        "paperid": "2407.12857v2",
        "paper_path": "./papers/2407.12857v2.json",
        "figure_id": "2407.12857v2_figure_2.png",
        "caption": "Figure 2: The overall framework of SEA consists of three modules:\nStandardization, Evaluation and Analysis.",
        "qtype": "Design_Rationale",
        "response": "Mask1 refers to GPT-4.  The authors observed that off-the-shelf open-source models (e.g. Mistral-7B, GPT-3.5) tended to just concatenate multiple reviews, whereas GPT-4 would truly integrate them into a single, evidence-backed, uniformly formatted review.  However, calling GPT-4 on the entire training set would be prohibitively expensive and inflexible.  To get the best of both worlds, they therefore treated GPT-4 as a “teacher” on a small (20 %) slice of the data, generated high-quality standardized reviews with it, and then fine-tuned Mistral-7B (the “student”) to mimic those GPT-4 outputs.  In short, they distilled GPT-4’s superior standardization ability into an open-source model via supervised fine-tuning."
    },
    {
        "question": "What reasoning underlies employing SEA-A mismatch-driven self-correction to refine SEA-E reviews?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Now, we step into the Analysis module, where a mismatch score is proposed to measure the consistency between papers and their generated reviews.",
            "The smaller the absolute value of the mismatch score, the higher the consistency between the review and the paper.",
            "After SEA-A is trained, we further introduce a self-correction strategy to analyze each review generated by SEA-E. When the estimated mismatch score is larger than a pre-set threshold θ, we regenerate the review by adding the current mismatch score as additional prompt to ensure the consistency between the paper and the review."
        ],
        "final_answer": "SEA-A computes a mismatch score to quantify how much a generated review deviates from the paper’s content, under the assumption that larger deviations imply lower consistency and review quality. When this mismatch exceeds a predefined threshold, the framework automatically triggers a self-correction—re‐invoking SEA-E with the mismatch feedback—to produce a more consistent and better‐aligned review.",
        "relevant_elements": [
            "SEA-A",
            "self-correction",
            "SEA-E"
        ],
        "id": 1078,
        "masked_question": "What reasoning underlies employing [mask1] mismatch-driven self-correction to refine SEA-E reviews?",
        "masked_number": 1,
        "masked_elements": [
            "SEA-A"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.12857v2_figure_2.png",
        "paperid": "2407.12857v2",
        "paper_path": "./papers/2407.12857v2.json",
        "figure_id": "2407.12857v2_figure_2.png",
        "caption": "Figure 2: The overall framework of SEA consists of three modules:\nStandardization, Evaluation and Analysis.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning for why SEA uses a “mismatch-driven self-correction” (i.e. the red‐boxed SEA-A) to refine the reviews produced by SEA-E:\n\n1. **Quantifying consistency via SEA-A**  \n   – After SEA-E generates a review for a paper, SEA-A computes a “mismatch score” that measures how much that review deviates from the paper’s content and from the consensus rating of real reviewers.  \n   – Concretely, SEA-A is a lightweight regression model trained to predict  \n     | reviewer’s rating – weighted average rating |  \n     so that large scores flag overly-positive or overly-negative, and hence potentially low-quality, reviews.\n\n2. **Detecting low-quality reviews**  \n   – If SEA-A’s predicted mismatch for a generated review exceeds a preset threshold, it signals that the review is inconsistent or biased (it doesn’t faithfully reflect the paper’s strengths/weaknesses).\n\n3. **Feeding the mismatch back to SEA-E**  \n   – Rather than discard the review, SEA automatically re-prompts SEA-E, appending the mismatch score (and an instruction to reduce it) into the generation prompt.  \n   – This “self-correction loop” steers SEA-E to produce a new review that is more balanced, evidence-based, and aligned with the paper—as quantified by a lower mismatch.\n\n4. **Outcome: higher‐quality, consistent reviews**  \n   – By continuously measuring and then penalizing excessive deviation, SEA ensures its final reviews stay close to the paper’s actual content and to the consensus view, yielding more reliable and constructive peer reviews."
    },
    {
        "question": "What steps convert GPT-4's integrated multi-review outputs into SEA-S's fine-tuning dataset for Mistral-7B?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Specifically, we first randomly select 20% of the papers from the training set along with their reviews, where m is the number of selected papers and n_i is the number of reviews corresponding to paper i.",
            "Next, for each paper i, we input all its reviews along with the customized instruction s into GPT-4, which in turn yields the standardized review r_i.",
            "In this way, we can construct the instruction dataset for the data standardization model SEA-S that takes Mistral-7B as the base model.",
            "Formally, the triplet in the dataset is <instruction, multiple reviews, standardized review>, which is further served for SFT."
        ],
        "final_answer": "They randomly sample 20% of the papers and their reviews, send each paper’s full set of reviews plus a custom instruction to GPT-4 to produce a single integrated (standardized) review, and collect for each paper the tuple (instruction, original reviews, GPT-4’s standardized review) as the SFT training examples to fine-tune Mistral-7B (SEA-S).",
        "relevant_elements": [
            "GPT-4",
            "SEA-S",
            "Mistral-7B"
        ],
        "id": 1079,
        "masked_question": "What steps convert [mask1]'s integrated multi-review outputs into SEA-S's fine-tuning dataset for Mistral-7B?",
        "masked_number": 1,
        "masked_elements": [
            "GPT-4"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.12857v2_figure_2.png",
        "paperid": "2407.12857v2",
        "paper_path": "./papers/2407.12857v2.json",
        "figure_id": "2407.12857v2_figure_2.png",
        "caption": "Figure 2: The overall framework of SEA consists of three modules:\nStandardization, Evaluation and Analysis.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed component is GPT-4.  The process that turns GPT-4’s fused “multi-review” outputs into the SEA-S fine-tuning set for Mistral-7B is as follows:\n\n1.  Sampling  \n    •  Randomly pick 20 % of the papers in the (original) training pool, together with all of their individual peer reviews.  \n\n2.  Prompting GPT-4  \n    •  For each sampled paper i, feed GPT-4:  \n       –  A brief “standardization” instruction (i.e. how to merge, reformat, and cite evidence), plus  \n       –  All of paper i’s raw reviews.  \n    •  GPT-4 produces one integrated, unified review Sᵢ that (a) highlights strengths and weaknesses in a consistent format and (b) cites evidence from the original reviews.  \n\n3.  Building the SFT Triplets  \n    •  Pack each example into a training triplet  \n       〈 instruction,  {raw reviews of paper i},  Sᵢ 〉  \n    •  This yields an “instruction dataset” of size 0.2·(number of training papers).  \n\n4.  Supervised Fine‐Tuning (Distillation)  \n    •  Take Mistral-7B as the base model.  \n    •  Run standard SFT over those triplets, teaching it to imitate GPT-4’s formatting and argumentation.  \n\nAfter this stage, the resulting checkpoint is SEA-S.  At that point you can throw the *remaining* 80 % of paper–review sets into SEA-S to produce integrated reviews for the full training corpus."
    },
    {
        "question": "How does SEA-A's mismatch score inform SEA-E's regeneration mechanism during self-correction?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "After SEA-A is trained, we further introduce a self-correction strategy to analyze each review generated by SEA-E. When the estimated mismatch score is larger than a pre-set threshold, we regenerate the review by adding the current mismatch score as additional prompt to ensure the consistency between the paper and the review."
        ],
        "final_answer": "SEA-A computes a mismatch score that measures how inconsistent a generated review is with its paper. If this score exceeds a predefined threshold, the system triggers SEA-E to regenerate the review, supplying the computed mismatch score as an extra prompt to guide the new review toward better consistency.",
        "relevant_elements": [
            "SEA-A",
            "SEA-E",
            "self-correction"
        ],
        "id": 1080,
        "masked_question": "How does [mask1]'s mismatch score inform SEA-E's regeneration mechanism during self-correction?",
        "masked_number": 1,
        "masked_elements": [
            "SEA-A"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.12857v2_figure_2.png",
        "paperid": "2407.12857v2",
        "paper_path": "./papers/2407.12857v2.json",
        "figure_id": "2407.12857v2_figure_2.png",
        "caption": "Figure 2: The overall framework of SEA consists of three modules:\nStandardization, Evaluation and Analysis.",
        "qtype": "Implementation_Details",
        "response": "First, recall that the little red‐boxed module in the rightmost “Analysis” column is SEA-A, which takes a paper–review pair and spits out a single scalar “mismatch score” measuring how far SEA-E’s review deviates from the paper’s own (weighted) rating.  SEA-A is trained so that smaller |mismatch| means better consistency.\n\nIn our self‐correction loop, we do exactly this:\n\n1.  SEA-E generates a review for paper P.  \n2.  SEA-A scores that review against P, producing a mismatch score m.  \n3.  If |m| exceeds a preset threshold T, we treat that as “the review isn’t consistent enough.”  \n4.  We then call SEA-E a second time, *this time appending m (and a short instruction like “Your last draft was off by m points; please revise to better match the paper.”) to the prompt.*  \n\nIn this way SEA-A’s mismatch score both (a) decides **when** to regenerate, and (b) provides **feedback** within the prompt so that SEA-E can produce a more on-target, self-corrected review."
    },
    {
        "question": "What is the motivation behind fusing semantic, driving, and context data before generating complexity-infused features?",
        "relevant_section_ids": [
            "1",
            "1"
        ],
        "relevant_context": [
            "Driving behavior, such as speed adjustments in response to poor visibility or narrow lanes, is also influenced by scene complexity. Speed and acceleration patterns adjust based on obstacles and conditions [12]. Integrating behavior data with scene information deepens our understanding of driver interactions with their environment, improving crash risk modeling.",
            "Extracting hidden context from this combined data is essential. Previous studies have shown that fusing situational and memory-based features [27], as well as road graph and motion history data [16], enhances situation awareness and motion prediction, respectively. Building on this, we incorporate feature fusion to capture both explicit and implicit features of roadway scenes."
        ],
        "final_answer": "The fusion of semantic, driving, and contextual features is motivated by the need to form a richer, more holistic representation of roadway complexity. By combining imagery-derived semantics with driver behavior and higher-level context, the model can extract both explicit and hidden (implicit) scene characteristics. This fused feature set improves our understanding of how drivers interact with complex environments and, as a result, enhances the accuracy of crash-risk prediction.",
        "relevant_elements": [
            "Semantic",
            "Driving",
            "Context"
        ],
        "id": 1,
        "masked_question": "What is the motivation behind fusing [mask1], [mask2], and context data before generating complexity-infused features?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic",
            "Driving"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17886v1_figure_1.png",
        "paperid": "2411.17886v1",
        "paper_path": "./papers/2411.17886v1.json",
        "figure_id": "2411.17886v1_figure_1.png",
        "caption": "Figure 1: The model structure. The model takes raw images as input and generates semantic, driving, and contextual features from various sources to predict the crash likelihood as the output. It consists of an encoder that learns hidden features from the semantic, driving, and contextual data, which are infused with the complexity index. The prediction model then utilizes all the available features, including the complexity-infused features, to predict the crash likelihood. Example data is shown above each feature source.",
        "qtype": "Design_Rationale",
        "response": "Here’s a step-by-step breakdown of why the authors first fuse the semantic features (mask₁), the driving features (mask₂), and the contextual data before they go on to compute their “complexity‐infused” representations:\n\n1.  Defining roadway complexity  \n    –  In the Introduction they argue that “roadway complexity” isn’t just about what you see in the camera (semantic)—it also depends on how the driver is behaving and on the environmental conditions.  \n    –  Object density, road markings, weather, pavement state, speed, acceleration and so on all interact to affect a driver’s cognitive load and crash risk.\n\n2.  Lessons from prior work  \n    –  They cite studies showing that fusing situational (scene) and historical/motion data improves situation awareness and trajectory prediction.  \n    –  By analogy, fusing semantic, driving and context data should let the model pick up on hidden, higher-order cues that any single modality would miss.\n\n3.  The role of the encoder  \n    –  Their two-stage framework uses an encoder whose job is to “learn hidden features from the fusion of scene-level semantic, driving-level, and contextual features.”  \n    –  Only once those three streams are combined can the encoder tease out complex interactions—e.g. “drivers brake harder when object density is high and visibility is poor”—and package them into a single “complexity index.”\n\n4.  Downstream benefit  \n    –  Those complexity-infused features then feed into the crash-likelihood predictor alongside the original semantic, driving and context streams.  \n    –  Empirically, this richer, fused representation improves the ability to forecast crash risk.\n\nIn short, the motivation for fusing mask₁ (semantic), mask₂ (driving) and the contextual inputs up front is to let the model capture the implicit, cross-modal relationships that define true roadway complexity—relationships you simply cannot learn if you treat each modality in isolation."
    },
    {
        "question": "What rationale supports integrating Amazon Mechanical Turk and GPT-4o for complexity index generation?",
        "relevant_section_ids": [
            "1",
            "3.3.1"
        ],
        "relevant_context": [
            "We compare the complexity index annotations from Amazon Mechanical Turk and Large Language Models (LLMs) in terms of their capability to predict crash likelihood and find that LLMs-generated annotations consistently exhibited better predictive performance. This can enhance the development of real-time crash prediction systems and inspire the integration of automated annotation tools for improved accuracy and scalability.",
            "The complexity index was generated from two sources: AI and humans. For AI, the GPT-4o-2024-08-06 model was used along with the contextual feature generation process, as shown in Fig. 3. In this approach, the model generated a complexity score on a scale from 0 to 10 to describe the complexity and demanding level of the roadway scenes.",
            "The human-generated complexity indices relied on Amazon Mechanical Turk (MTurk) for annotations. The task was designed to assess the complexity level of roadway scenes. Workers were shown image frames and asked to rate the complexity of each scene on a scale from 1 to 10. Only workers with a high approval rating, at least 500 completed tasks, and residing in the US were selected. A pilot study was conducted with 500 images, where 10 workers annotated the same image. The results showed a relatively high level of agreement among workers. Based on this, in the official round, each scene was annotated by 3 workers, and the final complexity score was determined by averaging their responses."
        ],
        "final_answer": "By collecting complexity ratings both from human annotators on Mechanical Turk and from the GPT-4o model, the authors anchor their ground‐truth in human perception while simultaneously exploiting the scalability and consistency of an automated LLM.  They then compare the two sets of scores—and find that the GPT-4o–generated complexity index not only aligns well with human judgments (as demonstrated by the high MTurk inter‐rater agreement) but also yields better crash‐prediction performance—thereby justifying the integration of both sources.",
        "relevant_elements": [
            "Amazon Mechanical Turk",
            "GPT-4o",
            "Complexity Index"
        ],
        "id": 2,
        "masked_question": "What rationale supports integrating [mask1] and GPT-4o for complexity index generation?",
        "masked_number": 1,
        "masked_elements": [
            "Amazon Mechanical Turk"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17886v1_figure_1.png",
        "paperid": "2411.17886v1",
        "paper_path": "./papers/2411.17886v1.json",
        "figure_id": "2411.17886v1_figure_1.png",
        "caption": "Figure 1: The model structure. The model takes raw images as input and generates semantic, driving, and contextual features from various sources to predict the crash likelihood as the output. It consists of an encoder that learns hidden features from the semantic, driving, and contextual data, which are infused with the complexity index. The prediction model then utilizes all the available features, including the complexity-infused features, to predict the crash likelihood. Example data is shown above each feature source.",
        "qtype": "Design_Rationale",
        "response": "Here’s the chain of reasoning:\n\n1.  In Fig. 1 the red‐boxed module is Amazon Mechanical Turk (MTurk), and it sits alongside GPT-4o in the “Complexity Index” ground-truth generator.  \n2.  In § 3.3.1 the authors explain that they want two independent sources of scene‐complexity ratings:\n    - a) Human judgments collected via MTurk (workers rate each image 1–10)  \n    - b) Automatic judgments produced by repeatedly prompting GPT-4o (LLM scores 0–10)  \n3.  By integrating both MTurk and GPT-4o they can  \n    - Cross-validate: check that the LLM’s scores align with human perception,  \n    - Benchmark: directly compare predictive power of human vs. LLM annotations for crash likelihood,  \n    - Scale: if GPT-4o matches or outperforms humans, they have a fast, cost-effective way to label vast amounts of data.  \n4.  Indeed, their experiments show LLM-generated indices yield better crash-prediction performance than MTurk, justifying the shift toward automated, real-time annotation.  \n\nAnswer:  \nThey combine Amazon Mechanical Turk and GPT-4o so as to obtain both human‐perceived complexity scores and automated LLM scores, allowing cross-validation of annotations, head-to-head benchmarking of their predictive utility, and—if the LLM proves as good or better—a scalable, low-latency alternative to crowd-sourced labeling."
    },
    {
        "question": "How does the encoder integrate semantic, driving, and context features to compute the complexity index?",
        "relevant_section_ids": [
            "3.4.1"
        ],
        "relevant_context": [
            "The complexity-infused encoder uses a fully connected neural network structure with either 16 or 32 hidden neurons.",
            "The input to the network is threefold: (1) the 17 semantic features, (2) the combination of the 17 semantic features and 9 driving features, or (3) the combination of all features—17 semantic features, 9 driving features, and 19 contextual features.",
            "The input variables are normalized to a 0-1 range to ensure consistency across features and improve the stability of the model during training.",
            "After the hidden layer, a ReLU activation function is applied.",
            "The output of the network is the complexity index."
        ],
        "final_answer": "The encoder first concatenates and normalizes the chosen features (either semantic alone, semantic plus driving, or all three—semantic, driving, and contextual) into a single input vector. It then feeds this vector through a fully-connected neural network (with 16 or 32 hidden units and a ReLU activation) whose output node produces the final complexity index.",
        "relevant_elements": [
            "Encoder",
            "Complexity Index"
        ],
        "id": 3,
        "masked_question": "How does the [mask1] integrate semantic, driving, and context features to compute the complexity index?",
        "masked_number": 1,
        "masked_elements": [
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17886v1_figure_1.png",
        "paperid": "2411.17886v1",
        "paper_path": "./papers/2411.17886v1.json",
        "figure_id": "2411.17886v1_figure_1.png",
        "caption": "Figure 1: The model structure. The model takes raw images as input and generates semantic, driving, and contextual features from various sources to predict the crash likelihood as the output. It consists of an encoder that learns hidden features from the semantic, driving, and contextual data, which are infused with the complexity index. The prediction model then utilizes all the available features, including the complexity-infused features, to predict the crash likelihood. Example data is shown above each feature source.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is the complexity-infused encoder.  In practice it works as follows:\n\n1.  Feature concatenation  \n    – Take your three feature vectors (17 semantic features, 9 driving features, 19 contextual features), concatenate them into one long input vector.  \n    – Normalize every element to the [0,1] range.\n\n2.  Single hidden-layer MLP  \n    – Feed that normalized, concatenated vector into a fully-connected neural net with one hidden layer (either 16 or 32 neurons).  \n    – Apply a ReLU activation after the hidden layer.  \n    – The post‐ReLU activations are what the paper calls the “complexity‐infused features.”\n\n3.  Output layer  \n    – Map those complexity-infused features down to a single scalar, the complexity index (0 – 10).  \n    – If you’re using LLM-derived labels, you can treat that scalar as either continuous (optimize RMSE) or discretized into buckets (optimize accuracy).  For the human (MTurk) labels it’s always treated as a continuous target.\n\nBy training this small MLP to predict human or GPT-4–generated complexity ratings, the encoder learns a nonlinear fusion of semantic, driving and contextual cues and spits out a single “complexity index” for each frame."
    },
    {
        "question": "How does the crash prediction module combine complexity-infused and semantic features to produce the crash likelihood output?",
        "relevant_section_ids": [
            "3.4.2"
        ],
        "relevant_context": [
            "After generating the complexity-infused features from the encoder, they were used to predict the level of crash likelihood in combination with the corresponding input feature sets.",
            "For example, if the complexity-infused features were trained on only semantic features, the input for the crash prediction model would consist of both the complexity-infused and semantic features."
        ],
        "final_answer": "The crash prediction module concatenates the complexity-infused features with the semantic features to form the input vector. This combined feature set is normalized and then fed into a prediction model (e.g., a seven-layer fully connected neural network or other classifiers), which produces the crash likelihood output.",
        "relevant_elements": [
            "Complexity-infused Features",
            "Crash Likelihood"
        ],
        "id": 4,
        "masked_question": "How does the crash prediction module combine [mask1] and semantic features to produce the [mask2] output?",
        "masked_number": 2,
        "masked_elements": [
            "Complexity-infused Features",
            "Crash Likelihood"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17886v1_figure_1.png",
        "paperid": "2411.17886v1",
        "paper_path": "./papers/2411.17886v1.json",
        "figure_id": "2411.17886v1_figure_1.png",
        "caption": "Figure 1: The model structure. The model takes raw images as input and generates semantic, driving, and contextual features from various sources to predict the crash likelihood as the output. It consists of an encoder that learns hidden features from the semantic, driving, and contextual data, which are infused with the complexity index. The prediction model then utilizes all the available features, including the complexity-infused features, to predict the crash likelihood. Example data is shown above each feature source.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  In Fig. 1, the red‐outlined block is labeled “Complexity‐infused Features” – this is what the question refers to as [mask1].  \n2.  The blue‐outlined block at the far right is “Crash Likelihood” – that is [mask2].  \n3.  Section 3.4.2 of the text (“Crash Prediction Model”) explains exactly how these are combined:\n    - After the encoder produces the “complexity-infused features,” these are concatenated with the original semantic feature vector (and, in other variants, also with driving and contextual features).  \n    - This joint feature vector (i.e. [mask1] + semantic features) is then normalized (all inputs scaled to [0,1]) and fed into a standard predictor (Random Forest, GBDT, KNN or a 7‐layer fully connected neural network).  \n    - The predictor is trained on this combined input to output the final crash likelihood score.\n\nSo, in a single sentence:\n\nThe crash prediction module takes the complexity-infused features ([mask1]), concatenates them with the semantic feature vector, normalizes the combined vector, and passes it through a learned classifier/regressor (e.g. a fully-connected neural network) to produce the crash likelihood ([mask2])."
    },
    {
        "question": "How does Relevance prediction task leverage Encoder and Decoder to contrast positive and negative docids?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "To generate high-quality pseudo-queries for the original documents, we employ a LLM using the prompt: “Given the following document {d}, generate {X} insightful queries that a reader might have after reading the content. Ensure the queries cover key concepts.” When the prompt is combined with a document d and the required number of pseudo-queries X as input, we obtain well-written pseudo-queries. They share the same docids as the input original document.",
            "Similarly, we ensure that the model tends to generate relevant docids than irrelevant ones. In the same mini-batch, the loss L_r in the t-th iteration is:"
        ],
        "final_answer": "In the relevance prediction task, each pseudo-query is first encoded by the model’s encoder, then the decoder is used to generate a distribution over candidate docids. A contrastive loss L_r is applied in each mini-batch that pushes up the generated probability of the true (positive) docid for that query and pushes down the probabilities of all other (negative) docids, thereby explicitly contrasting positive against negative docids.",
        "relevant_elements": [
            "Relevance prediction task",
            "Encoder",
            "Decoder"
        ],
        "id": 6,
        "masked_question": "How does [mask1] leverage Encoder and Decoder to contrast positive and negative docids?",
        "masked_number": 1,
        "masked_elements": [
            "Relevance prediction task"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.11504v1_figure_1.png",
        "paperid": "2407.11504v1",
        "paper_path": "./papers/2407.11504v1.json",
        "figure_id": "2407.11504v1_figure_1.png",
        "caption": "Figure 1: The bootstrapped pre-training pipeline of BootRet. (1) The initial docids ℐ𝒟0superscriptsubscriptℐ𝒟0\\mathcal{I}_{\\mathcal{D}}^{0}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT are obtained with the initial model parameters θ0superscript𝜃0\\theta^{0}italic_θ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT. (2) To perform the t𝑡titalic_t-th iteration, we design the corpus indexing task and relevance prediction task for pre-training. We construct noisy documents and pseudo-queries with a LLM, and design contrastive losses (the yellow and the orange rectangles) and a semantic consistency loss (the green rectangle) to learn the corpus and relevance information discriminatively. After pre-training, the model updates from θt−1superscript𝜃𝑡1\\theta^{t-1}italic_θ start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. (3) The bootstrapped θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is used to dynamically update the docids ℐ𝒟t−1superscriptsubscriptℐ𝒟𝑡1\\mathcal{I}_{\\mathcal{D}}^{t-1}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to ℐ𝒟tsuperscriptsubscriptℐ𝒟𝑡\\mathcal{I}_{\\mathcal{D}}^{t}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, i.e., bootstrapped docids, which are further used in the next iteration. (Figure should be viewed in color.)",
        "qtype": "Implementation_Details",
        "response": "Here’s what the red‐boxed “relevance-prediction” module is doing, step by step:\n\n1.  Take a pseudo–query q (produced by the LLM) and run it through the **Encoder** to get a query embedding.\n2.  Condition the **Decoder** on that embedding and ask it to generate document‐ID tokens.  In practice we force‐decode two kinds of IDs:\n    –  The *positive* (correct) docid for q, and  \n    –  a set of *negative* (incorrect) docids drawn from the rest of the batch.\n3.  For each candidate docid d, we compute the log-likelihood  \n       log Pθ(d | q)  \n   by summing the token‐generation probabilities across the decoder’s output sequence.\n4.  We then form a contrastive (InfoNCE-style) loss:\n   \n   L_relevance = –  \n     ⁽¹⁻|B|⁾ ∑_{i∈B}  \n     log  \n     exp( log Pθ(d⁺_i|q_i) / τ )  \n     ∑_{j∈B} exp( log Pθ(d_j|q_i) / τ )\n\n   where B is the mini-batch, d⁺_i is the true docid for query q_i, τ is a temperature, and the denominator sums over both the positive and all negative docids.\n   \nBy doing this the model is *explicitly* driven to assign higher generation probability to the correct (positive) docid than to any of the negatives, so that at inference time the decoder will naturally favor retrieving the true relevant docid for a new query."
    },
    {
        "question": "How do contrastive losses complement semantic consistency loss relative to classical contrastive learning objectives?",
        "relevant_section_ids": [
            "3.3.1",
            "3.3.3"
        ],
        "relevant_context": [
            "Semantic consistency loss: It aims at maintaining overall semantic consistency between original and noisy documents.",
            "Contrastive losses for corpus indexing: Conditioned on original document–docid pairs, we encourage the model to generate a docid that corresponds to the document rather than the docids of other documents. In the same mini-batch, we aim for the model to generate the docid corresponding to the document with a higher probability than generating others. Inspired by contrastive learning Khosla et al. (2020), this loss is formalized as: … Similarly, for noisy pairs, the loss LC is: …",
            "Note, Eq. (2) and Eq. (3) ensure that the model’s probability of generating the corresponding docid is greater than generating other docids. Eq. (7) does not explicitly contrast with other docids."
        ],
        "final_answer": "The semantic consistency loss pulls the representations of an original document and its noisy variants together, ensuring they remain aligned. The contrastive losses then build on this by explicitly contrasting the correct docid against all other docids in the same batch—encouraging the model to assign higher generation probability to the positive docid and lower probability to negatives. In this way, the contrastive losses play the same role as classical contrastive learning (pulling positives together and pushing negatives apart), complementing the semantic consistency objective with stronger discrimination among similar documents.",
        "relevant_elements": [
            "contrastive losses",
            "semantic consistency loss"
        ],
        "id": 7,
        "masked_question": "How do [mask1] complement semantic consistency loss relative to classical contrastive learning objectives?",
        "masked_number": 1,
        "masked_elements": [
            "contrastive losses"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.11504v1_figure_1.png",
        "paperid": "2407.11504v1",
        "paper_path": "./papers/2407.11504v1.json",
        "figure_id": "2407.11504v1_figure_1.png",
        "caption": "Figure 1: The bootstrapped pre-training pipeline of BootRet. (1) The initial docids ℐ𝒟0superscriptsubscriptℐ𝒟0\\mathcal{I}_{\\mathcal{D}}^{0}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT are obtained with the initial model parameters θ0superscript𝜃0\\theta^{0}italic_θ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT. (2) To perform the t𝑡titalic_t-th iteration, we design the corpus indexing task and relevance prediction task for pre-training. We construct noisy documents and pseudo-queries with a LLM, and design contrastive losses (the yellow and the orange rectangles) and a semantic consistency loss (the green rectangle) to learn the corpus and relevance information discriminatively. After pre-training, the model updates from θt−1superscript𝜃𝑡1\\theta^{t-1}italic_θ start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. (3) The bootstrapped θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is used to dynamically update the docids ℐ𝒟t−1superscriptsubscriptℐ𝒟𝑡1\\mathcal{I}_{\\mathcal{D}}^{t-1}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to ℐ𝒟tsuperscriptsubscriptℐ𝒟𝑡\\mathcal{I}_{\\mathcal{D}}^{t}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, i.e., bootstrapped docids, which are further used in the next iteration. (Figure should be viewed in color.)",
        "qtype": "Literature_Background",
        "response": "Here “ [mask1] ” is simply the set of contrastive losses boxed in red (the yellow/orange rectangles).  These contrastive objectives play a very different role from the green “semantic consistency” term:\n\n1. The semantic‐consistency loss only pulls each noisy variant back towards its original document (i.e. it guarantees that d and dⁿ have similar representations), but it says nothing about how d should sit relative to *other* documents.  \n2. The red‐boxed contrastive losses, by contrast, treat every *other* docid in the batch as a negative.  They force the model to assign a higher generation‐probability to the correct docid than to any of those distractors.  \n\nIn other words, whereas classical contrastive learning also pushes negatives away in embedding space, these red-boxed losses (a) operate directly on the decoder’s docid‐generation probabilities instead of just on cosine‐similarities, and (b) tie that discrimination back into the seq2seq training so the model not only keeps semantically similar texts together but also learns to *distinguish* each document from all others via its unique identifier."
    },
    {
        "question": "How does pseudo-query generation via LLM differ from earlier pseudo-query methods in generative retrieval pre-training?",
        "relevant_section_ids": [
            "1",
            "3.3.2"
        ],
        "relevant_context": [
            "For example, Zhou et al. (2022) proposed indexing- and retrieval-based pre-training tasks; document pieces or pseudo-queries are used as input, and docids (e.g., product quantization code) are predicted as output with maximum likelihood estimation (MLE). Similarly, Chen et al. (2022) proposed retrieval-based tasks, which aim to construct and learn pairs of pseudo-queries and docids (i.e., Wikipedia titles) from the corpus.",
            "To generate high-quality pseudo-queries for the original documents, we employ a LLM using the prompt: “Given the following document {d}, generate {X} insightful queries that a reader might have after reading the content. Ensure the queries cover key concepts.” When the prompt is combined with a document d and the required number of pseudo-queries X as input, we obtain well-written pseudo-queries. They share the same docids as the input original document."
        ],
        "final_answer": "Earlier generative-retrieval pre-training methods constructed pseudo-queries directly from the corpus (for example by using document snippets or titles) and paired them with docids for MLE training. In contrast, BootRet uses a large language model with a targeted prompt to generate high-quality, concept-covering pseudo-queries—resulting in more coherent and semantically rich queries than the heuristic or metadata-based queries used in prior work.",
        "relevant_elements": [
            "LLM",
            "pseudo-queries"
        ],
        "id": 8,
        "masked_question": "How does pseudo-query generation via [mask1] differ from earlier pseudo-query methods in generative retrieval pre-training?",
        "masked_number": 1,
        "masked_elements": [
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.11504v1_figure_1.png",
        "paperid": "2407.11504v1",
        "paper_path": "./papers/2407.11504v1.json",
        "figure_id": "2407.11504v1_figure_1.png",
        "caption": "Figure 1: The bootstrapped pre-training pipeline of BootRet. (1) The initial docids ℐ𝒟0superscriptsubscriptℐ𝒟0\\mathcal{I}_{\\mathcal{D}}^{0}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT are obtained with the initial model parameters θ0superscript𝜃0\\theta^{0}italic_θ start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT. (2) To perform the t𝑡titalic_t-th iteration, we design the corpus indexing task and relevance prediction task for pre-training. We construct noisy documents and pseudo-queries with a LLM, and design contrastive losses (the yellow and the orange rectangles) and a semantic consistency loss (the green rectangle) to learn the corpus and relevance information discriminatively. After pre-training, the model updates from θt−1superscript𝜃𝑡1\\theta^{t-1}italic_θ start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT. (3) The bootstrapped θtsuperscript𝜃𝑡\\theta^{t}italic_θ start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT is used to dynamically update the docids ℐ𝒟t−1superscriptsubscriptℐ𝒟𝑡1\\mathcal{I}_{\\mathcal{D}}^{t-1}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT to ℐ𝒟tsuperscriptsubscriptℐ𝒟𝑡\\mathcal{I}_{\\mathcal{D}}^{t}caligraphic_I start_POSTSUBSCRIPT caligraphic_D end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT, i.e., bootstrapped docids, which are further used in the next iteration. (Figure should be viewed in color.)",
        "qtype": "Literature_Background",
        "response": "In BootRet, “pseudo‐queries” are not just lifted out of the text (e.g. as sliding windows, titles or section-headings) and then trained with a straight MLE loss.  Instead, we take each document and feed it into an external LLM with a prompt like:\n\n    “Given the following document {d}, generate X insightful queries that a reader might have after reading the content.  Ensure the queries cover key concepts.”  \n\nThat LLM generates full-sentence, semantically rich questions for each passage.  We then pair those machine-written questions with the document’s current doc-id and train the seq-to-seq model with a *contrastive* objective (so it learns to score the correct doc-id higher than all the others).\n\nBy contrast, all of the earlier GR-pretraining schemes that used pseudo-queries simply extracted pieces of the document (or used titles/anchor-text) and trained under plain MLE.  They never brought in a dedicated LLM to *generate* diverse, high-quality queries, nor did they couple that generation with an explicit contrastive ranking loss."
    },
    {
        "question": "How does Masking Joints contrast with feedforward Decoder processing in prior methodologies?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Section 1: “Later works [5, 35] enhanced the efficiency by employing neural networks that predict joint angles and robot poses in a single feed‐forward pass.”",
            "Section 3.1: “We employ a masking‐based pre‐training strategy tailored for robotic applications … Masks are selected to occlude the regions around four randomly selected robot joints … With the unmasked patches as context, a Vision Transformer encoder produces context embeddings … These context embeddings are then passed to a VIT‐based predictor, which infers embeddings for all patches of the original image … The embeddings for the masked patches … are used to compute the L1 loss during training. … This trains the encoder to infer the robot’s joint‐related information based on the surroundings.”"
        ],
        "final_answer": "Prior methods use an encoder whose output embeddings are fed directly into a decoder (Keypoint Net, Joint Net, etc.) in a single feed‐forward pass to predict joint angles and poses. In contrast, RoboPEPP’s Masking Joints pre‐training deliberately occludes regions around robot joints and trains an encoder–predictor pair: the encoder processes only the unmasked context, and the predictor must reconstruct the embeddings of the masked joint regions. This forces the network to learn to infer joint appearances and spatial relationships from surrounding cues, rather than relying on a single pass through a straightforward decoder.",
        "relevant_elements": [
            "Masking Joints",
            "Decoder"
        ],
        "id": 9,
        "masked_question": "How does [mask1] contrast with feedforward [mask2] processing in prior methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Masking Joints",
            "Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17662v1_figure_1.png",
        "paperid": "2411.17662v1",
        "paper_path": "./papers/2411.17662v1.json",
        "figure_id": "2411.17662v1_figure_1.png",
        "caption": "Figure 1: Comparison of an existing robot pose estimation method [5] with our RoboPEPP framework. RoboPEPP integrates joint masking-based pre-training (b.1) to enhance the encoder’s grasp of the robot’s physical model, combined with downstream networks, and keypoint filtering (b.2) to achieve high accuracy.",
        "qtype": "Literature_Background",
        "response": "In the figure, the red‐boxed module “Masking Joints” is our self-supervised pre-training step: we deliberately cover up (mask) the image patches around a handful of joints and train an encoder-predictor to reconstruct their embeddings from only the surrounding, unmasked context. In contrast, the blue-boxed component in the top row (the “feed-forward” encoder–decoder with Keypoint, Rotation, Joint and Depth nets) is exactly the traditional one-shot approach: the network sees the full image and, in a single forward pass, spits out joint angles and camera pose without ever having to infer missing or occluded joint information. In short, our masking scheme forces the model to learn the robot’s geometric structure by “hallucinating” masked regions, whereas prior methods simply learn a direct mapping from complete images to outputs."
    },
    {
        "question": "How does Predictor enhance Joint Net training beyond traditional Encoder-Decoder schemes?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "These context embeddings are then passed to a VIT-based predictor, which infers embeddings for all P patches of the original image, denoted \\{\\hat{z}_p\\} for p = 1,…,P.",
            "Our approach differs from JEPA [3] by using context-informed masking at joint locations. While JEPA learns deeper semantic representations by randomly masking the input for tasks like object detection, we focus on encoding the robot’s physical properties by specifically masking joint regions. This trains the encoder to infer the robot’s joint-related information based on the surroundings, emulating a predictive understanding similar to how humans or animals deduce missing information about physical structures.",
            "The pre-trained encoder and predictor are then fine-tuned, where they extract embeddings E for P from images, which are used by the Joint Net and Keypoint Net to predict joint angles and 2D keypoints, respectively. To further increase occlusion robustness, random masks covering up to 20% of the image are applied during training. Consistent with Sec. 3.1, the predictor outputs all patch embeddings, including masked ones."
        ],
        "final_answer": "By inserting a Predictor between the encoder and the Joint Net, RoboPEPP forces the encoder to learn to \"hallucinate\" or predict the embeddings of masked joint regions from their surrounding context. During pre-training the Predictor reconstructs the representations of occluded joints, teaching the encoder to capture the robot’s physical structure. At fine-tuning time the Predictor supplies full patch embeddings—including those for originally occluded joints—to the Joint Net, yielding richer, context-aware features and improving joint-angle estimation beyond what a standard encoder–decoder trained only on unmasked images can achieve.",
        "relevant_elements": [
            "Predictor",
            "Joint Net"
        ],
        "id": 10,
        "masked_question": "How does [mask1] enhance Joint Net training beyond traditional Encoder-Decoder schemes?",
        "masked_number": 1,
        "masked_elements": [
            "Predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17662v1_figure_1.png",
        "paperid": "2411.17662v1",
        "paper_path": "./papers/2411.17662v1.json",
        "figure_id": "2411.17662v1_figure_1.png",
        "caption": "Figure 1: Comparison of an existing robot pose estimation method [5] with our RoboPEPP framework. RoboPEPP integrates joint masking-based pre-training (b.1) to enhance the encoder’s grasp of the robot’s physical model, combined with downstream networks, and keypoint filtering (b.2) to achieve high accuracy.",
        "qtype": "Literature_Background",
        "response": "First, let’s be crystal‐clear on what “[mask1]” is.  In panel (b.2) it’s the little red box around the module labelled “Predictor.”  So the question really asks:\n\n  “How does the Predictor enhance Joint Net training beyond what you’d get from a vanilla encoder–decoder?”  \n\nHere’s the chain of thought:\n\n1.  In a standard encoder–decoder you  \n    • encode the visible pixels into some latent,  \n    • then decode that latent into your final outputs (e.g. joint angles).  \n    There is no explicit mechanism that forces the network to imagine or “hallucinate” what lies behind occlusions.  \n\n2.  In our RoboPEPP scheme, before we ever touch joint‐angle labels, we pre‐train an encoder + Predictor on a self-supervised task: we randomly mask out the neighborhoods of some joints, run the remaining patches through the encoder, and then train the Predictor to reconstruct the embeddings of the masked patches (using an L₁ loss against a frozen “teacher” backbone).\n\n3.  That masking‐and‐reconstruction game teaches the encoder + Predictor two things simultaneously:  \n    a.  strong spatial context priors around robot joints (it must learn the kinematic correlations between visible limbs and hidden joints),  \n    b.  robustness to partial occlusion (since up to 20% of the image is blanked out).  \n\n4.  When you now hook up that same encoder + Predictor to the downstream Joint Net, the Joint Net is no longer starting from scratch.  Instead, it ingests embeddings in which every patch—visible or previously masked—already carries a physically consistent, context-infused guess of “what joint geometry should be here.”  \n\n5.  In effect, the Predictor has “filled in” missing joint information and baked those inferences into the feature map.  So the Joint Net sees a far richer, more kinematically aware representation than it would if it just got a pass-through encoder output.  This extra structure makes angle refinement more accurate and stable, especially under partial view or occlusion.\n\nIn short: by inserting and pre‐training the Predictor to reconstruct embeddings for masked joint regions, we turn the encoder’s latent space into a physically grounded, occlusion-robust canvas.  Feeding those predictive embeddings into the Joint Net yields notably better angle estimates than a plain encoder–decoder could."
    },
    {
        "question": "How does masking joints affect the encoder-predictor’s ability to infer masked joint embeddings?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Masks are selected to occlude the regions around four randomly selected robot joints, or a random area if a joint is outside the camera’s field of view.",
            "This trains the encoder to infer the robot’s joint-related information based on the surroundings, emulating a predictive understanding similar to how humans or animals deduce missing information about physical structures."
        ],
        "final_answer": "By masking out patches around randomly selected joints, the encoder-predictor is forced to rely on contextual cues from the unmasked regions to reconstruct the embeddings of the occluded joints. This targeted masking thus teaches the model to infer joint-related embeddings from surrounding image information.",
        "relevant_elements": [
            "Masking Joints",
            "Encoder",
            "Predictor"
        ],
        "id": 11,
        "masked_question": "How does [mask1] affect the [mask2]-predictor’s ability to infer masked joint embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "Masking Joints",
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17662v1_figure_1.png",
        "paperid": "2411.17662v1",
        "paper_path": "./papers/2411.17662v1.json",
        "figure_id": "2411.17662v1_figure_1.png",
        "caption": "Figure 1: Comparison of an existing robot pose estimation method [5] with our RoboPEPP framework. RoboPEPP integrates joint masking-based pre-training (b.1) to enhance the encoder’s grasp of the robot’s physical model, combined with downstream networks, and keypoint filtering (b.2) to achieve high accuracy.",
        "qtype": "Experimental_Results",
        "response": "In our pre‐training (Fig. b.1) the red‐boxed operation is exactly “Masking Joints”: we deliberately occlude the image patches around a random subset of four joints.  Those masked patches go into the blue‐boxed “Encoder→Predictor” branch, which is then tasked with re‐creating (i.e. predicting) the embeddings for exactly those missing patches.  By hiding the joint regions during encoding, we force the encoder–predictor to learn the spatial and kinematic relationships of the robot from the surrounding, unmasked context.  In practice this means that the predictor cannot cheat by merely copying low‐level pixel patterns; instead it must infer joint embeddings from the rest of the arm’s structure, which greatly improves its ability to hallucinate (and later regress) the features at those joint locations."
    },
    {
        "question": "How do predicted keypoints and forward kinematics feed into PnP for pose computation?",
        "relevant_section_ids": [
            "3",
            "3.3"
        ],
        "relevant_context": [
            "Predicted joint angles and forward kinematics yield 3D joint coordinates, which, combined with detected 2D keypoints, are used in a PnP solver to estimate pose (Sec. 3.3).",
            "The robot’s pose is estimated using the EPnP algorithm [19  ###reference_b19###] with the filtered 2D-3D correspondences and known camera intrinsics.",
            "As keypoints are defined on joints, we obtain the 3D points corresponding to the 2D keypoints using the robot’s forward kinematics and predicted joint angles."
        ],
        "final_answer": "The Keypoint Net produces 2D keypoint detections and the Joint Net predicts joint angles, which are passed through the robot’s forward kinematics to generate corresponding 3D joint coordinates. These 2D–3D correspondences, together with known camera intrinsics, are then input into an EPnP solver to compute the robot’s pose in the camera frame.",
        "relevant_elements": [
            "Keypoint Net",
            "Forward Kinematics",
            "PnP"
        ],
        "id": 12,
        "masked_question": "How do predicted [mask1] and [mask2] feed into PnP for pose computation?",
        "masked_number": 2,
        "masked_elements": [
            "Keypoint Net",
            "Forward Kinematics"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17662v1_figure_1.png",
        "paperid": "2411.17662v1",
        "paper_path": "./papers/2411.17662v1.json",
        "figure_id": "2411.17662v1_figure_1.png",
        "caption": "Figure 1: Comparison of an existing robot pose estimation method [5] with our RoboPEPP framework. RoboPEPP integrates joint masking-based pre-training (b.1) to enhance the encoder’s grasp of the robot’s physical model, combined with downstream networks, and keypoint filtering (b.2) to achieve high accuracy.",
        "qtype": "Experimental_Results",
        "response": "The two shaded modules in (b.2) simply provide the 2D–3D correspondences that go into EPnP.  In practice we\n\n  1. run the red-boxed Keypoint Net to get 2D joint locations in the image,  \n  2. run the blue-boxed Forward‐Kinematics block (taking the Joint‐Net’s angle predictions) to produce the corresponding 3D joint coordinates in the robot frame,  \n\nand then hand those paired 2D–3D points to EPnP to solve for the rigid pose T."
    },
    {
        "question": "How does Emergent Feature Checking and Correction ensure trajectory smoothness and reliability?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In addition, the framework incorporates an Emergent Feature Checking and Correction module to address emergent features that appear in the output trajectory of large VLM.",
            "By detecting and mitigating these emergent features, the module ensures that the planned trajectory remains smooth and reliable.",
            "This correction process helps maintain accurate and safe navigation by refining the trajectory to avoid being skewed by misleading or atypical data points."
        ],
        "final_answer": "Emergent Feature Checking and Correction identifies and mitigates unexpected or anomalous features in the raw trajectory output from the large VLM. By detecting these ‘‘emergent’’ artifacts and correcting them—i.e., refining the path to remove or adjust misleading or atypical points—it keeps the resulting trajectory smooth, accurate, and reliable.",
        "relevant_elements": [
            "Emergent Feature Checking and Correction",
            "Planned Trajectory"
        ],
        "id": 14,
        "masked_question": "How does [mask1] ensure trajectory smoothness and reliability?",
        "masked_number": 1,
        "masked_elements": [
            "Emergent Feature Checking and Correction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.09251v2_figure_1.png",
        "paperid": "2408.09251v2",
        "paper_path": "./papers/2408.09251v2.json",
        "figure_id": "2408.09251v2_figure_1.png",
        "caption": "Figure 1: Overview of V2X-VLM Framework. The framework integrates data from vehicle and infrastructure cameras alongside descriptive text prompts to create a comprehensive E2E system for cooperative autonomous driving. Using a large VLM as the backbone, the framework processes and synthesizes diverse input types to generate optimized trajectories. A contrastive learning technique enhances scene understanding by aligning visual and textual feature representations, while an Emergent Feature Checking and Correction module mitigates the impact of emergent abilities, ensuring accurate and reliable trajectory planning outcomes.",
        "qtype": "Experimental_Results",
        "response": "Chain of thought  \n1. In the diagram the red‐boxed module is labeled “Emergent Feature Checking and Correction.”  \n2. The text (§III-A) tells us that this module sits between the raw VLM output and the final trajectory τ.  \n3. Its job is to “address emergent features that appear in the output trajectory of large VLM” (i.e. sudden spurious jumps, outliers or artifacts that the VLM may hallucinate).  \n4. By automatically detecting those aberrant points and then “mitigating” or smoothing them out, the module removes unwanted kinks and enforces temporal consistency.  \n5. The result is a trajectory that no longer carries the VLM’s occasional misleading spikes, but instead is a smooth, physically feasible, and reliable path for the ego vehicle.  \n\nAnswer  \nThe Emergent Feature Checking and Correction module inspects the raw VLM–generated waypoints for spurious “emergent” anomalies, flags any local outliers or abrupt jumps, and then re‐interpolates or smooths across those points.  In doing so it filters out the VLM’s occasional artifacts and enforces a temporally consistent, physically plausible trajectory—thus guaranteeing both smoothness and reliability in the final plan."
    },
    {
        "question": "What are the limitations of Contrastive Learning in ensuring robust multimodal alignment under noisy infrastructure data?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Contrastive Learning"
        ],
        "id": 15,
        "masked_question": "What are the limitations of [mask1] in ensuring robust multimodal alignment under noisy infrastructure data?",
        "masked_number": 1,
        "masked_elements": [
            "Contrastive Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.09251v2_figure_1.png",
        "paperid": "2408.09251v2",
        "paper_path": "./papers/2408.09251v2.json",
        "figure_id": "2408.09251v2_figure_1.png",
        "caption": "Figure 1: Overview of V2X-VLM Framework. The framework integrates data from vehicle and infrastructure cameras alongside descriptive text prompts to create a comprehensive E2E system for cooperative autonomous driving. Using a large VLM as the backbone, the framework processes and synthesizes diverse input types to generate optimized trajectories. A contrastive learning technique enhances scene understanding by aligning visual and textual feature representations, while an Emergent Feature Checking and Correction module mitigates the impact of emergent abilities, ensuring accurate and reliable trajectory planning outcomes.",
        "qtype": "Others",
        "response": "Let’s first pin down what [mask1] refers to.  In the red‐outlined block of Fig. 1, we see the “Contrastive Learning” module.  Concretely, it takes the pooled image feature z and text feature h, builds an N×N similarity matrix, and applies an InfoNCE‐style loss to pull correct (image, text) pairs together and push all others apart.\n\nNow, what shortcomings does that bring when one of your modalities—in this case the infrastructure camera stream—is noisy or unreliable?\n\n1. reliance on clean positive pairs  \n   • Contrastive learning presumes that every “positive” (image, prompt) pair truly matches the scene.  If the infrastructure image is blurred, occluded or mis-timestamped, you end up “pulling” a bad feature into your joint embedding, teaching the model to associate noise with the correct text.  \n\n2. global‐pooling only captures coarse semantics  \n   • The framework uses a single, pooled feature vector per view.  Local misalignments (e.g. a pedestrian cropped out, a traffic light glare) get averaged away, so the contrastive loss can’t correct fine‐grained mismatches introduced by noisy infrastructure data.  \n\n3. sensitivity to false negatives / insufficient negatives  \n   • In practice you need many negative pairs in a batch for InfoNCE to work well.  If your infrastructure pairs are noisy, you risk accidental “false negatives” (two actually matching scenes treated as negatives) which hurts the sharpness of the learned embedding.  \n\n4. no explicit noise modelling or confidence weighting  \n   • There is no mechanism in vanilla contrastive learning here to downweight or ignore especially low‐quality frames.  All images enter the loss equally, so a stretch of very poor infrastructure footage drags the entire embedding space off course.  \n\n5. hyperparameter brittleness  \n   • The temperature τ and the relative weighting λ of the contrastive term must be carefully tuned; under noisy conditions small shifts in τ can lead to collapsed or overly spread embeddings, reducing robustness even further.  \n\nIn short, while contrastive learning is a powerful way to glue the visual and textual streams together, its vanilla form is ill-equipped to (a) discriminate valid from corrupted infrastructure inputs, (b) focus on local, fine-grained alignment, or (c) dynamically adjust to varying noise levels in the roadside camera feed."
    },
    {
        "question": "How might Emergent Feature Checking and Correction introduce biases or delay in real-time trajectory adjustments during emergencies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Emergent Feature Checking and Correction"
        ],
        "id": 16,
        "masked_question": "How might [mask1] introduce biases or delay in real-time trajectory adjustments during emergencies?",
        "masked_number": 1,
        "masked_elements": [
            "Emergent Feature Checking and Correction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.09251v2_figure_1.png",
        "paperid": "2408.09251v2",
        "paper_path": "./papers/2408.09251v2.json",
        "figure_id": "2408.09251v2_figure_1.png",
        "caption": "Figure 1: Overview of V2X-VLM Framework. The framework integrates data from vehicle and infrastructure cameras alongside descriptive text prompts to create a comprehensive E2E system for cooperative autonomous driving. Using a large VLM as the backbone, the framework processes and synthesizes diverse input types to generate optimized trajectories. A contrastive learning technique enhances scene understanding by aligning visual and textual feature representations, while an Emergent Feature Checking and Correction module mitigates the impact of emergent abilities, ensuring accurate and reliable trajectory planning outcomes.",
        "qtype": "Others",
        "response": "Chain-of-thought  \n1. In the figure, the red box highlights the “Emergent Feature Checking and Correction” module inserted between the VLM’s raw trajectory output and the final planned trajectory.  \n2. During normal operation this module scans for “emergent” (i.e. unexpected or out‐of‐distribution) features in the VLM’s trajectory and applies corrections or smoothing.  \n3. In an emergency you often need abrupt, high‐magnitude maneuvers (hard braking, sharp swerves), which this module may flag as anomalous.  \n4. Flagging a genuine sudden maneuver as an “emergent artifact” will trigger the correction logic—smoothing out or even rejecting it—thus biasing the system toward more conservative trajectories.  \n5. Moreover, the extra detection + correction steps add computational overhead, increasing end‐to‐end latency.  \n6. Together, misclassifying critical high‐urgency moves as spurious and the extra processing time can delay or dampen necessary real‐time adjustments in emergencies.  \n\nAnswer  \nBy design, the Emergent Feature Checking and Correction block looks for “surprising” trajectory bits and then smooths or rejects them. In an emergency—when abrupt, low‐probability maneuvers are exactly what’s needed—it may falsely treat those urgent adjustments as spurious and either damp them down or reroute them through its correction logic. That both biases the vehicle toward overly conservative moves and adds extra compute steps, so the net effect is slower, and potentially unsafe, real-time trajectory updates under crisis conditions."
    },
    {
        "question": "What limitations might arise from multi-scale alignment on extremely small or blurred lesions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Multi-Scale Fine-Grained Mask-Attribute Alignment"
        ],
        "id": 17,
        "masked_question": "What limitations might arise from [mask1] on extremely small or blurred lesions?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-Scale Fine-Grained Mask-Attribute Alignment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.15744v1_figure_1.png",
        "paperid": "2410.15744v1",
        "paper_path": "./papers/2410.15744v1.json",
        "figure_id": "2410.15744v1_figure_1.png",
        "caption": "Figure 1: Overview of the training and inference framework of Malenia. The key contributions of our work are two simple but effective designs: the Multi-scale Fine-Grained Mask-Attribute Alignment and the Cross-Modal Knowledge Injection module. Incorporating these designs empowers our vision-language pre-training framework, resulting in a strong zero-shot segmentation model.",
        "qtype": "Others",
        "response": "First, let us restate what “[mask1]” is in this figure: it is the “Multi-Scale Fine-Grained Mask-Attribute Alignment” block (outlined in red in Fig. 1(a)).  Its job is to take the learned mask‐token embeddings at several decoder resolutions and align each of those mask embeddings with the text embeddings of eight structured lesion attributes via a multi-positive contrastive loss.\n\nNow, why might that particular module struggle on extremely small or blurred lesions?\n\n1.  Coarse‐to‐fine hierarchy can lose tiny objects  \n    – At the coarser decoder scales (Block 1 or Block 2), the spatial resolution is low.  Very small lesions may simply vanish or be down‐sampled into a single or even zero pixels, so the cross‐attention between mask tokens and image features will not “see” the lesion.  \n    – By the time you get to the finest scale (Block 3 or Block 4), although the resolution is higher, a tiny lesion may still occupy so few pixels that the network has difficulty generating a confident mask proposal (the sigmoid threshold may never fire strongly).\n\n2.  Blurred or low‐contrast boundaries hurt proposal matching  \n    – The pipeline generates binary mask proposals by applying sigmoid to token–feature dot-products and then does bipartite matching to the ground-truth masks.  If a lesion’s edges are blurred or have very low contrast, the proposals will be sloppy (fuzzy, multi-modal), so the one‐to‐one matching can fail or match the mask token to only part of the lesion.  This in turn corrupts the positive pairs used in the contrastive loss.\n\n3.  Weak or noisy contrastive signal  \n    – When a mask token covers only a tiny or partially matched region, its embedding will be an unreliable “average” over a few pixels.  Pushing that noisy embedding closer to the correct attribute texts (and away from all other texts) can become unstable—especially since the multi-positive NCE loss assumes that each positive pair is a good, informative anchor.\n\n4.  Missing supervision  \n    – In the extreme, if no mask proposals survive the matching step (because the lesion is too small or too indistinct), then you effectively have zero positive pairs for that lesion at some scales.  That means that scale contributes no learning signal for those hard cases.\n\nIn combination, these effects mean that, while multi-scale alignment greatly helps medium‐sized, well-defined lesions, it can underperform on minute or extremely blurred lesions simply because the mask proposals never reliably capture them at any decoder resolution, and the contrastive machinery cannot attach a stable training signal to those tiny, fuzzy regions."
    },
    {
        "question": "What motivates using multi-scale mask-attribute alignment for fine-grained lesion segmentation?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Malenia first leverages multi-scale mask representations with inherent boundary information to capture diverse lesion regions, then matches fine-grained visual features of lesions with text embeddings, effectively bridging the gap between the contrastive pre-training task and the per-pixel dense prediction task.",
            "Existing methods (Jiang et al., 2024; Lin et al., 2024) overlook the advantage of leveraging multi-scale visual features during cross-modal alignment. In contrast, we match the hierarchical mask token embeddings from different Transformer decoder blocks with text features. This approach enables the model to accurately capture and segment across a range of mask sizes. ... This variation in feature resolution across blocks ensures mask-text alignment at different scales, which is crucial for segmenting classes with large size variations, such as tumors."
        ],
        "final_answer": "Multi-scale mask-attribute alignment is motivated by the need to bridge the gap between image‐level contrastive pre-training and per-pixel segmentation, by capturing boundary-rich, lesion-level features at multiple scales. This allows the model to precisely localize and segment lesions of varying sizes and shapes, delivering fine-grained segmentation even for large or irregular tumors.",
        "relevant_elements": [
            "Multi-Scale Fine-Grained Mask-Attribute Alignment"
        ],
        "id": 19,
        "masked_question": "What motivates using [mask1] for fine-grained lesion segmentation?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-Scale Fine-Grained Mask-Attribute Alignment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.15744v1_figure_1.png",
        "paperid": "2410.15744v1",
        "paper_path": "./papers/2410.15744v1.json",
        "figure_id": "2410.15744v1_figure_1.png",
        "caption": "Figure 1: Overview of the training and inference framework of Malenia. The key contributions of our work are two simple but effective designs: the Multi-scale Fine-Grained Mask-Attribute Alignment and the Cross-Modal Knowledge Injection module. Incorporating these designs empowers our vision-language pre-training framework, resulting in a strong zero-shot segmentation model.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module in Fig. 1 is exactly our “multi-scale fine-grained mask–attribute alignment” (trained with a multi-positive contrastive loss).  We were driven to introduce it by two observations:\n\n 1.  Standard CLIP-style pre‐training only aligns whole‐image embeddings with text, but segmentation is a per-pixel (or per-mask­proposal) task.  There is a big mismatch between the global contrastive objective and the need to pinpoint exact lesion boundaries and shapes.\n\n 2.  Lesions come in wildly different sizes, shapes and with very fuzzy edges—especially unseen ones.  By extracting mask token embeddings at multiple decoder scales, and then explicitly pulling each of those regional (“mask”) embeddings close to its eight textual attribute embeddings (shape, density, location, etc.) while pushing away all the non-matching attributes, we force the model to learn truly fine-grained, attribute-conditioned visual representations.  \n\nIn short, the multi-scale mask–attribute contrastive alignment was motivated by the need to bridge global image–text alignment with local, boundary-sensitive prediction, and to give the model the flexibility to segment lesions of arbitrary size and appearance—even those it’s never seen before."
    },
    {
        "question": "What reasoning underlies integrating deep fusion in the Cross-Modal Knowledge Injection module?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Furthermore, we propose a novel Cross-Modal Knowledge Injection (CMKI) module in Malenia, inspired by the observation that visual and textual embeddings, after feature alignment, are complementary and can mutually reinforce each other.",
            "The deep fusion of vision and language offers two key benefits: 1) Mask representations are enriched with textual information from language models, resulting in more context-aware segmentation. 2) Text embeddings enhance their descriptive capabilities by attending to visual features, enabling segmentation conditioned on specific text prompts."
        ],
        "final_answer": "Malenia integrates deep fusion in its CMKI module because visual and textual embeddings are complementary and can mutually reinforce each other. By deeply fusing mask tokens with attribute embeddings, the module enriches mask representations with contextual language information for more context-aware segmentation, and simultaneously refines text embeddings with visual cues to support segmentation conditioned on textual prompts.",
        "relevant_elements": [
            "Cross-Modal Knowledge Injection module",
            "Deep Fusion"
        ],
        "id": 20,
        "masked_question": "What reasoning underlies integrating [mask1] in the Cross-Modal Knowledge Injection module?",
        "masked_number": 1,
        "masked_elements": [
            "Deep Fusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.15744v1_figure_1.png",
        "paperid": "2410.15744v1",
        "paper_path": "./papers/2410.15744v1.json",
        "figure_id": "2410.15744v1_figure_1.png",
        "caption": "Figure 1: Overview of the training and inference framework of Malenia. The key contributions of our work are two simple but effective designs: the Multi-scale Fine-Grained Mask-Attribute Alignment and the Cross-Modal Knowledge Injection module. Incorporating these designs empowers our vision-language pre-training framework, resulting in a strong zero-shot segmentation model.",
        "qtype": "Design_Rationale",
        "response": "Here is the step-by-step reasoning that leads to why the red-boxed component (“Deep Fusion”) is inserted into the CMKI module:\n\n1.  **Observation of Complementarity**  \n    After the multi-scale mask–attribute contrastive alignment, we end up with two sets of embeddings:  \n    -  Mask tokens  (purely visual, encoding boundary-and-shape information)  \n    -  Attribute tokens  (purely textual, encoding high-level semantic cues like “round,” “enhanced CT,” etc.)  \n    Empirically, these two modalities are **complementary**—the masks know where the lesion is, the text knows *what* it looks like.\n\n2.  **Limitation of “Frozen” Text Embeddings**  \n    Prior work typically keeps the text branch *fixed* at inference time, or only lets visual features attend to text once. That rigid separation means you never allow the language side to *learn from* the actual image features it’s describing, or let the image side fully absorb the nuances of the textual attributes.\n\n3.  **Injecting Cross-Modal Knowledge**  \n    Malenia’s CMKI module therefore does two things at once:  \n    a.  **Updates the mask tokens** by letting them attend to—and *absorb*—the textual attribute embedding (so the visual query gets “dressed up” with language context).  \n    b.  **Updates the attribute tokens** by letting them attend to the visual mask features (so the semantic descriptions become *grounded* in actual image appearance).\n\n4.  **Deep Fusion via Repeated Cross- and Self-Attention**  \n    The red box shows exactly how that mutual enrichment is implemented: a small stack of cross-attention + self-attention layers on each branch.  \n    -  In each cross-attention block, the mask-branch queries the attribute-branch keys/values (and vice-versa) to pull in complementary signals.  \n    -  In each following self-attention block, each branch “digests” the newly gathered information in its own feature space.\n\n5.  **Why This Matters for Zero-Shot Segmentation**  \n    -  **Context-aware masks:** Mask tokens enriched with text help the decoder resolve ambiguous or atypical edges by referring back to, say, “heterogeneous enhancement.”  \n    -  **Visual-grounded text:** Attribute tokens that have seen the actual image features can produce more precise, image-conditioned segmentations when they re-query the pixel grid.  \n\nBy stacking several rounds of this bidirectional, cross-modal attention (the red-boxed “Deep Fusion”), CMKI ensures that neither branch remains siloed. The result is a pair of highly entangled, mutually refined queries—one from the visual stream, one from the textual stream—that, once re-projected onto the 3D feature map, yield significantly sharper, more generalizable zero-shot segmentations of unseen lesions."
    },
    {
        "question": "What is the motivation behind concatenating Mean Hand Gesture Feature with Text Embedding?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "To tackle issue (b), in the second stage, we optimize the text embedding for each image by integrating the hand gesture representation in the first stage to ensure alignment between text and hand image.",
            "In the second stage of training, the main objective is to integrate text embedding with hand gesture features to facilitate the diffusion model to generate realistic hand images."
        ],
        "final_answer": "The Mean Hand Gesture Feature is concatenated with the text embedding in order to inject hand‐specific gesture information into the text condition, thereby aligning the textual description with the corresponding hand appearance and guiding the diffusion model to generate anatomically correct, realistic hand images.",
        "relevant_elements": [
            "Mean Hand Gesture Feature",
            "Text Embedding"
        ],
        "id": 21,
        "masked_question": "What is the motivation behind concatenating [mask1] with [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Mean Hand Gesture Feature",
            "Text Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.15461v2_figure_2.png",
        "paperid": "2408.15461v2",
        "paper_path": "./papers/2408.15461v2.json",
        "figure_id": "2408.15461v2_figure_2.png",
        "caption": "Figure 2: The proposed Hand1000 is designed with a three-stage training process. In Stage I, the primary objective is to compute mean hand gesture feature from images. Stage II builds on this by concatenating the mean hand gesture feature obtained in Stage I with the corresponding text embeddings. These concatenated features are then mapped into a fused embedding, which is further enhanced by linearly fusing it with the original text embedding, resulting in a double-fused embedding. This embedding is optimized using a reconstruction loss through a frozen Stable Diffusion model, ensuring that the final embedding is well-optimized. Stage III involves fine-tuning the Stable Diffusion model for image generation, leveraging the frozen optimized embedding obtained from Stage II.",
        "qtype": "Design_Rationale",
        "response": "Mask₁ is the Mean Hand Gesture Feature (the average feature vector extracted in Stage I) and Mask₂ is the original CLIP Text Embedding for that image’s caption.  They are concatenated so that the caption‐level semantics (Mask₂) carry along an explicit “prior” for the correct hand pose (Mask₁).  In other words, by fusing the gesture feature with the text embedding, the diffusion model is given both what the text says and a concrete structural signal about how the hand should look, which greatly improves the alignment between text prompts and anatomically correct hand generation."
    },
    {
        "question": "What reasoning guides freezing Optimized Embedding when fine-tuning the Stable Diffusion Model?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "In line with image editing works (Kawar et al. 2023 ###reference_b16###), it is essential to keep the Optimized Embedding frozen at this stage to preserve the hand gesture information integrated during the second phase.",
            "This freezing strategy ensures that the embedding remains consistently aligned with the hand features and continues to accurately match the hand images."
        ],
        "final_answer": "The Optimized Embedding is kept frozen during fine-tuning to preserve the hand gesture information integrated in Stage II and to ensure the embedding stays aligned with the hand features and accurately matches the hand images.",
        "relevant_elements": [
            "Optimized Embedding",
            "Stable Diffusion Model"
        ],
        "id": 22,
        "masked_question": "What reasoning guides freezing [mask1] when fine-tuning the Stable Diffusion Model?",
        "masked_number": 1,
        "masked_elements": [
            "Optimized Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.15461v2_figure_2.png",
        "paperid": "2408.15461v2",
        "paper_path": "./papers/2408.15461v2.json",
        "figure_id": "2408.15461v2_figure_2.png",
        "caption": "Figure 2: The proposed Hand1000 is designed with a three-stage training process. In Stage I, the primary objective is to compute mean hand gesture feature from images. Stage II builds on this by concatenating the mean hand gesture feature obtained in Stage I with the corresponding text embeddings. These concatenated features are then mapped into a fused embedding, which is further enhanced by linearly fusing it with the original text embedding, resulting in a double-fused embedding. This embedding is optimized using a reconstruction loss through a frozen Stable Diffusion model, ensuring that the final embedding is well-optimized. Stage III involves fine-tuning the Stable Diffusion model for image generation, leveraging the frozen optimized embedding obtained from Stage II.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In the figure, the red box highlights the “Optimized Embedding” produced at the end of Stage II.  \n2. Stage III uses that embedding as a *fixed* condition to fine-tune the Stable Diffusion model.  \n3. By freezing the embedding, we prevent it from drifting away from the hand‐gesture information it learned in Stage II.  \n4. This forces the diffusion model’s parameters (and only its parameters) to adapt to a stable, gesture-enriched signal, ensuring consistent alignment between embedding and generated images.\n\nAnswer:\n\nWe freeze the Optimized Embedding so that its hand-gesture information remains intact and stable during model fine-tuning; this way the diffusion network learns to adjust its weights to a fixed, gesture-rich condition rather than letting the embedding itself shift."
    },
    {
        "question": "How does the Gesture Recognition Model aggregate frame-specific features into the Mean Hand Gesture Feature?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To extract hand features associated with this gesture, we feed the images into a gesture recognition model (i.e. Mediapipe hands (Zhang et al. 2020 ###reference_b59###)) to obtain features from the final layer of the network.",
            "Subsequently, these features are averaged to obtain a Mean Hand Gesture Feature representation of the gesture, which is used for training in the following stages."
        ],
        "final_answer": "The model takes the per‐frame features output by the gesture recognition network and computes their element‐wise average to form the Mean Hand Gesture Feature.",
        "relevant_elements": [
            "Gesture Recognition Model",
            "Mean Hand Gesture Feature"
        ],
        "id": 23,
        "masked_question": "How does the [mask1] aggregate frame-specific features into the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Gesture Recognition Model",
            "Mean Hand Gesture Feature"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.15461v2_figure_2.png",
        "paperid": "2408.15461v2",
        "paper_path": "./papers/2408.15461v2.json",
        "figure_id": "2408.15461v2_figure_2.png",
        "caption": "Figure 2: The proposed Hand1000 is designed with a three-stage training process. In Stage I, the primary objective is to compute mean hand gesture feature from images. Stage II builds on this by concatenating the mean hand gesture feature obtained in Stage I with the corresponding text embeddings. These concatenated features are then mapped into a fused embedding, which is further enhanced by linearly fusing it with the original text embedding, resulting in a double-fused embedding. This embedding is optimized using a reconstruction loss through a frozen Stable Diffusion model, ensuring that the final embedding is well-optimized. Stage III involves fine-tuning the Stable Diffusion model for image generation, leveraging the frozen optimized embedding obtained from Stage II.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the Mediapipe-hands gesture recognizer in Stage I; Mask2 is the “Mean Hand Gesture Feature.”  The recognizer processes each of the N training images of the same gesture, extracts a feature vector from its final layer for each frame, and then simply takes the element-wise mean (average pooling) of those N vectors to produce the single Mean Hand Gesture Feature."
    },
    {
        "question": "How does the hyperparameter λ influence the linear fusion of Fused Embedding and Text Embedding?",
        "relevant_section_ids": [
            "3.3",
            "3.5"
        ],
        "relevant_context": [
            "A linear fusion of the fused embedding with the original text embeddings is subsequently performed to obtain the Double Fused Embedding: where λ is the hyperparameter,  is Double Fused Embedding,  is Text Embedding, and  is Fused Embedding.",
            "The Text Embedding and the Fused Embedding are linearly fused to produce the Double Fused Embedding: where λ is the hyperparameter,  is Double Fused Embedding,  is Text Embedding, and  is Fused Embedding."
        ],
        "final_answer": "λ controls the relative weighting between the original text embedding and the fused embedding in forming the double fused embedding: the text embedding is scaled by λ and the fused embedding by (1−λ).",
        "relevant_elements": [
            "λ",
            "Fused Embedding",
            "Text Embedding"
        ],
        "id": 24,
        "masked_question": "How does the hyperparameter [mask1] influence the linear fusion of Fused Embedding and Text Embedding?",
        "masked_number": 1,
        "masked_elements": [
            "λ"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.15461v2_figure_2.png",
        "paperid": "2408.15461v2",
        "paper_path": "./papers/2408.15461v2.json",
        "figure_id": "2408.15461v2_figure_2.png",
        "caption": "Figure 2: The proposed Hand1000 is designed with a three-stage training process. In Stage I, the primary objective is to compute mean hand gesture feature from images. Stage II builds on this by concatenating the mean hand gesture feature obtained in Stage I with the corresponding text embeddings. These concatenated features are then mapped into a fused embedding, which is further enhanced by linearly fusing it with the original text embedding, resulting in a double-fused embedding. This embedding is optimized using a reconstruction loss through a frozen Stable Diffusion model, ensuring that the final embedding is well-optimized. Stage III involves fine-tuning the Stable Diffusion model for image generation, leveraging the frozen optimized embedding obtained from Stage II.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step-by-step reasoning:\n\n1. In Stage II the paper defines a “Double Fused Embedding” by linearly combining the original Text Embedding (E_text) and the Fused Embedding (E_fused) coming out of the FC layer.  \n2. The diagram’s red box highlights two multipliers: one multiplies E_fused by (1 – λ), the other multiplies E_text by λ, and then they’re summed.  \n3. Therefore the hyperparameter in [mask1] is λ, and the fusion is  \n   Double_Fused = (1 – λ)·E_fused + λ·E_text  \n4. By adjusting λ between 0 and 1 you shift the balance:  \n   – λ→0 ⇒ mostly the gesture‐aware (fused) embedding is retained  \n   – λ→1 ⇒ mostly the original text embedding is retained  \n\nAnswer:  \n[mask1] is the scalar λ that weights the two embeddings in the linear fusion. It governs the trade-off between Fused Embedding and Text Embedding by assigning λ to the latter and (1 – λ) to the former."
    },
    {
        "question": "How does the Label retrieval module apply thresholding on CLAP Audio Encoder embeddings for audio label selection?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The input audio is then fed into CLAP’s audio encoder to obtain audio embedding. The cosine similarities between the text embeddings and the audio embedding are calculated.",
            "Only labels whose similarity exceeds a threshold are adopted as the audio-label l_a. Here, the threshold is a predefined constant set between 0 and 1, K represents the number of label categories, and 1 is assigned if a certain label is detected, while 0 is assigned if it is not."
        ],
        "final_answer": "The module computes cosine similarities between the CLAP audio embedding and each text embedding (obtained by prompting “this is sound of {label name}”). It then applies a fixed threshold (a constant between 0 and 1) and selects only those labels whose similarity score exceeds this threshold as the audio labels.",
        "relevant_elements": [
            "Label retrieval",
            "CLAP Audio Encoder"
        ],
        "id": 25,
        "masked_question": "How does the [mask1] module apply thresholding on [mask2] embeddings for audio label selection?",
        "masked_number": 2,
        "masked_elements": [
            "Label retrieval",
            "CLAP Audio Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.11729v1_figure_2.png",
        "paperid": "2409.11729v1",
        "paper_path": "./papers/2409.11729v1.json",
        "figure_id": "2409.11729v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed method DETECLAP. To enhance CAV-MAE with object information, we apply CLAP and object detector to the videos in the dataset, thereby acquiring audio-visual labels. Based on these labels, we train CAV-MAE with audio-visual label prediction loss.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed “Label retrieval” block is nothing fancy – it simply takes the embedding that comes out of the CLAP audio encoder (the blue‐boxed module), computes its cosine‐similarity against a library of text‐prompt embeddings (e.g. “this is sound of trumpet”, “this is sound of dog”, etc.), and then binarizes those scores.  In practice you pick a fixed threshold τ (e.g. 0.3 or whatever you set at design time), and for each candidate label you do  \n   s_i = cos( f_audio(x), f_text(“this is sound of label_i”) )  \n   y_i = 1 if s_i ≥ τ, else 0.  \nAll labels whose similarity exceeds τ become your audio labels."
    },
    {
        "question": "How does Label Prediction Loss back-propagate through the Cross-modal Encoder to adjust audio-visual label predictions?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The audio-visual label prediction loss is calculated using the audio-visual labels y. Given the mean-pooled vectors \\bar{Z}_v and \\bar{Z}_a in Eq (3), we add a single linear perceptron layer with weight matrices W_v and W_a and a sigmoid activation function σ to enable the vectors to recognize objects.",
            "We train the CAV-MAE with object information by optimizing the loss: L_total = L_c + L_m + λ (L_v2l + L_a2l)."
        ],
        "final_answer": "The label prediction losses L_v2l and L_a2l are computed on top of linear classifier heads attached to the mean-pooled outputs of the cross-modal encoder. During training, the gradients of these losses flow backward through the sigmoid and linear layers into the mean-pooled vectors and further through the cross-modal encoder itself. This back-propagation updates both the classifier weights and the cross-modal encoder parameters so that its representations become more discriminative for audio-visual object labels.",
        "relevant_elements": [
            "Label Prediction Loss",
            "Cross-modal Encoder"
        ],
        "id": 26,
        "masked_question": "How does [mask1] back-propagate through the Cross-modal Encoder to adjust audio-visual label predictions?",
        "masked_number": 1,
        "masked_elements": [
            "Label Prediction Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.11729v1_figure_2.png",
        "paperid": "2409.11729v1",
        "paper_path": "./papers/2409.11729v1.json",
        "figure_id": "2409.11729v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed method DETECLAP. To enhance CAV-MAE with object information, we apply CLAP and object detector to the videos in the dataset, thereby acquiring audio-visual labels. Based on these labels, we train CAV-MAE with audio-visual label prediction loss.",
        "qtype": "Implementation_Details",
        "response": "First, the red‐boxed “[mask1]” in the diagram is the audio-visual label prediction loss  \n\\[L_{\\text{label}} = L_{v2l} + L_{a2l}\\]  \nwhere  \n•  L_{v2l} is the binary cross‐entropy between the sigmoid-activated visual-label head and the merged labels,  \n•  L_{a2l} is the same for the audio-label head.  \n\nThese two losses are computed by taking the mean‐pooled cross-modal embedding z (Eq. 3 in the paper), passing it through two small linear classifiers (with weight matrices W_v and W_a), and applying a sigmoid.  The gradients of L_{v2l}+L_{a2l} then flow backwards through:  \n1.  the sigmoid activations and their linear layers W_v, W_a,  \n2.  the mean‐pooling operation that produced z,  \n3.  the Cross‐modal Encoder itself (and thence into the audio and visual encoders).  \n\nIn this way the Cross-modal Encoder’s weights are nudged so that its joint representations become better at predicting the presence or absence of each object in both modalities."
    },
    {
        "question": "How does audio-visual label prediction loss extend CAV-MAE's contrastive learning framework?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Specifically, DETECLAP introduces an audio-visual prediction loss during training, allowing the model to predict hundreds of audio-visual object labels from the extracted features.",
            "The audio-visual label prediction loss is calculated using the audio-visual labels y. Given the mean-pooled vectors \\bar{z}_v and \\bar{z}_a in Eq (3), we add a single linear perceptron layer with weight matrices W_v and W_a and a sigmoid activation function σ to enable the vectors \\bar{z}_v and \\bar{z}_a to recognize objects: \\hat{y}_v = σ(W_v \\bar{z}_v), \\hat{y}_a = σ(W_a \\bar{z}_a), where L_{v2l} and L_{a2l} are visual-to-label and audio-to-label binary cross-entropy losses. We train the CAV-MAE with object information by optimizing the loss: L = L_c + L_m + L_{v2l} + L_{a2l}."
        ],
        "final_answer": "DETECLAP augments CAV-MAE’s original contrastive and reconstruction objectives by adding an auxiliary audio-visual label prediction loss. After mean-pooling the cross-modal audio and visual embeddings, it passes each through a lightweight linear+sigmoid head to predict object labels, then includes the resulting audio-to-label and visual-to-label binary cross-entropy losses alongside the contrastive loss. This encourages the model not only to align audio and visual representations but also to learn fine-grained object semantics.",
        "relevant_elements": [
            "audio-visual label prediction loss",
            "CAV-MAE",
            "contrastive learning"
        ],
        "id": 27,
        "masked_question": "How does [mask1] extend CAV-MAE's contrastive learning framework?",
        "masked_number": 1,
        "masked_elements": [
            "audio-visual label prediction loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.11729v1_figure_2.png",
        "paperid": "2409.11729v1",
        "paper_path": "./papers/2409.11729v1.json",
        "figure_id": "2409.11729v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed method DETECLAP. To enhance CAV-MAE with object information, we apply CLAP and object detector to the videos in the dataset, thereby acquiring audio-visual labels. Based on these labels, we train CAV-MAE with audio-visual label prediction loss.",
        "qtype": "Literature_Background",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In Fig. 2 of DETECLAP, the red‐boxed term “[mask1]” sits on the arrow from the fused audio–visual labels back into the CAV-MAE pre-training network.  By referring to the caption and the text of Sec. III-A, we see that that term is the “audio-visual label prediction loss,” i.e.  \n    Lv→l + La→l  \n    (the visual-to-label and audio-to-label binary cross-entropy losses).\n\n2.  In vanilla CAV-MAE, the objective is  \n    – a contrastive loss between audio and visual embeddings (Lc)  \n    – plus the MAE reconstruction loss (Lm).  \n    These drive the model to align coarse audio and visual features and to recover masked tokens, but they don’t explicitly teach it to recognize object categories.\n\n3.  By adding Lv→l + La→l on top of Lc + Lm, DETECLAP injects direct object-level supervision into the same embedding space used for contrastive learning.  \n    – Two linear heads (one on the pooled audio vector, one on the pooled visual vector) predict a multi-hot vector of object labels (obtained via CLAP for audio and YOLOv8 for vision).  \n    – Each is trained with a sigmoid cross-entropy to match those retrieved labels.\n\n4.  In this way, the model’s embeddings are not only pulled together when audio and video match (contrastive) and trained to reconstruct masked tokens (MAE), but are also pushed to contain fine-grained object information (label prediction).  \n\nAnswer:  \n[mask1] is the auxiliary audio-visual label prediction loss (Lv→l + La→l), which extends CAV-MAE’s contrastive learning by adding multi-label classification heads on the pooled audio/visual embeddings and training them with binary cross-entropy against object labels. This forces the learned embeddings to encode explicit object semantics in addition to the coarse cross-modal alignment learned by contrastive loss."
    },
    {
        "question": "How does Spatial Clue Aggregator enhance or reinterpret PoseNet's channel reduction strategy?",
        "relevant_section_ids": [
            "2.3",
            "3.3"
        ],
        "relevant_context": [
            "“As shown in Fig. 2, the model processes channel-wise concatenated monocular video frames, which are then passed through several convolutional layers for channel reduction, followed by an average pooling layer to produce a tensor of shape … This tensor, representing a combination of three Euler angles and three translational components, lacks interpretability for geometric modeling and robustness in scenarios involving moving objects.” (Section 2.3)",
            "“Having obtained the feature flow S_i, absolute feature position P_i, their corresponding confidence C_i, and the downsampled dense point cloud X_i, we proceed to encode them into a homogeneous positional embedding space E_i^p. First, we normalize S_i, P_i and X_i into the range [−1,1] using linear mapping, facilitating a uniform feature representation across different scales. Subsequently, these three positional priors are integrated into positional embeddings E_i^p as follows: E_i^p = W_{p2}(ReLU(W_{p1}([S_i^norm, P_i^norm, X_i^norm]))), where W_{p1} and W_{p2} are two consecutive convolutional layers with learnable parameters that map 2D or 3D position vectors into a higher embedding dimension.” (Section 3.3)"
        ],
        "final_answer": "Whereas traditional PoseNet applies generic convolutional layers purely to shrink (i.e. ‘reduce’) the channel dimension of concatenated frames, the Spatial Clue Aggregator replaces that blind channel reduction with a learned embedding of explicit spatial priors.  It first gathers dense 2D feature flows, pixel coordinates, confidence scores, and 3D point-cloud locations, normalizes each, concatenates them, and then uses a small two-layer convolutional network to generate a compact positional embedding.  In effect, channel reduction is reinterpreted as a fusion of geometry-aware features rather than mere feature compression, yielding richer, more interpretable inputs for subsequent pose regression.",
        "relevant_elements": [
            "Spatial Clue Aggregator",
            "channel reduction"
        ],
        "id": 29,
        "masked_question": "How does [mask1] enhance or reinterpret PoseNet's channel reduction strategy?",
        "masked_number": 1,
        "masked_elements": [
            "Spatial Clue Aggregator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.05283v1_figure_2.png",
        "paperid": "2407.05283v1",
        "paper_path": "./papers/2407.05283v1.json",
        "figure_id": "2407.05283v1_figure_2.png",
        "caption": "Figure 2: An illustration of our proposed SCIPaD framework. Compared with the traditional PoseNet [14] architecture, it comprises three main parts: (1) a confidence-aware feature flow estimator, (2) a spatial clue aggregator, and (3) a hierarchical positional embedding injector.",
        "qtype": "Literature_Background",
        "response": "In the original PoseNet you take the last CNN feature‐map (C×H×W), run a single 1×1 conv or “channel‐reduction” layer down to six channels, average‐pool it over H×W and call that your 6‐DoF pose.  Our “Spatial Clue Aggregator” (the red–boxed module) completely re-casts that naïve channel‐reduction step as a learned, multi-modal embedding of geometric cues rather than a blind squeeze of all information.  Concretely:\n\n 1.  Instead of immediately collapsing C→6 channels, we first assemble three kinds of per‐pixel “spatial clues” – the 2D feature flow (the Sᵢ maps), the raw grid coordinates, and the depth‐lifted 3D points – plus a learned confidence for every flow vector.  \n 2.  A small two-layer convolutional network then “reduces” these rich spatial inputs into a compact channel embedding (so formally it *is* a channel reduction, but now it carries explicit geometry).  \n 3.  Finally, this geometry‐laden channel embedding is gated and fused into the semantic feature stream at each scale (rather than thrown away by an all-pooled 6-channel bottleneck).\n\nIn this way, the Spatial Clue Aggregator turns the old one-shot C→6 channel reduction into a *hierarchical, geometry-aware* channel embedding, preserving—and indeed capitalizing on—positional and depth information for far more robust 6-DoF regression."
    },
    {
        "question": "In what way does Confidence-Aware Feature Flow Estimator extend CNN-based feature extractor's capability for pose estimation?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "To address the aforementioned issues, we first propose a confidence-aware feature flow estimator (CAFFE) to calculate and adjust dense feature correspondences with the consideration of pixel-wise confidence levels. This module explicitly extracts abundant positional clues regarding 2D feature translations, which provides strong constraints for ego-motion recovery.",
            "Unlike previous work [55], which primarily emphasizes feature flow generation across consecutive frames, our proposed CAFFE also produces pixel-wise confidence levels for reweighting the feature flow."
        ],
        "final_answer": "CAFFE extends the basic CNN feature extractor by computing dense, differentiable 2D feature correspondences (feature flows) between frames and assigning each correspondence a pixel-wise confidence score—thereby supplying explicit geometric (positional) cues that bolster pose estimation.",
        "relevant_elements": [
            "Confidence-Aware Feature Flow Estimator",
            "CNN-based feature extractor"
        ],
        "id": 30,
        "masked_question": "In what way does [mask1] extend [mask2]'s capability for pose estimation?",
        "masked_number": 2,
        "masked_elements": [
            "Confidence-Aware Feature Flow Estimator",
            "CNN-based feature extractor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.05283v1_figure_2.png",
        "paperid": "2407.05283v1",
        "paper_path": "./papers/2407.05283v1.json",
        "figure_id": "2407.05283v1_figure_2.png",
        "caption": "Figure 2: An illustration of our proposed SCIPaD framework. Compared with the traditional PoseNet [14] architecture, it comprises three main parts: (1) a confidence-aware feature flow estimator, (2) a spatial clue aggregator, and (3) a hierarchical positional embedding injector.",
        "qtype": "Literature_Background",
        "response": "Step 1: Identify what each box actually is  \n- The blue‐outlined module in the “Traditional PoseNet” (mask₂) is simply a CNN encoder that pools its deepest semantic feature map down into a single 6-dimensional vector for pose regression. It learns predominantly “what” is in the scene, not “where” things have moved.  \n- The red‐outlined module in SCIPaD (mask₁) is the Confidence-Aware Feature Flow Estimator (CAFFE). It explicitly computes dense 2D correspondences (feature-flow) between the reference and target frames, along with a per-pixel confidence score.\n\nStep 2: Compare their roles in pose estimation  \n- The vanilla CNN encoder (mask₂) passively “summarizes” appearance into a fixed-length code and lets a small MLP predict a single pose. It has no built-in mechanism for reasoning about geometric displacements or whether those displacements are reliable.  \n- CAFFE (mask₁) on the other hand actively measures how each pixel moves from one frame to the next (via a differentiable 2D soft-argmax), and it also learns to down-weight unreliable matches (via the confidence map). Those explicit, per-pixel motions and their certainties become hard geometric constraints that feed into the later pose‐decoder.\n\nStep 3: State how mask₁ extends mask₂’s capability  \nIn short, CAFFE turns the black-box, semantics-only feature extractor of traditional PoseNet into a geometry-aware module that  \n  1. explicitly localizes correspondences with sub-pixel accuracy, and  \n  2. re-weights them by confidence,  \nthereby injecting strong positional (epipolar) constraints into the pose estimation that the plain CNN encoder cannot provide."
    },
    {
        "question": "How does the confidence-aware feature flow estimator generate confidences to guide the Spatial Clue Aggregator?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Another crucial piece of information conveyed by A^i is the confidence level c^i, which indicates the quality of the calculated feature flow.",
            "We argue that c depends on two factors:\n\n• Magnitude of affinity values. If all the affinity values are relatively small, it suggests a lack of strong feature correspondences within the specified window. For example, if a moving object occupies the entire window and occludes the original matched pixel, this can result in smaller affinity values in the entire window.\n\n• Distribution of affinity values. If the largest affinity values are closely clustered, it suggests the presence of texture-less areas or keypoints that are difficult to discriminate.",
            "To avoid these aforementioned issues and lower their impact on matched correspondences, we formulate the feature matching confidence level c as follows:\n\nwhere c tends to approach 1 only when there is a unique large affinity value within the given window, indicating high confidence in the feature correspondence. This formulation assists in assessing the reliability of feature matches by considering both the magnitude and the distribution of affinity values across spatial dimensions.",
            "Having obtained the feature flow ΔP, absolute feature position P, their corresponding confidence C, and the downsampled dense point cloud V, we proceed to encode them into a homogeneous position embedding space."
        ],
        "final_answer": "The confidence‐aware feature flow estimator computes a per‐pixel confidence by analysing its cross‐frame affinity volume: it measures both the absolute strength of the highest affinity response and how dominant that response is relative to the rest of the window (via a softmax‐style normalization). This confidence score—which approaches 1 only when there is a single, strong match—is then output alongside the sub‐pixel flow and used in the Spatial Clue Aggregator to reweight and fuse the positional clues.",
        "relevant_elements": [
            "Confidence-Aware Feature Flow Estimator",
            "Spatial Clue Aggregator"
        ],
        "id": 31,
        "masked_question": "How does the [mask1] generate confidences to guide the Spatial Clue Aggregator?",
        "masked_number": 1,
        "masked_elements": [
            "Confidence-Aware Feature Flow Estimator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.05283v1_figure_2.png",
        "paperid": "2407.05283v1",
        "paper_path": "./papers/2407.05283v1.json",
        "figure_id": "2407.05283v1_figure_2.png",
        "caption": "Figure 2: An illustration of our proposed SCIPaD framework. Compared with the traditional PoseNet [14] architecture, it comprises three main parts: (1) a confidence-aware feature flow estimator, (2) a spatial clue aggregator, and (3) a hierarchical positional embedding injector.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in the figure is our “Confidence–Aware Feature Flow Estimator.”  What it does is two–fold:\n\n 1.  It first computes a small local correlation (affinity) volume between each target–frame pixel and a window of candidate pixels in the reference frame (by normalizing both feature patches to unit length and doing a dot‐product for every offset in that window).  \n 2.  It then turns that set of raw dot–products into a probability distribution via a softmax (this is exactly the same 2D “soft-argmax” trick we borrow from [56] – it gives us sub–pixel flow and also a clean way to get confidences).  \n\nConcretely, if A(p,Δ) is the affinity score at target pixel p for shift Δ, we define  \n   \n   P(p,Δ) = exp A(p,Δ) ∕ Σ_{Δ′} exp A(p,Δ′)  \n\nand then   \n\n   – the flow f(p) = Σ_{Δ} P(p,Δ)·Δ  \n   – the confidence C(p) = max_{Δ} P(p,Δ).  \n\nBecause P comes from a softmax, C(p) is close to 1 only when there is a single, very strong match in the window (large magnitude + sharp peak).  Those confidence values C get passed on to the Spatial Clue Aggregator to re–weight each pixel’s flow according to how reliable that match actually was."
    },
    {
        "question": "How does the Hierarchical Positional Embedding Injector balance semantic and positional features across scales?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "In this work, our proposed hierarchical positional embedding injector aims to effectively integrate low-level positional embeddings P into high-level semantic features F^s across different scales.",
            "For the features F^s and P from the i-th stage, we first employ a channel reduction block to transform F^s into compact embeddings. Subsequently, the compressed positional embeddings are integrated into the semantic features F^s with a learnable gate g, which automatically modulates the importance of semantic and spatial information.",
            "The motivation for introducing the gating mechanism lies in leveraging the strengths of different network layers: the shallower layers of the network encode more precise positional embeddings, while the deeper layers preserve richer semantic information. In contrast to prior arts which indiscriminately fuse the cross-modal information, our approach ensures the network adaptively focuses on semantic and positional information with different scales.",
            "Afterwards, the selectively fused features are combined with those from the preceding layer, yielding spatial-semantic co-attentive feature representations."
        ],
        "final_answer": "The injector first compresses the semantic features via a channel-reduction block, then uses a learnable gate to weight and fuse them with the positional embeddings at each scale. This gating lets the network automatically allocate more weight to positional cues in shallow layers and more to semantic cues in deeper layers, and it combines the fused outputs hierarchically to produce spatial-semantic co-attentive representations.",
        "relevant_elements": [
            "Hierarchical Positional Embedding Injector"
        ],
        "id": 32,
        "masked_question": "How does the [mask1] balance semantic and positional features across scales?",
        "masked_number": 1,
        "masked_elements": [
            "Hierarchical Positional Embedding Injector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.05283v1_figure_2.png",
        "paperid": "2407.05283v1",
        "paper_path": "./papers/2407.05283v1.json",
        "figure_id": "2407.05283v1_figure_2.png",
        "caption": "Figure 2: An illustration of our proposed SCIPaD framework. Compared with the traditional PoseNet [14] architecture, it comprises three main parts: (1) a confidence-aware feature flow estimator, (2) a spatial clue aggregator, and (3) a hierarchical positional embedding injector.",
        "qtype": "Experimental_Results",
        "response": "The mystery “red‐boxed” module is in fact the paper’s so‐called Hierarchical Positional Embedding Injector.  Its whole job is to take the semantic feature maps produced at each stage, squeeze them down to a compact form, and then add in the positional embeddings from the confidence‐aware flow/point‐cloud branch – but not in a hard‐wired 50/50 way.  Instead, at stage i it learns a little gating tensor G_i (the same spatial size as the feature map) and computes  \n  \n  H_i = F^s_i  +  G_i ⊙ E^p_i  +  C_i(H_{i−1})  \n  \nwhere  \n •  F^s_i is the stage-i semantic feature,  \n •  E^p_i is the stage-i positional embedding,  \n •  G_i is a per-location “importance” gate, and  \n •  C_i(·) is a small conv/skip block carrying forward the fused H_{i−1}.  \n  \nBy learning G_i at each resolution, the network can automatically up- or down-weight geometry versus semantics (shallow layers favour geometry, deep layers favour semantics) and then pass the mixed features on to the next scale.  In this way it “balances” the two modalities in a truly hierarchical, data-driven fashion."
    },
    {
        "question": "How does the Group Prompts Aggregation Module balance semantic diversity and integrity through group token aggregation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In order to obtain finer-grained spatial features, an effective semantic grouping of spatial features is performed.",
            "Subsequently, we designed several group prompts that can be updated, with the number of group prompts being M.",
            "In order to better group and aggregate image features, we send the image features F_i and G T_i representing group prompts to the encoder of Transformer model for aggregation, where N is the number of image tokens, D is the feature dimension of the token.",
            "The above processes are expressed as: [Transformer aggregation formula]. However, the above operations can only roughly group features.",
            "In order to obtain more refined group features for subsequent encoding of semantic information, we recombine these updateable group prompts G T_i with the original features F_i. Here, G T_i serves as query and F_i serves as key and value. This step further refines the semantic information of each group in order to complete visual–semantic projection locally.",
            "Among them, G T_i is the group semantic vector, which is a clustering of local semantic information."
        ],
        "final_answer": "The module first introduces multiple learnable \"group prompts\" that coarsely partition the backbone’s spatial features into M distinct clusters via a Transformer encoder—this ensures a diverse set of semantic groupings. It then takes each updated prompt token and attends back to the original feature map (using the prompt as query and the pixels as key/value) to refine and restore the detailed semantics within each group. In this two‐stage process, the initial grouping preserves diversity across clusters, and the subsequent prompt‐to‐feature attention step reintegrates the fine‐grained information, thereby balancing semantic diversity with integrity.",
        "relevant_elements": [
            "Group Prompts Aggregation Module"
        ],
        "id": 33,
        "masked_question": "How does the [mask1] balance semantic diversity and integrity through group token aggregation?",
        "masked_number": 1,
        "masked_elements": [
            "Group Prompts Aggregation Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.12253v2_figure_2.png",
        "paperid": "2408.12253v2",
        "paper_path": "./papers/2408.12253v2.json",
        "figure_id": "2408.12253v2_figure_2.png",
        "caption": "Figure 2: Pipeline of Epsilon. The feature representation of the image is first obtained through a pre-trained backbone network that is frozen and cannot be updated. The image features are then applied to the Group Prompts Aggregation Module (GPA Module), which represents local semantic generation, and the Global Forward Propagation Module (GFP Module), which represents global diversity semantic generation. Finally, the output of the two modules is integrated to obtain the complete semantics. (zoom in for a better view)",
        "qtype": "Experimental_Results",
        "response": "The short answer is that nothing mystical is hiding behind that red‐boxed “mystery module” – it is simply a tiny two‐stage transformer that uses a handful of learnable “group prompts” to softly cluster the ViT’s patch‐tokens into H disjoint semantic groups.  By doing so it naturally drives two competing desiderata at once:\n\n 1.  **Semantic diversity**  – because you have H distinct prompts (each one a small trainable vector) and each prompt “queries” the full set of image tokens, the model is encouraged to push each prompt to specialize on a different part of the scene (blue sky, sunset glow, foreground clouds, etc.).  In other words, the multiple prompts act like H different “lenses,” each pulling out its own unique semantic slice.  \n 2.  **Semantic integrity**  – at the same time, all H prompts share the same Transformer‐encoder weights, the same normalization layers, and the same softmax‐based attention machinery.  That shared backbone forces each group to be a *coherent* summary of the pixels it attends to, and it also guarantees that you never lose sight of the global structure of the feature map.  \n\nConcretely, what happens inside the red box is:\n\n a.  You append H learnable tokens (“group prompts”) to the ViT’s patch tokens.  \n b.  You run a single pass of Transformer‐encoder layers.  Now each prompt has absorbed a *coarse* summary of the entire image.  \n c.  You detach those H prompt‐outputs and re-use them as *queries* against the *original* patch‐tokens (as keys and values) in a second cross‐attention step.  The softmax‐attention automatically figures out which patches belong to which prompt, so each prompt ends up with a *refined*, *locally coherent* semantic vector.  \n\nBecause each prompt can only “own” a fraction of the attention mass, they are forced to carve up the scene into H different pieces (diversity), yet they all attend back into the same frozen feature bank and share normalization and projection weights (integrity).  That is exactly how the Group Prompts Aggregation Module balances diversity and integrity in one neat, fully differentiable package."
    },
    {
        "question": "How does the Global Forward Propagation Module utilize multiple feature blocks from Split Conv to enrich global semantics?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We first encode the feature F_i into Φ feature blocks, respectively F_i^G. Then, Φ identical modules are applied to each feature block.",
            "Taking F_i^{Gm} as an example, we first use a multi-layer perceptron (MLP) to re-represent the features. Then, the features are fed into softmax to obtain the weight representation of each feature point in the feature map A_i^m.",
            "After obtaining A_i^m, we perform a dot product with the original input feature F_i. Then, all features are summed to obtain the global semantics S_i^m corresponding to F_i^{Gm}.",
            "After obtaining all the global semantics S_i^m, we use the concatenate operation to combine S_i^1, S_i^2, … and get F_i^G."
        ],
        "final_answer": "The Global Forward Propagation Module first splits the image’s global feature into multiple blocks via Split Conv. Each block is fed into an identical sub-module that (1) uses an MLP followed by softmax to compute personalized spatial weights, (2) applies these weights by dot-product with the original feature to highlight important global information, and (3) sums the weighted features to form a per-block global semantic vector. Finally, all per-block semantic vectors are concatenated to produce a richer, more diverse global semantic representation.",
        "relevant_elements": [
            "Split Conv",
            "Global Forward Propagation Module"
        ],
        "id": 34,
        "masked_question": "How does the [mask1] utilize multiple feature blocks from [mask2] to enrich global semantics?",
        "masked_number": 2,
        "masked_elements": [
            "Global Forward Propagation Module",
            "Split Conv"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.12253v2_figure_2.png",
        "paperid": "2408.12253v2",
        "paper_path": "./papers/2408.12253v2.json",
        "figure_id": "2408.12253v2_figure_2.png",
        "caption": "Figure 2: Pipeline of Epsilon. The feature representation of the image is first obtained through a pre-trained backbone network that is frozen and cannot be updated. The image features are then applied to the Group Prompts Aggregation Module (GPA Module), which represents local semantic generation, and the Global Forward Propagation Module (GFP Module), which represents global diversity semantic generation. Finally, the output of the two modules is integrated to obtain the complete semantics. (zoom in for a better view)",
        "qtype": "Experimental_Results",
        "response": "First, note what the two masked regions are:\n\n  • [mask2] (blue box) is the “Split Conv” operation that chops the single backbone feature map Fᵢ into M smaller feature‐blocks  \n  • [mask1] (red box) is one of the M identical “Global Forward Propagation Modules” (GFP modules)\n\nHere is the step–by–step of how the red box (GFP Module) uses the multiple blocks produced by the blue box to build a richer global representation:\n\n1.  Split into blocks (blue box):  \n    – The input feature Fᵢ (size H×W×C) is first passed through a light “Split Conv” layer which partitions it into M disjoint sub‐features  \n      {Fᵢ^{G1}, Fᵢ^{G2}, …, Fᵢ^{GM}}  \n    – Each of these has the same spatial extent but a reduced number of channels (so that concatenation later restores the original C).\n\n2.  Per‐block global weighting (red box):  \n    – Take one block Fᵢ^{Gm}  \n    – Feed it through a small MLP (1×1 convolutions or fully-connected layers across the channel dimension) to produce a raw score map  \n    – Apply softmax over the H×W locations ⇒ Aᵢ^{M} (a spatial attention map that sums to 1)  \n    – Element–wise multiply Aᵢ^{M} ⊙ Fᵢ^{Gm} to re-weight the block, so that the most “globally important” locations in that block are highlighted\n\n3.  Token–level pooling:  \n    – Sum (or average) the weighted features over the H×W positions to get a single semantic vector Sᵢ^{M} for that block\n\n4.  Multi‐view aggregation:  \n    – Repeat steps 2–3 for each of the M blocks  \n    – You obtain M vectors {Sᵢ^{1},…,Sᵢ^{M}} that each capture a different “facet” of the global feature  \n    – Concatenate them into one long global semantic embedding\n\nBy splitting the feature into M blocks and then learning a separate spatial weighting and pooling for each block, the model forces itself to look at M different “views” of global context.  Concatenating all M pooled vectors therefore yields a far more diverse and information‐rich global representation than would a single top‐layer pooling or channel attention alone."
    },
    {
        "question": "What limitations arise from freezing backbone features before the Group Prompts Aggregation Module?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Group Prompts Aggregation Module"
        ],
        "id": 35,
        "masked_question": "What limitations arise from freezing backbone features before the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Group Prompts Aggregation Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.12253v2_figure_2.png",
        "paperid": "2408.12253v2",
        "paper_path": "./papers/2408.12253v2.json",
        "figure_id": "2408.12253v2_figure_2.png",
        "caption": "Figure 2: Pipeline of Epsilon. The feature representation of the image is first obtained through a pre-trained backbone network that is frozen and cannot be updated. The image features are then applied to the Group Prompts Aggregation Module (GPA Module), which represents local semantic generation, and the Global Forward Propagation Module (GFP Module), which represents global diversity semantic generation. Finally, the output of the two modules is integrated to obtain the complete semantics. (zoom in for a better view)",
        "qtype": "Others",
        "response": "Chain-of-thought  \n1. In Fig. 2 (highlighted in red) the authors state that they take a pre-trained ViT-B/16 backbone and “freeze” it – i.e. its weights are not updated during Epsilon’s training.  \n2. By freezing the backbone features at that point, no gradient flows back into any of the convolutional or transformer layers of the backbone.  \n3. Consequently the model cannot adapt the low-level and mid-level visual representations to the particular demands of the Group Prompts Aggregation (GPA) module or the downstream zero-shot task.  \n4. Practically this means:  \n   • The features may carry biases or “blind spots” from the original pre-training domain (e.g. ImageNet) that are never corrected.  \n   • Fine-tuning benefits – such as sharpening task-specific edges, textures or object details – are lost.  \n   • The grouping prompts must operate on a fixed feature manifold, which can limit how precisely they can cluster and refine local semantics.  \n5. In short, freezing the backbone before the red-boxed GPA block simplifies training but at the cost of flexibility: the visual encoder cannot specialize or improve for the new multi-label zero-shot setting.  \n\nAnswer  \nBy freezing the backbone features before the Group Prompts Aggregation block, the model loses the ability to adapt or fine-tune its early and mid-level visual representations to the specific multi-label zero-shot task. As a result, the grouping prompts must work with a fixed, potentially sub-optimal feature space (with any biases inherited from the pre-training domain), which can hamper the precision of local semantic grouping and overall task performance."
    },
    {
        "question": "What alternative strategy could replace the Global Forward Propagation Module for enriching global semantics?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In existing methods, the extraction of global features is rough. Most methods use the top-level features and design some channel (or global) attention structures to achieve this.",
            "Although our GFP module has some structural similarities with the traditional channel attention model, our distribution of weights is not limited to the channel dimension."
        ],
        "final_answer": "Instead of the Global Forward Propagation Module, one could fall back on a traditional channel (or global) attention mechanism applied to the top-level features to enrich global semantics.",
        "relevant_elements": [
            "Global Forward Propagation Module"
        ],
        "id": 36,
        "masked_question": "What alternative strategy could replace the [mask1] for enriching global semantics?",
        "masked_number": 1,
        "masked_elements": [
            "Global Forward Propagation Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.12253v2_figure_2.png",
        "paperid": "2408.12253v2",
        "paper_path": "./papers/2408.12253v2.json",
        "figure_id": "2408.12253v2_figure_2.png",
        "caption": "Figure 2: Pipeline of Epsilon. The feature representation of the image is first obtained through a pre-trained backbone network that is frozen and cannot be updated. The image features are then applied to the Group Prompts Aggregation Module (GPA Module), which represents local semantic generation, and the Global Forward Propagation Module (GFP Module), which represents global diversity semantic generation. Finally, the output of the two modules is integrated to obtain the complete semantics. (zoom in for a better view)",
        "qtype": "Others",
        "response": "In the paper’s Global Forward Propagation (GFP) module the red‐boxed “black‐box” is in fact just an MLP followed by a softmax that produces a spatial weighting over that block’s features.  If you wanted to swap out that little MLP+softmax for something else that can also produce a rich global descriptor, the two obvious off–the–shelf replacements would be:\n\n  1. A standard channel‐attention or “SE-block” style module (i.e. global pooling → small MLP → sigmoid) to re-weight channels instead of locations, or  \n  2. A principal-vector–based pooling step (e.g. take the first principal component, as in Ben-Cohen et al. or Zhang et al.) to extract the “principal” global feature.  \n\nEither of these would slot in wherever the paper used that tiny MLP+softmax and still give you a single, enriched global semantic vector."
    },
    {
        "question": "What trade-offs might arise from integrating CDWConv and FreqSSM for balancing low- and high-frequency motion features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "FreqSSM",
            "CDWConv"
        ],
        "id": 37,
        "masked_question": "What trade-offs might arise from integrating [mask1] and [mask2] for balancing low- and high-frequency motion features?",
        "masked_number": 2,
        "masked_elements": [
            "CDWConv",
            "FreqSSM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17532v1_figure_1.png",
        "paperid": "2411.17532v1",
        "paper_path": "./papers/2411.17532v1.json",
        "figure_id": "2411.17532v1_figure_1.png",
        "caption": "Figure 1: Overview of FTMoMamba. FTMoMamba is built upon the Diffusion model within FTMamba modules, which explores the frequency-domain information to guide motion generation, as well as text-semantic information to ensure text-motion consistency in the latent space. Specifically, the diffusion model compresses and decompresses the raw motion sequence, reducing the interference of redundant information in motion generation. FTMamba, as the core of the denoising module, consists of FreqMamba and TextMamba. The former decomposes motion sequences into low- and high-frequency components to guide the generation of static and fine-grained motions, respectively. The latter aligns textual semantics with sequential features to ensure text-motion consistency.",
        "qtype": "Others",
        "response": "Here is a step‐by‐step unpacking of the two pieces in question and the kinds of compromises you have to juggle when you put them together:\n\n1. Identify [mask₁] and [mask₂]  \n   - [mask₁] (red box) is the CDWConv block – the cascaded, depth-wise convolutions with growing dilations that “scan” very local, short-term temporal patterns.  \n   - [mask₂] (blue box) is the FreqSSM – the frequency-aware State-Space Model that explicitly splits the signal into low- and high-frequency bands, then re-weights them with learnable α/β coefficients in its A-matrix.\n\n2. What each brings to the table  \n   - CDWConv gives you fine-grained, local temporal detail (good for capturing jitter, very fast transients, micro-gestures).  \n   - FreqSSM provides a way to balance long-range (low-frequency) drifts and quick (high-frequency) motions in a principled, “signal decomposition” sense.\n\n3. Where the tension shows up  \n   a. Local detail vs. global consistency  \n      – If you lean too heavily on CDWConv you may capture all the tiny wiggles but lose the smoothness of a stable posture.  \n      – If you over-emphasize the low-freq pole of FreqSSM you risk washing out the micro-motions that really make a walk look alive.  \n   b. Computational and optimization cost  \n      – CDWConv is lightweight, but stacking many dilated layers raises memory and FLOP counts.  \n      – FreqSSM brings in wavelet transforms + extra learnable gates (α/β), which adds parameters and slows down the backward pass.  \n   c. Interference and gradient competition  \n      – Both modules share the same upstream activations.  If their losses or signal-gating pull in different “frequency directions,” you can get unstable training or have one module “dominate” the other.  \n   d. Hyper-parameter tuning  \n      – You must find the right dilation pattern for CDWConv (kernel sizes, dilation rates) and the right initialization or regularization for the α/β that balance low vs. high bands in FreqSSM.  That is extra knobs to turn.\n\n4. Net effect  \n   – When done well, you get crisp high-frequency detail riding on a stable low-frequency backbone.  \n   – But in practice you trade off:  \n     • richer local detail vs. a smoother overall trajectory,  \n     • faster inference (few CDW layers, coarse SSM) vs. highest fidelity,  \n     • simpler, more stable training vs. the risk of “over-engineering” your frequency split.\n\nIn short, the integration of the lightweight CDWConv for local feature extraction with the more heavyweight FreqSSM for global, frequency-aware state updates forces you to balance detail against smoothness, cost against expressivity, and stability against sensitivity."
    },
    {
        "question": "What biases could frozen CLIP embeddings introduce during text-guided motion synthesis in TextMamba?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "TextMamba",
            "CLIP"
        ],
        "id": 38,
        "masked_question": "What biases could frozen [mask1] embeddings introduce during text-guided motion synthesis in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP",
            "TextMamba"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17532v1_figure_1.png",
        "paperid": "2411.17532v1",
        "paper_path": "./papers/2411.17532v1.json",
        "figure_id": "2411.17532v1_figure_1.png",
        "caption": "Figure 1: Overview of FTMoMamba. FTMoMamba is built upon the Diffusion model within FTMamba modules, which explores the frequency-domain information to guide motion generation, as well as text-semantic information to ensure text-motion consistency in the latent space. Specifically, the diffusion model compresses and decompresses the raw motion sequence, reducing the interference of redundant information in motion generation. FTMamba, as the core of the denoising module, consists of FreqMamba and TextMamba. The former decomposes motion sequences into low- and high-frequency components to guide the generation of static and fine-grained motions, respectively. The latter aligns textual semantics with sequential features to ensure text-motion consistency.",
        "qtype": "Others",
        "response": "Here is a concise chain of thought leading to the answer:\n\n1. In the red box the model is using a frozen CLIP text encoder to produce the text embedding fᵗ.  \n2. In the blue box (TextMamba/TextSSM) these frozen CLIP embeddings are the sole source of sentence‐level supervision for aligning text and motion.  \n3. Because the CLIP encoder is frozen, any systematic biases or gaps it learned during its large‐scale, vision–language pretraining (e.g. cultural stereotypes, gender/ethnicity slants, under-representation of niche actions or phrasings, etc.) will be passed directly into the TextMamba alignment.  \n4. In practice this can mean that, when asked to generate certain motions, the model will reproduce the same selectional preferences or omissions that are implicit in CLIP’s embedding space—leading to stereotyped or semantically incomplete motion outputs.\n\nAnswer  \nWhat biases could frozen CLIP embeddings introduce during text-guided motion synthesis in TextMamba?  \n\nThey can carry over any cultural, gender, or situational stereotypes and vocabulary gaps that CLIP picked up in pretraining; because those embeddings are never fine-tuned, those priors directly shape the TextSSM’s alignment and can cause the generated motions to reflect CLIP’s original biases (e.g. stereotypical action assignments, poor handling of rare or nuanced instructions, or systematic semantic omissions)."
    },
    {
        "question": "What motivates multiplicative gating between FreqSSM output and motion features in FreqMamba?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "FreqMamba … employs the frequency state space model (FreqSSM) to capture high- and low-frequency information, guiding the model to generate fine-grained motions.",
            "We further perform information selection through a multiplication-based method to obtain the motion feature f^{mo}_n guided by frequency-domain information."
        ],
        "final_answer": "The multiplicative gating is motivated by the need to perform information selection—using frequency-domain cues from FreqSSM to modulate (gate) the original motion features so that the model can emphasize static poses and fine-grained motions captured in the frequency domain.",
        "relevant_elements": [
            "FreqMamba",
            "FreqSSM"
        ],
        "id": 39,
        "masked_question": "What motivates multiplicative gating between [mask1] output and motion features in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "FreqSSM",
            "FreqMamba"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17532v1_figure_1.png",
        "paperid": "2411.17532v1",
        "paper_path": "./papers/2411.17532v1.json",
        "figure_id": "2411.17532v1_figure_1.png",
        "caption": "Figure 1: Overview of FTMoMamba. FTMoMamba is built upon the Diffusion model within FTMamba modules, which explores the frequency-domain information to guide motion generation, as well as text-semantic information to ensure text-motion consistency in the latent space. Specifically, the diffusion model compresses and decompresses the raw motion sequence, reducing the interference of redundant information in motion generation. FTMamba, as the core of the denoising module, consists of FreqMamba and TextMamba. The former decomposes motion sequences into low- and high-frequency components to guide the generation of static and fine-grained motions, respectively. The latter aligns textual semantics with sequential features to ensure text-motion consistency.",
        "qtype": "Design_Rationale",
        "response": "Let me first restate what the two masks pick out:\n\n  • [mask₁] is the red‐outlined block inside FreqMamba – that is the Frequency State-Space Module (FreqSSM).  \n  • [mask₂] is the entire blue-outlined module – the FreqMamba subnetwork.  \n\nInside FreqMamba, after running the noisy latent through a small linear → CDWConv → FreqSSM pipeline, the paper says:\n\n  “We further perform information selection through a multiplication-based method to obtain the motion feature f ╳ guided by frequency-domain information:\n   fₙᵐᵒ = σ(…FreqSSM output…) ⊙ (linear projection of the original motion feature).”\n\nIn other words, the numeric output of the FreqSSM is turned into a sigmoid gating vector which is then multiplied element-wise into the motion feature.  \n\nWhy do they do this?  \n\nChain of thought:  \n1. In motion generation we have two very different kinds of signal hiding in the same latent stream – the slow, “static” postural components (low-frequency) and the fast, “fine-grained” movements (high-frequency).  \n2. The raw motion feature by itself does not distinguish these two and so cannot easily learn to emphasize one or the other.  \n3. The FreqSSM block is explicitly designed to tease apart low- vs. high-frequency content, producing a frequency-aware “mask.”  \n4. By passing that mask through a sigmoid and then multiplying it into the original motion feature, the model can selectively amplify or suppress parts of the signal based on whether they are the kinds of motions (low- vs. high-freq) it needs at that diffusion step.  \n5. This multiplicative gating thus serves as a learned “frequency filter,” letting FreqMamba dynamically pick out static posture cues or fine-grained motion details, rather than forcing the network to learn both simultaneously in one undifferentiated projection.  \n\nSo in one sentence:\n\n• They introduce multiplicative gating between the FreqSSM output and the motion features in FreqMamba to let the frequency-aware module dynamically select (i.e. amplify or suppress) portions of the raw motion embedding, so that low-frequency (static) and high-frequency (dynamic) components can be cleanly and adaptively fused during denoising."
    },
    {
        "question": "What motivates integrating TextSSM after CDWConv for sentence-level alignment in TextMamba?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In text-driven human motion generation tasks, the generated motion is directly constrained by the text instructions. Therefore, effectively understanding the semantic information of the text and achieving precise alignment between semantics and motion is crucial for generating motions [36,5]. However, the above method directly concatenates text features and motion features. Such simple concatenation introduces text-motion semantic inconsistency as the motions become more complex.",
            "Then, it extracts the latent temporal features of X_t^mo using CDWConv, and TextSSM aligns the text and motion features.",
            "In TextSSM, we first compute the state equation for the motion feature, updating the hidden state. Then, the sentence-level feature f^t extracted by CLIP is summed with the output matrix C, achieving text-to-motion alignment with minimal computational cost."
        ],
        "final_answer": "Because simply concatenating text and motion features leads to semantic inconsistencies when motions become complex, TextMamba uses CDWConv to extract temporal motion features and then applies TextSSM to inject CLIP’s sentence-level text embeddings into the SSM’s output matrix C. This design precisely aligns text and motion at the sentence level and guarantees cross-modal consistency with minimal computational overhead.",
        "relevant_elements": [
            "TextMamba",
            "TextSSM"
        ],
        "id": 40,
        "masked_question": "What motivates integrating [mask1] after CDWConv for sentence-level alignment in TextMamba?",
        "masked_number": 1,
        "masked_elements": [
            "TextSSM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17532v1_figure_1.png",
        "paperid": "2411.17532v1",
        "paper_path": "./papers/2411.17532v1.json",
        "figure_id": "2411.17532v1_figure_1.png",
        "caption": "Figure 1: Overview of FTMoMamba. FTMoMamba is built upon the Diffusion model within FTMamba modules, which explores the frequency-domain information to guide motion generation, as well as text-semantic information to ensure text-motion consistency in the latent space. Specifically, the diffusion model compresses and decompresses the raw motion sequence, reducing the interference of redundant information in motion generation. FTMamba, as the core of the denoising module, consists of FreqMamba and TextMamba. The former decomposes motion sequences into low- and high-frequency components to guide the generation of static and fine-grained motions, respectively. The latter aligns textual semantics with sequential features to ensure text-motion consistency.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In TextMamba we first extract a residual motion feature fmo via CDWConv to get its latent temporal structure.  \n2.  Simply concatenating CLIP’s sentence embedding ft with that temporal feature was found to introduce semantic drift (i.e. text and motion stop lining up as sequences grow).  \n3.  To stop that drift, the authors insert a small “TextSSM” block (highlighted in red) immediately after CDWConv.  \n4.  TextSSM is a light-weight state-space fusion module that (a) takes the CDWConv output as its motion input, (b) adds in the full sentence embedding ft at its output step, and (c) then re-projects through the SSM’s observation C‐matrix.  \n5.  This does two things at once: it aligns the global sentence meaning with every timestep of the motion latent, and it does so with almost zero extra cost (no heavy cross‐attention).  \n\nIn short, the red-boxed TextSSM is placed right after CDWConv so that, once the pure motion feature has been temporally “mixed,” it can be immediately and efficiently aligned—sentence-by-sentence—with the CLIP embedding, ensuring tight text–motion consistency."
    },
    {
        "question": "What motivates freezing the vision encoder and linear layer while tuning only virtual tokens?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To reduce the computing resources, all parameters θ of LVLM are frozen during training, except for the newly introduced parameters φ of virtual tokens. For instance, with the addition of 20 virtual tokens, only M parameters are trainable, accounting for just 0.0012% of the total model parameters. This significantly reduces the computational costs while preserving the notable optimization effects on multi-modal object hallucinations, details are demonstrated in Section 4.3."
        ],
        "final_answer": "Freezing the vision encoder and linear layer (i.e., all original LVLM parameters) and tuning only the new virtual tokens is motivated by a desire to drastically reduce computing resources and parameter updates. By training just the small set of virtual token embeddings (only 0.0012% of total parameters in an example), PATCH achieves efficient optimization against object hallucinations without the high cost of full-model fine-tuning.",
        "relevant_elements": [
            "vision encoder",
            "linear",
            "virtual tokens"
        ],
        "id": 41,
        "masked_question": "What motivates freezing the [mask1] and [mask2] while tuning only virtual tokens?",
        "masked_number": 2,
        "masked_elements": [
            "vision encoder",
            "linear"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.06795v1_figure_1.png",
        "paperid": "2410.06795v1",
        "paper_path": "./papers/2410.06795v1.json",
        "figure_id": "2410.06795v1_figure_1.png",
        "caption": "Figure 1: The architecture of LVLMs with PATCH (taking MiniGPT-v2 as an example) where the visual encoder, linear projection layer, and the LLM remain frozen during the training phase. The only updated component during fine-tuning is the parameters of the virtual tokens. A frozen pre-trained Cascade Mask R-CNN head is adopted to obtain the object information in the test images.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  Identify the masked modules  \n    -  [mask1] (red box) is the Vision Encoder  \n    -  [mask2] (blue box) is the Linear (projection) layer that maps visual features into the LLM’s embedding space  \n\n2.  What the paper actually does  \n    -  At fine-tuning time it freezes every parameter of the LVLM (vision encoder, linear projector, and the language model)  \n    -  The only trainable parameters are the embeddings of a small set of “virtual tokens.”  \n\n3.  Why freeze the vision encoder & linear layer?  \n    a.  Preserve Pre-trained Knowledge  \n        •  Both modules have already been pre-trained on massive amounts of image (and image-text) data.  \n        •  Changing them risks “forgetting” that hard-won representation.  \n    b.  Drastically Cut Compute & Memory  \n        •  Tuning a multi-billion-parameter model end-to-end is very expensive.  \n        •  By freezing all but the virtual tokens (~0.0012% of total params), training uses far less GPU memory and FLOPs.  \n    c.  Avoid Overfitting & Maintain Flexibility  \n        •  The virtual tokens act as a lightweight adapter that injects object-detection cues without disturbing the backbone.  \n        •  At inference you can “plug in” or “unplug” these tokens depending on whether you have detection results.  \n\n4.  The succinct answer  \n    “They freeze both the vision encoder and its linear projection to retain the backbone’s pre-trained representations, to avoid the compute and memory cost of full model fine-tuning, and to prevent catastrophic forgetting—only the small set of virtual‐token embeddings is updated to inject object‐detection information with minimal overhead.”"
    },
    {
        "question": "Why position virtual tokens between linear outputs and object embeddings for cross-modal alignment?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Inspired by Zhu et al. (2024  ###reference_b33###), we insert a set of M virtual tokens T_v between the image features F_i and the detection information F_e.",
            "The PATCH strategy enhances the model’s ability to utilize detection results, enabling LVLMs to interpret image content with greater accuracy. By narrowing the representational gap between visual features and text in the semantic space, PATCH optimally aligns cross-modal features, particularly for tasks that benefit from enriched detection prompts."
        ],
        "final_answer": "The virtual tokens are placed between the visual (linear) outputs and the object detection embeddings so that their trainable embeddings can bridge the representational gap between the two modalities. By inserting and fine-tuning these tokens at that junction, PATCH can optimally align visual features with textual (object) embeddings in the shared semantic space, improving cross-modal alignment and reducing object hallucinations.",
        "relevant_elements": [
            "linear",
            "virtual tokens",
            "object embeddings"
        ],
        "id": 42,
        "masked_question": "Why position [mask1] between [mask2] outputs and object embeddings for cross-modal alignment?",
        "masked_number": 2,
        "masked_elements": [
            "virtual tokens",
            "linear"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.06795v1_figure_1.png",
        "paperid": "2410.06795v1",
        "paper_path": "./papers/2410.06795v1.json",
        "figure_id": "2410.06795v1_figure_1.png",
        "caption": "Figure 1: The architecture of LVLMs with PATCH (taking MiniGPT-v2 as an example) where the visual encoder, linear projection layer, and the LLM remain frozen during the training phase. The only updated component during fine-tuning is the parameters of the virtual tokens. A frozen pre-trained Cascade Mask R-CNN head is adopted to obtain the object information in the test images.",
        "qtype": "Design_Rationale",
        "response": "Here’s a step‐by‐step reasoning for why the authors chose to insert the red‐boxed “virtual tokens” between the blue‐boxed linear outputs and the object embeddings:\n\n1.  What each piece is doing  \n    •  Blue‐box (Linear): takes the raw vision‐encoder features and projects them into the LLM’s token embedding space.  \n    •  Object embeddings: discrete tokens encoding detected object classes & boxes.  \n\n2.  The alignment problem  \n    •  Even after that linear “vision→text” projection, the continuous image embeddings and the symbolic object tokens still live in slightly different subspaces.  If you just concatenate them and hand them to the LLM you’ll often get spurious or “hallucinated” objects, because the model hasn’t seen this precise mix of continuous vs. discrete embeddings during pre‐training.  \n\n3.  What virtual tokens buy you  \n    •  By slotting in a small set of trainable vectors (“virtual tokens”) right between those two modalities, you give the model a place to learn how to translate/align the continuous vision vectors into the same semantic manifold as the object tokens (and vice versa).  \n    •  Those vectors can softly “anchor” or “calibrate” the gap, smoothing out discrepancies so that, at inference time, the LLM sees one unified sequence of embeddings rather than a jarring switch from one embedding type to another.\n\n4.  Why that particular location  \n    •  If you put them before the linear layer, they never see the object tokens—they’d only massage raw vision features.  \n    •  If you put them after the object embeddings, they’d never get to adjust the vision→text projection itself.  \n    •  Sandwiched in between, they see both sides and can learn exactly how to nudge the projected image vectors to line up neatly with the detection tokens, yielding much tighter cross‐modal alignment and fewer hallucinations.\n\nIn short: the virtual tokens are placed exactly between the linear‐projected image features and the object embeddings so they can serve as a small, trainable “adapter” that bridges the two modalities and enforces a unified semantic space."
    },
    {
        "question": "How does Linear projection map vision encoder outputs into a compatible embedding space for subsequent virtual token integration?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Linear",
            "Vision Encoder"
        ],
        "id": 43,
        "masked_question": "How does [mask1] projection map vision encoder outputs into a compatible embedding space for subsequent virtual token integration?",
        "masked_number": 1,
        "masked_elements": [
            "Linear"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.06795v1_figure_1.png",
        "paperid": "2410.06795v1",
        "paper_path": "./papers/2410.06795v1.json",
        "figure_id": "2410.06795v1_figure_1.png",
        "caption": "Figure 1: The architecture of LVLMs with PATCH (taking MiniGPT-v2 as an example) where the visual encoder, linear projection layer, and the LLM remain frozen during the training phase. The only updated component during fine-tuning is the parameters of the virtual tokens. A frozen pre-trained Cascade Mask R-CNN head is adopted to obtain the object information in the test images.",
        "qtype": "Implementation_Details",
        "response": "Step 1 – Identify [mask1]  \nLooking at the red‐boxed module in Figure 1, [mask1] is simply the “Linear” projection layer sitting immediately on top of the frozen vision encoder.  \n\nStep 2 – What this layer must do  \n• The vision encoder (e.g. a ViT) spits out a sequence of feature vectors of size Dv (say 1024).  \n• The language‐model into which we will inject “virtual” tokens expects vectors of size Dt (say 768) in its own embedding space.  \n• In order to stitch the two together, we need a one‐to‐one mapping from R^{Dv}→R^{Dt} that aligns the image features with the LLM’s token embeddings.  \n\nStep 3 – How it works  \n• The “Linear” block is nothing more exotic than a (frozen) fully‐connected layer: y=W x + b.  \n• Here x∈R^{Dv} is each image‐patch or <BOS>/<EOS> embedding from the vision encoder, W∈R^{Dt×Dv} and b∈R^{Dt} are the learned projection weights (kept frozen at fine-tuning time).  \n• The output y∈R^{Dt} now sits in the *same* vector space as the LLM’s word‐piece embeddings.  \n\nStep 4 – Why this enables virtual token integration  \n• Once the image features have been linearly mapped into R^{Dt}, they can be concatenated with any additional “virtual” tokens (also of dimension Dt) and then fed as a single continuous embedding stream into the LLM.  \n• This unified embedding space is what lets PATCH slip in trainable soft tokens alongside the projected image features without further costly re-training of the vision encoder or the LLM."
    },
    {
        "question": "How are Virtual Tokens initialized and updated to align Cascade Mask R-CNN detection outputs with LLM embeddings?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Inspired by Zhu et al. (2024), we insert a set of k virtual tokens V between the image features FI and the detection information OD. The embeddings of these tokens are optimized during training, with parameters E, where E is the token embedding size of the LVLM.",
            "To reduce the computing resources, all parameters θ of LVLM are frozen during training, except for the newly introduced parameters E of virtual tokens.",
            "In the inference phase, we extend the model’s vocabulary by incorporating several special tokens (e.g., [ref1], [ref2], …, [refn]) whose embeddings are initialized by the fine-tuned virtual token embeddings."
        ],
        "final_answer": "A small set of virtual tokens—each with an embedding vector of the same dimension as the LVLM’s token embeddings—is inserted between the encoded image features and the Cascade Mask R-CNN’s detection outputs. These new embeddings (denoted E) are the only parameters that are trained: the rest of the LVLM is frozen and all gradient updates go into tuning these virtual tokens so that they learn to bridge the representational gap between detector outputs and the LLM’s semantic space. At inference time, the learned embeddings are added into the model’s vocabulary as special reference tokens (e.g., [ref1], [ref2], …), effectively aligning the detector’s outputs with the LLM’s embedding space.",
        "relevant_elements": [
            "Virtual Tokens",
            "Cascade Mask R-CNN",
            "Large Language Model (LLM)"
        ],
        "id": 44,
        "masked_question": "How are [mask1] initialized and updated to align [mask2] detection outputs with LLM embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "Virtual Tokens",
            "Cascade Mask R-CNN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.06795v1_figure_1.png",
        "paperid": "2410.06795v1",
        "paper_path": "./papers/2410.06795v1.json",
        "figure_id": "2410.06795v1_figure_1.png",
        "caption": "Figure 1: The architecture of LVLMs with PATCH (taking MiniGPT-v2 as an example) where the visual encoder, linear projection layer, and the LLM remain frozen during the training phase. The only updated component during fine-tuning is the parameters of the virtual tokens. A frozen pre-trained Cascade Mask R-CNN head is adopted to obtain the object information in the test images.",
        "qtype": "Implementation_Details",
        "response": "First, let us restate what the two masked regions refer to:\n\n- [mask1] (red box) is the block labeled “Virtual Tokens.”  \n- [mask2] (blue box) is the frozen, pre-trained Cascade Mask R-CNN that produces object detections.  \n\nThe question is asking: “How are these Virtual Tokens initialized and how are they updated so that the detection outputs from the Cascade Mask R-CNN can be aligned with the LLM’s embedding space?”\n\nChain-of-Thought:\n\n1.  In the PATCH paper we freeze all of the original LVLM parameters during fine-tuning: that means the vision encoder, linear projector, and LLM are untouched.  The only parameters we allow to move are the embeddings of a small bank of new “virtual tokens.”  \n2.  These virtual tokens are simply learned vectors of the same dimensionality *d* as the LLM’s word embeddings.  We usually initialize them randomly (for example with a standard Gaussian or Xavier initializer), or in some implementations one could even initialize them from the LLM’s own <BOS>/<EOS> embeddings — in any case they start off as free-form, randomly seeded vectors.  \n3.  At each training step the raw image is passed through the frozen vision backbone; in parallel the Cascade Mask R-CNN (also frozen) spits out a set of object descriptors (class IDs, box coordinates, RoI features, etc.).  \n4.  We then insert our K virtual token embeddings in between the sequence of image‐region features and the sequence of object embeddings.  This slotting gives the virtual tokens direct “access” to both the visual backbone output and the detector output.  \n5.  The only gradients we allow to flow come back into those K virtual token vectors.  By back‐propagating the training loss (e.g. cross‐entropy on the model’s yes/no answer for object hallucination), we nudge the virtual‐token embeddings so that in effect they learn to “translate” the frozen detector’s output into a form that the LLM can consume without catastrophic misalignment.  \n6.  After fine‐tuning, those K vectors now sit in the same semantic space as the LLM’s other token embeddings, and they bridge the gap between the Cascade Mask R-CNN detections and the LLM’s autoregressive decoder.  \n\nPutting it all together:\n\nAnswer  \nThe virtual tokens (mask1) are simply initialized as a small set of random embedding vectors of the same dimension as the LLM’s word inputs.  During fine‐tuning they are the *only* parameters that are unfrozen: gradients from the hallucination‐detection loss flow back into those K vectors and update them by standard back‐propagation.  In this way, they learn to absorb and re‐encode the frozen Cascade Mask R-CNN object‐detection outputs (mask2) into the LLM’s embedding space, aligning the detector’s raw outputs with what the LLM “expects” at its input."
    },
    {
        "question": "How does the A2R-OT algorithm integrate Frame Identification Algorithm outputs and Frame Vector Representation into iterative model updates?",
        "relevant_section_ids": [
            "3",
            "3.3"
        ],
        "relevant_context": [
            "The devised Metaverse traffic classifier, Discren-XR receives the Metaverse network traffic at the A2R-(OT) that invokes the FVR and FIA with the required segment size to form statistical frame vectors that are used in finding the classification model in training.",
            "The proposed A2R-(OT) algorithm, presented in Algorithm 1, adopts the random forest algorithm, which continuously refines the Metaverse classifier by iterating through various segment sizes to find the optimal segment size (s*), number of training segments (N), and final classification model (final model).",
            "The algorithm starts by forming segments. The FVR forms the vectors of the respective segments. Split function helps splitting the segment vectors into train and validation data at ratio r. Random forest is trained with train data until the validation meet the stopping criteria: 1) zero error conditions and 2) early stopping conditions."
        ],
        "final_answer": "At each iteration the A2R-(OT) loop (i) selects a candidate segment size, (ii) invokes the Frame Identification Algorithm (FIA) on the raw packet data to find frame-related packets, (iii) applies the Frame Vector Representation (FVR) to each segment—combining the four raw features plus frame counts, inter-arrival times and durations—into a fixed statistical vector, and (iv) feeds those vectors into a random-forest classifier.  The algorithm then evaluates training versus validation error under zero-error or early-stop rules, adjusts the segment size or increases the number of segments, and—using warm starts—aggregates and retains the partial models across iterations until the overall classifier converges.",
        "relevant_elements": [
            "Frame Identification Algorithm",
            "Frame Vector Representation",
            "Augmentation, Aggregation, and Retention-Online Training Algorithm"
        ],
        "id": 45,
        "masked_question": "How does the A2R-OT algorithm integrate [mask1] outputs and [mask2] into iterative model updates?",
        "masked_number": 2,
        "masked_elements": [
            "Frame Identification Algorithm",
            "Frame Vector Representation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05184v1_figure_1.png",
        "paperid": "2411.05184v1",
        "paper_path": "./papers/2411.05184v1.json",
        "figure_id": "2411.05184v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed solution. (a) Metaverse testbed to capture Metaverse network traffic, and (b) block diagram of the Discern-XR solution.",
        "qtype": "Implementation_Details",
        "response": "The very short answer is:\n\nA2R‐OT simply “plugs” the two upstream blocks – the Frame Identification Algorithm (the red box) and the Frame Vector Representation (the blue box) – into its inner training loops.  Concretely, at every candidate segment size:\n\n  1.  The A2R‐OT sampler carves the raw packet stream into segments of length s.  \n  2.  It hands each segment to the Frame Identification Algorithm (mask 1), which marks out exactly which packets belong to video frames.  \n  3.  It then hands the same packets plus the FIA’s frame‐labels into the Frame Vector Representation (mask 2), which computes the 13‐dimensional statistical feature–vector for each segment.  \n  4.  Those vectors become the new “training examples” that are (a) appended to the growing training set (Augmentation), (b) thrown into a Random Forest trainer that is warm-started from the last iteration so that old trees are not thrown away (Retention), and (c) whose resulting intermediate forests are held and eventually en-sem-bled into a single robust classifier (Aggregation).  \n  5.  The algorithm then checks its two stopping conditions (zero‐error or early‐stop) on the held-out validation subset; if neither is met, it tweaks the segment size or number of segments and repeats steps 1–5.  \n\nIn this way the FIA outputs feed directly into the FVR, whose vectors feed directly into the online Random Forest loop.  Every pass over a new segment size both augments the training set and “retains” already learned trees, and at the end the various partial models are aggregated into the final Metaverse traffic classifier."
    },
    {
        "question": "How does the Traffic Manager convert raw .pcap captures into network traffic data for the online training pipeline?",
        "relevant_section_ids": [
            "3",
            "2"
        ],
        "relevant_context": [
            "The rendered traffic is tapped on a cloud computer using a traffic sniffer, i.e., Wireshark [12]. Wireshark extracts the captured traffic in packet captures (.pcap) files from which network traffic data is extracted into comma-separated values (CSV). The extracted CSV for a given service consists of four application-level features.",
            "Each packet p_{i,j} is a vector with four raw features: time, packet length, packet direction, and packet inter-arrival time."
        ],
        "final_answer": "The Traffic Manager uses Wireshark to capture the Metaverse traffic into .pcap files and then exports those packet captures into CSV format. Each CSV entry corresponds to a packet represented by four application-level features (time, packet length, packet direction, and packet inter-arrival time), which become the network traffic data fed into the online training pipeline.",
        "relevant_elements": [
            "Traffic Manager",
            "Network Traffic Data"
        ],
        "id": 46,
        "masked_question": "How does the [mask1] convert raw .pcap captures into [mask2] for the online training pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "Traffic Manager",
            "Network Traffic Data"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05184v1_figure_1.png",
        "paperid": "2411.05184v1",
        "paper_path": "./papers/2411.05184v1.json",
        "figure_id": "2411.05184v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed solution. (a) Metaverse testbed to capture Metaverse network traffic, and (b) block diagram of the Discern-XR solution.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the “Traffic Manager” box (in red), and Mask2 is the “Network Traffic Data” box (in blue).  In our pipeline the Traffic Manager simply runs Wireshark (or tshark) on the live IP link, collects everything into .pcap files, and then immediately exports those pcaps into a flat CSV of packet‐level records.  Each row in that CSV carries the four “raw” fields (timestamp, packet length, direction, inter‐arrival time).  Those CSV records are exactly what we call the “Network Traffic Data” that feed into the online training (A2R-OT) stage.  In short, the Traffic Manager uses Wireshark to parse the .pcap into CSV, yielding the Network Traffic Data."
    },
    {
        "question": "How does Frame Identification Algorithm inform statistical feature extraction in Frame Vector Representation?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "The FIA algorithm relies on the traffic behaviour, including packet length and inter-arrival time, to accurately identify video frames. … The disparity in packet length allows the algorithm to define a minimum packet length threshold for identifying frames … and the reliability in frame packet inter-arrival times allows the algorithm to define the maximum frame duration as the difference in mode inter-arrival times.",
            "The FVR algorithm represents a given traffic segment into a statistical frame vector, which contains 13 statistical features derived from the four raw features, … The final three features are derived from the frame-related traffic data: frame count, average frame inter-arrival time, and total frame duration, which provide unique information about Metaverse traffic services."
        ],
        "final_answer": "The Frame Identification Algorithm (FIA) first parses the raw packet stream using thresholds on packet length and inter-arrival time to identify which packets belong to each video frame.  Frame Vector Representation (FVR) then uses the output of FIA—namely the delineated frame boundaries and timings—to compute three frame-based statistical features (frame count, average frame inter-arrival time, and total frame duration) that are appended to the other ten raw-feature statistics to form the 13-dimensional frame vector.",
        "relevant_elements": [
            "Frame Identification Algorithm",
            "Frame Vector Representation"
        ],
        "id": 47,
        "masked_question": "How does [mask1] inform statistical feature extraction in Frame Vector Representation?",
        "masked_number": 1,
        "masked_elements": [
            "Frame Identification Algorithm"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05184v1_figure_1.png",
        "paperid": "2411.05184v1",
        "paper_path": "./papers/2411.05184v1.json",
        "figure_id": "2411.05184v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed solution. (a) Metaverse testbed to capture Metaverse network traffic, and (b) block diagram of the Discern-XR solution.",
        "qtype": "Literature_Background",
        "response": "Here’s a step-by-step of how the red-boxed module – the Frame Identification Algorithm (FIA) – feeds into the Frame Vector Representation (FVR):\n\n1. Role of the FIA  \n   - FIA scans the raw packet stream and, using two simple behavioral heuristics  \n     a) A minimum‐packet‐length threshold to single out “big” video packets  \n     b) An inter‐arrival‐time threshold (derived from the first two peaks in the inter‐arrival histogram) to group packets that belong to the same frame  \n   - It thus partitions the downlink traffic into discrete “frames,” each consisting of a burst of consecutive video packets.\n\n2. What FVR needs from FIA  \n   The FVR produces a 13‐dimensional summary vector for each traffic segment:  \n   – The first ten features are generic statistics (means, variances, percentiles, etc.) over the raw packet fields.  \n   – The last three features are frame‐specific, namely:  \n     • total number of frames,  \n     • average frame inter‐arrival time,  \n     • total frame duration.\n\n3. How FIA informs those three features  \n   - By telling FVR exactly where one frame starts and ends, FIA allows FVR to count how many frames occurred in the segment.  \n   - Because FIA has already measured the timestamp of each frame’s first packet, FVR can compute the inter‐arrival times between successive frames and average them.  \n   - Since FIA also knows the duration of each frame burst (from the first to the last packet in that burst), FVR can sum or average those durations.\n\nIn short, the red-boxed Frame Identification Algorithm provides the raw “frame boundaries” and timings that the Frame Vector Representation uses to calculate its three frame‐based statistical features (frame count, mean frame spacing, and total frame duration), which greatly enhance the discriminative power of the 13-dimensional feature vector."
    },
    {
        "question": "How does Augmentation, Aggregation, and Retention-Online Training Algorithm leverage Frame Vector Representation for dynamic model updates?",
        "relevant_section_ids": [
            "3",
            "3.3"
        ],
        "relevant_context": [
            "The devised Metaverse traffic classifier, Discren-XR receives the Metaverse network traffic at the A2R-(OT) that invokes the FVR and FIA with the required segment size to form statistical frame vectors that are used in finding the classification model in training.",
            "The algorithm start by forming segment. The FVR forms the vectors of the respective segments. Split function helps splitting the segment vectors into train and validation data at ratio r. Random forest is trained with train data until the validation meet the stopping criteria: 1) zero error conditions and 2) early stopping conditions.",
            "The proposed A2R-(OT) algorithm ... continuously refines the Metaverse classifier by iterating through various segment sizes to find the optimal segment size (s*), number of training segments (K*), and final classification model (final_model).",
            "The A2R-(OT) algorithm operates on three core principles: Augmentation, where new network traffic segments are continuously added to improve generalization; Aggregation, where multiple models trained on different segments are combined for a more robust final model; and Retention, which ensures the model retains and builds on previous knowledge in dynamic environments like Metaverse traffic, ensuring sustained accuracy and efficiency."
        ],
        "final_answer": "The A2R-(OT) algorithm uses the Frame Vector Representation (FVR) module to transform each newly formed traffic segment into a 13-dimensional statistical frame vector. During Augmentation, these FVR vectors are appended as fresh training examples; during Aggregation, models trained on different segment sizes (and hence different sets of FVR vectors) are combined to yield a more robust forest; and during Retention, the algorithm warm-starts the random forest so that each update builds on previously learned FVR-based patterns. In this way, by continuously invoking FVR on incoming segments, A2R-(OT) dynamically updates and refines the classifier in an online fashion.",
        "relevant_elements": [
            "Augmentation, Aggregation, and Retention-Online Training Algorithm",
            "Frame Vector Representation"
        ],
        "id": 48,
        "masked_question": "How does [mask1] leverage [mask2] for dynamic model updates?",
        "masked_number": 2,
        "masked_elements": [
            "Augmentation, Aggregation, and Retention-Online Training Algorithm",
            "Frame Vector Representation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.05184v1_figure_1.png",
        "paperid": "2411.05184v1",
        "paper_path": "./papers/2411.05184v1.json",
        "figure_id": "2411.05184v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed solution. (a) Metaverse testbed to capture Metaverse network traffic, and (b) block diagram of the Discern-XR solution.",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning, step by step:\n\n1.  Identify the two masked modules in the figure  \n    •  [mask1] (red‐boxed) is the “Augmentation, Aggregation, and Retention – Online Training” (A2R-OT) algorithm.  \n    •  [mask2] (blue‐boxed) is the “Frame Vector Representation” (FVR) block, which turns each raw traffic segment into a 13-dimensional statistical frame vector.  \n\n2.  What FVR produces  \n    •  FVR ingests the four raw packet‐level features (packet size, inter-arrival time, etc.) plus the FIA-extracted frame counts, frame inter-arrival times, and total frame duration.  \n    •  It outputs a fixed-length “frame vector” for each traffic segment, capturing both overall traffic statistics and video-frame–specific statistics.  \n\n3.  How A2R-OT uses these frame vectors  \n    •  **Augmentation**: as new Metaverse traffic arrives, FVR continuously converts every new segment into its 13-dim vector; A2R-OT appends these to its growing training pool.  \n    •  **Aggregation**: A2R-OT trains a random-forest model on each set of segment vectors (trying different segment sizes), then ensembles or combines those sub-models into a single robust classifier.  \n    •  **Retention**: rather than throwing away what it has already learned, A2R-OT “warm-starts” each new round of random-forest training from the previous model’s trees and stops early if no real gain is seen, thus preserving and building on past knowledge.  \n\n4.  The net effect on dynamic model updates  \n    •  By feeding FVR’s vectors in real time into its loop of “augment → re-train → aggregate → retain,” the A2R-OT algorithm can continuously refine its Metaverse-traffic classifier on the fly.  \n    •  This tight integration—frame vectors in, updated forest out—gives the system the ability to track shifting traffic patterns and update its model without ever halting service."
    },
    {
        "question": "How does MLP adaptation complement token pruning decisions for dynamic computation allocation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MLP",
            "token pruning"
        ],
        "id": 49,
        "masked_question": "How does [mask1] adaptation complement token pruning decisions for dynamic computation allocation?",
        "masked_number": 1,
        "masked_elements": [
            "MLP"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.05010v1_figure_2.png",
        "paperid": "2407.05010v1",
        "paper_path": "./papers/2407.05010v1.json",
        "figure_id": "2407.05010v1_figure_2.png",
        "caption": "Figure 2: \nIllustration of the inference process of PRANCE. PRANCE is a lightweight framework for ViTs that jointly optimizes model structure and data. First of all, the framework divides the ViT model into four groups according to the inference sequence, each containing multiple ViT blocks. During inference, the selector utilizes the features of each group step by step to decide the model channel dimensions and token numbers for them, aiming to minimize FLOPs while ensuring accuracy. Moreover, PRANCE supports three main token optimization methods: pruning, merging, and pruning-then-merging.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the little red‐boxed module in the picture with the text: that red box is simply the “MLP” (the feed‐forward sub‐layer) inside each Transformer group.  In PRANCE, the agent doesn’t just decide “how many tokens to keep” in each group (i.e. token pruning), it also simultaneously decides “how wide should my MLP be from here on.”  \n\nHere is how the two interact to give truly sample-adaptive compute:  \n1.  At each group boundary, a lightweight PPO selector looks at the current features and emits two things:  \n   •  A token-keep ratio  (pruning or pruning+merging)  \n   •  A channel-width ratio for the MLP (and likewise for the MHSA) in the next blocks  \n2.  If the selector has just pruned aggressively (few tokens), it will typically also shrink the MLP width to avoid “wasting” a big feed-forward layer on only a handful of tokens.  Conversely, if the selector finds that it must keep most tokens (a hard sample), it will keep the MLP wide enough to process all those tokens thoroughly.  \n3.  Technically, the MLP layers were trained as a “channel-elastic” supernet (via weight‐sharing), so at inference time you can simply slice off the top  of the hidden dimension and run a narrower MLP on the remaining tokens.  \n\nBy coupling the token‐pruning decision with a matching MLP width decision, PRANCE achieves fine‐grained control over its FLOPs: easy images ⇒ few tokens + narrow MLP ⇒ big savings, hard images ⇒ more tokens + wider MLP ⇒ preserve accuracy."
    },
    {
        "question": "How does Token Optimization coordinate pruning and merging across sequential transformer blocks?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Token optimization consists of two steps: (1) token importance ranking and (2) token optimization. In the first step, tokens are sorted by their contributions to the task, so that a specific token optimization method can be applied in the second step according to the token keep ratio.",
            "After preprocessing the tokens, we consider three representative token reduction strategies to obtain the tokens for the remaining MHSA layers and MLP layers in -th group: (i) pruning, (ii) merging, and (iii) pruning-then-merging.",
            "For token pruning, unimportant tokens will be discarded for each sample according to [20].",
            "For token merging, the sorted tokens will be divided into two categories based on the token keep ratio: important tokens S_k = {t_i: i ≤ k} and unimportant tokens S_u = {t_i: i > k}. Subsequently, each unimportant token t_j will be merged into an optimal important token t_i* that is most similar to it, to formulate a new S_k for next layers: S_k′.",
            "For pruning and merging, we adopt the pruning-then-merging [25] scheme. Specifically, the token keep ratio is divided into a token pruning ratio α along with a token merging ratio β, i.e., α + β = r.",
            "After the token optimization, the remaining tokens will go through the latter Transformer blocks within this group, with the architectural decisions based on δ."
        ],
        "final_answer": "PRANCE applies token optimization in each group of three Transformer blocks. Within a group, it first ranks tokens by importance (using the ⟨CLS⟩ token’s query inner product), then applies one of three strategies: pruning (dropping the lowest-ranked tokens), merging (dividing tokens into ‘important’ and ‘unimportant’ sets and merging each unimportant token into its most similar important token), or a combined pruning-then-merging scheme (splitting the keep ratio into separate pruning and merging ratios). The reduced token set is then fed into the remaining blocks of that group. This groupwise process repeats sequentially across all transformer blocks, coordinating pruning and merging decisions throughout the network.",
        "relevant_elements": [
            "Token Optimization",
            "pruning",
            "merging"
        ],
        "id": 50,
        "masked_question": "How does [mask1] coordinate pruning and merging across sequential transformer blocks?",
        "masked_number": 1,
        "masked_elements": [
            "Token Optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.05010v1_figure_2.png",
        "paperid": "2407.05010v1",
        "paper_path": "./papers/2407.05010v1.json",
        "figure_id": "2407.05010v1_figure_2.png",
        "caption": "Figure 2: \nIllustration of the inference process of PRANCE. PRANCE is a lightweight framework for ViTs that jointly optimizes model structure and data. First of all, the framework divides the ViT model into four groups according to the inference sequence, each containing multiple ViT blocks. During inference, the selector utilizes the features of each group step by step to decide the model channel dimensions and token numbers for them, aiming to minimize FLOPs while ensuring accuracy. Moreover, PRANCE supports three main token optimization methods: pruning, merging, and pruning-then-merging.",
        "qtype": "Literature_Background",
        "response": "Here is the step‐by‐step unpacking of how the red-boxed “[mask1]” module (i.e. the “Token Optimization” block) glues together pruning and merging as the network marches through its Transformer layers:\n\n1.  **Location in the pipeline**  \n    – In each Transformer *group* (every three blocks in PRANCE’s design) the Token‐Optimization module sits between the first MHSA layer and the succeeding MLP (and on to the next MHSA of the group).  Its job is to take the raw, full set of tokens coming out of MHSA and whittle them down before onward processing.\n\n2.  **Input from the selector**  \n    – A tiny PPO-driven “selector” before each group decides two things for that group, for the *current sample*:  \n       a) a continuous token‐keep ratio π ∈ (0, 1]  \n       b) which reduction strategy to use: prune-only, merge-only, or prune-then-merge.  \n\n3.  **Token importance ranking**  \n    – The module grabs the *query* vector of the CLS token (from the group’s first MHSA) and computes its inner‐product with every other token.  \n    – Sorting by that score yields a descending list from “most important” to “least important.”\n\n4.  **Pruning step**  \n    – If pruning is part of the strategy, the bottom ⌈(1 – π) · N⌉ tokens in that ranking are simply dropped outright.\n\n5.  **Merging step**  \n    – If merging is chosen (either alone or after pruning), then for each *unimportant* token still in play the module finds the one *kept* token whose embedding has the highest cosine similarity.  \n    – It adds that unimportant token’s vector into its “best match.”  In effect you collapse several low‐value tokens into their nearest neighbor, reducing token count but retaining some “local” information.\n\n6.  **Passing to next layers**  \n    – The remaining, possibly merged, token set is then fed into the rest of that group’s MLP + MHSA layers.  \n    – At the end of the group, the next PPO selector will issue fresh (π, strategy) signals, and the same token‐optimization block repeats its pruning/merging on the *new* feature set.\n\nBy slotting this module between MHSA and MLP in each group and driving it with per-group, per-sample keep-ratios plus a choice of “prune,” “merge,” or “prune-then-merge,” PRANCE is able to cascade fine-grained token reduction decisions in lockstep with the flow of Transformer blocks."
    },
    {
        "question": "How does MSA-driven token optimization distinguish between pruning, merging, and pruning-merging decisions?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "After preprocessing the tokens, we consider three representative token reduction strategies to obtain the tokens for the remaining MHSA layers and MLP layers in k-th group: (i) pruning, (ii) merging, and (iii) pruning-then-merging.",
            "For token pruning, unimportant tokens will be discarded for each sample according to φ_k [20].",
            "For token merging, the sorted tokens will be divided into two categories based on the token keep ratio φ_k: important tokens X_k^+ and unimportant tokens X_k^-.\nSubsequently, each unimportant token x_i^- will be merged into an optimal important token x_j^+ that is most similar to it, to formulate a new X_k for next layers: where p represents the number of kept tokens and the indexes j are chosen to maximize the cosine similarity cos(x_i^-, x_j^+).",
            "For pruning and merging, we adopt the pruning-then-merging [25] scheme. Specifically, the token keep ratio φ_k is divided into a token pruning ratio δ_k along with a token merging ratio μ_k, i.e., δ_k + μ_k = φ_k."
        ],
        "final_answer": "MSA-driven token optimization first ranks tokens by importance (inner product with <CLS>). Then:\n• Pruning: it simply discards the lowest-ranked (unimportant) tokens according to the keep ratio φ_k.\n• Merging: it splits the sorted tokens into important and unimportant sets and merges each unimportant token into the most similar important token (by cosine similarity), yielding fewer tokens.\n• Pruning-then-merging: the overall keep ratio φ_k is split into a pruning fraction δ_k and a merging fraction μ_k, so that δ_k + μ_k = φ_k; the method first prunes δ_k of the tokens and then merges according to μ_k.",
        "relevant_elements": [
            "MSA",
            "Token Optimization"
        ],
        "id": 51,
        "masked_question": "How does [mask1]-driven token optimization distinguish between pruning, merging, and pruning-merging decisions?",
        "masked_number": 1,
        "masked_elements": [
            "MSA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.05010v1_figure_2.png",
        "paperid": "2407.05010v1",
        "paper_path": "./papers/2407.05010v1.json",
        "figure_id": "2407.05010v1_figure_2.png",
        "caption": "Figure 2: \nIllustration of the inference process of PRANCE. PRANCE is a lightweight framework for ViTs that jointly optimizes model structure and data. First of all, the framework divides the ViT model into four groups according to the inference sequence, each containing multiple ViT blocks. During inference, the selector utilizes the features of each group step by step to decide the model channel dimensions and token numbers for them, aiming to minimize FLOPs while ensuring accuracy. Moreover, PRANCE supports three main token optimization methods: pruning, merging, and pruning-then-merging.",
        "qtype": "Experimental_Results",
        "response": "First, note that the little red box in the figure is simply around the first Multi-Head Self-Attention (MSA) layer in each Transformer group.  In our method, that MSA is *the* workhorse we use to drive all of our token‐optimization decisions.  Here’s how it gives rise to three different behaviors:\n\n1.  MSA-driven importance scoring  \n    –  We take the query vector of the class token (“<CLS>”) from the *first* MSA in the group and compute its inner product with every other token’s key.  This produces an importance score \\(s_i\\) for each token \\(i\\).\n\n2.  Pure pruning  \n    –  We sort tokens in descending order of \\(s_i\\), keep the top-\\(k\\) (where \\(k\\) is the desired keep ratio) and **drop** the rest outright.\n\n3.  Pure merging  \n    –  Again we sort by \\(s_i\\).  We split the full set into  \n      • “important” tokens (the top-\\(k\\)), and  \n      • “unimportant” tokens (the remainder).  \n    –  For each unimportant token, we find the one important token whose embedding is most *cosine-similar* and we *merge* the unimportant token into it (i.e. add its features to that single most‐similar kept token).  \n\n4.  Pruning-then-merging  \n    –  We simply choose two ratios, \\(r_p\\) for pruning and \\(r_m\\) for merging, such that \\(r_p + r_m =\\) the overall keep ratio.  \n    –  First we prune the bottom \\(r_p\\) fraction.  Then on the remainder we perform the merging step as above to collapse the bottom \\(r_m\\) fraction into the top tokens.\n\nIn every case the *only* thing that changes is **how** we use the MSA-computed importance scores to decide whether a token gets thrown away, merged into another, or both."
    },
    {
        "question": "How does multi-stage MLP dimension selection interact with token keep ratio optimization across transformer blocks?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Therefore, the selector is formulated as: where k is the group index, r_k represents the token keep ratio for k-th Transformer group, s_k denotes the structures decision of k-th Transformer group, and h_k is the feature extracted by the k-th Transformer group, representing the abstracted data information up to the current block in the ViT.",
            "Specifically, A_k denotes the decided MLP ratios for the l_F MLP layers in this Transformer group, where 0 < a_kl \\le 1, and B_k denotes the decided embedding dimension ratios for the l_M MSA layers in this Transformer group.",
            "The token t_k represents the token pruning keep ratio, token merging keep ratio, or a combination of both, denoted as t_k. Depending on the selected token optimization policies, it serves as a basis for conducting sample-specific token optimization.",
            "After the token optimization, the remaining tokens will go through the latter Transformer blocks within this group, with the architectural decisions based on s_k."
        ],
        "final_answer": "At each group of three transformer blocks, the lightweight PPO selector jointly outputs both an MLP dimension reduction schedule (A_k) and a token keep ratio (r_k). First, it applies the chosen token keep ratio to prune or merge tokens. The surviving tokens are then processed through the group’s MLP layers whose channel dimensions have been reduced according to the MLP ratios. This per‐group sequence repeats across all transformer stages, enabling multi‐stage co‐optimization of MLP dimensions and token counts.",
        "relevant_elements": [
            "MLP",
            "Token Optimization",
            "Transformer Blocks"
        ],
        "id": 52,
        "masked_question": "How does multi-stage [mask1] dimension selection interact with token keep ratio optimization across [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "MLP",
            "Transformer Blocks"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.05010v1_figure_2.png",
        "paperid": "2407.05010v1",
        "paper_path": "./papers/2407.05010v1.json",
        "figure_id": "2407.05010v1_figure_2.png",
        "caption": "Figure 2: \nIllustration of the inference process of PRANCE. PRANCE is a lightweight framework for ViTs that jointly optimizes model structure and data. First of all, the framework divides the ViT model into four groups according to the inference sequence, each containing multiple ViT blocks. During inference, the selector utilizes the features of each group step by step to decide the model channel dimensions and token numbers for them, aiming to minimize FLOPs while ensuring accuracy. Moreover, PRANCE supports three main token optimization methods: pruning, merging, and pruning-then-merging.",
        "qtype": "Experimental_Results",
        "response": "Let’s unpack what the two colored boxes are pointing at and then see how they work together:\n\n1.  The red box is drawn around the small MLP‐ratio block inside each three‐block “group.”  In the text this is the lightweight selector’s choice of the MLP expansion ratio (i.e. how many hidden channels its feed‐forward network will actually use).\n\n2.  The blue box encloses the entire “meta‐network” pre‐training stage (Stage 1–2 in Fig. 3), in which we learn a channel‐elastic ViT (i.e. a ViT that can later be sliced to any valid embedding dimension or MLP ratio).\n\nSo the question becomes:\n\n  “How does multi‐stage MLP dimension selection interact with token keep‐ratio optimization across the meta‐network?”\n\nAnswer, step by step:\n\n–  During meta‐training (the blue‐boxed phase) we first learn a ViT whose MHSA and MLP layers can run at many different channel widths (by randomly slicing out embeddings and MLP expansions at each step).  That builds the “supernet” which can later be shrunk.\n\n–  At inference time, we break that supernet into groups of three Transformer blocks.  Before each group, a tiny PPO‐based selector looks at the group’s <CLS> feature and predicts two things at once:\n   1.  Exactly which channel widths (“MLP dimension”) to activate in the next MLP (and in each MHSA)  \n   2.  What fraction of the tokens to keep (the “token keep ratio”) and how aggressively to prune or merge the others.\n\n–  Because both choices come from the same network at each stage, the selector can learn how many MLP channels it can afford for a given token‐budget, or conversely how few tokens it can pass along if it wants to boost MLP capacity.  In effect, the “MLP dimension” selection and the “token keep ratio” are jointly output at each stage, yielding a tightly coupled, sample‐adaptive trade‐off between model width (channels) and data size (tokens) across the entire meta‐network."
    },
    {
        "question": "How does initialization of normal and common reflectance parameters enhance BRDF parameter convergence based on training outcomes?",
        "relevant_section_ids": [
            "4.4",
            "5.6"
        ],
        "relevant_context": [
            "To enhance the optimization process and improve robustness, the model is initially trained for a specific warm-up iteration (1000 iterations) without incorporating the full-spectra spectrum maps. Following this, the common BRDF parameters and normals for the full-spectra are initialized (see Fig. 1) using the average values from all other spectra, and this initialization step is integrated into the training process. By including these adequate priors, the optimization of parameters is guided more effectively, leading to better outcomes as demonstrated in the quantitative and qualitative analysis.",
            "The results presented in Table 7 clearly indicate that incorporating information from other spectra leads to improved average performance metrics for the rendered output across different real-world scenes. The higher average values achieved regarding PSNR and SSIM and the lower LPIPS values demonstrate enhancements when utilizing additional spectral information, highlighting the effectiveness of this approach in improving rendering quality and material asset estimation."
        ],
        "final_answer": "Initializing normals and common reflectance parameters to the average values from other spectral bands after a warm-up phase provides strong priors that guide the optimization. This leads to faster, more stable convergence of BRDF parameters and yields improved quantitative metrics (higher PSNR and SSIM, lower LPIPS) as well as better qualitative rendering results.",
        "relevant_elements": [
            "Initialization",
            "BRDF parameters"
        ],
        "id": 53,
        "masked_question": "How does [mask1] of normal and common reflectance parameters enhance [mask2] convergence based on training outcomes?",
        "masked_number": 2,
        "masked_elements": [
            "Initialization",
            "BRDF parameters"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.06975v1_figure_1.png",
        "paperid": "2408.06975v1",
        "paper_path": "./papers/2408.06975v1.json",
        "figure_id": "2408.06975v1_figure_1.png",
        "caption": "Figure 1: The proposed spectral Gaussian splatting framework:\nSpectral Gaussian model predicting BRDF parameters, distilled feature fields, and light per spectrum from multi-view spectrum-maps. The full-spectra maps and learnable parameters are introduced later in the training process by initializing them with priors from all other spectra.",
        "qtype": "Experimental_Results",
        "response": "Let’s first unpack what the two masked boxes actually are:\n\n- Mask 1 (red box) is the “Initialization” step in which, after a short warm-up (1 k iterations), we take the normals and the “common” reflectance parameters (diffuse albedo, roughness, specular tint) that have already been learned in all the other spectral bands and average them to form a good prior for the as-yet un-trained bands.  \n- Mask 2 (blue box) is the stage in which the network is fitting the full per-band BRDF parameters (plus lighting and the distilled feature field) in one big joint optimization.\n\nNow, how does that Initialization (mask 1) help the BRDF fitting stage (mask 2) to converge better?\n\n1.   Without any prior, each spectrum’s BRDF parameters (normals, albedo, etc.) must be learned from scratch, which is a highly under-constrained, ill-posed problem — especially early in training when the Gaussians themselves are still shifting around.  \n2.   By doing a short “warm-up” (i.e. fitting only the single‐band Gaussians and their appearance with a standard per‐band loss) we obtain reasonable normals and reflectance in each band independently.  \n3.   We then compute the average of these normals and reflectance vectors over all the *other* bands and *initialize* the new, full‐spectrum normals/reflectance in every band to that average.  \n4.   This provides a *strong prior* that (a) ties together all spectra, (b) keeps normals coherent, and (c) places albedos/roughness into a plausible region of parameter space.  \n5.   As a result, when we enter the joint BRDF optimization block (mask 2), the solver is no longer “jumping around” in a huge unconstrained space — it already starts close to a physically plausible solution.  \n6.   Empirically (see the ablation in Sec. 5.4) this initialization yields:\n     •  Faster convergence (fewer iterations to reach a stable loss)  \n     •  Higher PSNR/SSIM  \n     •  Lower LPIPS  \n     •  Qualitatively crisper specular highlights and more accurate diffuse colors  \n\nIn short, the “Initialization” of shared normals and reflectance acts as a learned spectral‐average prior that drastically narrows the search for the subsequent BRDF + lighting + feature‐field fit. This both stabilizes and accelerates the convergence of those BRDF parameters (mask 2) and leads to measurably better final reconstructions."
    },
    {
        "question": "What drives integrating CMT into both Image Encoder and Text Encoder for early cross-modal temporal feature fusion?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "In this work, we aim at making SAM2 wiser, by addressing these limitations without fine-tuning SAM2 weights, thereby preserving its original capabilities, and without outsourcing modality interaction to external, heavy models. To overcome challenges i) Text understanding and ii) Temporal modeling, we design a learnable Adapter [12] module, named Cross-Modal Temporal Adapter (CMT), with two key principles in mind: a) enabling mutual contamination between visual and linguistic modalities; and b) encoding temporal cues into visual features.",
            "We build on this popular Adapter framework [12] and propose a novel Cross-Modal Temporal Adapter (CMT) which models temporal dynamics within visual features while contaminating each modality with the other.",
            "We integrate the Cross-Modal Temporal Adapter (CMT) into the frozen text and visual encoders at every intermediate layer ℓ."
        ],
        "final_answer": "They integrate CMT into both the image and text encoders so that visual and linguistic features can interact and fuse early—allowing mutual contamination of modalities—and so that motion cues (temporal information) are embedded directly into the features before mask decoding.",
        "relevant_elements": [
            "CMT",
            "Image Encoder",
            "Text Encoder"
        ],
        "id": 59,
        "masked_question": "What drives integrating [mask1] into both Image Encoder and Text Encoder for early cross-modal temporal feature fusion?",
        "masked_number": 1,
        "masked_elements": [
            "CMT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17646v1_figure_2.png",
        "paperid": "2411.17646v1",
        "paper_path": "./papers/2411.17646v1.json",
        "figure_id": "2411.17646v1_figure_2.png",
        "caption": "Figure 2: Overview of SAMWISE.\nWe build on a frozen SAM2 and a frozen Text Encoder to segment images in video given a textual description.\nWe incorporate the Cross-Modal Temporal Adapter (CMT) into the text and visual encoders at every intermediate layer k𝑘kitalic_k to model temporal dynamics within visual features while contaminating each modality with the other. Then, we extract the [CLS] and verb embeddings, namely Contextual and Motion prompts, from the adapted textual features and project them through a learnable MLP. The final embedding is used to prompt the Mask Decoder, which outputs the segmentation mask. Finally, the Conditional Memory Encoder detects when a new candidate object, aligned with the caption, appears in the frame, enabling SAM2 to dynamically refocus its tracking.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed blocks are the Cross-Modal Temporal (CMT) Adapters, and they are injected into every layer of both the image and the text encoder.  The reason for doing this is two-fold:\n\n1. Mutual contamination of modalities  – by letting each visual layer attend to text (and each text layer attend back to vision) we force the network to learn fine‐grained, early alignments between words (both object and action terms) and pixel‐level features.  \n2. Temporal cue injection  – at the same time those adapters run a lightweight, localized spatio-temporal attention over neighboring frames so that motion information is woven into the visual features from the very first encoder layers.\n\nTogether, these two principles drive the decision to integrate the CMT adapter into both encoders for early cross-modal, cross-frame feature fusion."
    },
    {
        "question": "What motivates comparing memory-less tokens and Mask Decoder outputs in Conditional Memory Encoder to correct tracking bias?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "On the other hand, we observe that the memory-less features: i) contain an unbiased representation of the current frames, ii) are aligned with the textual prompt via our CMT (cf. Fig. 5), and iii) can thus be used to propose candidate instances that match the prompt without being biased by past predictions.",
            "Building on these intuitions, we derive a memory-less token T_ml from a cross-attention between the unbiased feature maps and the prompt. Such token represents a summary of the visual features that match the prompt. The idea is to compare it with the mask token T_mask generated by the Mask Decoder, to detect when they represent different objects, i.e., to detect when SAM2 is tracking an object that is not the one currently most aligned with the caption."
        ],
        "final_answer": "Because memory-less features provide an unbiased, text-aligned summary of the current frame’s contents, comparing the memory-less token to the Mask Decoder’s token lets the system detect when SAM2’s memory-based tracking is following the wrong object and thus correct tracking bias.",
        "relevant_elements": [
            "Mask Decoder",
            "Conditional Memory Encoder"
        ],
        "id": 60,
        "masked_question": "What motivates comparing memory-less tokens and [mask1] outputs in [mask2] to correct tracking bias?",
        "masked_number": 2,
        "masked_elements": [
            "Mask Decoder",
            "Conditional Memory Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17646v1_figure_2.png",
        "paperid": "2411.17646v1",
        "paper_path": "./papers/2411.17646v1.json",
        "figure_id": "2411.17646v1_figure_2.png",
        "caption": "Figure 2: Overview of SAMWISE.\nWe build on a frozen SAM2 and a frozen Text Encoder to segment images in video given a textual description.\nWe incorporate the Cross-Modal Temporal Adapter (CMT) into the text and visual encoders at every intermediate layer k𝑘kitalic_k to model temporal dynamics within visual features while contaminating each modality with the other. Then, we extract the [CLS] and verb embeddings, namely Contextual and Motion prompts, from the adapted textual features and project them through a learnable MLP. The final embedding is used to prompt the Mask Decoder, which outputs the segmentation mask. Finally, the Conditional Memory Encoder detects when a new candidate object, aligned with the caption, appears in the frame, enabling SAM2 to dynamically refocus its tracking.",
        "qtype": "Design_Rationale",
        "response": "They do it so that the model can spot when its long‐term, memory‐driven tracker has drifted onto the wrong object, and then gently “pull” it back to the correct one without throwing away all past context. Concretely:\n\n1.  The “memory-less” token is a summary of the current frame’s features that match the text prompt but has no bias from previous frames.  \n2.  The Mask Decoder’s token carries exactly the opposite bias—it encodes what the model has been following so far.  \n3.  By feeding both tokens into the Conditional Memory Encoder and comparing them, the model learns to detect when they point at different objects (i.e., when the tracker has drifted).  \n4.  At that moment, it softly injects the memory-less prediction back into the memory bank, correcting the bias and re-aligning tracking with the caption."
    },
    {
        "question": "Why integrate Variance-based Sample Pruning before LLM-based Sample Scoring?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Although it is possible to present all identified noisy samples $\\mathcal{S}_{nd}$ to the LLMs for scoring, this approach would be prohibitively time-consuming due to the massive interactions in the recommender system.",
            "Hard sample candidates $\\mathcal{S}_{H}$ are collected by … With the increasing $k$, more candidates will be selected in latter training iterations and provided to LLM-based Sample Scoring to identify hard samples further."
        ],
        "final_answer": "Variance-based Sample Pruning is introduced to dramatically cut down the number of candidate samples that need costly LLM inference. By first selecting a small subset of high‐variance (i.e. likely hard) samples, the framework avoids the prohibitive expense of scoring all noisy samples with the LLM, making the subsequent LLM‐based Sample Scoring module computationally feasible.",
        "relevant_elements": [
            "Variance-based Sample Pruning",
            "LLM-based Sample Scoring"
        ],
        "id": 61,
        "masked_question": "Why integrate [mask1] before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Variance-based Sample Pruning",
            "LLM-based Sample Scoring"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.10343v1_figure_2.png",
        "paperid": "2409.10343v1",
        "paper_path": "./papers/2409.10343v1.json",
        "figure_id": "2409.10343v1_figure_2.png",
        "caption": "Figure 2: \nThe overview of the LLMHD framework.\nLLMHD leverages LLMs to differentiate hard and noisy samples, thereby enhancing the denoising recommender training task.\nThe framework identifies hard samples through three main modules: (1) Variance-based Sample Pruning, (2) LLM-based Sample Scoring, and (3) Iterative Preference Updating.",
        "qtype": "Design_Rationale",
        "response": "Let’s unpack what each boxed module does and why their order matters:\n\n1. What Variance-based Sample Pruning (red box, [mask1]) does  \n   - It cheaply computes the variance of model prediction scores for each sample over recent epochs (Eq. 17).  \n   - Samples whose positive or negative scores fluctuate the most are likely to be “hard” candidates (as prior work shows hard examples have higher variance).  \n   - By selecting only the top-variance positives and negatives (controlled by a growing budget γₜ), it prunes away the vast majority of samples that are almost certainly “easy” or pure noise.  \n\n2. What LLM-based Sample Scoring (blue box, [mask2]) does  \n   - It invokes an LLM to read your current user‐preference summary plus an item’s text profile, and returns a compatibility—or “hardness”—score.  \n   - This is far more expensive (in API calls or model compute) than the simple variance calculation.  \n\n3. Why put Pruning before Scoring?  \n   - If you naively fed every noisy‐sample candidate into the LLM scorer, you’d pay huge compute/API costs.  \n   - By first filtering with the very cheap variance‐based test, you cut down to a small, high-value subset of true “hard” candidates.  \n   - Only those survivors then go through the expensive LLM pass, making the pipeline both accurate (LLM disambiguates hard vs. noise) and efficient (you only score ≪ batch-size samples).  \n\nIn short: you integrate Variance-based Sample Pruning before LLM-based Sample Scoring to drastically reduce the number of expensive LLM calls, by cheaply pre-selecting just the most promising hard‐sample candidates."
    },
    {
        "question": "What justifies Iterative Preference Updating following LLM-based Sample Scoring?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "Accurate user preference θ is critical for effective LLM sample scoring.",
            "However, the θ summarized based on interacted items do not fully capture user interests due to the inclusion of disliked items, i.e., false-positives, and the exclusion of liked items, i.e., false-negatives.",
            "To mitigate this problem, we refine user preferences iteratively by excluding dislikes and incorporating likes."
        ],
        "final_answer": "Iterative Preference Updating is justified because the initial user‐preference summary—derived from historical interactions—can be biased by false‐positive (disliked) and false‐negative (overlooked liked) items. By iteratively removing dislikes and adding overlooked likes, the module refines the user preference representation, ensuring more accurate LLM‐based sample scoring.",
        "relevant_elements": [
            "Iterative Preference Updating",
            "LLM-based Sample Scoring"
        ],
        "id": 62,
        "masked_question": "What justifies [mask1] following LLM-based Sample Scoring?",
        "masked_number": 1,
        "masked_elements": [
            "Iterative Preference Updating"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.10343v1_figure_2.png",
        "paperid": "2409.10343v1",
        "paper_path": "./papers/2409.10343v1.json",
        "figure_id": "2409.10343v1_figure_2.png",
        "caption": "Figure 2: \nThe overview of the LLMHD framework.\nLLMHD leverages LLMs to differentiate hard and noisy samples, thereby enhancing the denoising recommender training task.\nThe framework identifies hard samples through three main modules: (1) Variance-based Sample Pruning, (2) LLM-based Sample Scoring, and (3) Iterative Preference Updating.",
        "qtype": "Design_Rationale",
        "response": "Chain of thought  \n1. In our pipeline (Fig. 2) we first prune a small set of candidate “hard”/noisy samples via variance, then we ask the LLM to score each candidate’s hardness against the *current* user‐preference summary.  \n2. But that user‐preference summary was originally just a concatenation of all past interacted items—it can easily include items the user actually *disliked* (false positives) and omit items they really *liked* (false negatives).  \n3. Since the very next step (LLM–based Sample Scoring) uses that summary as the *sole* context for deciding whether a sample is truly “hard,” any errors in the summary will corrupt our hardness scores.  \n4. Therefore, *before* we move on to use those scores in training, we must *refine* or *update* the user‐preference summary: we drop the confidently disliked (false‐positive) items, we add the confidently liked (false‐negative) ones, and then we re‐prompt the LLM to produce a cleaner description.  \n5. This iterative Preference Updating block (the red‐boxed module) is thus justified by the fact that *accurate* preference text is *essential* for the LLM to give *meaningful* hardness scores.  \n\nAnswer  \nIterative Preference Updating is justified because LLM–based Sample Scoring critically depends on a *correct* textual summary of the user’s likes and dislikes—and the initial summary is polluted by false positives/negatives—so we must iteratively refine it to ensure the LLM hardness scores remain valid."
    },
    {
        "question": "How does Variance-based Sample Pruning compute and rank positive and negative variances to select hard sample candidates?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Specifically, hard sample candidates are selected based on the observation of previous work (2020  ###reference_b3###), which demonstrated that hard samples exhibit relatively higher prediction score variance compared to noisy samples.",
            "Therefore, for samples  D_b , we calculate the prediction scores variance of positive  v_{p,b}  and negative  v_{n,b}  items across multiple epochs (see Equation 17  ###reference_7###).",
            "Then sort them in descending order based on  v_{p,b}  and  v_{n,b}  respectively, where  n_p  and  n_n  denotes the number of positive and negative items in the  D_b  respectively.",
            "Hard sample candidates  C_b  are collected by , where  β  denotes the proportion of hard samples."
        ],
        "final_answer": "Variance-based Sample Pruning first computes, for each mini-batch D_b, the variance of the recommender’s predicted scores over multiple epochs separately for its positive items (v_{p,b}) and its negative items (v_{n,b}). It then sorts the positive‐item variances in descending order by v_{p,b} and the negative‐item variances in descending order by v_{n,b}. Finally, it takes the top β-fraction of each sorted list (i.e., the highest‐variance positive and negative items) and unions them to form the set of hard sample candidates C_b.",
        "relevant_elements": [
            "Variance-based Sample Pruning"
        ],
        "id": 63,
        "masked_question": "How does [mask1] compute and rank positive and negative variances to select hard sample candidates?",
        "masked_number": 1,
        "masked_elements": [
            "Variance-based Sample Pruning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.10343v1_figure_2.png",
        "paperid": "2409.10343v1",
        "paper_path": "./papers/2409.10343v1.json",
        "figure_id": "2409.10343v1_figure_2.png",
        "caption": "Figure 2: \nThe overview of the LLMHD framework.\nLLMHD leverages LLMs to differentiate hard and noisy samples, thereby enhancing the denoising recommender training task.\nThe framework identifies hard samples through three main modules: (1) Variance-based Sample Pruning, (2) LLM-based Sample Scoring, and (3) Iterative Preference Updating.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is nothing mystical – it is simply “Variance–based Sample Pruning.”  Concretely, for every training example in the current mini–batch B we keep track of its model scores over the last M epochs and compute two scalar variances:\n\n  • vₚ(x)  =  variance of the *positive* score history for x  \n  • vₙ(x)  =  variance of the *negative* score history for x  \n\nWe then build two ranked lists:\n\n  1. Sort all examples by vₚ(x) in descending order and take the top ⌈ρ·|B|⌉ entries.  \n  2. Sort all examples by vₙ(x) in descending order and take the top ⌈ρ·|B|⌉ entries.  \n\nFinally we form our hard–candidate set by taking the union of those two “high‐variance” slices.  Intuitively, examples whose positive or negative scores have been most volatile across epochs are much more likely to be genuine hard cases rather than static noise, so we only pass those on to the much more expensive LLM‐based scorer."
    },
    {
        "question": "How does Iterative Preference Updating identify confident false positives for exclusion in preference summaries?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "For every epoch t, we calculate the variance score σⁿ_{u,i} of user–item pairs (u,i) … We divided variance scores into two groups, positive and negative samples, and ordered from lowest to highest, where i⁻_{k,u} and i⁺_{k,u} are the k-th negative and positive sample respectively.",
            "To identify whether a sample is a false positive or false negative in the t-th epoch, we use the indicators ω_{u,i} and φ_{u,i} respectively. The threshold κ employed here follows the same definition as introduced in Equation 7.",
            "We design a robust mechanism to select confident items for preference updates. Formalized as follows, … The δ is a confidence threshold."
        ],
        "final_answer": "Iterative Preference Updating first computes, for each user–item interaction, the variance of the model’s prediction scores over the last T epochs and ranks both interacted (positive) and non-interacted (negative) samples by that variance.  It then flags potential false positives (i.e., interacted items that are likely dislikes) by setting ω_{u,i}=1 for any positive sample whose variance falls below the dynamic threshold κ.  Finally, among those flagged by ω_{u,i}, it only excludes from the user preference summary those whose indicator confidence exceeds a fixed threshold δ—thus ensuring that only items confidently deemed false positives are removed.",
        "relevant_elements": [
            "Iterative Preference Updating"
        ],
        "id": 64,
        "masked_question": "How does [mask1] identify confident false positives for exclusion in preference summaries?",
        "masked_number": 1,
        "masked_elements": [
            "Iterative Preference Updating"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.10343v1_figure_2.png",
        "paperid": "2409.10343v1",
        "paper_path": "./papers/2409.10343v1.json",
        "figure_id": "2409.10343v1_figure_2.png",
        "caption": "Figure 2: \nThe overview of the LLMHD framework.\nLLMHD leverages LLMs to differentiate hard and noisy samples, thereby enhancing the denoising recommender training task.\nThe framework identifies hard samples through three main modules: (1) Variance-based Sample Pruning, (2) LLM-based Sample Scoring, and (3) Iterative Preference Updating.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ is the “Iterative Preference Updating’’ box in the red frame.  In that module the system weeds out its own mistaken “positives’’ by looking for items whose model‐predicted positive score has been almost perfectly stable over many epochs (i.e. very low variance) and then only keeping the very lowest‐variance ones as *confident* false positives to drop from the summary.  Concretely:\n\n 1. At epoch *t* it computes for each user–item pair *(u,i)* the variance  \n      var_{u,i} = Var( ŷ_{u,i}^{t−w+1}, …, ŷ_{u,i}^{t} )  \n    over the last *w* prediction scores.  \n\n 2. It splits these variances into two lists — one for true positives and one for true negatives — and sorts the positive list in ascending order.  Let v_{u,p,b} be the *b*-th smallest positive variance.  \n\n 3. All positive items whose variance ≤ v_{u,p,b} become *candidate* false positives Eₚ.  \n\n 4. It then applies a tighter *confidence* cutoff βₜ (a fraction of the batch) and takes only the top βₜ of those candidates — i.e. those with the very lowest variance — as the *confident* false positives Eₚ^c.  \n\n 5. Finally, Eₚ^c is fed into an LLM prompt (P_UP^FP) whose sole task is to *remove* any mention of those items from the user’s textual preference summary.  \n\nIn this way, the red-boxed module isolates and excludes only those “positives’’ whose prediction scores were unnaturally too stable to reflect real user interest, then instructs the LLM to excise them from the user profile."
    },
    {
        "question": "How does the Predictor fuse FC1 Input and FC1 Weights to generate active neuron scores?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Predictor",
            "FC1 Input",
            "FC1 Weights"
        ],
        "id": 65,
        "masked_question": "How does the [mask1] fuse FC1 Input and FC1 Weights to generate active neuron scores?",
        "masked_number": 1,
        "masked_elements": [
            "Predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.14740v2_figure_2.png",
        "paperid": "2410.14740v2",
        "paper_path": "./papers/2410.14740v2.json",
        "figure_id": "2410.14740v2_figure_2.png",
        "caption": "Figure 2: Overall Architecture of M2Cache.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in Figure 2 is nothing magic beyond a tiny “scoring” network (what we call the Predictor) that cheaply computes a per-neuron importance by looking at both the FC1 input and the FC1 weight rows.  Concretely, it fuses them by essentially taking a dot-product (or a low-rank approximation of a dot-product) between the d-dimensional FC1 input vector x and each of the n FC1 weight‐rows wᵢ.  The result is an n-dimensional “score” vector s where  \n   \n   sᵢ ≃ wᵢ · x  \n   \nEach sᵢ tells you how “active” neuron i is for the current input, and those scores drive both the sparse masking and the mixed-precision splitting."
    },
    {
        "question": "How does the Preloader manage layer prefetching from SSD to DRAM under the Preloading Policy?",
        "relevant_section_ids": [
            "5.4"
        ],
        "relevant_context": [
            "Based on the tradeoff analysis of the two schemes mentioned above, we propose pattern-aware SSD preloading, as shown in Figure 8. It consists of two main modules: 1) preloader, which is used to preload the next a few layers of neurons to be used, load them from the SSD, and insert them into DRAM. And 2) the two-level DRAM cache, which stores and manages the preloaded layers.",
            "To design a preloader, there are two main factors we need to determine: 1) when to preload the neurons of one layer based on the inference progress such that the loading latency can be hidden, and 2) which neurons in a certain layer should be loaded such that there will be no explicit accuracy impact.",
            "First, based on our experiments, the one-layer neuron preloading time (from SSD to DRAM cache) is approximately twice as long as the one layer inference time. Therefore, we only need to preload the neuron from the layer that is two or more layers ahead of the current layer inference.",
            "Second, we propose to preload the entire layer to DRAM by identifying the missing neurons in DRAM."
        ],
        "final_answer": "Under the Preloading Policy the Preloader watches the decoding progress and fetches whole layers of FFN neurons from SSD into DRAM two layers ahead of the current inference layer (because SSD→DRAM takes roughly twice a layer’s inference time), and it loads every neuron missing in DRAM so that when the GPU advances to that layer the data are already resident.",
        "relevant_elements": [
            "Preloader",
            "SSD",
            "DRAM"
        ],
        "id": 66,
        "masked_question": "How does the [mask1] manage layer prefetching from SSD to DRAM under the Preloading Policy?",
        "masked_number": 1,
        "masked_elements": [
            "Preloader"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.14740v2_figure_2.png",
        "paperid": "2410.14740v2",
        "paper_path": "./papers/2410.14740v2.json",
        "figure_id": "2410.14740v2_figure_2.png",
        "caption": "Figure 2: Overall Architecture of M2Cache.",
        "qtype": "Implementation_Details",
        "response": "The box outlined in red is simply the “Preloader.”  In our Pattern-Aware Preloading scheme the Preloader is nothing more exotic than a little SSD→DRAM DMA engine that kicks off whole-layer fetches a couple of layers ahead of the GPU’s compute.  Concretely:  \n1. At runtime, as soon as you finish computing layer L, the Preloader fires off an asynchronous read of layer L+2 (or L+3, etc.) from SSD into DRAM.  \n2. Because we know from profiling that fetching one layer costs about twice as much time as doing one layer’s worth of FFN work, prefetching two layers ahead completely hides the SSD latency under useful compute.  \n3. Rather than pulling in every neuron blindly, the Preloader only brings in the “missing” neurons for that layer (all of them the first time, thereafter only new ones), and carves them into our two-tier DRAM cache (the fixed area for the first K layers and the sliding “dynamic” area thereafter).  \n\nBy always staying two layers ahead, and by only copying exactly those weights that aren’t yet resident in DRAM, the Preloader presentation-hides the SSD’s high latency and drives nearly 100% cache hit rates for the downstream GPU inference kernels."
    },
    {
        "question": "How does HBM Cache's LRU-like cache policy extend classical in-memory caching algorithms?",
        "relevant_section_ids": [
            "5.1",
            "5.3"
        ],
        "relevant_context": [
            "5.1: The multi-level cache complements MP Inference with a two-level caching strategy: 1) GPU-DRAM Cache: Utilizing an LRU cache mechanism, this level stores frequently accessed active neurons directly in the GPU cache.",
            "5.3: Cache Policy: The cache policy is used to update the neurons in each separate cache during inference for different tokens. Here, we employ the Adjacent Token Update (ATU) cache policy. ATU only updates the neurons that differ between tokens, and we don’t use algorithms like sliding windows proposed by LLM-in-a-Flash or the most widely used LRU. ATU is a trade-off between cache hit ratio and cache management overhead. With the proposed high-performance layer-based HBM cache with ATU, the cache hit ratio can reach about 80%, and the cache management overhead is nearly zero."
        ],
        "final_answer": "Rather than relying on a generic recency-based eviction as in classical LRU, the HBM cache is partitioned per transformer layer into contiguous blocks and drives cache updates with an Adjacent Token Update (ATU) policy. ATU exploits the fact that ~80% of active neurons overlap between successive tokens: at each decode step it only loads the small set of neurons that change, achieving LRU-like hit ratios (~80%) while incurring almost zero management overhead.",
        "relevant_elements": [
            "HBM Cache",
            "LRU-like cache policy"
        ],
        "id": 68,
        "masked_question": "How does [mask1]'s LRU-like cache policy extend classical in-memory caching algorithms?",
        "masked_number": 1,
        "masked_elements": [
            "HBM Cache"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.14740v2_figure_2.png",
        "paperid": "2410.14740v2",
        "paper_path": "./papers/2410.14740v2.json",
        "figure_id": "2410.14740v2_figure_2.png",
        "caption": "Figure 2: Overall Architecture of M2Cache.",
        "qtype": "Literature_Background",
        "response": "The short answer is that nothing magical is hiding behind the red box – M2Cache simply takes the same “keep the recently‐used items in your fastest memory, evict the old ones” idea that every operating-system and database has been using for decades, and re-packages it for the special case of weight-matrix rows (neurons) on a GPU.  In practice that means:\n\n 1.   Instead of one big monolithic cache, HBM is carved up into  L “mini-caches,” one per transformer layer, each just large enough to hold the neurons that are actually active (under the mixed-precision budget).  \n\n 2.   Within each of those layer-caches we keep the neurons that were used by the last few tokens, evicting old ones in LRU fashion.  \n\n 3.   Rather than walking a full recency list or doing a costly sort on the GPU, we exploit two facts: (a) around 80% of the neurons are reused from one token to the next, and (b) we already have a “predictor” that tells us which neurons will be needed.  So on each new token we perform an “Adjacent-Token Update” (ATU) – we compare the set of neurons used last time with the set needed this time, drop only the ones no longer needed, and bring in only the new ones.  That gives you an LRU-style hit rate without ever launching complicated CPU→GPU copy kernels or walking long data structures.  \n\nPut more generically: M2Cache’s “LRU-like” policy is just LRU turned into a lean, layer-aware, predictor-guided GPU native cache.  By delegating “recently used” tracking to the existing token overlap and predictor logic, it extends the textbook in-memory LRU algorithm into a form that (a) lives entirely inside HBM, (b) works at neuron-block granularity, and (c) has virtually zero eviction bookkeeping overhead."
    },
    {
        "question": "How does STFT with multiple windows parallel multiscale area attention's division of time-frequency spatial information?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Xu et al. [25] proposed a multiscale area attention, which applies the transformer-type attention mechanism to the CNN-based model.",
            "This significantly improves the recognition performance by dividing the time-frequency spatial information into granular perspectives.",
            "We preprocess the speech signals with different and overlapping window sizes using short-term Fourier transformation (STFT)."
        ],
        "final_answer": "By applying STFT with multiple window sizes and overlaps, the model produces several spectrograms at different time-frequency resolutions. In effect, each window setting yields a ‘scale’ of representation, mirroring multiscale area attention’s approach of dividing the time-frequency spatial plane into multiple granular perspectives.",
        "relevant_elements": [
            "STFT with multiple windows"
        ],
        "id": 69,
        "masked_question": "How does [mask1] parallel multiscale area attention's division of time-frequency spatial information?",
        "masked_number": 1,
        "masked_elements": [
            "STFT with multiple windows"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.04007v1_figure_1.png",
        "paperid": "2409.04007v1",
        "paper_path": "./papers/2409.04007v1.json",
        "figure_id": "2409.04007v1_figure_1.png",
        "caption": "Figure 1: The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.",
        "qtype": "Literature_Background",
        "response": "In our pipeline the red‐boxed module is simply “STFT with multiple windows.”  What this does is to take the 6 s speech clip and compute short‐time Fourier transforms with several different window lengths (and often overlapping hops), producing a set of log-Mel spectrograms each at a different time–frequency resolution.  \n\nThat parallels Xu et al.’s “multiscale area attention” in the spatial domain: just as multiscale area attention carves the feature map into patches of different sizes (fine to coarse) so the model can attend to local and global spatial cues, our multi-window STFT carves the time–frequency plane into fine and coarse temporal slices (short and long windows) so the network sees both rapid fluctuations and longer‐term spectral patterns."
    },
    {
        "question": "How does combining ECA block with convolution block compare to spectral temporal channel attention's spatial-channel fusion?",
        "relevant_section_ids": [
            "1",
            "2.2",
            "4.3"
        ],
        "relevant_context": [
            "Guo et al. [27] proposed spectral temporal channel attention, which is a modified version of bottleneck attention module (BAM) [30, 31, 32]. Therefore, it used not only focus on spatial features but also attention to channel features. In addition, it has an independent attention learning structure in all the axes of the input features.",
            "However, channel attention requires more learning parameters than spatial attention because of the two multi-layer perceptron (MLP) layers. More trainable parameters are required when examining the attention structure and considering the more diverse aggregated input features [33]. However, an increase in trainable parameters causes overfitting problems when trainable samples are leaked, such as in SER [34].",
            "To achieve this, the ECA uses a 1-D convolution layer. Therefore, it is highly efficient because it requires only a few trainable parameters equal to the kernel size [37].",
            "ECA can efficiently improve the channel feature of the convolution layer, which is essential for classifying speech emotions. Fig. 5 shows the ECA block used after the convolution block."
        ],
        "final_answer": "Spectral temporal channel attention combines spatial and channel attention via independent modules on each feature axis, but it relies on two-layer MLPs per branch and so introduces a substantial number of extra parameters (and hence a higher overfitting risk). In contrast, inserting the ECA block after a convolution block focuses purely on channel-wise attention, using a single 1-D convolution to learn inter-channel relationships. This yields comparable or better channel representation with only a handful of additional parameters (equal to the ECA kernel size), making it far more parameter-efficient than spectral temporal channel attention’s spatial-channel fusion.",
        "relevant_elements": [
            "ECA Block",
            "Convolution block"
        ],
        "id": 70,
        "masked_question": "How does combining [mask1] with [mask2] compare to spectral temporal channel attention's spatial-channel fusion?",
        "masked_number": 2,
        "masked_elements": [
            "ECA Block",
            "Convolution block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.04007v1_figure_1.png",
        "paperid": "2409.04007v1",
        "paper_path": "./papers/2409.04007v1.json",
        "figure_id": "2409.04007v1_figure_1.png",
        "caption": "Figure 1: The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.",
        "qtype": "Literature_Background",
        "response": "Here’s the intuition in plain terms:\n\n1. What STC (“spectral‐temporal‐channel” attention) does  \n   - It is essentially a two‐headed bottleneck module that learns spatial attention and channel attention jointly.  \n   - The spatial path is a small conv net, the channel path is a two–fully‐connected‐layer MLP.  \n   - Because it must learn both “where” in the time–frequency map to look and “which” feature-maps to amplify, it carries the overhead of two MLP layers and a parallel spatial‐attention branch.\n\n2. What you get when you simply take the 3×3 convolutional block (blue) and follow it by an ECA block (red)  \n   - The blue block (3×3 Conv + BN + ReLU) takes care of all your spatial/time-frequency filtering.  \n   - The red block (ECA) then only learns channel re-weighting by a single 1-D conv over the globally pooled channels and a sigmoid gate—no MLPs, no extra spatial branch.  \n   - In other words you’ve decoupled “spatial filtering” (the normal CNN block) from “channel attention” (the lightweight ECA).\n\n3. The upshot in comparison  \n   - Both STC and “Conv→ECA” are trying to give you spatial & channel attention.  \n   - STC fuses them in one big bottleneck (heavier, two‐MLP channel path + spatial path).  \n   - Conv→ECA splits the job: the conv block handles spatial, ECA handles channel, and ECA only needs a tiny 1-D kernel rather than two full FC layers.  \n   - Result: almost the same ability to focus on important time/frequency regions and channels, but with a fraction of the extra parameters and much less risk of overfitting on small SER datasets."
    },
    {
        "question": "How does integrating ECA blocks after convolution blocks affect channel feature representation efficiency?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "To overcome this situation, we adopt the ECA [35] in the CNN-based model, which can effectively improve the representation of the filters.",
            "ECA can efficiently improve the channel feature of the convolution layer, which is essential for classifying speech emotions."
        ],
        "final_answer": "Integrating ECA blocks after convolution blocks applies a lightweight channel‐wise attention mechanism that learns inter‐filter relationships and weights important channels, thereby boosting the representation capacity of convolutional filters with minimal extra parameters and improving the efficiency of channel feature extraction.",
        "relevant_elements": [
            "ECA block",
            "Convolution block"
        ],
        "id": 71,
        "masked_question": "How does integrating [mask1] after [mask2] affect channel feature representation efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "ECA block",
            "Convolution block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.04007v1_figure_1.png",
        "paperid": "2409.04007v1",
        "paper_path": "./papers/2409.04007v1.json",
        "figure_id": "2409.04007v1_figure_1.png",
        "caption": "Figure 1: The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  In our CNN backbone (the “blue” box), each convolutional block spits out a set of feature‐maps across C channels.  Alone, that block treats every channel equally, so when you crank up C you begin to dilute each filter’s “voice.”\n\n2.  The red box is the ECA (Efficient Channel Attention) module.  It takes those C feature‐maps, collapses each one to a single number via global average‐pooling (so no extra parameters), then runs a tiny 1-D convolution + sigmoid to produce a C-length weight vector.\n\n3.  By inserting that ECA block immediately after the convolutional block, you dynamically re-scale each channel’s activation according to how “important” it is for the current input.  \n\n4.  The result is a much sharper, more discriminative set of channel features—even as you grow the number of filters—yet you’ve only added a handful of extra weights (the 1-D conv) rather than a full attention head.\n\n5.  In practice this means better emotional‐cue extraction (high signal channels get boosted, low‐value ones get suppressed) without a blow‐up in parameter count or overfitting.\n\nAnswer:  \nIntegrating the ECA block (red) immediately after each convolution block (blue) lets the network re-weight its channel outputs on the fly—amplifying the most informative filters and damping the rest—thus substantially improving channel‐feature representation efficiency with only a tiny extra parameter cost."
    },
    {
        "question": "How does varying STFT window sizes enhance log-Mel spectrogram representational robustness?",
        "relevant_section_ids": [
            "3.1",
            "5.2",
            "5.4",
            "5.6"
        ],
        "relevant_context": [
            "Section 3.1: \"If the windowing length is longer, the frequency resolution increases; however, the resolution in time decreases. If the windowing length is shorter, the frequency resolution decreases, however, the time resolution increases. Therefore, we need to determine which features are more important in terms of time or frequency. For this purpose, we performed our experiment by using eight different settings during preprocessing.\"",
            "Section 5.2: \"We prepared the different versions of the datasets to search for more effective preprocessing settings with different window sizes and overlaps in the STFT. Therefore, an interval was set based on previous studies. As listed in Table III, most previous studies set the window size from 16 ms to 50 ms. Based on this, we chose eight different window sizes at 5 ms intervals within a slightly wider range of 15 ms to 50 ms. The overlap size was adjusted to obtain the same size of input data.\"",
            "Section 5.4: \"In experiments with different versions of datasets, except [version 5], the best performance of each model can be observed in the higher versions of the datasets. This implies that a larger window size can effectively represent emotional features.\"",
            "Section 5.6: \"As shown in Fig. 9, the model performance tended to increase from dataset versions 1 to 8. In particular, ... version 8 dataset showed better results than the other version datasets in most cases. This indicates that a large-sized window in emotional speech preprocessing is effective.\""
        ],
        "final_answer": "By generating log-Mel spectrograms with multiple STFT window lengths (from 15 ms up to 50 ms), the system captures complementary time–frequency trade-offs—short windows preserve fine temporal changes while long windows yield higher frequency resolution.  This multi-window strategy produces a richer set of spectral features, and empirically the larger window versions (e.g., 50 ms) consistently improve emotional-feature representation, making the learned spectrogram inputs more robust for emotion classification.",
        "relevant_elements": [
            "STFT with Multiple Windows",
            "Log-Mel Spectrograms"
        ],
        "id": 72,
        "masked_question": "How does varying [mask1] enhance log-Mel spectrogram representational robustness?",
        "masked_number": 1,
        "masked_elements": [
            "STFT with Multiple Windows"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.04007v1_figure_1.png",
        "paperid": "2409.04007v1",
        "paper_path": "./papers/2409.04007v1.json",
        "figure_id": "2409.04007v1_figure_1.png",
        "caption": "Figure 1: The overall pipeline of speech emotion recognition with CNN-based efficient channel attention architectures.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what’s in the red‐boxed region.  If you look at the three blue blocks in the bottom flowchart (“Select Signal → Signal Segment → STFT with Multiple Windows → Log-Mel Spectrograms → Model Training…”), the red box is exactly around the “STFT with Multiple Windows” step.  In other words, [mask1] = “using multiple STFT window sizes (and overlaps) to produce a set of spectrograms at different time–frequency resolutions.”\n\nChain of thought:\n\n1. A single STFT window length trades off time resolution against frequency resolution:  \n   – Short windows → fine time detail but coarse frequency bins  \n   – Long windows → fine frequency detail but coarse time bins\n\n2. If you pick only one fixed window length you force the network to learn from that one time–frequency trade‐off, which may miss cues that live at other scales (rapid transients vs. narrow spectral peaks).\n\n3. By varying the STFT window size (and correspondingly the hop/overlap) you generate multiple versions of the log-Mel spectrogram, each emphasizing different aspects of the signal (e.g. one emphasizes rapid fluctuations, another emphasizes harmonic detail).\n\n4. Feeding all of these multi-resolution spectrograms into the CNN (either as separate channels or via data-augmentation) gives the model a richer, more diverse set of features.  The network can thereby latch on to whichever time‐frequency patterns best correlate with each emotion, rather than being stuck with a single resolution.\n\n5. Empirically, the paper shows that (a) larger window sizes tend to work better for emotion (probably because many emotional cues are in slower spectral changes), but (b) combining all eight window settings in a data-augmentation scheme yields the highest overall UA/WA.  That tells you that the multi-window (“multi-STFT”) approach makes the representation more robust and generalizable.\n\nAnswer:  \nBy varying the STFT window lengths (and overlaps) we produce multiple log-Mel spectrograms at different time–frequency resolutions.  This multi‐resolution set of inputs augments the data and supplies complementary cues (short‐term transients vs. fine spectral details), forcing the CNN to learn features that are invariant across scales.  In practice, this leads to a more robust, generalizable spectro‐temporal representation for speech emotion recognition."
    },
    {
        "question": "How does Observer feedback refine storyboard generator outputs before agent manager proceeds?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "The second step focuses on generating the storyboard. Here, the agent manager provides the story descriptions  and protagonist videos  to the storyboard generator, which produces a series of images aligned with  and . Similar to the previous step, the storyboard results undergo user or observer evaluation until they meet the desired criteria.",
            "Observer. The observer is an optional agent within the framework, and it acts as a critical evaluator, tasked with assessing the outputs of other agents, such as the storyboard generator, and signaling the agent manager to proceed or provide feedback for optimizing the results.",
            "However, existing MLLMs still have limited capability in evaluating images or videos. As demonstrated in our experiments in Appendix A.5, these models cannot distinguish between ground-truth and generated storyboards. Therefore, we implemented the LAION aesthetic predictor as the core of this agent, which can effectively assess the quality of storyboards in certain cases and filter out some low-quality results."
        ],
        "final_answer": "After the storyboard generator produces an initial set of images, the Observer examines them—using an aesthetic quality assessment model (the LAION predictor) or a human review—to score and filter out low-quality frames. If the outputs do not yet meet the desired criteria, the Observer returns feedback to the storyboard generator (via the agent manager) requesting revisions. This loop continues—generate, evaluate, refine—until the Observer signals approval, at which point the agent manager moves on to the next stage.",
        "relevant_elements": [
            "Observer",
            "Agent Manager",
            "Storyboard Generator"
        ],
        "id": 73,
        "masked_question": "How does [mask1] feedback refine storyboard generator outputs before agent manager proceeds?",
        "masked_number": 1,
        "masked_elements": [
            "Observer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04925v2_figure_2.png",
        "paperid": "2411.04925v2",
        "paper_path": "./papers/2411.04925v2.json",
        "figure_id": "2411.04925v2_figure_2.png",
        "caption": "Figure 2: Our multi-agent framework’s video creation process. Yellow blocks represent the next agent’s input, while blue blocks indicate the current agent’s output. For example, the Storyboard Generator (SG)’s input includes story results and reference videos, and its output consists of storyboard results and the subject mask of the reference videos. The Agent Manager (AM) automatically selects the next agent to execute upon receiving signals from different agents and may request the Observer to evaluate the results when other agents complete their tasks.",
        "qtype": "Experimental_Results",
        "response": "Here’s what happens in practice when the (red‐boxed) observer steps in between the Storyboard Generator and the Agent Manager:\n\n1.  The Storyboard Generator produces its batch of frames (the “storyboard results”).  \n2.  Before the Agent Manager will advance to the next agent, it explicitly “asks for check” by routing those frames through the Observer.  \n3.  The Observer employs its Aesthetic Quality Assessment (in our prototype the LAION predictor, though it could equally be a human or an MLLM-based judge) to score each frame for things like:  \n    •  Visual appeal and composition  \n    •  Faithfulness to the subject (consistency in character appearance and pose)  \n    •  Clarity of background and key actions  \n4.  If the Observer flags any frame as sub-par (“not good”), it returns that verdict to the Agent Manager.  \n5.  The Agent Manager then instructs the Storyboard Generator to re-work or re-sample those specific shots—often by tweaking prompts, adjusting masks, or regenerating only the offending frames.  \n6.  This loop continues—Storyboard Generator → Observer → Agent Manager—until the Observer finally signals “good” or until a preset iteration limit is reached.  \n7.  Only once the Observer gives its approval does the Agent Manager hand off the approved storyboards to the Video Creator.\n\nIn this way, the observer’s “good?/not good” feedback enforces an explicit quality‐control loop, forcing the Storyboard Generator to refine any weak or inconsistent panels before the multi‐agent pipeline moves on."
    },
    {
        "question": "How does video creator utilize outputs from storyboard generator and subject masks to animate consistent videos?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "During removal, each storyboard I_j undergoes subject segmentation using algorithms like LangSAM, resulting in the subject mask M_j. For redrawing, a user-provided subject image with its background removed is selected, and StoryAnyDoor, fine-tuned based on AnyDoor with {(I_j, M_j)}, fills the mask locations M_j with the customized subject.",
            "Given the reference videos V, the storyboard I, and the story descriptions D, the goal of the video creator is to animate the storyboard following the story descriptions D to form the storytelling videos with consistent subjects of in V.",
            "To reduce the interference of background information and make the trainable parameters focus on learning the identity of the new subject, we further introduce a localization loss L_loc applied on the cross‐attention maps. Specifically, the similarity map S between the encoded subject token embedding and the latent videos is calculated for each cross-attention module, and the subject mask M is leveraged to maximize the values of S inside the subject locations."
        ],
        "final_answer": "The video creator takes the redrawn storyboard images produced by the storyboard generator—where subject masks have been used to segment out and replace the protagonist consistently across shots—and, together with the reference videos and story descriptions, animates these frames into a multi‐shot video. During training, it employs a LoRA‐BE fine‐tuning strategy on a latent diffusion model, using the subject masks in a localization loss on cross‐attention maps so that the model focuses its learning on the subject regions and preserves consistency of the protagonist throughout the generated video.",
        "relevant_elements": [
            "Video Creator",
            "Storyboard Generator"
        ],
        "id": 74,
        "masked_question": "How does [mask1] utilize outputs from storyboard generator and subject masks to animate consistent videos?",
        "masked_number": 1,
        "masked_elements": [
            "Video Creator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04925v2_figure_2.png",
        "paperid": "2411.04925v2",
        "paper_path": "./papers/2411.04925v2.json",
        "figure_id": "2411.04925v2_figure_2.png",
        "caption": "Figure 2: Our multi-agent framework’s video creation process. Yellow blocks represent the next agent’s input, while blue blocks indicate the current agent’s output. For example, the Storyboard Generator (SG)’s input includes story results and reference videos, and its output consists of storyboard results and the subject mask of the reference videos. The Agent Manager (AM) automatically selects the next agent to execute upon receiving signals from different agents and may request the Observer to evaluate the results when other agents complete their tasks.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the red-boxed Video Creator agent (i.e. the LoRA-BE module) turns the storyboard + subject masks into a consistent multi-shot video:\n\n1. Inputs it receives  \n   - The storyboard frames (one per shot) produced by the Storyboard Generator  \n   - The subject masks extracted from the reference videos  \n   - The original reference videos of the protagonist  \n\n2. Fine-tuning for a new character (“Miffy,” say)  \n   – It takes a pre-trained latent video diffusion backbone (DynamiCrafter).  \n   – It injects a small LoRA adapter into every self-, cross- and temporal-attention layer.  These LoRA parameters learn to shift the model’s “domain” so it can faithfully render this out-of-distribution character.  \n   – In parallel it learns 16 block-wise token embeddings (one per cross-attention block) so that the new token (e.g. <Miffy>) carries character-specific appearance information.  \n\n3. Mask-guided localization loss  \n   – During fine-tuning, it compares the model’s cross-attention maps against the provided subject masks.  \n   – A localization loss drives the attention to “light up” precisely where the mask says the character is.  This forces the LoRA-adapter and token embeddings to focus on learning the subject’s identity rather than background clutter.  \n\n4. Generation (inference)  \n   – At inference time the noisy latent video is conditioned both on:  \n     • The storyboard frames (as per-frame image condition)  \n     • The learned character token embeddings + LoRA adapters  \n   – The model denoises shot by shot, applying temporal attention across frames, yet always using the same learned adapters and embeddings.  \n\nBecause  \n • the storyboard fixes the rough layout and motion,  \n • the LoRA adapters + block-wise embeddings lock in the exact look of the protagonist, and  \n • the mask-driven loss has taught the model to concentrate on the masked region,  \nthe end result is a multi-shot video in which your cartoon character remains visually consistent from scene to scene."
    },
    {
        "question": "What privacy or bias issues might affect Zero-shot Detection when matching expert-defined topics?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Zero-shot Detection"
        ],
        "id": 78,
        "masked_question": "What privacy or bias issues might affect [mask1] when matching expert-defined topics?",
        "masked_number": 1,
        "masked_elements": [
            "Zero-shot Detection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05930v2_figure_1.png",
        "paperid": "2411.05930v2",
        "paper_path": "./papers/2411.05930v2.json",
        "figure_id": "2411.05930v2_figure_1.png",
        "caption": "Figure 1: The BERTrend Framework processes data in time-sliced batches, undergoing preprocessing that includes unicode normalization and paragraph segmentation for very long documents. It applies a BERTopic model to extract topics for each batch, which are merged with prior batches using a similarity threshold to form a cumulative topic set. This data helps track topic popularity over time, identifying strong and weak signals based on dynamically chosen thresholds. Additionally, the framework includes a zero-shot detection feature for targeted topic monitoring, providing more fine-grained results due to document-level matching with topics defined by the expert.",
        "qtype": "Others",
        "response": "First, let’s be clear that “[mask1]” in the question is pointing to the red‐boxed “Zero-shot Detection” (or “Targeted Zero-shot Topic Monitoring”) module in Figure 1.  In that step, an expert writes down a handful of topic descriptions (“Diseases, Outbreaks, Illnesses, Viruses,” or “Respiratory diseases and nuclear power plants” in the callout) and the system goes through every incoming document, embeds it, compares it by cosine similarity against each expert topic embedding, and then flags everything above a low threshold as “in topic.”  \n\nWhat privacy or bias concerns attend exactly that operation?  A short list:\n\n 1. Privacy of the source material  \n    •  Document-level matching means you may end up surfacing entire passages (or even entire documents) that contain personal, proprietary, or otherwise sensitive information—PII in social-media posts or confidential corporate reports—that the expert never intended to see.  \n    •  If the embeddings are stored or logged, there is a well-known risk that rare or outlier phrases can be “reconstructed” from the embedding space, leaking sensitive text.  \n\n 2. Expert-selection and confirmation bias  \n    •  Experts choose their own topic descriptions (and similarity thresholds).  Their mental model of the field—what counts as “nuclear power plant” or “respiratory disease”—inevitably carries their subjective blind spots.  Anything phrased outside of their vocabulary or frame may never be retrieved.  \n    •  Because the threshold is set low to maximize recall (0.4–0.6), the system will happily cherry-pick anything that even loosely matches the expert’s preconceptions, reinforcing their prior hypotheses and drowning out genuinely novel signals.  \n\n 3. Embedding-model bias  \n    •  The off-the-shelf sentence-transformer (“all-mpnet-base-v2”) was trained on large web corpora and news data that reflect myriad social biases (gender, racial, cultural).  When you compute cosine similarity between an embedded expert topic and an embedded document, those latent biases can skew which documents are deemed “close enough.”  \n    •  Dialectal, sociolectal, or non-standard language registers may be under-represented in the embedding space, so documents from marginalized or non-English-centric sources will systematically score lower and be ignored.  \n\n 4. Threshold and sampling bias  \n    •  A single low‐similarity cutoff for all topics assumes uniform semantic granularity.  In practice, some topics (e.g. “viruses”) are described with very precise technical jargon—in which case a 0.45 threshold might miss half the relevant docs—whereas other topics (“environment”) are so broad that a 0.45 threshold swamps you with false positives.  \n\nIn short, the zero-shot detection stage trades the statistical “deniability” of a purely unsupervised topic model for the granular recall of expert-defined fuzzy matching—but it inherits all of the expert’s blind spots, opens a path for privacy leaks at the document level, and is vulnerable to whatever biases lurk in the underlying embedding model and threshold choice."
    },
    {
        "question": "Why dynamically estimate thresholds from popularity distribution rather than fixed values?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "Using percentiles calculated dynamically over a sliding window offers several advantages:",
            "Adaptability to datasets: The retrospective parameter allows the method to adapt to the input data’s velocity and production frequency.",
            "Forget gate mechanism: The sliding window avoids the influence of outdated signals on current threshold calculations.",
            "Robustness to outliers: Calculating thresholds based on the popularity distribution reduces sensitivity to outlier popularities and prevents thresholds from approaching zero when many signals have faded away."
        ],
        "final_answer": "Dynamically estimating thresholds from the popularity distribution rather than using fixed values provides three key benefits: it adapts to the data’s velocity and production frequency, it ‘forgets’ outdated signals by relying on a sliding window of recent popularity values, and it is robust to outliers by deriving thresholds from the empirical distribution of topic popularities.",
        "relevant_elements": [
            "Calculate Popularities",
            "Estimate Thresholds"
        ],
        "id": 80,
        "masked_question": "Why dynamically [mask1] from popularity distribution rather than fixed values?",
        "masked_number": 1,
        "masked_elements": [
            "Estimate Thresholds"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05930v2_figure_1.png",
        "paperid": "2411.05930v2",
        "paper_path": "./papers/2411.05930v2.json",
        "figure_id": "2411.05930v2_figure_1.png",
        "caption": "Figure 1: The BERTrend Framework processes data in time-sliced batches, undergoing preprocessing that includes unicode normalization and paragraph segmentation for very long documents. It applies a BERTopic model to extract topics for each batch, which are merged with prior batches using a similarity threshold to form a cumulative topic set. This data helps track topic popularity over time, identifying strong and weak signals based on dynamically chosen thresholds. Additionally, the framework includes a zero-shot detection feature for targeted topic monitoring, providing more fine-grained results due to document-level matching with topics defined by the expert.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning, step by step:\n\n1.  What the red–boxed step does  \n    In the diagram, after we compute each topic’s “popularity” over time, BERTrend “Estimates Thresholds” (the red box) by taking, for example, the 10ᵗʰ and 50ᵗʰ percentiles of the popularity values within a recent rolling window.\n\n2.  Why not use a fixed threshold?  \n    •  Non-stationary data: In a streaming setting the absolute number of documents (and hence the raw popularities) can rise and fall over time. A fixed cutoff (say, “popularity > 100”) might be too lax at one point (when volumes are low) or too strict at another (when volumes spike).  \n    •  Outliers and fading topics: If yesterday’s most-popular topic went from 1,000 down to 10 documents, a fixed threshold could either drown out all weak emergers or forever flag dying topics as “strong.”  \n    •  No manual tuning: By using percentiles you remove the need to pick a magic number for every new dataset or time granularity.\n\n3.  What dynamic percentiles buy us  \n    •  Adaptivity: The 10ᵗʰ/50ᵗʰ percentile automatically rescales as overall document counts shift.  \n    •  Forgetting: Because it’s computed over a sliding window, very old peaks or troughs don’t unduly influence today’s threshold.  \n    •  Robustness: Percentiles are insensitive to single extreme outliers, so you won’t get thresholds that collapse toward zero if a single topic temporarily goes viral.\n\nIn short, BERTrend “dynamically estimates thresholds from the popularity distribution” so that its notions of “weak” vs. “strong” signals remain calibrated as the stream of text ebbs and flows—something a fixed cutoff simply cannot do without constant manual re-tuning."
    },
    {
        "question": "What is the motivation behind combining L_affinity, L_dispersion, and L_compactness objectives?",
        "relevant_section_ids": [
            "2.2",
            "3.1"
        ],
        "relevant_context": [
            "Module Specialization – The key goal of MODA is to promote modularity within a multi-layer DNN by cultivating distinct specialized sub-networks tailored to different functionalities, i.e., to predicting particular classes. This functional specialization implies that, ideally, each hidden unit of a layer should activate exclusively in response to input samples from a single class. To this end, we propose two novel training objectives that foster the functional specialization of hidden units in each layer by shaping their activation patterns: (1) intra-class affinity promotes similar subsets of units consistently activated to predict samples within the same class, and (2) inter-class dispersion enforces distinct subsets of units activated to predict samples from different classes.",
            "Module Compactness – In principle, intra-class affinity and inter-class dispersion are sufficient to promote modularity by regulating activations of hidden units within a layer. However, in practice, hidden units with low activation values would still be present across decomposed modules. Specifically, when non-essential neuron activations contribute, even only marginally, to the target predictions of multiple classes, their corresponding weights end up shared unnecessarily in the modules. To address this, we propose the third modularity objective, compactness, whose direct aim is to “disable” unnecessary hidden units used for predicting samples.",
            "As discussed above, our strategy promotes modularity within model M with three novel objectives: (1) intra-class affinity – M should activate similar subsets of neurons in L for samples of the same class; (2) inter-class dispersion – M should activate dissimilar subsets of neurons in L for samples from different classes; and (3) compactness – M should activate a minimal number of neurons in each subset."
        ],
        "final_answer": "They are combined to guide the network to form truly modular sub-networks by (1) encouraging the same subset of neurons to fire for samples of the same class (affinity), (2) pushing different classes to use distinct neuron subsets (dispersion), and (3) enforcing sparsity so that only the essential neurons remain active (compactness), thus yielding fine-grained, compact, accuracy-preserving modules.",
        "relevant_elements": [
            "L_affinity",
            "L_dispersion",
            "L_compactness"
        ],
        "id": 81,
        "masked_question": "What is the motivation behind combining [mask1], L_dispersion, and L_compactness objectives?",
        "masked_number": 1,
        "masked_elements": [
            "L_affinity"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01074v1_figure_1.png",
        "paperid": "2411.01074v1",
        "paper_path": "./papers/2411.01074v1.json",
        "figure_id": "2411.01074v1_figure_1.png",
        "caption": "Figure 1. High-level overview of MODA",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed term in the figure is simply the intra–class affinity loss, L_affinity.  In MODA’s design the three losses play complementary roles:\n\n 1.  L_affinity (intra–class affinity) encourages the same small set of neurons to fire on all examples of one class (so that each class “owns” a consistent sub-network).  \n 2.  L_dispersion (inter–class dispersion) pushes those per‐class sub-networks apart so that different classes do not share the same neurons.  \n 3.  L_compactness drives unimportant, near-zero activations all the way to zero, pruning away any marginally active units and keeping each module as small and tight as possible.  \n\nBy combining all three objectives you get modules that are (a) highly specialized to a single class, (b) disjoint from one another, and (c) as sparse and compact as possible—precisely the properties you need for clean, reusable DNN sub-modules without additional masking layers or costly retraining."
    },
    {
        "question": "What reasoning supports using a Module Recognizer prior to Structured Modularization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Module Recognizer",
            "Structured Modularization"
        ],
        "id": 82,
        "masked_question": "What reasoning supports using a [mask1] prior to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Module Recognizer",
            "Structured Modularization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01074v1_figure_1.png",
        "paperid": "2411.01074v1",
        "paper_path": "./papers/2411.01074v1.json",
        "figure_id": "2411.01074v1_figure_1.png",
        "caption": "Figure 1. High-level overview of MODA",
        "qtype": "Design_Rationale",
        "response": "The little “module‐recognizer” block in red is nothing mystical – it is simply a tiny front-end classifier whose job is to tell you which of your class‐specific modules (the blue boxed “Structured Modularization” step) you ought to run.  In MODA, each extracted module only knows how to score one class, so if you simply dropped them all on an input you’d either have to run every one in parallel (wasteful) or you’d have no way of choosing which one to fire.  By inserting a lightweight recognizer first, you cheaply pick out the single module you actually need to execute, both preserving the fine‐grained, reusable structure of your network and dramatically cutting down on unnecessary computation."
    },
    {
        "question": "How are intra-class affinity and inter-class dispersion balanced during modular training to shape neuron activations?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "As discussed above, our strategy promotes modularity within model M with three novel objectives: (1) intra-class affinity – M should activate similar subsets of neurons in layer l for samples of the same class; (2) inter-class dispersion – M should activate dissimilar subsets of neurons in layer l for samples from different classes; and (3) compactness – M should activate a minimal number of neurons in each subset.",
            "Inter-class dispersion: … To maximize dispersion between these patterns, we minimize inter-class dispersion loss L_dispersion while training the model M.",
            "Intra-class affinity: … maximizing affinity between activation patterns involves minimizing intra-class affinity loss L_affinity during training.",
            "In summary, incorporating intra-class affinity, inter-class dispersion, and compactness into our loss function yields a unified loss function L for training the model M: L = L_cross_entropy + α·L_affinity + β·L_dispersion + γ·L_compactness. Through mini-batch gradient descent, modular training aims to reduce L_cross_entropy to improve classification accuracy, while simultaneously decreasing the affinity and dispersion losses to encourage similar activations within each class and dissimilar activations across classes."
        ],
        "final_answer": "During modular training, MODA computes two cosine-similarity-based losses on each layer’s activations: an intra-class affinity loss (L_affinity), which it minimizes to encourage the same subset of neurons to fire for samples of the same class, and an inter-class dispersion loss (L_dispersion), which it also minimizes (effectively maximizing angular distance) to push apart activations for samples of different classes. Both losses are combined—alongside the standard cross-entropy and a compactness term—into a single weighted objective. By tuning their weights (α for affinity, β for dispersion), the optimizer jointly enforces that neuron activations become both more consistent within each class and more distinct across classes.",
        "relevant_elements": [
            "L_affinity",
            "L_dispersion",
            "Modular Training"
        ],
        "id": 83,
        "masked_question": "How are intra-class affinity and inter-class dispersion balanced during [mask1] to shape neuron activations?",
        "masked_number": 1,
        "masked_elements": [
            "Modular Training"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01074v1_figure_1.png",
        "paperid": "2411.01074v1",
        "paper_path": "./papers/2411.01074v1.json",
        "figure_id": "2411.01074v1_figure_1.png",
        "caption": "Figure 1. High-level overview of MODA",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed step in Figure 1 is exactly the “activation‐driven training” phase (i.e. the modular training step), where MODA simultaneously pulls together the activations of samples from the same class (intra‐class affinity) and pushes apart the activations of samples from different classes (inter‐class dispersion).  Concretely, at each layer l we compute:  \n\n  •  an intra‐class affinity loss   \n     – measures the cosine similarity between the ReLU activation vectors of all pairs of examples within the same class; minimizing this loss forces same‐class vectors to align, i.e. to “use” the same small subset of neurons.  \n\n  •  an inter‐class dispersion loss   \n     – measures the cosine similarity between activation vectors of examples from different classes; minimizing this loss drives those vectors to be orthogonal (low cosine similarity), i.e. to activate disjoint subsets of neurons.  \n\nThese two terms (together with a compactness term and the usual cross‐entropy) are simply added into one joint loss  \n   L = L₍cross‐entropy₎ + λ₁ L₍affinity₎ + λ₂ L₍dispersion₎ + λ₃ L₍compactness₎  \nand optimized by mini‐batch gradient descent.  By tuning the weights λ₁ and λ₂, MODA finds a sweet spot where neurons specialize––samples of the same class consistently fire the same small group of units, while samples of different classes fire largely non-overlapping groups."
    },
    {
        "question": "How does compactness loss gradient steer Module Recognizer neuron selection for each class module during structured modularization?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "The compactness loss L_c is designed based on L1 norm (Ma et al., 2019 ###reference_b34###), and derives the desired properties from it, as discussed below: \nwhere L_c is the L1 norm of the activation vector a_i^l. During training, L_c facilitates feature selection, ensuring that only the essential neurons are activated for a class. It promotes sparse activation vectors by reducing near-zero activations towards zero, thus resolving the undesired shared influences of neurons encountered in the inter-class dispersion loss.",
            "Once the modular model is trained, MODA identifies the layers’ units that are frequently activated while classifying samples of each class. These units, along with their associated weights, are extracted to form distinct modules. Specifically, the frequency of neuron activation with respect to a particular class C_i is calculated by the number of times a neuron u_j^l is activated in response to input samples from C_i. We define ρ as a threshold to determine whether u_j^l should be included in the module for class C_i based on the frequency of its activations. Formally, the module for class C_i is defined as: \nS_i = { u_j^l | f_{i,j}^l ≥ ρ }"
        ],
        "final_answer": "By including an L1-based compactness term in the loss, MODA’s training gradient continuously pushes non-essential neuron activations toward zero. As a result, after training the network exhibits sparse activation patterns in which only the key neurons remain highly active for each class. During structured modularization, the Module Recognizer measures how often each neuron fires for a given class and selects only those whose activation frequency exceeds a fixed threshold—i.e., the very neurons left ‘unsuppressed’ by the compactness-driven gradient are the ones chosen for each class module.",
        "relevant_elements": [
            "L_compactness",
            "Module Recognizer",
            "Structured Modularization"
        ],
        "id": 84,
        "masked_question": "How does [mask1] loss gradient steer [mask2] neuron selection for each class module during structured modularization?",
        "masked_number": 2,
        "masked_elements": [
            "L_compactness",
            "Module Recognizer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.01074v1_figure_1.png",
        "paperid": "2411.01074v1",
        "paper_path": "./papers/2411.01074v1.json",
        "figure_id": "2411.01074v1_figure_1.png",
        "caption": "Figure 1. High-level overview of MODA",
        "qtype": "Implementation_Details",
        "response": "Here’s the high-level mechanism in two steps:\n\n1.  During training, the red-boxed loss  \n   L_compactness  \n   is nothing more than an L₁–penalty on the activation vector for each class:  \n   L_compactness=\\|\\mathbf a\\|₁.  \n   Its gradient w.r.t. each neuron’s activation a_j is essentially  \n   ∂L/∂a_j=sign(a_j),  \n   so every small nonzero activation is driven straight toward zero.  Only those neurons whose total downstream gradient (from cross-entropy + affinity/dispersion) is strong enough to overcome this uniform shrinkage remain “on.”\n\n2.  At structured decomposition time, the blue-boxed Module Recognizer simply counts, for each class, which neurons fire “above threshold” across the training set.  Because L_compactness has already squeezed out all the weak, incidental activations, the remaining fired neurons form a very sparse, stable set.  Those are exactly the neurons the Module Recognizer picks to constitute the class-specific module."
    },
    {
        "question": "How does the attentive statistics pool integrate outputs of the ViT backbone into a single detection embedding?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The feature extractor f adopts a ViT backbone, which splits the spectrogram into patches, encodes each patch as an embedding by a linear layer, and processes them by stacks of Transformer blocks, outputting a series of patch features.",
            "An attentive statistical pooling layer [19] is appended to the ViT backbone to fuse these patch features into an utterance embedding h, and a linear layer is employed to map h to a low-dimensional detection embedding y, which is further processed by the anomaly detector g."
        ],
        "final_answer": "The attentive statistics pool applies an attentive statistical pooling layer over the sequence of patch-level outputs from the ViT backbone to fuse them into a single utterance embedding h, and then a linear layer projects h into the final low-dimensional detection embedding y.",
        "relevant_elements": [
            "ViT backbone",
            "Attentive Statistics Pool"
        ],
        "id": 85,
        "masked_question": "How does the [mask1] integrate outputs of the [mask2] into a single detection embedding?",
        "masked_number": 2,
        "masked_elements": [
            "Attentive Statistics Pool",
            "ViT backbone"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.14753v1_figure_1.png",
        "paperid": "2408.14753v1",
        "paper_path": "./papers/2408.14753v1.json",
        "figure_id": "2408.14753v1_figure_1.png",
        "caption": "Figure 1: Architecture of the ASD Model in CoopASD. The feature extractor f (⋅)𝑓⋅f(\\cdot)italic_f ( ⋅ ) is updated globally and shared among factories, while the linear classifier ci (⋅)subscript𝑐𝑖⋅c_{i}(\\cdot)italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) and KNN detector gi (⋅)subscript𝑔𝑖⋅g_{i}(\\cdot)italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) are uniquely constructed and preserved locally.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step of how the two modules work together:\n\n1. The ViT “backbone” (blue box) chops the input spectrogram into a sequence of patch embeddings and runs them through a stack of Transformer layers, producing one D-dimensional feature vector per patch.\n\n2. The Attentive Statistical Pool (red box) then takes that sequence of patch vectors and learns an attention score for each patch.  \n   – Using those scores it computes a weighted mean (and often a weighted standard-deviation) over the patch features.  \n   – This attention-weighted statistic collapses the entire sequence of patch outputs into one fixed-length “utterance” vector.\n\n3. Finally, a small linear layer projects that pooled utterance vector into the low-dimensional detection embedding yⱼ.\n\nIn short, the Attentive Statistical Pool “listens” to the stream of patch features coming out of the ViT, uses learned attention weights to compute a single summary statistic (mean ± std), and hands that summary off to the linear projector to produce the one detection embedding."
    },
    {
        "question": "How does the KNN detector utilize the detection embedding to compute anomaly score from local templates?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The anomaly detector g_i of factory i is a simple KNN detector. A local memory bank M_i of factory i is first set up by the embeddings of the local training dataset X_i: Since X_i only consists of normal audio, M_i serves as a set of normality templates in the feature space.",
            "For each query embedding y_j of the local test dataset Y_i, g_i infers a subset S_j of M_i, which consists of the top-K closest embeddings of M_i to y_j (using cosine distance as the metric).",
            "The anomaly score is defined as the mean distance of S_j to y_j."
        ],
        "final_answer": "The KNN detector first builds a memory bank of normal detection embeddings. For a test embedding, it retrieves its K nearest neighbors from this bank using cosine distance and then defines the anomaly score as the average of those K cosine distances.",
        "relevant_elements": [
            "KNN Detector g_i",
            "Detection Embedding y_j"
        ],
        "id": 86,
        "masked_question": "How does the [mask1] utilize the [mask2] to compute anomaly score from local templates?",
        "masked_number": 2,
        "masked_elements": [
            "KNN Detector g_i",
            "Detection Embedding y_j"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.14753v1_figure_1.png",
        "paperid": "2408.14753v1",
        "paper_path": "./papers/2408.14753v1.json",
        "figure_id": "2408.14753v1_figure_1.png",
        "caption": "Figure 1: Architecture of the ASD Model in CoopASD. The feature extractor f (⋅)𝑓⋅f(\\cdot)italic_f ( ⋅ ) is updated globally and shared among factories, while the linear classifier ci (⋅)subscript𝑐𝑖⋅c_{i}(\\cdot)italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) and KNN detector gi (⋅)subscript𝑔𝑖⋅g_{i}(\\cdot)italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) are uniquely constructed and preserved locally.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is nothing more exotic than a plain k-nearest‐neighbour detector, and the blue‐boxed signal is exactly the embedding it operates on.  In practice what happens is:\n\n1. During training each factory builds up a small “memory bank” of detection embeddings  \n   y₁, y₂, …, yₙ  (all of which come from normal recordings).\n\n2. At test time a new segment is passed through the feature extractor + pooling + linear head  \n   to produce its detection embedding yⱼ (the blue box).\n\n3. The KNN detector gᵢ(·) (the red box) then measures the cosine distance between yⱼ and every template in the local memory bank,  \n   selects the k templates with smallest cosine distance, and\n\n4. Defines the anomaly score as the mean of those k distances.\n\nIn short, “gᵢ takes the new embedding yⱼ, finds its k nearest normal‐template neighbours in cosine‐space, and the average of those distances is its anomaly score.”"
    },
    {
        "question": "How does feature extractor f adapt ViT backbone design for robust spectrogram patch representation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For each normal recording x_i^j from the local dataset of factory i, it is first converted to a log-mel spectrogram, then sent to the feature extractor f. SpecAug [16] is applied to the spectrogram which masks a portion of the spectrogram to improve the robustness.",
            "The feature extractor f adopts a ViT [17] backbone, which splits the spectrogram into patches, encodes each patch as an embedding by a linear layer, and processes them by stacks of Transformer [18] blocks, outputting a series of patch features.",
            "An attentive statistical pooling layer [19] is appended to the ViT backbone to fuse these patch features into an utterance embedding h_i^j, and a linear layer is employed to map h_i^j to a low-dimensional detection embedding y_i^j, which is further processed by the anomaly detector.",
            "To improve the robustness, the ViT backbone is initialized from BEATs [20], a pre-trained ViT model for audio classification."
        ],
        "final_answer": "Feature extractor f first applies SpecAug to the log–mel spectrogram to improve robustness, then uses a ViT backbone that splits the spectrogram into patches, projects each patch via a linear layer, and passes them through Transformer blocks. It then pools the resulting patch features with an attentive statistical pooling layer into a single embedding, followed by a linear layer to produce the detection embedding. Finally, the ViT backbone is initialized from the pre-trained BEATs model to further enhance robustness.",
        "relevant_elements": [
            "feature extractor f",
            "ViT backbone"
        ],
        "id": 87,
        "masked_question": "How does [mask1] adapt ViT backbone design for robust spectrogram patch representation?",
        "masked_number": 1,
        "masked_elements": [
            "feature extractor f"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.14753v1_figure_1.png",
        "paperid": "2408.14753v1",
        "paper_path": "./papers/2408.14753v1.json",
        "figure_id": "2408.14753v1_figure_1.png",
        "caption": "Figure 1: Architecture of the ASD Model in CoopASD. The feature extractor f (⋅)𝑓⋅f(\\cdot)italic_f ( ⋅ ) is updated globally and shared among factories, while the linear classifier ci (⋅)subscript𝑐𝑖⋅c_{i}(\\cdot)italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) and KNN detector gi (⋅)subscript𝑔𝑖⋅g_{i}(\\cdot)italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) are uniquely constructed and preserved locally.",
        "qtype": "Literature_Background",
        "response": "Here the red‐boxed module is the global feature extractor f(·), which is nothing more than a vanilla ViT that has been “massaged’’ for spectrograms:\n\n1.  Patchify the log-Mel spectrogram into fixed-size tiles.  \n2.  Linearly project each tile into a token (just like the original ViT patch embedding).  \n3.  Run those tokens through the usual stack of Transformer blocks.  \n4.  Instead of taking only the [CLS] token, append an Attentive Statistical Pooling layer over all patch outputs so that you get a single, robust utterance embedding.  \n5.  Finally, use a small linear head to map that utterance vector into the low-dimensional detection embedding y.  \n\nTwo more tricks for extra robustness:  \n–  At training time we apply SpecAugment (random time/frequency masking) to the spectrogram before patching.  \n–  We initialize all of the Transformer weights from BEATs, a ViT already pre-trained on large-scale audio data.  \n\nTaken together, these changes adapt a plain ViT into a spectrogram-specialized, highly robust feature extractor."
    },
    {
        "question": "How does linear classifier c_i leverage ArcFace loss methods to constrain decision boundaries under non-iid conditions?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To further enforce the classification task, ArcFace loss [21] is adopted in CoopASD instead of cross-entropy loss, which further restricts the decision zones: where y_i is the label of x_i, C_i is the number of classes of factory i, and m and s are two hyperparameters that constrain the decision zones. θ_j is the angle between f(x_i) and the registered embedding of the j-th class, which is the j-th column of the weight W_i of the linear classifier c_i: cos(θ_j) = f(x_i)^T W_i^j / (||f(x_i)|| ||W_i^j||).",
            "Secondly, since the data are completely non-iid, the local linear classifiers of different factories yield distinct decision zones after local training. If a unified classifier is adopted for all factories, the model has to be updated frequently to ensure convergence, which imposes huge burdens on the communication network. Therefore, only the feature extractor f is uploaded and aggregated by the central server, while each linear classifier c_i is maintained locally."
        ],
        "final_answer": "Under completely non-iid conditions, each factory keeps its own linear classifier c_i and trains it locally using an ArcFace loss in place of standard cross-entropy. This loss adds an additive angular margin m and a scale s to the cosine similarity between the embedding f(x_i) and the class-weight vector W_i^j, effectively tightening the angular decision boundaries around each class and enforcing larger inter-class margins. By maintaining c_i locally, these margin-constrained decision zones remain specialized for each factory’s unique attribute distribution without requiring frequent global updates.",
        "relevant_elements": [
            "linear classifier c_i",
            "ArcFace loss"
        ],
        "id": 88,
        "masked_question": "How does [mask1] leverage ArcFace loss methods to constrain decision boundaries under non-iid conditions?",
        "masked_number": 1,
        "masked_elements": [
            "linear classifier c_i"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.14753v1_figure_1.png",
        "paperid": "2408.14753v1",
        "paper_path": "./papers/2408.14753v1.json",
        "figure_id": "2408.14753v1_figure_1.png",
        "caption": "Figure 1: Architecture of the ASD Model in CoopASD. The feature extractor f (⋅)𝑓⋅f(\\cdot)italic_f ( ⋅ ) is updated globally and shared among factories, while the linear classifier ci (⋅)subscript𝑐𝑖⋅c_{i}(\\cdot)italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) and KNN detector gi (⋅)subscript𝑔𝑖⋅g_{i}(\\cdot)italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( ⋅ ) are uniquely constructed and preserved locally.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify what “[mask1]” refers to in the red-boxed region of Figure 1.  If you look at the diagram, the red box encloses the “Linear Classifier cᵢ(·) (Training)” block.  In the text (Sec. III-B) they tell us that, instead of ordinary cross-entropy, each local linear classifier is trained with an ArcFace loss of the form\n\n ℒ = −1⁄N ∑ᵢ log  \n   exp[s·cos(θ_{yᵢ}+m)]  \n   -------------------------\n   exp[s·cos(θ_{yᵢ}+m)] + ∑_{j≠yᵢ} exp[s·cos(θ_j)]  \n\nwhere  \n– θ_j is the angle between the embedding and the jth column of the classifier weight matrix,  \n– m is an additive angular margin,  \n– s is a scaling factor.\n\nHow does this choice help under non-iid local data?  By replacing plain dot-product logits with s·cos(θ) and adding a fixed angular margin m on the correct‐class angle, ArcFace forces:  \n  • intra‐class features to cluster more tightly in angle (smaller θ), and  \n  • inter‐class weight vectors to open up a fixed angular gap ( ≥ m ).  \n\nEven though each factory’s attribute labels and data distribution are completely different (i.e. highly non-iid), imposing this angular margin regularizes each local decision boundary to be “spherical” and margin‐separated.  In practice that means every local classifier learns tightly compact, equally wide “decision cones” around each class.  When these locally discriminative but similarly regularized feature‐extractors are averaged across clients (FedAvg), the global model converges more stably despite the heterogeneity of client labels."
    },
    {
        "question": "How do Agent Module’s CoT reasoning and Time Engine’s activity scheduling methodologies interplay?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "Chain-of-Thought (CoT, Wei et al. (2022)) reasoning is incorporated, enabling the agent to generate reasoning alongside its actions.",
            "The agent’s activation is governed by the time engine, which stores the user’s hourly activity probability in a 24-dimension list. Based on these usage patterns, the time engine probabilistically activates the agent at specific times."
        ],
        "final_answer": "The Time Engine governs when each agent is activated (using a 24-hour activity probability schedule), and whenever an agent is activated it uses CoT reasoning within the Agent Module to generate its reasoning and decide on which actions to take.",
        "relevant_elements": [
            "Agent Module",
            "Time Engine"
        ],
        "id": 89,
        "masked_question": "How do [mask1]’s CoT reasoning and Time Engine’s activity scheduling methodologies interplay?",
        "masked_number": 1,
        "masked_elements": [
            "Agent Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11581v4_figure_2.png",
        "paperid": "2411.11581v4",
        "paper_path": "./papers/2411.11581v4.json",
        "figure_id": "2411.11581v4_figure_2.png",
        "caption": "Figure 2: The workflow of OASIS. During the registration phase, real-world or generated user information will be registered on the Environment Server. In the simulation phase, the Environment Server sends agent information, posts, and users’ relations to the RecSys, which then suggests posts to agents based on their social connections, interests, or hot score of posts. LLM agents receive the recommended posts and generate actions and rationales based on the contents. These actions ultimately update the state of the environment in real-time. The Time Engine manages the agents’ temporal behaviors, while the Scalable Inference handles large-scale inference requests from users.",
        "qtype": "Literature_Background",
        "response": "Chain-of-Thought  \n1. The red box in the diagram is the Agent Module, which houses an LLM that, when activated, performs CoT (Chain-of-Thought) reasoning to produce both “reasons” and “actions” (like, repost, comment, etc.).  \n2. The Time Engine is a separate component that holds a 24-dimensional probability vector for each agent, encoding that agent’s likelihood of being active in each hour of a simulated day.  \n3. Rather than firing every agent’s LLM inference all at once, the Time Engine “rolls the dice” at each time step (every 3 minutes internally, or according to an adjustable real‐to‐simulated time mapping) and only activates agents whose hourly probability threshold is met.  \n4. When the Time Engine activates an agent, it effectively calls into the Agent Module: the agent’s memory and recent context are assembled, the LLM is prompted, and its CoT reasoning chain is generated to explain why it chooses a particular action.  \n5. Once the CoT inference is complete, the derived action is sent back to the Environment Server and the agent’s memory is updated (including storing that CoT trace for future context).  \n6. In this way, the Time Engine paces and gates exactly when CoT reasoning runs, and the CoT process itself uses the implicit “current time” context (since the agent knows which hour it’s being activated in) to shape its reasoning.  \n\nFinal Answer  \nThe Agent Module’s CoT reasoning only fires when the Time Engine probabilistically activates that agent according to its hourly activity profile. The Time Engine schedules agents in time, and each scheduled activation invokes the Agent Module’s chain-of-thought process—prompting the LLM to generate a step-by-step rationale and then an action. In turn, the CoT output gets logged back into memory, and the next activation still respects the same per-hour probabilities, tightly coupling temporal scheduling with when and how agents “think through” their moves."
    },
    {
        "question": "How does RecSys’s post filtering methodology align with Scalable Inferencer’s asynchronous GPU inference strategy?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "RecSys",
            "Scalable Inferencer"
        ],
        "id": 90,
        "masked_question": "How does [mask1]’s post filtering methodology align with [mask2]’s asynchronous GPU inference strategy?",
        "masked_number": 2,
        "masked_elements": [
            "RecSys",
            "Scalable Inferencer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11581v4_figure_2.png",
        "paperid": "2411.11581v4",
        "paper_path": "./papers/2411.11581v4.json",
        "figure_id": "2411.11581v4_figure_2.png",
        "caption": "Figure 2: The workflow of OASIS. During the registration phase, real-world or generated user information will be registered on the Environment Server. In the simulation phase, the Environment Server sends agent information, posts, and users’ relations to the RecSys, which then suggests posts to agents based on their social connections, interests, or hot score of posts. LLM agents receive the recommended posts and generate actions and rationales based on the contents. These actions ultimately update the state of the environment in real-time. The Time Engine manages the agents’ temporal behaviors, while the Scalable Inference handles large-scale inference requests from users.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In the red box ([mask1]), the RecSys is responsible for “post filtering”: it draws from an agent’s follow‐network (“in‐network”), from the broader world (“out‐of‐network”), and ranks each stream (by popularity, recency, interest‐matching vectors, or a Reddit‐style hot score) to produce a concise list of candidate posts for each agent.  \n2. In the blue box ([mask2]), the Scalable Inferencer is a pool of GPU‐backed LLM inference servers.  Agents fire off reasoning‐and‐action requests against these LLM instances, and the inferencer uses an asynchronous request queue, multi‐GPU load‐balancer, and batching (via vLLM or similar) so that thousands—even millions—of inference calls can be handled in parallel without blocking.  \n3. Alignment: by culling the full post corpus down to a small, personalized slice, the RecSys ensures each agent only sends a handful of relevant items into the inference pipeline.  That lightweight, targeted input meshes perfectly with an asynchronous, high‐throughput GPU inference tier: fewer, more focused prompts means the inferencer can pack more agent requests into each GPU batch and more smoothly juggle thousands of concurrent threads.  \n4. In short, smart filtering up front (red box) and massively concurrent, nonblocking GPU inference behind it (blue box) form a “filter‐then‐infer” duo that scales – both limiting wasted work and keeping the GPUs busy with just the right payloads."
    },
    {
        "question": "How does asynchronous request handling in Scalable Inference optimize Agent Module throughput during high Time Engine activations?",
        "relevant_section_ids": [
            "2.6"
        ],
        "relevant_context": [
            "We design a highly concurrent distributed system where agents, the environment server, and inference services operate as independent modules, exchanging data through information communication channels.",
            "The system leverages asynchronous mechanisms to allow agents to send multiple requests concurrently, even while waiting for responses from previous interactions, and the environment module processes incoming messages in parallel.",
            "Inference services manage GPU resources through a dedicated manager, which balances agent requests across available GPUs to ensure efficient resource utilization."
        ],
        "final_answer": "By employing asynchronous communication channels, the Agent Module can dispatch many inference requests at once without stalling on prior calls. Incoming activations and replies are handled in parallel, and a dedicated GPU‐manager dynamically distributes these concurrent requests across available GPUs, which maximizes throughput when the Time Engine activates large numbers of agents simultaneously.",
        "relevant_elements": [
            "Scalable Inference",
            "Agent Module",
            "Time Engine"
        ],
        "id": 91,
        "masked_question": "How does asynchronous request handling in [mask1] optimize Agent Module throughput during high Time Engine activations?",
        "masked_number": 1,
        "masked_elements": [
            "Scalable Inference"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11581v4_figure_2.png",
        "paperid": "2411.11581v4",
        "paper_path": "./papers/2411.11581v4.json",
        "figure_id": "2411.11581v4_figure_2.png",
        "caption": "Figure 2: The workflow of OASIS. During the registration phase, real-world or generated user information will be registered on the Environment Server. In the simulation phase, the Environment Server sends agent information, posts, and users’ relations to the RecSys, which then suggests posts to agents based on their social connections, interests, or hot score of posts. LLM agents receive the recommended posts and generate actions and rationales based on the contents. These actions ultimately update the state of the environment in real-time. The Time Engine manages the agents’ temporal behaviors, while the Scalable Inference handles large-scale inference requests from users.",
        "qtype": "Experimental_Results",
        "response": "In our diagram the red‐boxed component under the Time Engine is the “Scalable Inferencer” (sometimes called the “Scalable Influencer” in the text).  Here is how its asynchronous request handling protects the Agent Module from becoming a bottleneck whenever the Time Engine suddenly turns on hundreds or thousands of agents at once:\n\n1. Non‐blocking call semantics  \n   – As soon as an agent is “activated” by the Time Engine it fires off its LLM request into the Scalable Inferencer via an asynchronous API (i.e. it doesn’t wait in a blocking RPC for the model to come back).  \n   – The agent immediately returns to its own scheduler, can be marked as “in‐flight,” and the Time Engine can continue activating other agents without delay.\n\n2. Centralized GPU‐manager with request queue  \n   – Behind the scenes the Scalable Inferencer maintains a request queue and a lightweight manager that dynamically assigns incoming LLM calls to any idle GPU.  \n   – This manager uses an event‐driven, multi‐threaded loop to dequeue pending requests, pack or batch them if possible, and then launch them on whichever GPU has capacity.\n\n3. Latency hiding via concurrency  \n   – Because agents never block waiting for a single inference call, dozens or hundreds of requests can be “in the pipeline” at once.  \n   – While the GPUs chew on one batch of prompts, the system can continue to accept new ones and immediately schedule them, pumping up utilization.\n\n4. Dynamic load balancing  \n   – If one GPU is temporarily overloaded the manager can shift new calls to a less busy card, or defer them in the queue until capacity frees up.  \n   – This adaptive dispatching keeps all inference hardware busy instead of letting some cards sit idle while others queue up long lists.\n\nThe net effect is that when the Time Engine “fires” a burst of agent activations, each agent can instantly hand off its LLM request and move on.  The Scalable Inferencer then transparently pipelines, balances, and batches those requests across the GPU cluster.  By decoupling request submission from response retrieval, we eliminate idle wait times, level out peaks in demand, and maximize end‐to‐end Agent Module throughput even under very high concurrency."
    },
    {
        "question": "How could dynamic relation updates in Environment Server affect RecSys recommendation freshness under rapid post influx?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Environment Server",
            "RecSys"
        ],
        "id": 92,
        "masked_question": "How could dynamic relation updates in [mask1] affect RecSys recommendation freshness under rapid post influx?",
        "masked_number": 1,
        "masked_elements": [
            "Environment Server"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.11581v4_figure_2.png",
        "paperid": "2411.11581v4",
        "paper_path": "./papers/2411.11581v4.json",
        "figure_id": "2411.11581v4_figure_2.png",
        "caption": "Figure 2: The workflow of OASIS. During the registration phase, real-world or generated user information will be registered on the Environment Server. In the simulation phase, the Environment Server sends agent information, posts, and users’ relations to the RecSys, which then suggests posts to agents based on their social connections, interests, or hot score of posts. LLM agents receive the recommended posts and generate actions and rationales based on the contents. These actions ultimately update the state of the environment in real-time. The Time Engine manages the agents’ temporal behaviors, while the Scalable Inference handles large-scale inference requests from users.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module is the Environment Server, which not only holds every user’s profile and every post, but also keeps an up-to-date “relations” table—that is, who follows whom, who liked what, who commented on whose posts, etc.  Every time an agent follows, unfollows, likes, replies, or mutes another agent or post, the Environment Server immediately writes that new edge (or deletes the old one) in its relations graph.\n\nSecond, the RecSys in OASIS splits candidate posts into two buckets—“in-network” (from your current followees) and “out-of-network” (everybody else)⁠—and then scores and ranks them differently.  In-network content is ranked largely by straightforward popularity, whereas out-of-network content is filtered by interest‐matching embeddings and recency.\n\nChain of thought on freshness under rapid post influx:\n\n1.  If relations are updated in real time, then as soon as you follow a new account (or that account posts something), its fresh content jumps immediately into your in-network pool.  Conversely, if you unfollow someone, you stop seeing their freshly minted posts.\n\n2.  When hundreds or thousands of posts flood the system every minute, having stale relation data means you’ll continue recommending content from accounts you no longer care about—or you’ll miss hot new posts from accounts you’ve just followed.\n\n3.  By contrast, a fast, dynamic relation update pipeline lets RecSys recompute each agent’s in-network set on the fly.  This keeps the “in-network” feed tightly aligned to each user’s up-to-the-second social graph, so new posts from newly followed accounts are surfaced immediately, and irrelevant old voices drop out just as fast.\n\n4.  In short, under a high‐velocity stream of incoming posts, frequent relation updates in the Environment Server directly translate into fresh, relevant recommendations: the RecSys is always ranking against the latest follow/unfollow and interaction graph, rather than a frozen-in-time snapshot.\n\nIf relation updates lag behind post creation, recommendation freshness degrades—old connections over-weight stale content and fresh posts from emerging connections get buried.  Keeping that relations table in sync is therefore critical for maintaining low latency and high relevance in the face of rapid post influx."
    },
    {
        "question": "How do cross-domain and video-language contrastive modules jointly enforce domain-invariant reward representations?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We present our methodology for leveraging contrastive learning to learn domain-invariant features by forming pairs across domains. Specifically, we hope that samples within the same category, irrespective of their domain origin, are positioned closely in feature space, while those from distinct classes are separated regardless of domain.",
            "To promote the model’s ability to capture semantically pertinent features across human and robot domains, we employ a video-language contrastive loss. This approach, distinct from conventional video-language alignment, aims to minimize discrepancies in both domains. It not only aligns temporal dynamics with task semantics but also enhances the adaptation of video features by leveraging the shared action semantics across both domains."
        ],
        "final_answer": "The cross-domain contrastive module pulls together video features of the same task from human and robot domains while pushing apart different tasks, ensuring that the visual representations are aligned across domains. The video-language contrastive module then aligns those domain-invariant video features with their corresponding language embeddings (and repels mismatched video–text pairs) across both human and robot data. Together, these two contrastive objectives minimize inter-domain discrepancies in the visual space and enforce semantic consistency between video and text, yielding a reward model whose representations generalize across human and robot domains.",
        "relevant_elements": [
            "cross-domain contrastive learning",
            "video-language contrastive learning"
        ],
        "id": 93,
        "masked_question": "How do [mask1] and video-language contrastive modules jointly enforce domain-invariant reward representations?",
        "masked_number": 1,
        "masked_elements": [
            "cross-domain contrastive learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.14872v1_figure_2.png",
        "paperid": "2407.14872v1",
        "paper_path": "./papers/2407.14872v1.json",
        "figure_id": "2407.14872v1_figure_2.png",
        "caption": "Figure 2: Adapt2Reward Architecture.\nWe propose Adapt2Reward which incorporates learnable failure prompts into the model s architecture. Our approach starts with clustering failure videos to discern specific patterns. Each identified cluster is then associated with a unique failure prompt. Additionally, we employ cross-domain contrastive learning and domain-specific prompt learning to align text and video representations between human and robot domains.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what “[mask1]” is.  If you look at the red‐boxed module in Figure 2 and compare it with the text of §3.2 (“Cross Domain Contrastive Learning”), you can see that [mask1] is exactly the “Cross-Domain Contrastive Learning” block.  \n\nNow: how do Cross-Domain Contrastive (CDC) and Video-Language Contrastive (VLC) learning work together to produce reward representations that are invariant to the human vs. robot domain shift?\n\nChain-of-thought:\n\n1.  **Goal**  \n    We want a single reward model R(s, τ) (where s is a video clip, τ is a task description) that (a) reads both human and robot videos, (b) reads text descriptions, and (c) does not get “distracted” by superficial domain differences (lighting, camera angle, body proportions, etc.).\n\n2.  **Cross-Domain Contrastive (CDC)**  \n    –  We treat each *successful* video (or video–text pair) as having a “label” equal to its natural-language description.  \n    –  For any robot video feature vᵣ and any human video feature vₕ that correspond to the *same* description, CDC pulls ‖vᵣ–vₕ‖ → 0.  \n    –  For videos with *different* descriptions, CDC pushes their features apart.  \n    –  Result: a shared visual‐only feature space where “opening the drawer” looks the same whether it was done by a human or a robot.\n\n3.  **Video-Language Contrastive (VLC)**  \n    –  We also have text embeddings t(τ) for each description τ.  \n    –  VLC pulls the video feature v (from either human or robot) toward its matching text embedding t(τ), and pushes it away from *all other* text embeddings in the batch.  \n    –  This forces the visual encoder to pay attention to the *semantics* of the task, not the domain stamps.\n\n4.  **Plugging them together**  \n    –  CDC alone only aligns human vs. robot *video* features, but doesn’t guarantee those features carry *task* semantics.  \n    –  VLC alone aligns video to text, but if done separately for human and robot it may still carve out two disjoint video subspaces (one for human, one for robot).  \n    –  By *simultaneously* optimizing both CDC and VLC over *mixed* batches of human+robot videos + their texts, we force the network to:  \n       • Learn video features that are both domain-agnostic (via CDC)  \n       • And semantically grounded in the natural-language description (via VLC).  \n\n5.  **Outcome**  \n    The shared encoder now produces reward scores (via a simple dot‐product or classification head) that generalize to new tasks and new domains, because the representation has been explicitly stripped of domain‐specific noise and aligned to the underlying task semantics.\n\nAnswer (concise):\n\nThey jointly enforce domain invariance by (1) using cross-domain contrastive learning to pull together human and robot videos of the *same* task (and push apart different tasks), and (2) using video–language contrastive learning to align those now-domain-agnostic video features with their shared text descriptions.  The two losses run in tandem on mixed human/robot batches, so the model can only succeed by discovering a representation that is both domain-independent and semantically faithful to the task, which in turn yields a robust, generalizable reward function."
    },
    {
        "question": "How does K-means clustering of failure videos guide failure prompt generation for nuanced failure modeling?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "By leveraging the distinct context of each failure, we seek to capture the unique precursors leading to each specific failure instance. Acknowledging the varied reasons for failures across different tasks, we propose the creation of a “failure prompts pool” to achieve this. This pool allows for flexible grouping and integration as input into the model, facilitating a nuanced and task-specific approach to understanding failures.",
            "For each task t, whose expression is z, the task-specific prompt pool consists of K learnable prompts: where P_j is a prompt with token length L_p and the same embedding size as z.",
            "We dynamically select suitable prompts for various videos depicting robotic task failures. For each task’s failure videos, we utilize spherical K-means clustering to iteratively update the clustering centers at the end of each training epoch. This process enables the assignment of new pseudo-labels to the failure videos, effectively uncovering distinct failure themes specific to each task.",
            "The assigned pseudo-labels are interpreted as indicators of the respective failure causes. Consequently, we select failure prompts based on their corresponding pseudo-label l, leveraging this alignment to foster understanding of failure dynamics."
        ],
        "final_answer": "K-means clustering is applied to the encoded features of each task’s failure videos to group them into K clusters, each corresponding to a different failure theme. The cluster assignments serve as pseudo‐labels for failure causes, and these pseudo‐labels are used to index into a pool of K learnable prompts. In this way, each failure video is paired with the prompt whose index matches its cluster, yielding task‐ and failure‐specific textual cues that let the model distinguish nuanced failure modes.",
        "relevant_elements": [
            "K-means clustering",
            "failure prompts"
        ],
        "id": 94,
        "masked_question": "How does [mask1] of failure videos guide [mask2] generation for nuanced failure modeling?",
        "masked_number": 2,
        "masked_elements": [
            "K-means clustering",
            "failure prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.14872v1_figure_2.png",
        "paperid": "2407.14872v1",
        "paper_path": "./papers/2407.14872v1.json",
        "figure_id": "2407.14872v1_figure_2.png",
        "caption": "Figure 2: Adapt2Reward Architecture.\nWe propose Adapt2Reward which incorporates learnable failure prompts into the model s architecture. Our approach starts with clustering failure videos to discern specific patterns. Each identified cluster is then associated with a unique failure prompt. Additionally, we employ cross-domain contrastive learning and domain-specific prompt learning to align text and video representations between human and robot domains.",
        "qtype": "Experimental_Results",
        "response": "Here’s the key idea, unpacked step by step:\n\n1.  Mask 1 (“the red‐boxed operation”) is the spherical K-means clustering of the robot’s failure-video embeddings.  At the end of each epoch, all failure videos for a given task are re-clustered into K groups (each group ideally capturing one distinct mode or cause of failure), and each video is assigned a cluster “pseudo-label.”\n\n2.  Mask 2 (“the blue‐boxed output”) is the selection (and subsequent learning) of a task-specific “failure prompt” from a small pool of learnable text prompts.  There are K learnable prompt vectors for each task, one per cluster.\n\n3.  By tying them together:  whenever the model processes a failure video, it looks up that video’s cluster index (from the K-means step) and then pulls in the corresponding prompt token from the pool.  This concatenated text—  \n     <failure_prompt_for_cluster_i> + “<robot> closing drawer”  \n   —becomes the textual input alongside the video.\n\n4.  In this way, each identifiable failure mode (as discovered by clustering in the red box) automatically “selects” its own specialized prompt (in the blue box), letting the model learn a distinct text embedding for each way the robot can fail.  This yields much richer, more nuanced failure signals than a single “generic failure” tag."
    },
    {
        "question": "What limitations arise from using K-means clustering to generate Failure Prompts for diverse failure modes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Failure Prompts",
            "K-means"
        ],
        "id": 95,
        "masked_question": "What limitations arise from using [mask1] clustering to generate Failure Prompts for diverse failure modes?",
        "masked_number": 1,
        "masked_elements": [
            "K-means"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.14872v1_figure_2.png",
        "paperid": "2407.14872v1",
        "paper_path": "./papers/2407.14872v1.json",
        "figure_id": "2407.14872v1_figure_2.png",
        "caption": "Figure 2: Adapt2Reward Architecture.\nWe propose Adapt2Reward which incorporates learnable failure prompts into the model s architecture. Our approach starts with clustering failure videos to discern specific patterns. Each identified cluster is then associated with a unique failure prompt. Additionally, we employ cross-domain contrastive learning and domain-specific prompt learning to align text and video representations between human and robot domains.",
        "qtype": "Others",
        "response": "First, let’s align the red‐boxed region in the figure with the text.  In the “Failure Prompts” section (page 8 of the context), the authors say:\n\n  •  “For each task’s failure videos, we utilize spherical K-means clustering to iteratively update the clustering centers …  \n  •  The assigned pseudo-labels are interpreted as indicators of the respective failure causes.  \n  •  We assume these pseudo-labels … succinctly encapsulate the semantic essence of each failed video …”\n\nIn the picture that region is literally labeled “K-means” and shows failure video embeddings being carved into several clusters, each of which then picks up its own “failure prompt.”\n\nSo [mask1] = “K-means.”\n\n---\n\nStep-by-step, the limitations of using plain K-means clustering to generate a diverse set of failure prompts are:\n\n1. fixed K and manual tuning  \n   – You must choose the number of clusters K up front, but you rarely know how many distinct “modes” of failure there really are.  If you set K too low you will merge genuinely different failures; if you set it too high you’ll fragment a single mode into spurious subclusters.\n\n2. spherical, equal-variance cluster assumption  \n   – K-means assumes each cluster is roughly spherical in the embedding space and of similar size.  In reality, some failure modes may span elongated or highly irregular regions of feature space (e.g. early-stage vs. late-stage failures), so K-means can misassign those samples.\n\n3. hard assignments only  \n   – Every failure video must belong to exactly one cluster.  If a failure video exhibits characteristics of two distinct modes it will still be forced into a single bucket, losing that nuance.\n\n4. sensitivity to initialization and instability over epochs  \n   – Although the paper realigns clusters each epoch, K-means is famously sensitive to initial centroid seeds and can “flip” cluster IDs over time, producing noisy pseudo-labels and hurting convergence.\n\n5. ignoring temporal structure or fine‐grained semantics  \n   – K-means operates purely on the final video‐embedding vectors.  It has no built-in way of knowing that two failures differ only in *when* they went wrong (e.g. early vs. late), or that they share some substructure.  Any such subtleties are lost.\n\n6. poor support for rare or outlier failures  \n   – If one particular kind of failure only shows up a handful of times in the robot data, it may be swallowed by a larger neighbor cluster or treated as noise, rather than spawning its own prompt (and so the model never learns to recognize it).\n\nAll of these issues can lead to “failure prompts” that are not semantically coherent, that miss important edge‐cases, or that overfit to whichever clusters happened to be formed in that epoch, and thus limit the method’s ability to cover truly diverse failure modes."
    },
    {
        "question": "What ethical concerns emerge from the Evaluation Module’s self-critique mechanism?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Evaluation Module",
            "self-critique mechanism"
        ],
        "id": 97,
        "masked_question": "What ethical concerns emerge from the [mask1]’s self-critique mechanism?",
        "masked_number": 1,
        "masked_elements": [
            "Evaluation Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.17406v1_figure_1.png",
        "paperid": "2410.17406v1",
        "paper_path": "./papers/2410.17406v1.json",
        "figure_id": "2410.17406v1_figure_1.png",
        "caption": "Figure 1. ProveRAG: automated retrieval-augmented generation & self-critique provenance.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n\n1.  The red-boxed portion of the figure is the “Evaluation Module,” where ProveRAG’s LLM inspects its own previous outputs (labeling them TP/FP/FN), produces a rationale, and cites provenance.  \n2.  In the paper this is touted as a way to reduce hallucination and build “trust” by showing evidence for every claim.  \n3.  However, outsourcing the act of critique entirely to the same or a sibling LLM layer raises several ethical flags:\n   •  Automation bias/over-reliance:  Users may defer unquestioningly to the model’s self-assessment, even when it’s mistaken or systematically biased.  \n   •  Accountability:  When the system “marks” its own errors, it becomes harder to determine who is responsible if something slips through—developers, operators, or the model itself?  \n   •  Transparency vs. opacity:  Although the module “shows evidence,” the underlying scoring and decision thresholds remain a black box, risking a false sense of clarity.  \n   •  Privacy and data-leak risks:  In surfacing provenance snippets, the module could inadvertently expose sensitive or proprietary information.  \n   •  Adversarial misuse:  An attacker might feed carefully crafted inputs to manipulate the self-critique into declaring poisoned or back-doored outputs “all clear.”  \n\nAnswer:\n\nBy giving the same or a sibling LLM authority to “self-critique,” ProveRAG risks creating automation bias and a dangerous over-reliance on the model’s own judgment.  Accountability becomes murky—if the evaluation layer misses or mislabels an error, who is at fault?  Although provenance snippets are surfaced, the critique process itself is still opaque, fostering a misleading sense of transparency.  Furthermore, exposing provenance runs the risk of leaking sensitive data, and an adversary could craft inputs to fool the self-critique into certifying malicious outputs.  All together, these factors raise ethical concerns about trust calibration, responsibility, transparency, privacy, and potential adversarial manipulation."
    },
    {
        "question": "What alternative retrieval strategies could augment the summary mitigation/exploitation information step?",
        "relevant_section_ids": [
            "4.1.1"
        ],
        "relevant_context": [
            "In the prompt-only experiment, the Relevant Information part is removed as we directly query the LLM about a specific CVE.",
            "We test on two retrieval techniques (to address RQ2). Figure 2 shows how each of these techniques is used. For the chunking technique, the LLM will use top-10 most similar chunks of 15,000 characters from the resources.",
            "In the chunking technique, we split the content of all the URLs into smaller chunks and embed these chunks as vectors using OpenAI’s text-embedding-ada-002 embedding model (by utilizing the LangChain framework...). These embeddings are then indexed to facilitate efficient retrieval. During runtime, the user’s query is also vectorized using the same embedding model, and a similarity search is conducted against the indexed chunks. The top-10 results are retrieved and fed as context into the Gen. LLM’s prompt..."
        ],
        "final_answer": "Besides the summarization-based retrieval, the paper evaluates two alternative strategies: 1) a prompt-only approach that skips any external retrieval and directly asks the LLM about the CVE, and 2) a chunking retrieval technique that splits the raw web content into fixed-size chunks, embeds them with a vector model, and retrieves the top-k semantically similar chunks to serve as context for the LLM.",
        "relevant_elements": [
            "Retr. LLM",
            "Summary mitigation/exploitation information"
        ],
        "id": 98,
        "masked_question": "What alternative [mask1] strategies could augment the [mask2] step?",
        "masked_number": 2,
        "masked_elements": [
            "Retr. LLM",
            "Summary mitigation/exploitation information"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.17406v1_figure_1.png",
        "paperid": "2410.17406v1",
        "paper_path": "./papers/2410.17406v1.json",
        "figure_id": "2410.17406v1_figure_1.png",
        "caption": "Figure 1. ProveRAG: automated retrieval-augmented generation & self-critique provenance.",
        "qtype": "Others",
        "response": "First, let’s align the masks to the figure and text:\n\n  1.  [mask1] is the box in red around the “Retr. LLM” icon – i.e. the retrieval step or “retrieval strategy.”  \n  2.  [mask2] is the box in blue around “Summary mitigation/exploitation information” – i.e. the summarization step that distills the raw retrieved content into a compact summary of exploits and fixes.\n\nSo the question becomes:\n\n  “What alternative retrieval strategies could augment the summarization step?”\n\nChain-of-thought to arrive at an answer:\n\n  •  ProveRAG currently experiments with two retrieval approaches:  \n     – Chunking (split all source URLs into 15 K-char pieces, embed via text-embedding-ada-002, similarity search, top-10 chunks)  \n     – Summarizing (an LLM walks NVD → CWE → hyperlinks, producing a single integrated exploitation/mitigation summary)  \n\n  •  Both feed into the “Summary mitigation/exploitation information” step, before the Gen. LLM generates the final advice.\n\n  •  We can therefore look at other retrieval paradigms—beyond basic vector chunking and LLM-driven summarization—that might give the summarizer richer or more targeted source material.\n\nHere are several alternative retrieval strategies you could plug in under [mask1] to bolster the summary step:\n\n1.  Hybrid Sparse + Dense Retrieval  \n    •  Combine a classic BM25 (or TF–IDF) retrieval over the raw HTML/text of NVD/CWE with a dense vector retrieval (e.g. DPR, Contriever) to capture both exact‐match signals and semantic matches.  \n    •  Re-rank the union of sparse and dense hits (e.g. with MonoBERT or a learned cross-encoder) before feeding them to the summarizer.  \n\n2.  Multi-hop or Graph-based Retrieval  \n    •  Build a knowledge graph linking CVE-IDs, CWE categories, vendor advisories, patch-notes, exploit-DB entries, etc., then traverse edges to gather second-order evidence (for instance: CVE→vulnerable library→exploit-DB write-ups).  \n    •  Summarize along each traversal path and then fuse the path-specific summaries.  \n\n3.  Query-Rewriting with Relevance Feedback  \n    •  Use an initial retrieval pass, then ask the LLM (or a lightweight classifier) to score which passages seem most on‐topic.  \n    •  Auto-expand or refine the original query (e.g. “CVE-2023-1234 remote code execution mitigation best practice”) and re-issue to the sources.  \n\n4.  Iterative “Ask for Missing Info” Retrieval  \n    •  After the first summary draft, prompt the LLM: “Is there any important exploit vector or mitigation approach you can’t fully elaborate because you lack direct evidence?”  \n    •  If it flags gaps, automatically send follow-up retrieval queries targeting those specific sub-topics.  \n\n5.  Retrieval via Domain-Specific Embeddings  \n    •  Fine-tune or adapt the embedding model on a corpus of vulnerability write-ups so that it better distinguishes distinctions like “attack vector” vs. “patch available.”  \n    •  Use those embeddings in place of a generic text-embedding-ada-002.  \n\n6.  Citation-Aware Retrieval  \n    •  Rather than treating each chunk as equal, weight chunks by the source’s trust level (e.g. official NIST vs. low-traffic blog).  \n    •  Feed the summarizer both the content and metadata (source, date, CVSS score) so it can prioritize higher-fidelity patches or exploits.  \n\nAny of these could be dropped in place of—or run in parallel with—the existing chunking/summarization approaches, yielding a richer set of evidence for the “Summary mitigation/exploitation information” step."
    },
    {
        "question": "What is the motivation behind separating retrieval and generation LLMs into distinct modules?",
        "relevant_section_ids": [
            "3",
            "4",
            "4.1.1"
        ],
        "relevant_context": [
            "RQ2: How to manage information overload in the context of RAG-based LLMs for complex queries? Another significant problem we aim to solve is the issue of information overload (i.e., arising from context window limitations). This problem of inability of LLMs to process massive amounts of information and identify relevant information can lead to less effective responses, as discussed in literature (Liu et al., 2024 ###reference_b17###). While more information is often seen as beneficial, our research shows that in the context of LLM-powered vulnerability analysis, an abundance of data can lead to inefficiencies (when chunking technique is used). The summarization technique we encountered in ProveRAG, can alleviate this issue. This technique can make the system especially effective in handling more complex cybersecurity issues, such as CVE mitigation queries that require more attention on additional relevant resources.",
            "ProveRAG works based on two main stages: The generation module and the evaluation module. Figure 1 shows the overall architecture. First, in the generation module, the Retr. LLM will load web content from external sources (starting from NVD) about a CVE vulnerability, and it will summarize the content with respect to the exploitation and mitigation information. Next, the Gen. LLM will look at the summaries and generates detailed exploitation and mitigation strategies for that specific CVE.",
            "A key advantage of the summarizing technique is its efficiency in handling large-scale data without relying extensively on document chunking, which enhances retrieval quality and accuracy of responses. This makes the system particularly suited for addressing more complex cybersecurity queries, such as those related to CVE mitigation that require exploration of more external resources."
        ],
        "final_answer": "By splitting the workflow into a retrieval LLM (which gathers and summarizes up-to-date external content) and a generation LLM (which uses that concise, relevant summary to produce exploitation and mitigation guidance), ProveRAG is able to (1) manage information overload and long document contexts, (2) overcome the temporal scope limits of pretrained models by integrating fresh data, and (3) provide more accurate, focused, and trustworthy responses.",
        "relevant_elements": [
            "Retr. LLM",
            "Gen. LLM"
        ],
        "id": 99,
        "masked_question": "What is the motivation behind separating [mask1] and [mask2] into distinct modules?",
        "masked_number": 2,
        "masked_elements": [
            "Retr. LLM",
            "Gen. LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.17406v1_figure_1.png",
        "paperid": "2410.17406v1",
        "paper_path": "./papers/2410.17406v1.json",
        "figure_id": "2410.17406v1_figure_1.png",
        "caption": "Figure 1. ProveRAG: automated retrieval-augmented generation & self-critique provenance.",
        "qtype": "Design_Rationale",
        "response": "Let’s start by mapping the two masked boxes back into the prose:\n\n  1. Mask 1 (red box) is the “Retr. LLM” in the Generation Module—the component responsible for going out to NVD, CWE, hyperlinks, etc., pulling in raw text, chunking or summarizing it, and returning a distilled set of “Relevant Information.”  \n  2. Mask 2 (blue box) is the “Gen. LLM” in the Generation Module—the component that takes only that distilled context and actually writes the exploitation and mitigation advice.\n\nWhy did the authors split retrieval and generation into two separate sub-modules?  \n\nChain-of-thought:  \n  • We know from RQ1 and RQ2 that ProveRAG must (a) cope with very fresh vulnerabilities (temporal limitation) and (b) avoid swamping the model with huge context windows (information overload).  \n  • The retrieval step has a very different goal from the generation step: it must browse multiple sources, chunk or summarize mega-documents, select just the exploitation- and mitigation-relevant facts, and produce a concise context.  \n  • The generation step, by contrast, must use only that carefully pre-filtered context to produce coherent, deterministic recommendations (temperature = 0) without having to scroll through gigabytes of raw text.  \n  • By making retrieval its own LLM call and generation its own, the system can:  \n     – Tune each prompt and hyperparameter to its narrowly defined job,  \n     – Control for hallucination (generation never “has” to ask the net for missing data),  \n     – Limit context size and complexity flowing into the Gen. LLM,  \n     – Trace and isolate failures to either “did we pull the wrong facts?” versus “did we generate a bad explanation?”  \n  • In short, separation enforces modularity: retrieval specializes in freshness and precision, generation specializes in clarity and trustworthiness.  \n\nAnswer (succinctly):  \nThey split Retrieval (mask 1) and Generation (mask 2) so that each LLM call can be narrowly tailored—Retrieval can aggressively fetch, chunk, and summarize up-to-the-minute external data (solving RQ1/RQ2), while Generation can take that lean, vetted context and produce deterministic, hallucination-resistant mitigation/exploitation advice (solving RQ3). This modular design lets you tune, debug, and trust each step independently."
    },
    {
        "question": "What is the rationale for integrating provenance evidence into the Evaluation Module?",
        "relevant_section_ids": [
            "3",
            "4.1.2"
        ],
        "relevant_context": [
            "One other key motivation for developing ProveRAG is the critical requirement to enhance the accuracy and trustworthiness of threat analysis by mitigating issues related to hallucination and omission errors. ProveRAG addresses this by integrating authoritative sources such as NVD and CWE into its framework. This integration ensures that the recommendations provided by ProveRAG are grounded in reliable and verifiable data, which we call provenance. To leverage the capabilities of LLMs, ProveRAG utilizes a structured, explicit, and concise prompt designed with a chain-of-thoughts technique (Wei et al., 2022). This approach compels the LLM to substantiate its responses with evidence, enhancing the credibility of the output.",
            "We carefully and iteratively designed structured, detailed, and succinct prompts with the chain-of-thought technique (Wei et al., 2022) to guide the model to think before answering while providing evidence. ... For these attributes, we ask the LLM to provide a value: True Positive (TP) ... rationale, and finally the provenance attribute where the LLM is tasked to show evidence for its response."
        ],
        "final_answer": "The Evaluation Module integrates provenance evidence to compel the LLM to substantiate its outputs with verifiable information from authoritative sources, thereby mitigating hallucinations and omissions, bolstering the accuracy and credibility of its threat analysis, and providing analysts with a transparent, auditable trail that they can trust.",
        "relevant_elements": [
            "provenance",
            "Evaluation Module"
        ],
        "id": 100,
        "masked_question": "What is the rationale for integrating [mask1] evidence into the Evaluation Module?",
        "masked_number": 1,
        "masked_elements": [
            "provenance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.17406v1_figure_1.png",
        "paperid": "2410.17406v1",
        "paper_path": "./papers/2410.17406v1.json",
        "figure_id": "2410.17406v1_figure_1.png",
        "caption": "Figure 1. ProveRAG: automated retrieval-augmented generation & self-critique provenance.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In Fig. 1 the red box around “provenance” in the Evaluation Module shows that after the LLM assigns a value (TP/FP/FN) and writes a rationale, it must also spit out the actual “Evidence” it used to judge its own previous answer (“Response”).  \n2. This “Evidence” comes from the same authoritative sources (NVD, CWE, hyperlinks) that were retrieved at evaluation time, so that every claim in the critique is backed up by a real snippet from an external source.  \n3. One of our core research questions (RQ3) is “How to enhance accuracy and trustworthiness by mitigating hallucination and omission?”  Simply labeling something “hallucinated” is not enough – you must show exactly where you pulled the contrary fact from.  \n4. By integrating the provenance or evidence step, ProveRAG creates an auditable trail: an analyst can see not only the LLM’s self-critique but the exact text it used to arrive at that critique.  \n5. This explicit evidence grounding (a) anchors the model’s verdicts in verifiable data, (b) makes detection of hallucinations/omissions transparent, and (c) builds human trust by allowing easy cross-checking of recommendations.  \n\nAnswer:\n\nThe evidence (“provenance”) box is included so that every self-critique is directly tied to concrete excerpts from the underlying authoritative sources. By forcing the Eval. LLM to show the actual snippets it compared against its own response, ProveRAG can both detect hallucinations/omissions and give users a transparent, auditable trail—thereby boosting accuracy and trust in its vulnerability analyses."
    },
    {
        "question": "What motivates integrating feature selection and data balancing sequentially in pre-processing stage?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Feature Selection",
            "Data Balancing"
        ],
        "id": 101,
        "masked_question": "What motivates integrating [mask1] and [mask2] sequentially in pre-processing stage?",
        "masked_number": 2,
        "masked_elements": [
            "Feature Selection",
            "Data Balancing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02670v1_figure_1.png",
        "paperid": "2411.02670v1",
        "paper_path": "./papers/2411.02670v1.json",
        "figure_id": "2411.02670v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed methodology",
        "qtype": "Design_Rationale",
        "response": "Mask₁ is the Feature‐Selection step and Mask₂ is the Data‐Balancing step.  We put them in that order because, first, we winnow the raw feature set down to only the most predictive attributes – removing noise, redundancy, and irrelevant measurements – which both speeds up learning and makes the model’s later SHAP explanations more meaningful.  Only once we have a compact, high-quality feature space do we apply oversampling/undersampling to correct the class skew.  Balancing on the reduced feature set avoids “hallucinating” synthetic examples in meaningless dimensions and gives us a fair, unbiased training set for the XAI classifier."
    },
    {
        "question": "Why group mean SHAP values for TP and FP before visual analysis in Decision Making Module?",
        "relevant_section_ids": [
            "3.2",
            "3.3.3"
        ],
        "relevant_context": [
            "Moreover, we generate group-wise feature explanation with mean SHAP values for each of the subgroups, such as true-positive (E_tp_mean), true-negative (E_tn_mean), false-positive (E_fp_mean), and false-negative (E_fn_mean) groups mean SHAP values along with the global mean SHAP.",
            "S1 (L1–L7 in Algorithm 1): Generating and storing the top contributing features’ (usually top 20 features) SHAP bar plots with global mean SHAP values for all four groups (e.g., TP, TN, FP, and FN).",
            "S2 (L8–L15 and L22–L25 in Algorithm 1): For each individual instance outcome, if the prediction is positive (meaning an attack traffic is predicted), then generate plots using the local feature SHAP value E_i by comparing the global SHAP values of the true-positive (E_tp_mean) and false-positive (E_fp_mean) group’s top features through a new overlapping bar graph.",
            "S3 (L16–L20 and L26–L30 in Algorithm 1): In this step, we observe the overlapping graphs to understand visually differentiable or similar feature contributions. We can infer that higher number of overlapping bars in these bar graphs, which we define as c, indicates a particular instance is closer to that corresponding group while the less overlapping scenario indicates distance from that group."
        ],
        "final_answer": "The decision‐making module first computes the mean SHAP values for the true‐positive and false‐positive groups so that each incoming instance’s local SHAP plot can be overlaid and compared against these group baselines. By having the group mean SHAP patterns for TP and FP, the analyst can visually inspect which group the new instance more closely resembles (via overlapping bar counts) and thereby decide whether the model’s positive prediction is likely correct (TP) or a false positive (FP).",
        "relevant_elements": [
            "Mean SHAP Value",
            "Decision Making Module"
        ],
        "id": 102,
        "masked_question": "Why group mean SHAP values for TP and FP before visual analysis in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Decision Making Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02670v1_figure_1.png",
        "paperid": "2411.02670v1",
        "paper_path": "./papers/2411.02670v1.json",
        "figure_id": "2411.02670v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed methodology",
        "qtype": "Design_Rationale",
        "response": "They compute the group‐level (mean) SHAP vectors for TP, FP (and likewise TN/FN) up front so that\n\n 1.  You have a fixed “prototype” feature‐importance profile for each of the four confusion‐matrix cells (so you know, on average, which features the model leans on when it is right vs. when it makes that specific error).\n\n 2.  Those prototypical SHAP profiles become the bars you overlay against any new incoming instance’s local SHAP bars.  By counting how many bars overlap you get a quick, visual similarity score (more overlap with TP means “probably a true positive,” more overlap with FP means “probably a false alarm,” etc.).\n\n 3.  Precomputing the group means also dramatically cuts down on per‐instance computation (you don’t have to re‐aggregate your test set SHAP values every time), and it smooths out noise so that your visual comparison is stable and interpretable.\n\nIn short, the mean SHAP values for TP vs. FP give you the reference “shape” of a correct positive vs. a mistaken positive so that your overlapping‐bar plot (Step S2/S3 in the text) has something concrete to compare each new instance against."
    },
    {
        "question": "How does Pre-Processing integrate feature selection and data balancing to ensure feature consistency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pre-Processing",
            "Feature Selection",
            "Data Balancing"
        ],
        "id": 103,
        "masked_question": "How does [mask1] integrate feature selection and data balancing to ensure feature consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Pre-Processing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02670v1_figure_1.png",
        "paperid": "2411.02670v1",
        "paper_path": "./papers/2411.02670v1.json",
        "figure_id": "2411.02670v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed methodology",
        "qtype": "Implementation_Details",
        "response": "The little red box on the left‐hand side of the diagram is simply the “Pre-Processing” stage of the Data Module.  In practice we make sure that feature selection and data balancing happen inside that one “Pre-Processing” block so that every sample – whether it ends up in the training or the test set – is described by exactly the same list of n features.  Concretely, the pipeline inside that box is:\n\n 1.  Clean the raw IDS traffic (remove NaNs, bad records, etc.)  \n 2.  Scale all numerical fields to a common range (e.g. min–max or standardization).  \n 3.  Run your feature‐selection procedure (filter, wrapper or embedded) to reduce the full feature set down to a fixed list F = {f1, f2, … , fn}.  \n 4.  Split into train/test (typically 80:20).  \n 5.  Apply oversampling (e.g. SMOTE), undersampling, or a hybrid approach on the training side only, so that both classes become equally represented—but only ever touching the features in F.  \n 6.  For the held-out test set we simply down‐sample the majority class (again using exactly the same n features) so that our performance numbers are measured on a balanced, feature-consistent test bed.  \n\nBecause feature selection happens before any sampling, and every sampling step works on the exact same n features, we guarantee that both classes (attack vs. benign) and both splits (train vs. test) reside in one common, fixed feature space."
    },
    {
        "question": "How does the Decision Making Module leverage overlapping SHAP bar plots for individual instance classification?",
        "relevant_section_ids": [
            "3.3.3"
        ],
        "relevant_context": [
            "S2 (L8-L15 and L22-L25 in Algorithm 1): For each individual instance outcome, if the prediction is positive (meaning an attack traffic is predicted), then generate plots using the local feature SHAP value Ei by comparing the global SHAP values of the true-positive (Etp_mean) and false-positive (Efp_mean) group’s top features through a new overlapping bar graph. On the other hand, if the prediction is negative (meaning a benign traffic is predicted), then the local features’ SHAP values would be mapped in overlapping bar graphs with the corresponding features from both the true-negative (Etn_mean) and false-negative (Efn_mean) groups.",
            "S3 (L16-L20 and L26-L30 in Algorithm 1): In this step, we observe the overlapping graphs to understand visually differentiable or similar feature contributions. We can infer that higher number of overlapping bars in these bar graphs which we define as N_overlap indicates a particular instance is closer to that corresponding group while the less overlapping scenario indicates distance from that group. Using this metrics from the respective graphs, an analyst can finally take the decision to mark a prediction as correct (TP, TN) or incorrect (FP, FN)."
        ],
        "final_answer": "The Decision Making Module overlays an individual instance’s local SHAP bar plot with the precomputed group‐level SHAP bar plots for the two relevant subgroups (TP vs. FP if the model predicted “attack,” or TN vs. FN if it predicted “benign”). It then counts how many feature bars overlap between the instance and each group. A higher count of overlapping bars indicates that the instance’s explanation is closer to that group’s characteristic pattern, guiding the analyst to decide whether the instance is correctly classified or is a false positive/false negative.",
        "relevant_elements": [
            "Decision Making Module",
            "Overlapping SHAP bar plots"
        ],
        "id": 104,
        "masked_question": "How does the [mask1] leverage overlapping SHAP bar plots for individual instance classification?",
        "masked_number": 1,
        "masked_elements": [
            "Decision Making Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.02670v1_figure_1.png",
        "paperid": "2411.02670v1",
        "paper_path": "./papers/2411.02670v1.json",
        "figure_id": "2411.02670v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed methodology",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed “Decision Making Module” is nothing more exotic than a tiny visual analytics loop around SHAP.  Concretely it works like this:\n\n1. Off-line you compute four “mean‐SHAP” bar-charts (one for TP, TN, FP and FN) by averaging the top k features’ SHAP values over each subgroup of your test set.  \n2. At run-time, whenever a new flow comes in you  \n   a. use your chosen model to predict Yᵢ (attack vs benign) and  \n   b. ask your SHAP Explainer for that single flow’s local SHAP vector Eᵢ (its feature‐importance bars).  \n3. Depending on Yᵢ:  \n   – If the model said “attack,” you overlay the flow’s bars on top of the TP and FP mean-bars.  \n   – If it said “benign,” you overlay on the TN and FN mean-bars.  \n4. You then count how many of your flow’s top‐feature bars “line up” (i.e. overlap) with each group’s mean-bars.  A higher overlap with the TP (or TN) bars says “this looks like a true positive (or true negative)”; a higher overlap with FP (or FN) says “this is likely a mistake.”  \n5. If the overlap counts are too close to call, you fall back to the raw predicted probability Pᵢ.\n\nIn this way the module “leverages overlapping SHAP bar plots” by turning visual similarity (bar-by-bar overlap) into a simple numeric proxy for “does this new instance behave more like the correct group or the misclassified group?”"
    },
    {
        "question": "How does the disentanglement process transform the physics prior map into distinct degradation region clusters?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "As a common practice, we estimate the illuminance map L by utilizing the maximum RGB channel of image I as L(x)=maxc∈{R,G,B}Ic(x). Then k-means is employed to acquire three clusters representing darkness, well-lit, and high-light regions. These clusters are aggregated as masks Mdark, Mwell, Mhigh.",
            "Under the assumption of local uniformity and homogeneity, a complete and irreducible set of invariants for the color illumination spectrum is given by R= (I_R ∂u I_G − I_G ∂u I_R)^2 + (I_G ∂u I_B − I_B ∂u I_G)^2 + (I_B ∂u I_R − I_R ∂u I_B)^2 + …, which captures features only related to illumination. Consequently, we assert that R functions as a light effects detector.",
            "To extract the light effects, ReLU and normalization functions are first applied to filter out minor disturbances. Then, by filtering invariant R with the well-lit mask Mwell, we obtain the light effects from the well-lit regions: Mlight = Norm(ReLU(R)) ⊙ Mwell, while the well-lit mask is refined: Mwell = Mwell ⊙ (1 − Mlight). With the initial disentanglement in Sec. 3.1, we obtain the final disentanglement: Mdark, Mhigh, Mwell, Mlight. All the masks are stacked to obtain the disentanglement map."
        ],
        "final_answer": "The process begins by computing a physics prior — the per‐pixel illuminance map L via the maximum RGB channel. K-means clustering on L produces three coarse region masks (darkness, well-lit, and high-light). Next, a color-invariant response R derived from the photometric model detects purely illumination‐driven light effects. ReLU and normalization filter R, and this result is masked by the well-lit region to isolate a light-effects mask. The well-lit mask is then refined by removing those light-effect pixels. Finally, the four binary masks (darkness, high-light, refined well-lit, and light effects) are stacked to form the complete disentangled degradation map.",
        "relevant_elements": [
            "Physics Prior",
            "Disentanglement Process"
        ],
        "id": 105,
        "masked_question": "How does the [mask1] transform the physics prior map into distinct degradation region clusters?",
        "masked_number": 1,
        "masked_elements": [
            "Disentanglement Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.14504v1_figure_1.png",
        "paperid": "2411.14504v1",
        "paper_path": "./papers/2411.14504v1.json",
        "figure_id": "2411.14504v1_figure_1.png",
        "caption": "Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) Vanilla structure regularization and the corresponding results. (c) The proposed disentangled regularization and the corresponding results.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step breakdown of what the red‐boxed “Disentanglement Process” does to turn the raw physics‐prior (the illuminance map) into four clean degradation masks:\n\n1.  Estimate an illuminance map L by taking, at each pixel, the maximum of its R, G, B values.  \n2.  Run k-means (k = 3) on L to get three preliminary clusters:  \n    – Darkness  \n    – Well-lit (diffuse reflectance under normal light)  \n    – High-light (saturated, over-exposed spots)  \n3.  To further split “well-lit” into (a) true diffuse regions and (b) light-effects (flares, glows, speculars), they invoke a small-scale photometric model (Kubelka–Munk) and derive a color‐invariant φ that responds only to illumination changes.  \n4.  Concretely, they:  \n   •  Compute φ via second- and third-order spatial derivatives of the RGB channels (using Gaussian filters + derivative kernels).  \n   •  Apply ReLU + normalization to φ to suppress noise.  \n   •  Mask φ by the original “well-lit” cluster so that only those pixels are tested for light‐effects.  \n5.  Any pixel in that masked invariant map that passes a threshold becomes the “light-effects” mask; the remainder stays in “well-lit.”  \n6.  The final four masks—darkness, high-light, well-lit, light-effects—are then stacked into the full “disentanglement map.”  \n\nIn this way, the module transforms a single physics‐prior illuminance map into four semantically meaningful degradation clusters."
    },
    {
        "question": "How is Neg. from the relative degradation selected to optimize push distances in contrastive learning?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Then, within each degradation region, the anchors A are randomly selected from the patches of generated daytime images G(x_n). The positive examples P are sampled from the same locations with the anchors in the source nighttime images x_n, and the negative examples N are randomly selected from other locations of x_n.",
            "Subsequently, the sample set with the same degradation type will be assigned weights and the contrastive loss will be computed in the following steps.",
            "Within each degradation matrix, a soft reweighting strategy is implemented. Specifically, for each anchor-negative pair, we apply optimal transport to yield an optimal transport plan, serving as a reweighting matrix associated with the disentangled results."
        ],
        "final_answer": "Negative examples for each anchor are drawn by randomly sampling patches from other spatial locations that share the same disentangled degradation label (e.g. well-lit, high-light, darkness, or light-effects). These ‘‘relative’’ negatives are then reweighted via an optimal‐transport–based scheme within each degradation block so that hard negatives receive higher attention when computing the push distances in the contrastive loss.",
        "relevant_elements": [
            "Neg. from the relative degradation",
            "push"
        ],
        "id": 106,
        "masked_question": "How is [mask1] selected to optimize push distances in contrastive learning?",
        "masked_number": 1,
        "masked_elements": [
            "Neg. from the relative degradation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.14504v1_figure_1.png",
        "paperid": "2411.14504v1",
        "paper_path": "./papers/2411.14504v1.json",
        "figure_id": "2411.14504v1_figure_1.png",
        "caption": "Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) Vanilla structure regularization and the corresponding results. (c) The proposed disentangled regularization and the corresponding results.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed patches in (c) are simply the “negatives from the relative degradation.”  In other words, once you have split the image into its four degradation regions (darkness, well-lit, high-light and light-effects) via the physics‐based disentangler, you only draw your negative samples from the same region that your anchor came from (instead of from all over the image as in vanilla NCE).  Finally, you solve an optimal‐transport problem within that block to re–weight those negatives so that the hardest (most similar) ones get the biggest “push.”  This both restricts your negatives to the correct degradation and automatically focuses your contrastive loss on the truly hard negatives, thus maximizing the push–apart distances."
    },
    {
        "question": "How can physics prior disentanglement leverage photometric color invariance techniques?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "To disentangle light effects from well-lit regions, we demonstrate both theoretically and empirically that a color-invariance property can effectively isolate light effects from well-lit regions.",
            "Under the assumption of local uniformity and homogeneity, a complete and irreducible set of invariants for the color illumination spectrum is given by: … Corollary 1 demonstrates that the invariant f^c captures the features only related to illumination. Consequently, we assert that f^c functions as a light effects detector because light effects are mainly related to the illumination. It allows us to design the illumination disentanglement module based on this physical prior.",
            "To extract the light effects, ReLU and normalization functions are first applied to filter out minor disturbances. Then, by filtering invariant f^c with the well-lit mask M_w, we obtain the light effects from the well-lit regions."
        ],
        "final_answer": "Physics-prior disentanglement uses a photometric model (from Kubelka–Munk theory) to derive a color-invariant response f^c that depends only on illumination, not on material reflectance. By computing this invariant over the image and then applying ReLU, normalization, and masking with the well-lit region map, the method isolates and detects purely illumination-driven ‘light effects,’ thereby disentangling them from other well-lit regions.",
        "relevant_elements": [
            "Physics Prior",
            "Disentangle"
        ],
        "id": 107,
        "masked_question": "How can [mask1] [mask2] leverage photometric color invariance techniques?",
        "masked_number": 2,
        "masked_elements": [
            "Physics Prior",
            "Disentangle"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.14504v1_figure_1.png",
        "paperid": "2411.14504v1",
        "paper_path": "./papers/2411.14504v1.json",
        "figure_id": "2411.14504v1_figure_1.png",
        "caption": "Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) Vanilla structure regularization and the corresponding results. (c) The proposed disentangled regularization and the corresponding results.",
        "qtype": "Literature_Background",
        "response": "Here’s a step‐by‐step of how the “Physics Prior → Disentanglement Process” pair makes use of photometric colour‐invariance:\n\n1.  **Physics Prior (Kubelka–Munk model).**  \n    –  We start by modelling the observed pixel colour as a combination of an illumination term   and a material reflectance term  (Eq. 1).  \n    –  Under the assumption of local homogeneity, Fresnel’s reflectance coefficient  switches between “reflectance‐dominated” (well-lit) and “illumination‐dominated” (light‐effects) regimes.  \n\n2.  **Initial disentanglement via illuminance clustering.**  \n    –  We compute a crude illuminance map  from the max-RGB channel and run K-means to carve out dark, well-lit and over-exposed (high-light) regions.  \n    –  At this point “light effects” still sit mixed in the well-lit cluster.  \n\n3.  **Photometric colour-invariant extraction.**  \n    –  From our Kubelka–Munk prior we derive (Corollary 1) a colour‐invariant  that—by construction—responds only to the illumination spectrum .  \n    –  Concretely, we form second‐ and third‐order combinations of the image’s Gaussian‐smoothed partial derivatives (Eqs. 5–8), giving us a single map  whose high values flag illumination‐dominated pixels.  \n\n4.  **Refining the disentanglement.**  \n    –  We threshold (ReLU + norm)  to suppress noise, then mask it by the well-lit region.  What remains is a precise “light effects” mask.  \n    –  Subtracting that from the original well-lit cluster yields a pure reflectance (true well-lit) mask.  \n\n5.  **Result: Four clean masks**  \n    –  Darkness, well-lit (reflectance), high-light (saturation), and light-effects (illumination).  \n\nIn short, the red-boxed “Physics Prior” provides the Kubelka–Munk reflectance/illumination model, and the blue-boxed “Disentanglement Process” uses a derived photometric colour‐invariant to pull out exactly those pixels whose appearance is driven purely by illumination, thereby cleanly separating light‐effects from genuine material reflectance."
    },
    {
        "question": "How does using Neg. from the relative degradation extend Neg. from the whole image sampling in contrastive learning?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "For unpaired image translation, contrastive learning has validated its effectiveness for the preservation of content. It targets to maximize the mutual information between patches in the same spatial location from the generated image and the source image as below: … The negative examples N represent patches with locations distinct from that of the anchor.",
            "Degradation-Aware Sampling. In this paper, N2D3 selects the anchor, positive, and negative patches under the guidance of the disentanglement results. Initially, based on the disentanglement mask obtained in the Sec. 3.2, we compute the patch count for different degradation types, denoting as C. Then, within each degradation region, the anchors are randomly selected from the patches of generated daytime images. The positive examples are sampled from the same locations with the anchors in the source nighttime images, and the negative examples are randomly selected from other locations of X."
        ],
        "final_answer": "Instead of drawing negatives uniformly from every other patch in the entire image, N2D3 first groups patches by their disentangled degradation type (e.g., darkness, well-lit, high-light, light effects) and then only samples negatives from within the same degradation group as the anchor. This ‘relative‐degradation’ negative sampling extends the vanilla whole‐image strategy by providing harder, more informative negatives that share the same local illumination/degradation characteristics, while discarding easy inter‐region negatives.",
        "relevant_elements": [
            "Neg. from the whole image",
            "Neg. from the relative degradation"
        ],
        "id": 108,
        "masked_question": "How does using [mask1] extend [mask2] in contrastive learning?",
        "masked_number": 2,
        "masked_elements": [
            "Neg. from the relative degradation",
            "Neg. from the whole image"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.14504v1_figure_1.png",
        "paperid": "2411.14504v1",
        "paper_path": "./papers/2411.14504v1.json",
        "figure_id": "2411.14504v1_figure_1.png",
        "caption": "Figure 1: Illustration of our motivation. (a) The disentanglement process leverages physical priors. (b) Vanilla structure regularization and the corresponding results. (c) The proposed disentangled regularization and the corresponding results.",
        "qtype": "Literature_Background",
        "response": "First, let’s be precise about what the two colored boxes represent in Fig. 1(b)/(c):  \n  • Blue box (“Neg. from the whole image”) is the vanilla NCE strategy—i.e. you draw your negative patches uniformly at random from any other location in the image.  \n  • Red box (“Neg. from the relative degradation”) is our disentangled strategy—i.e. you only draw your negative patches from other locations that share the *same* degradation label (darkness, well-lit, highlight or light-effects) as the anchor.  \n\nNow, **“How does using [mask1] extend [mask2] in contrastive learning?”**  \n\nChain of thought:  \n1. In standard (mask₂) contrastive learning, negatives come from anywhere in the image.  You get a huge but very heterogeneous pool of “easy” negatives (e.g. patches in totally different lighting or content).  \n2. In our disentangled version, we first segment the image into degradation categories (dark, well-lit, etc.).  When we sample negatives *only* from the same category as the anchor—i.e. from the red‐boxed set (mask₁)—we force the network to distinguish far subtler differences (two patches both in “darkness,” say, rather than “dark” vs “bright”).  \n3. In effect, we have “extended” the negative‐sampling scheme of mask₂ into a *degradation-aware* negative‐sampling scheme (mask₁).  This yields a block-diagonal similarity matrix (one block per degradation type) and pushes the network to learn structural consistency under each specific illumination degradation, rather than collapsing everything into a single “all-image” contrastive objective.  \n\nAnswer (in one sentence):  \nBy restricting negatives to come *only* from patches sharing the same disentangled degradation mask (mask₁), we turn the vanilla “whole‐image” negatives (mask₂) into a *degradation‐aware* negative set, so that the network must learn to tell apart much harder, within‐class negatives instead of easy, across‐class ones."
    },
    {
        "question": "How does trajectory clustering interact with policy retraining in explanation generation methodologies?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The embeddings are passed through the XMeans clustering algorithm introduced by Pelleg et al. (2000 ###reference_b18###). The implementation used by the authors is the one from Novikov (2019 ###reference_b16###). Using XMeans is an arbitrary choice and in Section 4.5 ###reference_### we will investigate other options.",
            "The cluster representations are embedded obtaining the representative embedding of given trajectories.",
            "The so-called complementary datasets are obtained. That is, for each cluster we create a different dataset where for each cluster j we retain all the data but those trajectories belonging to cluster j itself. We obtain then 10, 8, and 10 complementary datasets for the three environments respectively, and train for each complementary dataset new explanation policies and actions. In particular for Seaquest, Breakout and Q*Bert we use DiscreteSAC Christodoulou (2019 ###reference_b1###), whereas for HalfCheetah we employ SAC Haarnoja et al. (2018 ###reference_b8###).",
            "In the end, the decision made in a given state is attributed to a trajectory cluster."
        ],
        "final_answer": "After obtaining trajectory embeddings, the method applies XMeans to cluster them into semantically meaningful groups. For each cluster, it constructs a complementary dataset by removing the trajectories in that cluster, then retrains a separate explanation policy on each complementary dataset (using DiscreteSAC or SAC). By comparing each retrained policy against the original policy, the approach attributes changes in decisions back to the excluded cluster—thereby generating trajectory‐based explanations.",
        "relevant_elements": [
            "Trajectory Clustering",
            "Training Explanation Policies"
        ],
        "id": 110,
        "masked_question": "How does [mask1] interact with [mask2] in explanation generation methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Trajectory Clustering",
            "Training Explanation Policies"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.07200v1_figure_1.png",
        "paperid": "2411.07200v1",
        "paper_path": "./papers/2411.07200v1.json",
        "figure_id": "2411.07200v1_figure_1.png",
        "caption": "Figure 1:  Trajectory attribution process by Deshmukh et al. (2023)",
        "qtype": "Literature_Background",
        "response": "Here’s the high-level picture of how the red‐boxed step (“Trajectory Clustering”) feeds directly into the blue‐boxed step (“Training Explanation Policies”):\n\n1.  Trajectory Clustering (red box)  \n    – You begin with a pool of trajectory embeddings \\{t₁, t₂, …, tₙ\\}  \n    – An X-Means (or similar) algorithm groups them into m semantically coherent clusters C₁, C₂, …, Cₘ  \n\n2.  Forming Complementary Datasets  \n    – For each cluster Cⱼ, you create a “complementary” dataset by removing all trajectories in Cⱼ from the original data.  \n    – That gives you m+1 datasets in total: the full/original set (to recover π_orig) and one ablated set per cluster.  \n\n3.  Training Explanation Policies (blue box)  \n    – On the full set you train π_orig (the baseline policy).  \n    – On the dataset with C₁ omitted you train π₁, on the one with C₂ omitted you train π₂, …, up to πₘ.  \n    – Each πⱼ therefore “knows” everything except what was in cluster Cⱼ.  \n\n4.  Attribution by Counterfactual Comparison  \n    – At decision time you compare π_orig’s action to each πⱼ’s action.  \n    – If leaving out Cⱼ causes the policy to flip its recommendation, you infer that cluster Cⱼ was crucial to that decision.  \n\nIn short, the clustering step (mask 1) gives you the groups C₁…Cₘ that you then systematically hold out in the explanation‐policy training step (mask 2).  By retraining without each cluster, you turn each Cⱼ into a “counterfactual” experiment that isolates its influence on the agent’s choices."
    },
    {
        "question": "How does training explanation policies on complementary datasets validate trajectory cluster attribution methodology?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The so-called complementary datasets are obtained. That is, for each cluster we create a different dataset where for each cluster j we retain all the data but those trajectories belonging to cluster j itself. We obtain then 10, 8, and 10 complementary datasets for the three environments respectively, and train for each complementary dataset new explanation policies and actions.",
            "In the end, the decision made in a given state is attributed to a trajectory cluster."
        ],
        "final_answer": "By training a separate explanation policy on each complementary dataset—that is, on all trajectories except those in one cluster—we can observe how omitting each cluster affects the policy’s recommended action. The cluster whose removal causes the policy to choose a different action for a given state is then identified as responsible for the original decision. This procedure directly tests and validates that trajectories grouped into each cluster indeed drive the agent’s behavior in those states, confirming the cluster attribution methodology.",
        "relevant_elements": [
            "Training Explanation Policies",
            "Trajectory Cluster Attribution"
        ],
        "id": 111,
        "masked_question": "How does [mask1] on complementary datasets validate [mask2] methodology?",
        "masked_number": 2,
        "masked_elements": [
            "Training Explanation Policies",
            "Trajectory Cluster Attribution"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.07200v1_figure_1.png",
        "paperid": "2411.07200v1",
        "paper_path": "./papers/2411.07200v1.json",
        "figure_id": "2411.07200v1_figure_1.png",
        "caption": "Figure 1:  Trajectory attribution process by Deshmukh et al. (2023)",
        "qtype": "Experimental_Results",
        "response": "Here’s a step-by-step breakdown of how the two parts fit together:\n\n1. Mask₁ (“Training Explanation Policies on complementary datasets,” the red box)  \n   – For each cluster Cⱼ, remove all trajectories in that cluster from your dataset and re-train a new “explanation” policy πⱼ on the remaining data.  \n   – You thus end up with m new policies π₁,…,πₘ, each one blind to exactly one cluster.\n\n2. Mask₂ (“Trajectory Cluster Attribution,” the blue box)  \n   – At a query state s, you compare each πⱼ’s recommended action to the original policy π₀’s action.  \n   – Whichever πⱼ deviates the most (and whose complementary‐cluster embedding is closest to the original) is deemed to have “removed” the cluster that mattered most at s.\n\n3. How the first validates the second  \n   – By systematically taking one cluster out of the data and re-training (that’s Mask₁), you guarantee that any change in the new policy’s action can only be caused by the absence of that specific cluster.  \n   – When you then see which “leave-one-out” policy πⱼ actually switches the action in state s (and does so with minimal embedding drift), you have direct, causal evidence that cluster Cⱼ was responsible for π₀’s original decision—precisely the attribution procedure laid out in Mask₂.  \n   – In short, Mask₁ creates controlled experiments (one cluster missing at a time), and Mask₂ reads off those experiments to attribute responsibility back to individual clusters."
    },
    {
        "question": "How does the RL sequence encoder affect the semantic meaning captured by trajectory clusters?",
        "relevant_section_ids": [
            "4.5"
        ],
        "relevant_context": [
            "In Grid-World Environment the LSTM-based Seq2Seq encoding used by the authors has proven to be efficient. However, in this section we set out to experiment with different encoding techniques.",
            "We employed two kinds of pre-trained encoders: Trajectory Transformer … and BERT base model …",
            "Results: Experiments are performed over 250 trajectories. We defer the table of results … as we obtain no notable increase in performance across all metrics. Additionally, an inspection of high-level behaviors of clusters, as in section 5, highlights similar results."
        ],
        "final_answer": "Changing the RL sequence encoder (from LSTM-Seq2Seq to Trajectory Transformer or BERT) did not alter the semantic meaning captured by the trajectory clusters: high-level behaviors remained essentially the same and no cluster-level performance gains were observed.",
        "relevant_elements": [
            "RL Sequence Encoder",
            "Trajectory Clustering"
        ],
        "id": 112,
        "masked_question": "How does the [mask1] affect the semantic meaning captured by trajectory clusters?",
        "masked_number": 1,
        "masked_elements": [
            "RL Sequence Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.07200v1_figure_1.png",
        "paperid": "2411.07200v1",
        "paper_path": "./papers/2411.07200v1.json",
        "figure_id": "2411.07200v1_figure_1.png",
        "caption": "Figure 1:  Trajectory attribution process by Deshmukh et al. (2023)",
        "qtype": "Experimental_Results",
        "response": "Here the mystery “​[mask1]​” is nothing more exotic than the little red‐boxed module in step (a) of Fig. 1—namely the RL sequence encoder that turns a raw state–action trajectory into a fixed‐length embedding.  Its role is absolutely central to what “semantic meaning” your clustering will ultimately discover:\n\n1.  Trajectory ⇒ Encoder ⇒ Embedding  \n    •  The encoder ingests the ordered (s₀,a₀),(s₁,a₁),… sequence and tries to capture all the patterns that matter—state transitions, reward spikes, loops, detours, goal strategies, failure modes, etc.  \n2.  Embeddings ⇒ Clustering ⇒ Semantic clusters  \n    •  Whatever structure survives that encoding step is exactly what your clustering algorithm will latch onto.  If the encoder learns to highlight “goals reached” vs. “fell in lava” vs. “mid‐grid wander,” then the clusters will cleanly split along those lines.  If it misses or washes out certain temporal or reward cues, you end up with blobs of trajectories that no longer correspond to a human‐interpretable high‐level behavior.  \n3.  Changing the encoder changes the clusters  \n    •  In the paper you even see this: swapping in a Trajectory Transformer or a BERT‐style encoder didn’t radically improve metrics or alter the three archetypal behaviors they originally pulled out.  That’s your proof that it is the encoder’s capacity to preserve the right signals that governs whether XMeans or DBSCAN will find “meaningful” groups or just arbitrary lumps.  \n\nBottom line: the sequence encoder (​[mask1]​) is what defines the embedding space.  It therefore directly shapes what semantics survive into your clusters.  A more powerful, temporally aware encoder → richer, more interpretable trajectory clusters; a weak or overly lossy encoder → clusters that don’t align with any real high-level behavior."
    },
    {
        "question": "How does Spiking Attenuated Spatiotemporal Attention (TASA) leverage past spike correlations to enrich attention representation?",
        "relevant_section_ids": [
            "3.1",
            "3.1.1"
        ],
        "relevant_context": [
            "Notably, in DS2TA, we extend the attention from \"spatial-only\" to \"spatiotemporal,\" as illustrated in ➇ of Figure 1, where not only the spiking activities of these N neurons at time t but also those occurring before t are attended. This new mechanism allows DS2TA to attend to dependencies taking place in both time and space, and provides a means for fully exploring the spatiotemporal computing power of spiking neurons under the context of transformer models, as shown in Figure 2.",
            "Specifically, the input to the query/key/value neuron at location i in block l is based upon the firing activations of N output neurons from the prior (l–1)-th block that fall under a given Temporal Attention Window (TAW) τ:  s^{(l-1)}_{n,t0}, where w(τ) is the temporally-attenuated synaptic weight specifying the efficacy of a spike evoked by the n-th output neuron of block (l–1) t0 time-steps before on the neuron at location i in block l."
        ],
        "final_answer": "TASA incorporates a Temporal Attention Window so that, when forming Q, K and V, it not only multiplies by the current spike outputs but also aggregates spikes from the past τ time steps.  Each past spike is weighted by a learnable, temporally‐attenuated synaptic weight w(τ), enabling the attention mechanism to capture correlations over both space and time and thereby enrich the representation with spatiotemporal context.",
        "relevant_elements": [
            "Spiking Attenuated Spatiotemporal Attention (TASA)"
        ],
        "id": 113,
        "masked_question": "How does [mask1] leverage past spike correlations to enrich attention representation?",
        "masked_number": 1,
        "masked_elements": [
            "Spiking Attenuated Spatiotemporal Attention (TASA)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.15375v1_figure_1.png",
        "paperid": "2409.15375v1",
        "paper_path": "./papers/2409.15375v1.json",
        "figure_id": "2409.15375v1_figure_1.png",
        "caption": "Figure 1: The overview of denoising spiking transformer with intrinsic plasticity and spatiotemporal attention: DS2TA.",
        "qtype": "Experimental_Results",
        "response": "Here, “​[mask1]” is the little two‐stage module in the red box of the figure that the paper calls Spiking Attenuated Spatio­temporal Attention (TASA).  In the original spiking ViT you only ever multiply the D-dimensional spike vector at time t by your Q/K/V weight matrices – that is purely “spatial” attention.  What TASA does is:\n\n 1. It takes not just the D-dimensional spike output at the current time t, but it gathers together the spikes from a short history of T past time‐steps into one big N×(D·T) matrix.  \n 2. It applies the same Q/K/V projections to that flattened N×(D·T) tensor, but with each time‐lagged slice weighted by a learnable decay factor α^τ (implemented as cheap power‐of‐two shifts).  \n\nBecause you are multiplying in all of the past spikes—each one attenuated by how long ago it fired—the resulting attention scores now reflect correlations not just across space at time t, but also across time.  In other words, by stacking the last T spike vectors, applying a decaying temporal kernel, and then projecting, the model automatically “attends” to spatio­temporal patterns in the spike train rather than just a single snapshot."
    },
    {
        "question": "How does Nonlinear Spiking Attention Denoiser (NSAD) leverage hashmap-based mappings for efficient attention denoising?",
        "relevant_section_ids": [
            "3.2.1",
            "3.2.2"
        ],
        "relevant_context": [
            "Recognizing the central role of spiking attention maps, we propose a learnable hashmap-based Nonlinear Spiking Attention Denoiser (NSAD) to improve the overall transformer performance. NSAD serves the dual-purpose of denoising a given computed attention map, and equally importantly, introducing efficient element-wise nonlinear transformation to enhance expressive power.",
            "The proposed nonlinear spiking attention denoiser (NSAD) offers an efficient solution to addressing the above issues via element-wise hashmap-based nonlinear transformation without non-local memory access and computation, as illustrated in Figure 4. Each head in a transformer with h heads may have unique focuses and parameter distribution. As such, we establish a small hashmap H_j with M entries dedicated to each head j. Each entry in H_j is indexed (addressed) by a specific integer value falling within the range of possible attention values of S and Ŝ, i.e., H_j[v] specifies the integer value to which all entries with value v in the attention map associated with head j are transformed to.",
            "Since NSAD produces nonlinear transformed denoised maps using simple integer-based lookups of small hashmaps, it is computationally efficient and hardware-friendly. For a block of 12-head attention, only M integer values need to be stored in the hashmaps while there are O(h·N²) block-level weight parameters. The complexity of computing a denoised attention map is O(M) per head, which can also be easily parallelized on hardware."
        ],
        "final_answer": "NSAD builds a small lookup table (hashmap) per attention head, where each possible discrete attention value is mapped to a denoised output via a simple integer-based lookup. During inference, every entry in the raw spike-based attention map is replaced by its corresponding hashmap entry, enabling element-wise nonlinear denoising without expensive exponentials or global memory access. This lookup-based scheme requires only M stored integers per head and runs in O(M) time per head, making it both computationally and hardware efficient.",
        "relevant_elements": [
            "Nonlinear Spiking Attention Denoiser (NSAD)"
        ],
        "id": 114,
        "masked_question": "How does [mask1] leverage hashmap-based mappings for efficient attention denoising?",
        "masked_number": 1,
        "masked_elements": [
            "Nonlinear Spiking Attention Denoiser (NSAD)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.15375v1_figure_1.png",
        "paperid": "2409.15375v1",
        "paper_path": "./papers/2409.15375v1.json",
        "figure_id": "2409.15375v1_figure_1.png",
        "caption": "Figure 1: The overview of denoising spiking transformer with intrinsic plasticity and spatiotemporal attention: DS2TA.",
        "qtype": "Experimental_Results",
        "response": "Here is a step-by-step reasoning of how the red-boxed module – the Nonlinear Spiking Attention Denoiser (NSAD) – uses tiny per‐head hashmaps to do ultra-cheap, element-wise denoising of a spike-based attention map:\n\n1.  Image-Text Alignment  \n    - In the diagram, the red box encloses the “Nonlinear Spiking Attention Denoiser (NSAD)” block.  \n    - Textually, §3.2 explains that after Q·Kᵀ produces a raw, noisy attention map of small integer counts, NSAD must (a) suppress spurious activations and (b) inject nonlinearity without using global softmax or exponentials.\n\n2.  What NSAD Stores  \n    - For each transformer head h, we allocate a tiny lookup table (a hashmap) H⁽ʰ⁾ of size M (M is just the number of distinct integer values the raw attention can take, e.g. 0,1,…,7).  \n    - Entry H⁽ʰ⁾[v] tells us: “Whenever the raw attention entry equals v, replace it with H⁽ʰ⁾[v].”\n\n3.  How the Lookups Are Populated  \n    - During training each H⁽ʰ⁾ is tied to a continuous, 1-D nonlinear function f⁽ʰ⁾(·) (a learnable combination of linear, quadratic and sigmoid shapes).  \n    - For each possible integer v we compute f⁽ʰ⁾(v), then round it to the nearest integer to fill H⁽ʰ⁾[v].  \n    - Gradients flow back through f⁽ʰ⁾’s parameters, so the hash-table entries adapt to optimally denoise.\n\n4.  Runtime Denosing via Hash Lookup  \n    - Given an N×N raw integer attention map A generated by Q·Kᵀ, we produce the denoised map Â simply by  \n         for each position (i,j):  Âᵢⱼ = H⁽ʰ⁾[ Aᵢⱼ ]  \n      – no exponentials, no divisions, no row or column sums.  \n    - Complexity per head: O(N²) lookups, each O(1), fully parallelizable.  \n    - Compare that to O(N²·N) or O(N³) global softmax-style costs in a vanilla transformer block.\n\n5.  Why This Is Efficient and Expressive  \n    - Hardware-friendly integer table-lookup (no non-local memory traffic, no expensive fp-ops).  \n    - Per-head customization lets each head learn its own denoising curve.  \n    - Simple element-wise mapping still injects rich nonlinearity (amplify middling scores, clamp out tiny spikes, etc.).\n\nIn short, NSAD turns each head’s noisy, spike-count attention map into a clean, nonlinear version by indexing into a tiny learned hash table – an O(1) per-entry operation – rather than doing costly global softmaxes or exponentials."
    },
    {
        "question": "What are potential computational bottlenecks of Spiking Attenuated Spatiotemporal Attention under expanded temporal windows?",
        "relevant_section_ids": [
            "3.1.1",
            "3.1.2"
        ],
        "relevant_context": [
            "The spiking spatio-temporal attention is confined within a Temporal Attention Window (TAW) to limit computational complexity.",
            "The spatiotemporal attention in Eq. 1 involves T temporally-dependent weights W_{ij,τ} for a pair of presynaptic and postsynaptic neurons. We introduce a learnable scheme, called attenuated temporal weight replica, to reduce the number of temporally-dependent weights by a factor of T."
        ],
        "final_answer": "If the temporal window T grows large, the spiking attenuated spatiotemporal attention must gather and multiply spikes from all preceding T time-steps for every neuron, driving up compute cost linearly with T. Likewise, without the temporal weight-replica scheme, one would need to store and update T distinct sets of synaptic weights, ballooning parameter-memory requirements by a factor of T. Both the per-neuron summation over T past frames and the T× increase in weight matrices thus become the primary computational bottlenecks as the temporal window expands.",
        "relevant_elements": [
            "Spiking Attenuated Spatiotemporal Attention (TASA)"
        ],
        "id": 115,
        "masked_question": "What are potential computational bottlenecks of [mask1] under expanded temporal windows?",
        "masked_number": 1,
        "masked_elements": [
            "Spiking Attenuated Spatiotemporal Attention (TASA)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.15375v1_figure_1.png",
        "paperid": "2409.15375v1",
        "paper_path": "./papers/2409.15375v1.json",
        "figure_id": "2409.15375v1_figure_1.png",
        "caption": "Figure 1: The overview of denoising spiking transformer with intrinsic plasticity and spatiotemporal attention: DS2TA.",
        "qtype": "Others",
        "response": "First, let us identify exactly what “[mask1]” is.  If you look at the red‐boxed region in the encoder block (middle of the diagram), it contains three back‐to‐back copies of the “Spiking Attenuated Spatiotemporal Attention (TASA)” sub‐module (one each for Q, K, and V), each followed by BatchNorm and a LIF layer.  In other words, “[mask1]” ≃ the entire spatiotemporal attention front‐end that builds Q, K, V by attending over the last T timesteps of spike activations.\n\nNow:  if we expand the temporal attention window (i.e. increase T), what becomes expensive?  A step‐by‐step reasoning leads to two primary bottlenecks:\n\n 1.  **Matrix–vector (or matrix–matrix) multiplies that blow up in size with T.**  \n     –  The core of TASA is a weighted sum of the form  \n        S_ext ∈ ℝ^(N×(D·T))  times  W ∈ ℝ^((D·T)×D)  →  I ∈ ℝ^(N×D).  \n     –  As T grows, the “extended” firing tensor S_ext simply concatenates more history, so your GEMM cost grows O(N·D²·T).  \n     –  In practice you do this three times (for Q, K and V), so the compute cost is 3× heavier than spatial‐only attention, and linear in T.\n\n 2.  **Memory footprint and bandwidth to buffer and fetch T-step histories (plus attenuated weights).**  \n     –  You must store the last T spikes of each of the D neurons from the previous block.  That is O(N·D·T) state.  \n     –  You must also hold or generate (via cheap shifts) the decayed‐weight replicas for each temporal offset in [1…T], which adds O(D²·T) parameters or shift calculations.  \n     –  Every time you run TASA you read these T slices of history and the T decayed weight‐replicas—so your memory traffic is also O(N·D·T).\n\nAll in all, although the “attenuated‐replica” trick avoids having a full D×D×T tensor of independent weights, the *effective* compute and memory work in the TASA front‐end still scale linearly with the window size T.  Once you push T up (longer temporal context), the three large GEMMs and the ever‐bigger spike history buffers become the dominant computational bottlenecks."
    },
    {
        "question": "What alternative mechanism could replace learnable nonlinear denoise hashmaps in NSAD to improve efficiency?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Secondly, it has been shown that applying row or column-based nonlinear softmax operations to attention maps improves performance in ANN-based transformers. However, softmax induces exponential operations and non-local memory access and data summations, which are costly and not hardware-friendly (Dao et al., 2022 ###reference_b4###)."
        ],
        "final_answer": "Row- or column-based nonlinear softmax operations on the attention maps.",
        "relevant_elements": [
            "Nonlinear Spiking Attention Denoiser (NSAD)"
        ],
        "id": 116,
        "masked_question": "What alternative mechanism could replace [mask1] in [mask2] to improve efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "learnable nonlinear denoise hashmaps",
            "Nonlinear Spiking Attention Denoiser (NSAD)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.15375v1_figure_1.png",
        "paperid": "2409.15375v1",
        "paper_path": "./papers/2409.15375v1.json",
        "figure_id": "2409.15375v1_figure_1.png",
        "caption": "Figure 1: The overview of denoising spiking transformer with intrinsic plasticity and spatiotemporal attention: DS2TA.",
        "qtype": "Others",
        "response": "Here’s the intuition we used to arrive at the answer.  In the NSAD module (the blue‐boxed region) we are currently using a tiny per–head “hash map” (that red box) to do an arbitrary nonlinear re-mapping of each integer attention score.  That lookup table gives us a lot of freedom but at the cost of storing and indexing those tables in hardware.\n\nA very common hardware–friendly alternative is simply to replace the explicit lookup table by a small, piecewise–linear (PWL) or comparator-based function.  In other words you approximate your nonlinear denoiser by a few linear segments (or even by a single “keep top-K / zero out the rest” comparator), all of which can be implemented with just shifts, adds and comparators—no random memory lookups required.\n\nConcretely:\n  1.  Partition the integer attention domain into M intervals (e.g. [0,3), [3,7), [7,12), …).  \n  2.  On the fly, compare the raw attention score to the interval boundaries (cheap comparators)  \n  3.  For each interval apply a simple linear formula (y=ax+b with a and b power-of-two or small integers so it reduces to shifts and adds).  \n  4.  That single pass piecewise-linear mapping replaces the red-boxed hash map.\n\nThis PWL approximation:\n  •  Eliminates the need for per–head lookup tables  \n  •  Cuts memory and indexing overhead  \n  •  Still gives you a non-trivial, learnable nonlinearity (you learn the segment boundaries and the slope/intercept in each)  \n  •  Is one of the most common “softmax/lookup‐table” replacements in ultra-low-power or FPGA implementations."
    },
    {
        "question": "What alternative anchor video generation approaches could improve temporal consistency beyond image-based view synthesis?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Our overall method is agnostic to the specific technique used to generate the anchor frames in the first stage, and in this work we explore two different techniques: point-cloud sequence rendering, and multi-view per-frame image diffusion.",
            "Point Cloud Sequence Rendering. We begin by lifting the pixels from the input image plane into a 3D point cloud representation. For each frame of the source video  ,  , we independently estimate its depth map  using an off-the-shelf monocular depth estimator [6]. By combining the image with its depth map, the point cloud  can be initialized as: ... Next, we take as input the camera motion as a pre-defined trajectory of extrinsic matrices  , where each includes a rotation matrix and a translation matrix representing the camera’s pose (position and orientation), which are used to rotate and translate the point cloud in the camera’s coordinates. We then project the point cloud of each frame back onto the anchored camera plane using the function  to obtain a rendered image with perspective change:  .",
            "Multiview Image Diffusion for Each Frame. When a camera trajectory involves significant rotation and viewpoint changes, point cloud rendering usually fails [102]. To address this, we employ a multiview diffusion model [24]. This approach leverages the fact that multiview image datasets are generally easier to obtain compared to multiview video datasets. Specifically, as shown in Fig. 3, for each frame  of the source video, which represents the condition view, along with its corresponding camera parameters  , the model learns to estimate the distribution of the target image  where  is the target camera parameters which are also provided as input."
        ],
        "final_answer": "Rather than synthesizing each frame independently, you can lift video frames into a 3D point cloud and render the entire sequence under the new camera trajectory (“point-cloud sequence rendering”). This 3D-based approach enforces geometric and temporal coherence across frames and thus yields much more stable, temporally consistent anchor videos than per-frame image diffusion alone.",
        "relevant_elements": [
            "Image-based view synthesis",
            "Anchor video generation"
        ],
        "id": 117,
        "masked_question": "What alternative [mask1] approaches could improve temporal consistency beyond [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Anchor video generation",
            "Image-based view synthesis"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05003v1_figure_2.png",
        "paperid": "2411.05003v1",
        "paper_path": "./papers/2411.05003v1.json",
        "figure_id": "2411.05003v1_figure_2.png",
        "caption": "Figure 2: ReCapture consists, at setup time, of (a) Anchor video generation (b) Masked video fine-tuning using spatial and temporal LoRAs. To generate the clean output video with the new camera trajectory we simply perform inference of the video model.",
        "qtype": "Others",
        "response": "The key observation is that the brittleness in the “image-based view synthesis” block (the blue box) comes almost entirely from the fact that you treat each frame in isolation.  If you instead generate your anchor video with a method that inherently reasons about time, you will get much tighter temporal coherence right out of the gate.  For example, in place of per-frame diffusion or naïve point-cloud reprojection you could try:\n\n  •  A 4D neural‐radiance‐field (video-NeRF) approach (e.g. D-NeRF, HyperNeRF, Nerfies, etc.) that fits a single spatio-temporal volume over all frames and then renders novel views in one shot.  Because the geometry and appearance are learned as one continuous volume, the rendered frames tend to be almost flicker-free.  \n  •  A video-to-video diffusion or transformer model that takes in a short window of source frames (plus the new camera pose) and hallucinates all of those output frames jointly.  By working on a clip rather than one image at a time, these methods learn an explicit temporal prior and will naturally avoid per-frame jitter.  \n  •  An optical-flow plus inpainting pipeline that warps multiple past (or future) frames into the novel view and then fills holes with a small CNN or diffusion step—again, because you’re re-using pixels from several neighboring frames you get built-in smoothness.  \n\nAny of these “time-aware” view-synthesis schemes would serve as a superior anchor-video generator to the purely per-frame diffusion shown in the blue box, and would greatly reduce flicker before you even get to the LoRA fine-tuning stage."
    },
    {
        "question": "What motivates decoupling spatial context via Context-Aware Spatial LoRA and motion via Temporal-Motion LoRA?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The anchor video from the first stage may exhibit significant artifacts, such as revealed occlusions due to camera movement and temporal inconsistencies such as flickering.",
            "To address these issues, we propose a masked video fine-tuning strategy using temporal motion LoRAs.",
            "Although the video diffusion model with masked fine-tuning automatically fills the invalid regions of the anchor video, the filling may not be consistent with the original context or appearance, and might appear pixelated, as shown in Fig. 8 Line 2.",
            "We propose enhancing the spatial attention layers of the video diffusion model by incorporating a spatial LoRA, which is fine-tuned on the frames of the source video."
        ],
        "final_answer": "Because the noisy anchor video contains two distinct types of errors—temporal artifacts and inconsistencies (e.g., flickering and occlusion artifacts) and spatial/contextual artifacts (e.g., inconsistent appearance and pixelation)—the authors decouple the problem. Temporal-Motion LoRAs use a masked fine-tuning loss to learn correct motion patterns and enforce temporal consistency, while Context-Aware Spatial LoRA is trained on clean source frames to capture the original appearance and background context, ensuring that filled-in regions blend seamlessly with the rest of the video.",
        "relevant_elements": [
            "Context-Aware Spatial LoRA",
            "Temporal-Motion LoRA"
        ],
        "id": 119,
        "masked_question": "What motivates decoupling spatial context via [mask1] and motion via [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Context-Aware Spatial LoRA",
            "Temporal-Motion LoRA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05003v1_figure_2.png",
        "paperid": "2411.05003v1",
        "paper_path": "./papers/2411.05003v1.json",
        "figure_id": "2411.05003v1_figure_2.png",
        "caption": "Figure 2: ReCapture consists, at setup time, of (a) Anchor video generation (b) Masked video fine-tuning using spatial and temporal LoRAs. To generate the clean output video with the new camera trajectory we simply perform inference of the video model.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1.  In the fine‐tuning stage, the model must simultaneously  \n    – restore missing or corrupted pixels so that their appearance matches the rest of the scene,  \n    – and re-establish a smooth, flicker-free temporal flow that follows the new camera path.  \n\n2.  If you try to force a single set of LoRA updates to handle both “what things look like” (spatial structure, textures, colors) and “how things move” (frame-to-frame dynamics), you either get spatial artifacts (washed-out or mismatched fill-ins) or temporal artifacts (flicker, unnatural motion) because these two objectives compete.  \n\n3.  The red-boxed LoRA ([mask1] = context-aware spatial LoRA) only touches the spatial attention and convolution layers, is trained on *clean* source frames (no temporal coupling), and so it learns to faithfully reproduce the original scene’s appearance wherever you need to fill in holes.  \n\n4.  The blue-boxed LoRA ([mask2] = temporal-motion LoRA) only modifies the temporal transformer layers and is trained on the *noisy anchor video* with a masked loss (ignoring invalid pixels). This forces it to pick up the correct motion patterns and enforce consistency over time, without trying to relearn textures or colors.  \n\n5.  By decoupling them, each LoRA can specialize—one “knows” how the scene should look, the other “knows” how it should move—so that at inference the model can both fill in missing content seamlessly and keep it temporally coherent along the new camera trajectory."
    },
    {
        "question": "What rationale underlies image-based view synthesis in anchor video generation?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "Given the challenge of obtaining paired videos in the wild with varying camera movements, it is hard to solve this problem with a video-to-video pipeline in an end-to-end manner and we separate it into two steps instead.",
            "In more detail, the first stage consists of image-based view synthesis, in which we independently transform each input video frame to produce noisy anchor frames with the new camera pose, along with their validity masks. These frames are typically incomplete; they have artifacts such as missing information from revealed occlusions, and have structural deformations and temporal inconsistencies such as flickering."
        ],
        "final_answer": "Because collecting paired multi-view video data for end-to-end re-angling is impractical in the wild, the authors first approximate each new viewpoint by independently warping or synthesizing each frame to create a noisy “anchor video.” This image-based view synthesis step gives a rough, per-frame estimation of the desired camera motion (with masks for invalid regions) that can then be refined, without requiring full 4D reconstruction or paired training data.",
        "relevant_elements": [
            "Anchor Video Generation",
            "Image-Based View Synthesis"
        ],
        "id": 120,
        "masked_question": "What rationale underlies [mask1] in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Image-Based View Synthesis",
            "Anchor Video Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05003v1_figure_2.png",
        "paperid": "2411.05003v1",
        "paper_path": "./papers/2411.05003v1.json",
        "figure_id": "2411.05003v1_figure_2.png",
        "caption": "Figure 2: ReCapture consists, at setup time, of (a) Anchor video generation (b) Masked video fine-tuning using spatial and temporal LoRAs. To generate the clean output video with the new camera trajectory we simply perform inference of the video model.",
        "qtype": "Design_Rationale",
        "response": "In our pipeline there are two very different ways of getting an “anchor” video and a corresponding per‐frame validity mask.  The first (the blue‐boxed path) is just lifting each frame into a point‐cloud, re-projecting under the new camera pose, and marking any pixel that fell outside the original image as invalid (mask=0).  The second (the red‐boxed path, which we call “image-based view synthesis”) is exactly the same idea except that when the camera motion becomes too large – big pans, tilts or orbits – the simple point-cloud projection leaves holes or gross distortions.  In that case we fall back to a per-frame multiview diffusion model (à la CAT3D) to hallucinate the missing pixels.  We then generate the same validity mask by checking which pixels really came from the original reprojection and which ones were “filled in.”  \n\nThe raison d’être of the red‐boxed, image-based view synthesis inside the blue‐boxed “Anchor Video and Masks” is therefore two‐fold:\n\n1.  To handle camera trajectories that point-cloud rendering alone cannot (large rotations, big viewpoint jumps), by using a multiview diffusion model to synthesize plausible novel‐view imagery.  \n2.  To produce a binary mask alongside it—exactly as in the point-cloud case—that tells our later training stage which pixels are truly “known” (and should count toward the loss) and which are hallucinated (and must be ignored)."
    },
    {
        "question": "What motivates integrating the localization module and DreamBooth for progressive 3D Gaussian editing?",
        "relevant_section_ids": [
            "3",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Section 3 (Our Method): “The overall framework of GSEditPro … consists of two main stages. Firstly, we design an attention-based localization module … to locate the editing region in the 3D space using the keywords in the text prompt … Secondly, building upon 3D Gaussians, we implement scene editing leveraging Score Distillation Sampling (SDS) loss with DreamBooth … By integrating optimization and progressive localization, our method achieves precise and detailed local editing.”",
            "Section 3.2 (Locating Gaussian Editing Regions via Cross-Attention): “We propose an attention-based localization module that utilizes the 2D probability maps generated by the cross-attention layers … After that we backproject the 2D masks into 3D space and mark the Gaussians that need editing, enabling precise localization of the editing region explicitly in Gaussians before training.”",
            "Section 3.3 (Optimizing Editing Regions using DreamBooth): “After locating the editing regions, we propose an optimization scheme for 3D Gaussian editing. … DreamBooth possesses sufficient generation ability to guide the training of 3D Gaussians. We utilize the SDS loss … During each training iteration, SDS serves as a 2D guidance to optimize Gaussian parameters iteratively. This process matches the rendering results with the text guidance of the editing, obtaining desired editing results after sufficient training.”"
        ],
        "final_answer": "The method integrates the localization module and DreamBooth because the localization module gives accurate, view-consistent masks of the target editing regions in the explicit 3D-Gaussian representation, while DreamBooth (via fine-tuned diffusion guidance and the SDS loss) provides strong, text-driven generative forces to progressively update only those localized Gaussians. Together, this combination yields precise, high-quality local edits that preserve unrelated areas and remain consistent across views.",
        "relevant_elements": [
            "localization module",
            "DreamBooth"
        ],
        "id": 121,
        "masked_question": "What motivates integrating the [mask1] and [mask2] for progressive 3D Gaussian editing?",
        "masked_number": 2,
        "masked_elements": [
            "localization module",
            "DreamBooth"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10033v1_figure_1.png",
        "paperid": "2411.10033v1",
        "paper_path": "./papers/2411.10033v1.json",
        "figure_id": "2411.10033v1_figure_1.png",
        "caption": "Figure 1: Method Overview. GSEditPro edits scenes represented by 3D Gaussian Splatting using text prompt only. The key process of our method contains three parts: 1) Locating Gaussian editing regions via cross-attention, which assigns semantic labels to each Gaussian and determines whether the gradients can be propagated between them (Section 3.2); 2) Optimizing editing regions using DreamBooth, which uses ℒS⁢D⁢Ssubscriptℒ𝑆𝐷𝑆\\mathcal{L}_{SDS}caligraphic_L start_POSTSUBSCRIPT italic_S italic_D italic_S end_POSTSUBSCRIPT as the guidance to optimize Gaussian parameters iteratively (Section 3.3); 3) Preserving details with pixel-level guidance, which creates a pseudo-GT image to guide 3D Gaussian rendering for more detailed results (Section 3.4).",
        "qtype": "Design_Rationale",
        "response": "Here is the step-by-step reasoning:\n\n1. Our goal is to edit only the parts of the 3D Gaussian scene that correspond to the user’s text prompt, leaving everything else untouched.  \n2. To know *where* to edit, we tap into the diffusion model’s cross-attention maps (“mask₁” in the red box).  Those maps tell us, in each rendered view, which pixels the model thinks belong to the word or object we’re asking it to change.  \n3. Once we have these 2D attention priors, we lift them back into 3D to tag exactly which Gaussians should be mutable and which should be frozen.  This ensures any updates stay local.  \n4. But *knowing* where to edit is only half the battle; we also need *how* to edit.  That’s where the SDS (Score Distillation Sampling) loss (“mask₂” in the blue box) comes in.  SDS gives us a differentiable, text‐driven objective that tells the Gaussians how to change their color, opacity, covariance, etc., so that the rendered edits match the prompt.  \n5. By chaining these two pieces—cross-attention for localization and SDS for optimization—we get a **progressive** editing pipeline: first lock onto the right Gaussians, then iteratively nudge only those Gaussians to satisfy the text, all the while keeping the rest of the scene stable.  \n\nIn short, integrating the attention maps (mask₁) with the SDS loss (mask₂) lets us do **precise, view-consistent, local edits** in 3D Gaussian Splatting, and to drive those edits directly from text."
    },
    {
        "question": "Why combine SDS loss with DreamBooth fine-tuning during the optimizing stage to guide Gaussian manipulation?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Section 3.2: “Concretely, we sample rendering output in various views using COLMAP cameras and fine-tune the Stable Diffusion using DreamBooth. DreamBooth is a method that fine-tunes the large-scale text-to-image (T2I) model around a specific target subject, denoted as ‘*’ or other symbols, to ensure its ability to generate images similar to the input data. The preservation loss of DreamBooth will encourage the diffusion model to treat this special class as the default generating style, which increases the accuracy of attention maps as well.”",
            "Section 3.3: “After training on our target dataset, DreamBooth possesses sufficient generation ability to guide the training of 3D Gaussians. We utilize the SDS loss proposed by DreamFusion as the guiding loss function. … This loss is then employed during the back-propagation process to guide the cloning and splitting of the Gaussians, as well as the changes in their parameters. … Therefore, during each training iteration, SDS serves as a 2D guidance to optimize Gaussian parameters iteratively. This process matches the rendering results with the text guidance of the editing, obtaining desired editing results after sufficient training.”"
        ],
        "final_answer": "By first fine-tuning the diffusion model with DreamBooth on the target subject, the model gains strong, subject-specific generative capability and stable attention maps. The SDS loss then leverages that specialized diffusion model as a guidance signal—during each optimization step it back-propagates gradients only through the labeled Gaussians—to drive their cloning, splitting, and parameter updates so that the rendered scene matches the text prompt faithfully.",
        "relevant_elements": [
            "SDS loss",
            "DreamBooth"
        ],
        "id": 122,
        "masked_question": "Why combine [mask1] with [mask2] during the optimizing stage to guide Gaussian manipulation?",
        "masked_number": 2,
        "masked_elements": [
            "SDS loss",
            "DreamBooth"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10033v1_figure_1.png",
        "paperid": "2411.10033v1",
        "paper_path": "./papers/2411.10033v1.json",
        "figure_id": "2411.10033v1_figure_1.png",
        "caption": "Figure 1: Method Overview. GSEditPro edits scenes represented by 3D Gaussian Splatting using text prompt only. The key process of our method contains three parts: 1) Locating Gaussian editing regions via cross-attention, which assigns semantic labels to each Gaussian and determines whether the gradients can be propagated between them (Section 3.2); 2) Optimizing editing regions using DreamBooth, which uses ℒS⁢D⁢Ssubscriptℒ𝑆𝐷𝑆\\mathcal{L}_{SDS}caligraphic_L start_POSTSUBSCRIPT italic_S italic_D italic_S end_POSTSUBSCRIPT as the guidance to optimize Gaussian parameters iteratively (Section 3.3); 3) Preserving details with pixel-level guidance, which creates a pseudo-GT image to guide 3D Gaussian rendering for more detailed results (Section 3.4).",
        "qtype": "Design_Rationale",
        "response": "Here’s what’s going on in that red‐and‐blue box in the “Optimizing” panel:\n\n  1. The red box is simply where we feed in our *editing prompt* (“a * man wearing a pair of sunglasses”).\n  2. The blue box is the **SDS** (Score‐Distillation‐Sampling) loss coming out of DreamBooth/Stable Diffusion once you condition it on that prompt.\n\nWhy do we hook them together?  In plain English:\n\n– On its own, a 3D Gaussian splatting scene has no notion of “sunglasses” or “man with glasses.”  \n– The diffusion model *does* know those concepts—but only in 2D pixel space.  \n– By rendering the current 3D-GS scene to an image, passing that image (noised) into the diffusion model **with** the editing prompt, and then measuring the difference (the SDS loss) between the model’s predicted noise and the real noise we added, we get a *gradient* that tells us exactly how to tweak every Gaussian in the 3D scene so that its rendered view will look more like “a man wearing a pair of sunglasses.”  \n\nThus, combining the prompt (red) with the SDS loss (blue) is the mechanism that turns a *text* instruction into *3D* parameter updates. All of the Gaussian positions, covariances, colors, opacities, etc., get nudged so that, after enough iterations, the model reliably “paints” in the glasses (or whatever object you asked for) across *all* views."
    },
    {
        "question": "How does the Localization Module integrate attention maps and segmentation masks to label 3D Gaussians for editing?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Therefore, we propose an attention-based localization module that utilizes the 2D probability maps generated by the cross-attention layers as masks for each view, determining which regions need editing in 2D views.",
            "These 2D maps are then processed as point prompts for the large-scale segmentation model Segment Anything Model(SAM) to obtain a more precise mask for the target region.",
            "After that, we backproject the 2D masks into 3D space and mark the Gaussians that need editing, enabling precise localization of the editing region explicitly in Gaussians before training.",
            "When editing existing objects in the scene, our module chooses points of the processed maps as point prompts for the SAM, with the top 5 points selected based on the highest attention map values as positive ones, while the negative point prompts are chosen based on the lowest 3 values. After that SAM will segment a precise mask of the target for each view.",
            "Masks are back-projected during the differentiable rendering process similar to GaussianEditor and we only allow gradients to propagate within the labeled Gaussians whose weights of back-projection bigger than the threshold. Finally, our method finishes locating Gaussian editing regions explicitly and assigns the Gaussians their binary labels in 3D."
        ],
        "final_answer": "The Localization Module first collects per-view 2D attention maps from the cross-attention layers of a fine-tuned T2I model and thresholds or clusters them to form rough masks. It then feeds these attention-derived point prompts into SAM to produce precise segmentation masks in each view. These 2D masks are back-projected into the 3D Gaussian representation: any Gaussian whose projected contribution to a masked pixel exceeds a threshold is labeled as “to be edited,” while the rest remain unchanged. In this way, the system assigns binary edit/non-edit labels to individual 3D Gaussians before optimization.",
        "relevant_elements": [
            "Localization Module",
            "Attention Maps",
            "Masks"
        ],
        "id": 123,
        "masked_question": "How does the [mask1] integrate attention maps and segmentation masks to label 3D Gaussians for editing?",
        "masked_number": 1,
        "masked_elements": [
            "Localization Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.10033v1_figure_1.png",
        "paperid": "2411.10033v1",
        "paper_path": "./papers/2411.10033v1.json",
        "figure_id": "2411.10033v1_figure_1.png",
        "caption": "Figure 1: Method Overview. GSEditPro edits scenes represented by 3D Gaussian Splatting using text prompt only. The key process of our method contains three parts: 1) Locating Gaussian editing regions via cross-attention, which assigns semantic labels to each Gaussian and determines whether the gradients can be propagated between them (Section 3.2); 2) Optimizing editing regions using DreamBooth, which uses ℒS⁢D⁢Ssubscriptℒ𝑆𝐷𝑆\\mathcal{L}_{SDS}caligraphic_L start_POSTSUBSCRIPT italic_S italic_D italic_S end_POSTSUBSCRIPT as the guidance to optimize Gaussian parameters iteratively (Section 3.3); 3) Preserving details with pixel-level guidance, which creates a pseudo-GT image to guide 3D Gaussian rendering for more detailed results (Section 3.4).",
        "qtype": "Implementation_Details",
        "response": "The little “red‐box” sub‐block in Figure 1 is nothing mystical – it is simply our 2D-to-3D labeling step, which fuses the rough cross-attention heatmaps from the diffusion model with precise 2D object masks from SAM, and then back-projects the result into the Gaussian cloud.  In practice it works like this:  \n1.  During Img2Img DreamBooth, we grab the per-word cross-attention maps (e.g. “sunglasses”) at each sampled view.  A fixed threshold (0.5) plus a DBSCAN outlier-filtering pass turns those soft attention maps into rough binary masks.  \n2.  If we are editing an existing object, we refine those rough masks by using the top-5 highest-attention pixels as positive seeds and the bottom-3 as negatives for the Segment-Anything Model (SAM).  SAM then returns a crisp, per-view outline.  \n3.  Finally we take that per-view 2D mask, throw it back through our differentiable renderer and ask “which Gaussians fell inside this mask?”  Any Gaussian whose reprojection weight into the mask is above a small threshold is marked editable (its label bit is set to 1), while all others remain un­labeled (0).  \n\nOnce these binary labels exist on each Gaussian, all later DreamBooth/SDS gradients are gated so they only flow into the marked Gaussians – which is how we achieve truly local, text-guided editing in 3D."
    },
    {
        "question": "How does DreamBooth apply the SDS loss to iteratively update Gaussian parameters during optimization?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We utilize the SDS loss proposed by DreamFusion as the guiding loss function.",
            "After obtaining the prompt for editing and the images rendered from random views during training, they are collectively used as inputs to compute L_SDS in DreamBooth.",
            "This loss is then employed during the back-propagation process to guide the cloning and splitting of the Gaussians, as well as the changes in their parameters.",
            "Therefore, during each training iteration, L_SDS serves as a 2D guidance to optimize Gaussian parameters iteratively."
        ],
        "final_answer": "DreamBooth computes the SDS loss by feeding rendered views and the text prompt into the pre-trained diffusion model, measuring the squared-error between predicted and actual noise. During each optimization step, this loss is back-propagated only through the Gaussians marked for editing—guiding their cloning, splitting, and updates to positional, covariance, color, and opacity parameters iteratively until the rendered result aligns with the text guidance.",
        "relevant_elements": [
            "DreamBooth",
            "L_SDS"
        ],
        "id": 124,
        "masked_question": "How does [mask1] apply the SDS loss to iteratively update Gaussian parameters during optimization?",
        "masked_number": 1,
        "masked_elements": [
            "DreamBooth"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.10033v1_figure_1.png",
        "paperid": "2411.10033v1",
        "paper_path": "./papers/2411.10033v1.json",
        "figure_id": "2411.10033v1_figure_1.png",
        "caption": "Figure 1: Method Overview. GSEditPro edits scenes represented by 3D Gaussian Splatting using text prompt only. The key process of our method contains three parts: 1) Locating Gaussian editing regions via cross-attention, which assigns semantic labels to each Gaussian and determines whether the gradients can be propagated between them (Section 3.2); 2) Optimizing editing regions using DreamBooth, which uses ℒS⁢D⁢Ssubscriptℒ𝑆𝐷𝑆\\mathcal{L}_{SDS}caligraphic_L start_POSTSUBSCRIPT italic_S italic_D italic_S end_POSTSUBSCRIPT as the guidance to optimize Gaussian parameters iteratively (Section 3.3); 3) Preserving details with pixel-level guidance, which creates a pseudo-GT image to guide 3D Gaussian rendering for more detailed results (Section 3.4).",
        "qtype": "Implementation_Details",
        "response": "Here’s what the red‐boxed module (“[mask1]”) is doing during the optimization stage:\n\n1.  Start from your labeled 3D‐GS (where only the Gaussians you want to edit are “unlocked” for gradients).  \n2.  In each training iteration:\n    a.  Render a 2D view I_edit from the current 3D‐Gaussian model under a random camera.  \n    b.  Pick a random diffusion timestep t, add noise to I_edit to get z_t, and feed (z_t, text‐prompt) into the DreamBooth (i.e. fine-tuned Stable Diffusion) denoiser.  \n    c.  The network predicts ε̂_t(z_t; T); you compare that to the true noise ε you added and form the Score Distillation Sampling (SDS) loss  \n         L_SDS = w(t) · ‖ε – ε̂_t(z_t; T)‖².  \n    d.  Backpropagate ∂L_SDS through the denoiser and, crucially, through the differentiable Gaussian splatting renderer, so that the unlocked Gaussians’ parameters (positions, covariances, colors, opacities) receive gradient updates.  \n3.  Repeat. Over many iterations, this SDS‐guided loop sculpts the editable Gaussians to match the new text prompt while everything else stays locked."
    },
    {
        "question": "How does GSPR transform the Query 3D-GS Scene into a descriptor vector for matching?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.3"
        ],
        "relevant_context": [
            "Global Descriptor Generator is used to extract distinctive place recognition descriptors from the proposed MGS representations. To extract the high-level spatio-temporal features, we first voxelize the MGS scene, and then extract local and global features through a backbone network composed of 3D graph convolutions [30] and transformer [31] module. Finally, the spatio-temporal features are fed into NetVLAD-MLPs combos [4] and aggregated into discriminative descriptors.",
            "To tackle the disordered distribution of Gaussians, we first organize the MGS scene into a form that facilitates feature extraction through voxelization. … After the voxel encoding operation, the voxel set of shape  is encoded into an input form of . … Ultimately, the voxel downsampling operation imparts orderliness to the Gaussian scene and reduces the number of Gaussians that need to be processed.",
            "Inspired by the successful application of graph convolution in place recognition [35, 15], we use a 3D-GCN-based [30] graph convolution backbone network to fully exploit the local features in the scene. … We perform zero-mean normalization on the coordinates of the Gaussian graph and subsequently feed the Gaussian graph into stacked 3D graph convolution layers, 3D graph max-pooling layers [30], and ReLU nonlinear activation layers. The graph convolution backbone network generates output feature graph  based on the input features of Gaussian graph , which are then used for subsequent processing.",
            "We use transformers following the previous works [36, 6] to boost place recognition performance. … After adding the positional embeddings to the features and performing feature fusion through 3D graph convolution layers, we feed the position-encoded features into multi-head attention to fully extract the global spatio-temporal information in the scene."
        ],
        "final_answer": "GSPR first voxelizes the Query 3D-GS scene into a regular grid of voxel features. These voxels are encoded and organized into a graph which is processed by stacked 3D graph convolution layers to capture local spatio-temporal patterns. The resulting node features are enhanced with learnable positional embeddings and fed into a transformer (multi-head self-attention) module to aggregate global context. Finally, the fused features are passed through NetVLAD-MLP layers to produce a compact descriptor vector for matching.",
        "relevant_elements": [
            "GSPR",
            "Query 3D-GS Scene"
        ],
        "id": 125,
        "masked_question": "How does [mask1] transform the Query 3D-GS Scene into a descriptor vector for matching?",
        "masked_number": 1,
        "masked_elements": [
            "GSPR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.00299v1_figure_1.png",
        "paperid": "2410.00299v1",
        "paper_path": "./papers/2410.00299v1.json",
        "figure_id": "2410.00299v1_figure_1.png",
        "caption": "Figure 1: Effectively integrating different modalities is crucial for leveraging multimodal data. GSPR harmonizes multi-view RGB images and LiDAR point clouds into a unified scene representation based on Multimodal Gaussian Splatting. 3D graph convolution and transformer are utilized to extract both local and global spatio-temporal information embedded in the scene. This approach enables the generation of discriminative descriptors for place recognition.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is simply the GSPR block – in other words the “Global Scene Place Recognition” head of the system – and it works as follows:\n\n1. Voxelize the 3D-GS scene  \n   – Divide the continuous cloud of Gaussians into a fixed grid of voxels (in cylindrical coordinates) so that you end up with an ordered set of, say, N voxels each containing up to M Gaussians.  \n\n2. Encode each voxel with MeanVFE  \n   – For each voxel you take the positions (and other Gaussian attributes) of its Gaussians, run them through a tiny shared MLP (the MeanVFE) and average to get one D-dim feature per voxel.  \n\n3. Build a 3D graph and run 3D-GCN layers  \n   – Treat each occupied voxel as a node in a graph, connect it to its k nearest neighbors in space, and push those D-dim features through a stack of 3D graph-convolution + pooling + ReLU layers.  This captures local, spatially aware features.  \n\n4. Inject positional embeddings and feed into a Transformer  \n   – Add a learnable positional embedding to each node’s feature, then run a standard multi-head self-attention block over all of them to fuse global context.  \n\n5. Aggregate with NetVLAD + MLPs  \n   – Finally, take the Transformer’s outputs and run them through a NetVLAD pooling layer (plus a small MLP) to collapse all N node-features into one fixed-length (e.g. 256-dim) descriptor vector.  \n\nThat final vector is the “global descriptor” for the query 3D-GS scene, ready to be matched (via nearest neighbors or contrastive distance) against the reference descriptors."
    },
    {
        "question": "How does Multimodal Data integration yield the Reference 3D-GS Scene representation?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.1.1",
            "3.1.3"
        ],
        "relevant_context": [
            "In this paper, we propose a 3D Gaussian Splatting-based multimodal place recognition method namely GSPR, as shown in Fig. 1. We first design a Multimodal Gaussian Splatting (MGS) method to represent autonomous driving scenarios. We utilize LiDAR point clouds as a prior for the initialization of Gaussians, which helps to address the failures of structure-from-motion (SfM) in such environments. In addition, a mixed masking mechanism is employed to remove unstable features less valuable for place recognition. By doing so, we fuse multimodal data into a spatio-temporally unified Gaussian scene representation.",
            "As illustrated in Fig. 3, we introduce Multimodal Gaussian Splatting for autonomous driving scene reconstruction. The method processes multimodal data through the Image Branch and the LiDAR Branch, and then integrates different modalities into a spatio-temporally unified explicit scene representation through Gaussian Optimization.",
            "Using LiDAR point as position prior, the distribution of 3D Gaussian can be represented as: ... To fully utilize the spatio-temporal consistency between different modalities during the Gaussian initialization, we employ RGB images to perform LiDAR point cloud coloring. This approach provides a prior for initializing the spherical harmonic coefficients of the Gaussians.",
            "We employ Mask2Former, pre-trained on the Cityscapes dataset, as our semantic segmentation module to generate semantic labels for the training images. By integrating semantic labels with 2D ground-truth annotations, we can obtain instance-level mask representations. In light of the nature of unstable environmental features, we categorize the masked regions into static masks (e.g., sky and road surfaces) and dynamic masks (e.g., vehicles and pedestrians), each playing distinct roles during the Gaussian optimization process. ... This strategy mitigates the negative effects of dynamic objects and simultaneously maintains enough supervision for large-scale reconstruction compared to directly filtering out frames with dynamic objects."
        ],
        "final_answer": "The Reference 3D-GS Scene is produced by the Multimodal Gaussian Splatting (MGS) stage of GSPR.  In MGS, multi-view RGB images and LiDAR point clouds are fed into two parallel branches.  The LiDAR branch supplies 3D point positions to initialize each Gaussian’s location, while the image branch projects those points into the camera views to color the Gaussians and fit their spherical harmonic coefficients.  During Gaussian optimization, a mixed masking mechanism uses semantic segmentation masks to suppress unstable dynamic or uninformative regions.  Together these steps fuse the complementary geometry and appearance cues into a single, spatio-temporally consistent 3D Gaussian Splatting representation of the reference scene.",
        "relevant_elements": [
            "Multimodal Data",
            "Reference 3D-GS Scene"
        ],
        "id": 126,
        "masked_question": "How does [mask1] integration yield the [mask2] representation?",
        "masked_number": 2,
        "masked_elements": [
            "Multimodal Data",
            "Reference 3D-GS Scene"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.00299v1_figure_1.png",
        "paperid": "2410.00299v1",
        "paper_path": "./papers/2410.00299v1.json",
        "figure_id": "2410.00299v1_figure_1.png",
        "caption": "Figure 1: Effectively integrating different modalities is crucial for leveraging multimodal data. GSPR harmonizes multi-view RGB images and LiDAR point clouds into a unified scene representation based on Multimodal Gaussian Splatting. 3D graph convolution and transformer are utilized to extract both local and global spatio-temporal information embedded in the scene. This approach enables the generation of discriminative descriptors for place recognition.",
        "qtype": "Implementation_Details",
        "response": "Here mask₁ is the “raw” multimodal data block (the red‐boxed region showing synchronized LiDAR sweeps and multi–view RGB frames) and mask₂ is the fused 3D–GS scene (the blue‐boxed “Query 3D-GS Scene”).  The short answer is:\n\nBy running those two streams through our Multimodal Gaussian Splatting pipeline:\n\n 1.  **LiDAR branch**  \n    –  use the LiDAR point cloud to seed (initialize) the 3D Gaussians’ positions (and filter out ground or dynamic‐object points via bounding‐box erasure).  \n    –  project each LiDAR point into the RGB images to grab its color and initialize the spherical‐harmonic coefficients of the Gaussians.  \n\n 2.  **Image branch**  \n    –  feed in the synchronized multi–view images.  Use a pretrained Mask2Former to carve out static (sky, road) and dynamic (cars, pedestrians) regions.  \n    –  overlay static regions with background color (so Gaussians aren’t spawned there) and “detach” (ignore) the photometric loss inside dynamic masks rather than throwing away entire frames.  \n\n 3.  **Overfitting mitigation**  \n    –  sprinkle in a coarse spherical shell of Gaussians at the fringes of LiDAR coverage (also colored by the images) to fill in far‐field geometry and prevent the network from “pulling” distant structure into the near scene.  \n\n 4.  **Gaussian optimization**  \n    –  render the current Gaussian model into each training view, compute a photometric+SSIM loss only on the unmasked parts of the image, and back–propagate to update every Gaussian’s position, covariance, color and opacity.  \n\nAfter convergence, you end up with a single, spatio‐temporally consistent, view‐complete 3D Gaussian Splatting model of that location—i.e. the “Query 3D-GS Scene” (mask₂)."
    },
    {
        "question": "How does Data Augmentation improve Pretrain effectiveness compared to single-phase CNN training on synthetic PDN data?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Data Augmentation: As shown in Figure 1(b), we first augment the training data by applying multiple transformations to each image-based input which help improve the robustness of the model [19], especially when training data is not sufficient.",
            "Specifically, we apply the following five operations to each image-based input: vertical and horizontal flipping and three (counter-clockwise) rotations as shown in Figure 4 for a sample effective distance map. Next, a new testcase is generated by applying one of the five operations to an existing testcase; … This process results in a sixfold increase in the number of testcases, and enhances the diversity and robustness of the dataset [19]. It is applied to both artificially-generated data in pretrain phase, as well as real data in finetune phase."
        ],
        "final_answer": "By applying simple image transforms (horizontal/vertical flips and rotations) to the artificially-generated PDN inputs, data augmentation expands the synthetic training set sixfold and injects considerably more variation. This richer, more diverse pretraining data makes the AttUNet weights far more robust and less prone to overfitting than a conventional single-phase CNN trained only on unaugmented synthetic PDN data.",
        "relevant_elements": [
            "Data Augmentation",
            "Pretrain"
        ],
        "id": 129,
        "masked_question": "How does [mask1] improve Pretrain effectiveness compared to single-phase CNN training on synthetic PDN data?",
        "masked_number": 1,
        "masked_elements": [
            "Data Augmentation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.03292v1_figure_1.png",
        "paperid": "2408.03292v1",
        "paper_path": "./papers/2408.03292v1.json",
        "figure_id": "2408.03292v1_figure_1.png",
        "caption": "Figure 1: Components of our methodology: (a) image translation; (b) data augmentation and two-step training; (c) inference with attention U-Net; (d) saliency map generation; (e) explaintion and diagnosis of predicted hotspots.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify what “​[mask1]​” actually is.  If you look at the red box in Figure 1(b), it surrounds the block labeled “Data Augmentation (Image transformations).”  In our pipeline this block sits between the raw, synthetically‐generated PDN cases and the two–stage (pretrain/finetune) CNN training.\n\nNow, why does inserting that Data-Augmentation step make the pretraining so much more effective than simply doing a single–phase CNN fit on the original synthetic PDN images?  Here’s the logic, step by step:\n\n1.  Synthetic PDN data, by its nature, comes from a generator and typically represents only a handful of layout “styles.”  If you train a CNN straight on those few hundred or thousand cases, the network very quickly over-specializes to the exact geometries it has seen.\n\n2.  Overfitting to synthetic layouts means the learned filters do not generalize well when you later switch to real‐world PDN topologies.  The pretrain stage ends up memorizing idiosyncratic patterns that never occur in the real 20 designs.\n\n3.  By applying simple geometric transforms—vertical flips, horizontal flips and 90°/180°/270° rotations—to every one of those synthetic cases, we instantly multiply our effective pretrain set by six.  Each transformed image is still a valid PDN‐IR problem (the physics don’t change under those operations), but the network sees distinctly different pixel arrangements.\n\n4.  This forces the encoder to learn truly “intrinsic” features of current‐density maps, PDN density, layer resistances, etc., rather than latching onto, say, “all my stripe runs happen left-to-right” or “my power pads always sit at the top.”  The filters it learns become invariant to orientation and avoid co‐adaptations that only hold in the original synthetic cases.\n\n5.  The result is a much more robust set of pretrained weights.  When you then switch to finetuning on the 20 real designs—now with a low dropout and a cosine‐annealed LR—you are starting from a network that already “knows” the core IR‐drop phenomena in a wide variety of geometric contexts, rather than from one that has narrowly memorized a handful of synthetic layouts.\n\nIn short, Data Augmentation makes the pretrain phase far more effective by:\n\n– exponentially increasing the diversity of the fake‐PDN training set  \n– teaching the network to extract orientation‐invariant, general‐purpose IR‐drop features  \n– dramatically reducing overfitting on purely synthetic patterns  \n– yielding a much better initialization for the subsequent real‐data finetune  \n\nAll of which a single‐phase CNN fit on raw synthetic data simply cannot achieve."
    },
    {
        "question": "How does Saliency Map Generation differ from conventional gradient-based explainability in leveraging Image Translation inputs?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "Existing techniques for adding explainability to a deep neural network often require changing the network structure, for example by adding extra layer(s) which can in turn compromise the performance [14, 15, 16]. However, saliency maps are available tools which allow gaining some insights into model behavior very quickly (e.g., seconds in our problem).",
            "Figure 1(d) shows the process of generating the saliency maps. The first step is identifying high-drop pixels from the predicted IR-drop map. Next, a back-propagation is done from this high-drop-only output map to each individual pixel on the input side. Finally, a gradient is computed with respect to each input. Since all image-based inputs are normalized to [0, 1] range, the corresponding gradients are comparable across the pixels of different inputs.",
            "In the end a saliency map is generated for each image-based input, as shown in Figure 1(d)."
        ],
        "final_answer": "Unlike conventional gradient-based explainability methods—which typically focus on a single input image and often require modifying the network architecture—this work applies saliency maps directly to a multi-image-to-single-image translation problem. By back-propagating only from the designer-identified high-drop pixels into each of the normalized image-based inputs (current map, PDN density map, distance map, and per-layer resistance images), it produces one saliency map per input modality. Because all inputs are normalized to [0,1], their gradients become directly comparable, and no changes to the AttUNet model are needed.",
        "relevant_elements": [
            "Saliency Map Generation",
            "Image Translation"
        ],
        "id": 130,
        "masked_question": "How does [mask1] differ from conventional gradient-based explainability in leveraging Image Translation inputs?",
        "masked_number": 1,
        "masked_elements": [
            "Saliency Map Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.03292v1_figure_1.png",
        "paperid": "2408.03292v1",
        "paper_path": "./papers/2408.03292v1.json",
        "figure_id": "2408.03292v1_figure_1.png",
        "caption": "Figure 1: Components of our methodology: (a) image translation; (b) data augmentation and two-step training; (c) inference with attention U-Net; (d) saliency map generation; (e) explaintion and diagnosis of predicted hotspots.",
        "qtype": "Literature_Background",
        "response": "Here “mask1” is simply the \"Saliency Map Generation\" block (the red‐boxed step (d) in the figure).  What makes our Saliency Map Generation different from an off-the-shelf gradient saliency method is that:\n\n 1. We are not operating on one RGB image.  Our network takes in a *set* of heterogeneous, normalized “image–translation” inputs (current map, PDN density, distance map, plus per–layer resistance maps), each of which represents a completely different physical quantity.  \n 2. We first *select only the high–drop pixels* in the predicted IR-drop map (the designer’s true region of interest), and then back-propagate *from that sparse, focused output set* into *each* input image.  \n 3. Because all inputs were scaled into [0,1], the resulting gradients are directly *comparable* across those very different images, so we generate a *separate* saliency map for each input modality.  \n\nBy contrast, a conventional gradient–based saliency on a single image simply computes ∂score/∂pixels over one multi–channel tensor, does not distinguish among multiple input types, and often has to alter the network or lump everything together.  Our approach leverages the image–translation representation end‐to‐end, yields one focused saliency per input map, and requires no architectural changes."
    },
    {
        "question": "How does the custom loss function modify training dynamics in the finetune phase?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "As a final consideration during training, we employ a custom loss function. We note, the goal of IR-drop analysis is to predict the hotspot locations. Underestimating these is undesirable. Therefore, we define a custom loss function while training the model, which motivates the model to err on the side of overestimating the IR-drop, even if it results in a larger error.",
            "The loss function is set to punish more when a predicted value is less than the actual value."
        ],
        "final_answer": "In the finetune phase, the custom loss function biases the model to avoid underestimating IR-drop hotspots by applying a heavier penalty whenever a predicted drop is below the ground truth. This encourages the network to err on the side of overestimation, ensuring that high-drop regions (hotspots) are less likely to be missed.",
        "relevant_elements": [
            "Custom Loss Function",
            "Finetune"
        ],
        "id": 131,
        "masked_question": "How does the [mask1] modify training dynamics in the finetune phase?",
        "masked_number": 1,
        "masked_elements": [
            "Custom Loss Function"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.03292v1_figure_1.png",
        "paperid": "2408.03292v1",
        "paper_path": "./papers/2408.03292v1.json",
        "figure_id": "2408.03292v1_figure_1.png",
        "caption": "Figure 1: Components of our methodology: (a) image translation; (b) data augmentation and two-step training; (c) inference with attention U-Net; (d) saliency map generation; (e) explaintion and diagnosis of predicted hotspots.",
        "qtype": "Experimental_Results",
        "response": "Here is the step-by-step reasoning:\n\n1.   Identify what’s in the red box.  \n     If you look at the “Pretrain → Finetune” block in Figure 1(b), the only thing boxed in red is labeled “Custom Loss Function.”  \n\n2.   Recall the text that describes that loss.  \n     In Section 2.3 (Model Training), the authors say:  \n     – they want to bias the network so that it “errs on the side of overestimating the IR drop,”  \n     – so they build an asymmetric loss that punishes under-predictions more harshly than over-predictions,  \n     – mathematically they multiply the squared‐error by a factor α > 1 whenever the prediction ŷ is below the ground truth y.  \n\n3.   Ask how that custom loss changes training during Finetune.  \n     During Finetune they already dial back dropout, adopt a cosine‐annealed learning‐rate schedule, and work only on the small set of real cases.  Adding the custom loss on top of that means that, on every gradient step, any pixel where the network currently underestimates the true IR drop will incur a disproportionately large gradient.  In practice this forces the model in Finetune to shift its weights so as to raise those low-ball predictions (and hence “cover” all hotspots), even if that slightly worsens the overall mean-squared error.  \n\n4.   Summarize in plain English:  \n     The Custom Loss Function amplifies the gradient signal coming from any pixel where ŷ < y, so during the fine-tuning pass the network is driven to over-predict rather than risk missing a real IR-drop hotspot.  \n\nAnswer:\n\n“In the finetune phase the red-boxed ‘Custom Loss Function’ applies an asymmetric penalty—any time the network predicts an IR-drop below the true value it is multiplied by a constant α > 1.  As a result, under-predictions generate larger gradients, forcing the network during fine-tuning (with low dropout and a cosine-annealed learning rate) to ‘play it safe’ and overestimate rather than miss a hotspot.”"
    },
    {
        "question": "How are gradients from saliency map generation utilized to diagnose/explain high-drop pixels?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "The first step is identifying high-drop pixels from the predicted IR-drop map. … Next, a back-propagation is done from this high-drop-only output map to each individual pixel on the input side. Finally, a gradient is computed with respect to each input. Since all image-based inputs are normalized to [0,1] range, the corresponding gradients are comparable across the pixels of different inputs.",
            "Due to the complex nature of the neural network, F_j(X) is a highly non-linear function of X. However, given an input X₀, F_j(X) can be approximated with a linear function in the neighborhood of X₀ by computing the first-order Taylor expansion: F_j(X) ≈ F_j(X₀) + ∂F_j(X)/∂X|_{X₀} · (X – X₀). The magnitude of elements of ∂F_j(X)/∂X defines the importance of the corresponding pixels of X for the j-th output pixel.",
            "For a subset of output pixels S (for example representing the predicted high-drop pixels), a (combined) saliency map M_S is generated for input X representing all image-based inputs. This is done by computing the average gradient regarding the subset of output pixels:\nM_S = (1/|S|) ∑_{j ∈ S} |∂F_j(X)/∂X|.\nThe above will have dimension equal to X and further be broken into individual saliency maps representing each input-based image."
        ],
        "final_answer": "When the model predicts an IR-drop map, we first select the high-drop output pixels and back-propagate from that high-drop-only output to every input pixel. We then compute the gradient of each selected output pixel with respect to each input pixel. The magnitude of these gradients—made comparable by the [0,1] normalization of inputs—serves as a proxy for the contribution or importance of each input pixel to the high-drop outputs. By averaging these absolute gradients across the set of high-drop outputs, we form a saliency map per input image. High-magnitude entries in these saliency maps pinpoint which input pixels (and which inputs) are most responsible for the predicted hotspots, thus diagnosing and explaining the high-drop pixels.",
        "relevant_elements": [
            "Saliency Map Generation",
            "Diagnose/Explain high-drop pixels"
        ],
        "id": 132,
        "masked_question": "How are gradients from [mask1] utilized to diagnose/explain high-drop pixels?",
        "masked_number": 1,
        "masked_elements": [
            "Saliency Map Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.03292v1_figure_1.png",
        "paperid": "2408.03292v1",
        "paper_path": "./papers/2408.03292v1.json",
        "figure_id": "2408.03292v1_figure_1.png",
        "caption": "Figure 1: Components of our methodology: (a) image translation; (b) data augmentation and two-step training; (c) inference with attention U-Net; (d) saliency map generation; (e) explaintion and diagnosis of predicted hotspots.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed block in (d) is nothing more exotic than “take your high‐drop pixels, back-propagate from them to every input pixel, and look at the resulting gradients.”  In practice we do this as follows:\n\n 1.  After AttUNet spits out a full‐chip IR-drop image, we threshold or select the set  of “high-drop” output pixels.  \n 2.  We run a standard back-propagation pass from only those pixels in  back to the very first layer.  This gives us, for each input pixel x<sub>i</sub>, a partial derivative ∂y<sub>j</sub>/∂x<sub>i</sub> for each j∈D.  \n 3.  We collapse that into one saliency value per input pixel by averaging (or summing) the absolute gradients over all j∈D:  \n      S(i) = (1/|D|) ∑<sub>j∈D</sub> |∂y<sub>j</sub>/∂x<sub>i</sub>|  \n 4.  Because all of our input images were normalized to [0,1], these S(i) values are directly comparable from one input map to another.  We reshape S back into each input’s image dimensions and display it as a heat‐map.  \n\nThe bright spots in each saliency map tell you “if I nudge that particular input pixel up or down, I move my high-drop pixels the most.”  In other words, they rank which locations in the current map, PDN‐density map, resistance maps, etc., are driving your hotspots, and thus precisely diagnose/explain the source of your IR-drop problems."
    },
    {
        "question": "How does Magnify procedure ensure patch alignment across magnification levels without background contamination?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We achieve this by processing patches at  magnification levels , which form a geometric sequence, , to ensure patch alignment between levels.",
            "Filter retains only the  patches of highest importance, where  is a hyperparameter. Magnify queries the WSI in the same location as these patches, but at the subsequent resolution, effectively ‘zooming in’ on the selected patches, then removing resultant patches which consist only of background."
        ],
        "final_answer": "By choosing magnifications that form a geometric sequence, Magnify can request higher-resolution patches exactly co-located with previously selected patches (thus ensuring alignment). Any newly retrieved patches that contain only background are then discarded to avoid background contamination.",
        "relevant_elements": [
            "Magnify"
        ],
        "id": 133,
        "masked_question": "How does [mask1] procedure ensure patch alignment across magnification levels without background contamination?",
        "masked_number": 1,
        "masked_elements": [
            "Magnify"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18225v1_figure_1.png",
        "paperid": "2411.18225v1",
        "paper_path": "./papers/2411.18225v1.json",
        "figure_id": "2411.18225v1_figure_1.png",
        "caption": "Figure 1: Overview of our novel method, PATHS, which predicts a patient’s relative hazard level given a whole slide image using a top-down hierarchical process along the slide’s pyramidal structure, mimicking the workflow of a pathologist. The prediction y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG is made as a function of the slide-level features at each hierarchy level, F1,…,Fnsuperscript𝐹1…superscript𝐹𝑛F^{1},\\dots,F^{n}italic_F start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_F start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module is in fact the “Magnify” step of PATHS.  Here is what it is doing, and how it guarantees perfect alignment plus no background:\n\n1. Filter first chooses the top k patches (at magnification ℓ) by their learned importance scores.  \n2. Magnify then “zooms in” on exactly those k patches by re–querying the whole‐slide image at the next higher magnification (ℓ+1), using the same slide coordinates and the same pixel‐size window for each patch.  \n   – Because we pick our magnification levels in a geometric sequence (e.g. 1×, 2×, 4×, …) and keep the *pixel* dimensions of each patch constant, each new high-resolution patch exactly tiles the same *physical* region of tissue as its low-resolution parent.  That gives perfect spatial alignment.  \n3. Immediately after extraction, any of the newly fetched high-res patches that consist almost entirely of white (i.e. blank background) are thrown away.  In practice we simply compute a quick background mask or threshold, drop any patches above a background‐percentage cutoff, and renormalize the k slots if desired.  \n\nBy (a) re–querying the WSI at the same coordinates with a fixed pixel window and (b) pruning out purely blank crops, the Magnify step enforces exact cross‐magnification alignment while eliminating background‐only patches."
    },
    {
        "question": "How does the Recurrent unit use hierarchical context to contextualise patch features at subsequent magnification levels?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "For a patch  at magnification , the contextualised feature  is defined as\nwhere RNN denotes a learnable recurrent neural network, which is applied sequentially to the hierarchical context list . In this manner the RNN produces a feature offset which accounts for high-level properties of the tissue surrounding each patch, thus ‘contextualising’ the patch feature. Summation of the RNN output was chosen to enable easy representation of the identity function , for cases in which a patch’s surrounding tissue is not of high relevance.",
            "By sharing the weights of the RNN between all processors, this operation may be implemented efficiently: each processor carries out a single recurrent unit update step per patch, passing the resulting state to the corresponding patches at the subsequent magnification level."
        ],
        "final_answer": "The recurrent unit is a shared-weight RNN that is applied to the ordered list of parent-patch embeddings from all previous magnification levels (the hierarchical context). It processes this context sequentially to produce a feature offset vector, which is then added to each patch’s local embedding at the next magnification level. By carrying over its hidden state to the corresponding child patches, the RNN thus propagates macro-scale contextual information down the hierarchy and ‘contextualises’ high-magnification patch features.",
        "relevant_elements": [
            "Recurrent unit",
            "hierarchical context"
        ],
        "id": 134,
        "masked_question": "How does the [mask1] use hierarchical context to contextualise patch features at subsequent magnification levels?",
        "masked_number": 1,
        "masked_elements": [
            "Recurrent unit"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18225v1_figure_1.png",
        "paperid": "2411.18225v1",
        "paper_path": "./papers/2411.18225v1.json",
        "figure_id": "2411.18225v1_figure_1.png",
        "caption": "Figure 1: Overview of our novel method, PATHS, which predicts a patient’s relative hazard level given a whole slide image using a top-down hierarchical process along the slide’s pyramidal structure, mimicking the workflow of a pathologist. The prediction y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG is made as a function of the slide-level features at each hierarchy level, F1,…,Fnsuperscript𝐹1…superscript𝐹𝑛F^{1},\\dots,F^{n}italic_F start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_F start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in the figure is nothing more exotic than the paper’s “contextualisation RNN” (sometimes just called the recurrent unit).  Its job is to take, for each newly “zoomed‐in” patch, the stack of all its parent embeddings from the coarser levels above it (that list is what the authors call the hierarchical context C) and run a single RNN update over that list.  The hidden state that comes out of that recurrent pass is treated as a learned “feature offset” which is then simply added to the raw, high‐magnification patch embedding.  In that way the RNN “injects” the macro-scale information from the parents directly into the local patch feature (if the context proves unhelpful it can learn to add (near) zero, i.e. the identity).  Because the same RNN weights are used at every level, you only ever need one recurrent update per patch to carry forward and contextualise all of its ancestors’ information."
    },
    {
        "question": "What ethical considerations arise from automated Filter-driven region prioritisation in diagnostic workflows?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Filter"
        ],
        "id": 135,
        "masked_question": "What ethical considerations arise from automated [mask1]-driven region prioritisation in diagnostic workflows?",
        "masked_number": 1,
        "masked_elements": [
            "Filter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18225v1_figure_1.png",
        "paperid": "2411.18225v1",
        "paper_path": "./papers/2411.18225v1.json",
        "figure_id": "2411.18225v1_figure_1.png",
        "caption": "Figure 1: Overview of our novel method, PATHS, which predicts a patient’s relative hazard level given a whole slide image using a top-down hierarchical process along the slide’s pyramidal structure, mimicking the workflow of a pathologist. The prediction y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG is made as a function of the slide-level features at each hierarchy level, F1,…,Fnsuperscript𝐹1…superscript𝐹𝑛F^{1},\\dots,F^{n}italic_F start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_F start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n1. In the red-boxed step the model “Filter” operator ranks patches by their learned importance score αˡ and only retains the top-k for further inspection (“magnify”).\n2. In a clinical setting this amounts to an automated, attention- or importance-driven region prioritisation—i.e. only the model’s “most salient” tiles ever get high-resolution scrutiny.\n3. Ethical concerns immediately follow:\n    • Missed findings.  If the learned importance scores are overconfident or biased, rare but clinically critical features might be filtered out entirely, leading to false negatives.  \n    • Transparency & interpretability.  The priority function (the scalar α values) is learned and not human-readable; clinicians may not understand why some regions never reach their desk.  \n    • Over-reliance & deskilling.  Routine triage by an opaque algorithm can encourage pathologists to trust the model’s “priorities” too blindly, eroding vigilance on unselected areas.  \n    • Bias & fairness.  Training data imbalances (e.g. certain tumour morphologies under-represented) can skew which regions are deemed “important,” systematically disadvantaging some patient groups.  \n    • Accountability & liability.  If a missed diagnosis is later traced to a filtered-out region, it is unclear who is responsible—the tool’s designers, the hospital IT team, or the reporting pathologist.  \n    • Informed consent & governance.  Patients and regulators need to understand that not every pixel of their tissue is examined by a human; consent language, approval pathways and audit logs must reflect this automated triage.  \n4. Any deployment of attention-based region prioritisation must therefore include rigorous bias audits, explainability measures (e.g. heatmaps overlaid on the WSI), clinician-in-the-loop safeguards, and clear protocols for responsibility when filtered-out regions later prove diagnostically significant."
    },
    {
        "question": "How might replacing the non-differentiable Magnify operation impact training convergence and interpretability?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Magnify"
        ],
        "id": 136,
        "masked_question": "How might replacing the non-differentiable [mask1] operation impact training convergence and interpretability?",
        "masked_number": 1,
        "masked_elements": [
            "Magnify"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.18225v1_figure_1.png",
        "paperid": "2411.18225v1",
        "paper_path": "./papers/2411.18225v1.json",
        "figure_id": "2411.18225v1_figure_1.png",
        "caption": "Figure 1: Overview of our novel method, PATHS, which predicts a patient’s relative hazard level given a whole slide image using a top-down hierarchical process along the slide’s pyramidal structure, mimicking the workflow of a pathologist. The prediction y^^𝑦\\hat{y}over^ start_ARG italic_y end_ARG is made as a function of the slide-level features at each hierarchy level, F1,…,Fnsuperscript𝐹1…superscript𝐹𝑛F^{1},\\dots,F^{n}italic_F start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , … , italic_F start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.",
        "qtype": "Others",
        "response": "Here’s the key bit: the red‐boxed operation in Figure 1 is the “hard” top-k Filter + Magnify step that (a) throws away all but the K highest-importance patches and then (b) literally re-extracts those patches at the next resolution. Because that Filter uses an arg–top-k, it is not differentiable, so no gradient ever flows “through” the cropping step itself. In PATHS we side-step that by learning importance weights upstream and only back-propagating into those weights, but the act of selection remains a black-box from the point of view of the optimiser.\n\nIf you were to replace that non-differentiable mask–and–crop with a fully differentiable alternative (for instance a soft-top-k or Gumbel-softmax attention that “softly” blends neighboring patches rather than hard-dropping them), you would:\n\n1. Gain better gradient flow through the entire hierarchy.  \n  • Gradients could actually travel into the cropping parameters or sampling locations.  \n  • This tends to stabilise and often speed up convergence, since the network can learn exactly how to shift its focus rather than only indirectly via upstream importance scores.\n\n2. Lose the crisp, human-readable “these exact tiles were chosen” behaviour.  \n  • Hard top-k gives you a tiny handful of actual patches you can plop back on the slide for inspection.  \n  • Soft attention or differentiable sampling spreads your focus over many overlapping patches, so your final heatmap or “pathologist-style” zoom trace becomes blurrier and harder to audit.\n\nIn short, making the magnify step differentiable will usually help the optimiser (smoother loss surface, fewer zero-gradient regions) at the cost of the very sharp, discrete region‐of‐interest decisions that give PATHS its strong interpretability."
    },
    {
        "question": "What are potential privacy concerns when using identity embeddings in the ID-Preserving Module?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "ID-Preserving Module"
        ],
        "id": 137,
        "masked_question": "What are potential privacy concerns when using identity embeddings in the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "ID-Preserving Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18281v1_figure_2.png",
        "paperid": "2411.18281v1",
        "paper_path": "./papers/2411.18281v1.json",
        "figure_id": "2411.18281v1_figure_2.png",
        "caption": "Figure 2: \nFramework overview. Our proposed framework comprises three core components: the ID-Preserving Module, the Motion Control Module, and a composite loss function. The loss function incorporates a Region-Aware Loss to ensure high motion fidelity and an ID-Consistency Loss to maintain alignment with the reference ID image. During training, motion intensity ℳℳ\\mathcal{M}caligraphic_M is derived from optical flow. At inference, human animations are generated based on user-defined motion intensity ℳℳ\\mathcal{M}caligraphic_M and specified action phrases, enabling fine-grained and controllable video synthesis.",
        "qtype": "Others",
        "response": "Here’s a step-by-step reasoning on why harvesting and using identity embeddings in that red-boxed “ID-Preserving Module” can raise privacy flags:\n\n1. What the module does  \n   - It crops out a person’s face and runs it through two encoders (a CLIP image encoder for general context and ArcFace for fine-grained facial features) to produce a compact “ID embedding.”  \n   - That embedding then gets injected as a conditioning signal into the diffusion model so that every generated frame faithfully preserves that person’s identity.\n\n2. Why embeddings can be sensitive  \n   - Unlike simple labels, these embeddings encode biometric information—how your facial features are laid out in high detail.  \n   - They’re easily linkable back to a real person (e.g. if someone collects embeddings from multiple videos, they can tell it’s the same subject).\n\n3. Potential privacy concerns  \n   a. Embedding leakage or theft  \n     - If your system’s logs or model checkpoints aren’t tightly secured, an attacker who steals the embeddings can:  \n       • Re-identify or track the person across different videos or platforms  \n       • Launch “embedding inversion” attacks to reconstruct an approximate face image from the embedding  \n   b. Unauthorized cross-linking  \n     - Even if you only intended to use the embedding transiently, someone could reuse it elsewhere (e.g. feed it into a face-recognition database) and thereby link a private user to public datasets.  \n   c. Consent and data governance  \n     - Under many privacy regulations (GDPR, CCPA), extracting and storing a face-encoding without explicitly informing the subject can be non-compliant.  \n   d. Biometric profiling  \n     - Embeddings might inadvertently capture demographic attributes (age, gender, ethnicity) that could be used for unwanted profiling or discrimination.\n\nIn short, anytime you extract and hold onto a “face fingerprint,” you need to treat it almost like you would a real biometric key: encrypt it at rest, minimize how long you keep it, and obtain clear subject consent for each intended use."
    },
    {
        "question": "How might balancing Region-Aware Loss versus ID-Consistency Loss affect motion realism in intricate sequences?",
        "relevant_section_ids": [
            "3.2",
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Section 3.2, ID-Consistency Loss: “To address this issue, we introduce an ID-Consistency loss during training phase to maintain the identity information … the ID-Consistency loss L_ID across the sequence of N frames can be calculated by …”",
            "Section 3.3, Region-Aware Loss: “The fluency of the generated video heavily relies on the spatial coherence and realism of dynamic regions … we apply a region-aware loss to force the model to focus more on the high-motion regions … the region-aware loss L_region across all N frames can be compactly defined as …”",
            "Section 3.4, Overall Objective: “The total learning objective combines the Region-Aware Loss, which captures dynamic motion in high-activity regions, and the ID-Consistency Loss, which ensures identity consistency across frames. The hyperparameter λ balances the influence of identity preservation against motion fidelity.”"
        ],
        "final_answer": "Because the two losses pull the model in different directions—Region-Aware Loss encourages the network to sharpen and faithfully reproduce fine-grained motion in areas of high activity, while ID-Consistency Loss enforces stability of the subject’s identity across frames—adjusting their relative weight (via the λ hyperparameter) effectively trades off motion realism against identity fidelity. In particularly intricate motion sequences, increasing the weight on Region-Aware Loss will tend to boost dynamic detail and realism of subtle movements (e.g. wrinkles, lip shapes, eye blinks), whereas increasing the weight on ID-Consistency Loss will smooth out those nuances to better preserve a constant facial identity. Properly balancing the two is therefore critical: too little emphasis on region awareness can lead to overly static or blurred motion, while too much can introduce identity drift or artifacts.",
        "relevant_elements": [
            "Region-Aware Loss",
            "ID-Consistency Loss"
        ],
        "id": 138,
        "masked_question": "How might balancing [mask1] versus [mask2] affect motion realism in intricate sequences?",
        "masked_number": 2,
        "masked_elements": [
            "Region-Aware Loss",
            "ID-Consistency Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18281v1_figure_2.png",
        "paperid": "2411.18281v1",
        "paper_path": "./papers/2411.18281v1.json",
        "figure_id": "2411.18281v1_figure_2.png",
        "caption": "Figure 2: \nFramework overview. Our proposed framework comprises three core components: the ID-Preserving Module, the Motion Control Module, and a composite loss function. The loss function incorporates a Region-Aware Loss to ensure high motion fidelity and an ID-Consistency Loss to maintain alignment with the reference ID image. During training, motion intensity ℳℳ\\mathcal{M}caligraphic_M is derived from optical flow. At inference, human animations are generated based on user-defined motion intensity ℳℳ\\mathcal{M}caligraphic_M and specified action phrases, enabling fine-grained and controllable video synthesis.",
        "qtype": "Others",
        "response": "Here, [mask1] is the Region-Aware Loss (the red-boxed term) and [mask2] is the ID-Consistency Loss (the blue-boxed term).  Tuning the relative weight between them directly trades off motion fidelity in highly dynamic regions against strict identity preservation.  \n\nChain-of-thought:  \n1. The Region-Aware Loss up-weights noise-prediction errors in pixels with large optical-flow magnitudes, forcing the model to pay extra attention to fast-moving regions (e.g. lips, hands).  Raising its weight encourages sharper, more detailed micro-motions and better spatial coherence in intricate sequences.  \n2. The ID-Consistency Loss penalizes deviations of each generated frame’s face embedding from the reference identity.  Raising its weight locks down identity features more tightly, but can “over-smooth” or dampen motion, since the model hesitates to deform the face.  \n3. If you push the Region-Aware term too high, you’ll gain crisp, lifelike motion but begin to see identity drift or artefacts (the face may stretch or jitter).  If you push the ID-Consistency term too high, the face stays rock-solid, but the video can look stiff and fail to capture subtle but important movements.  \n4. The sweet spot lies in balancing the two: enough Region-Aware weighting to recover fine motions in complex sequences, yet enough ID-Consistency to prevent the identity from morphing.  \n\nSo, by adjusting the balance between these two losses, one can dial in how “alive” the motion feels versus how “fixed” the identity remains."
    },
    {
        "question": "What motivates integrating both Cross Attn and Motion Attn in the Motion Control Module?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We regard the control capacity of the model as lying in two aspects: one is the faithfulness of the motion description, and the other is the magnitude of motion intensity. To achieve this goal, we introduce extra action phrase and motion intensity as the conditions in the proposed model.",
            "As illustrated in Fig. 2, two parallel cross attention modules (Cross Attn and Motion Attn) are adopted in the motion control module to insert the action embedding and motion embedding. The process is formally represented as follows: Z = CrossAttn(Q, K_act, V_act) + α·MotionAttn(Q, K_motion, V_motion). The parameter α balances the influence of motion intensity within the combined attention output."
        ],
        "final_answer": "The module integrates both Cross Attn and Motion Attn so that it can separately encode the semantic intent of the action (via Cross Attn on the action phrase) and the strength or magnitude of the motion (via Motion Attn on the motion intensity). This design ensures faithful adherence to the described action and fine-grained control over the movement’s intensity.",
        "relevant_elements": [
            "Cross Attn",
            "Motion Attn",
            "Motion Control Module"
        ],
        "id": 139,
        "masked_question": "What motivates integrating both [mask1] and Motion Attn in the Motion Control Module?",
        "masked_number": 1,
        "masked_elements": [
            "Cross Attn"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18281v1_figure_2.png",
        "paperid": "2411.18281v1",
        "paper_path": "./papers/2411.18281v1.json",
        "figure_id": "2411.18281v1_figure_2.png",
        "caption": "Figure 2: \nFramework overview. Our proposed framework comprises three core components: the ID-Preserving Module, the Motion Control Module, and a composite loss function. The loss function incorporates a Region-Aware Loss to ensure high motion fidelity and an ID-Consistency Loss to maintain alignment with the reference ID image. During training, motion intensity ℳℳ\\mathcal{M}caligraphic_M is derived from optical flow. At inference, human animations are generated based on user-defined motion intensity ℳℳ\\mathcal{M}caligraphic_M and specified action phrases, enabling fine-grained and controllable video synthesis.",
        "qtype": "Design_Rationale",
        "response": "In our Motion Control Module we deliberately split motion‐conditioning into two parallel attention streams – the standard cross-attention (“Cross Attn”) over the action embedding and a dedicated “Motion Attn” over the flow-derived motion embedding – because we want to control two fundamentally different aspects of what the character does:\n\n 1.  Semantic “what” of the motion (i.e. the action phrase like “talking”) is handled by Cross Attn.  \n 2.  Quantitative “how much” the motion should move (i.e. the optical-flow–based intensity) is handled by Motion Attn.  \n\nBy integrating both, the model can faithfully follow the user’s described action while also matching the desired strength or speed of that action, yielding much finer‐grained and more controllable human video synthesis."
    },
    {
        "question": "What is the purpose of combining Region-Aware Loss and ID-Consistency Loss in the composite loss function?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "The total learning objective combines the Region-Aware Loss, which captures dynamic motion in high-activity regions, and the ID-Consistency Loss, which ensures identity consistency across frames.",
            "This dual objective guides the model to preserve both identity and motion fidelity in the generated videos."
        ],
        "final_answer": "The purpose is to guide the model to preserve both identity consistency and motion fidelity in the generated videos by focusing on dynamic motion in high-activity regions (via Region-Aware Loss) and ensuring consistent identity across frames (via ID-Consistency Loss).",
        "relevant_elements": [
            "Region-Aware Loss",
            "ID-Consistency Loss",
            "composite loss function"
        ],
        "id": 140,
        "masked_question": "What is the purpose of combining [mask1] and [mask2] in the composite loss function?",
        "masked_number": 2,
        "masked_elements": [
            "Region-Aware Loss",
            "ID-Consistency Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.18281v1_figure_2.png",
        "paperid": "2411.18281v1",
        "paper_path": "./papers/2411.18281v1.json",
        "figure_id": "2411.18281v1_figure_2.png",
        "caption": "Figure 2: \nFramework overview. Our proposed framework comprises three core components: the ID-Preserving Module, the Motion Control Module, and a composite loss function. The loss function incorporates a Region-Aware Loss to ensure high motion fidelity and an ID-Consistency Loss to maintain alignment with the reference ID image. During training, motion intensity ℳℳ\\mathcal{M}caligraphic_M is derived from optical flow. At inference, human animations are generated based on user-defined motion intensity ℳℳ\\mathcal{M}caligraphic_M and specified action phrases, enabling fine-grained and controllable video synthesis.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1.  In the diagram, the red‐boxed term ([mask1]) is the Region-Aware Loss, which re-weights the diffusion loss to focus the model on areas with high motion (as determined by an optical-flow mask).  This encourages sharp, coherent motion in those dynamic regions (e.g. facial movements).\n\n2.  The blue-boxed term ([mask2]) is the ID-Consistency Loss, which measures the distance between the face-recognition embeddings of the generated frames and the reference ID image.  This forces the model to keep the person’s identity fixed frame-to-frame.\n\n3.  By summing these two losses in the overall objective (with a balancing hyperparameter), the model is jointly trained to \n    •  preserve identity across all frames,  \n    •  and faithfully render the specified motion in the high-activity regions.\n\nSo, combining [mask1] and [mask2] ensures that the generated videos are both motion-accurate (in key regions) and identity-consistent."
    },
    {
        "question": "What motivates adapters transforming base features prior to quantization on sub-codebook branches?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Encoder. We regard the original VQGAN encoder as a base feature extractor. On top of that, K feature adapters are introduced to transform the base image features into their respective feature space.",
            "Second, maintaining factorized sub-codebooks and independent feature adapters allow the model to learn more diverse features."
        ],
        "final_answer": "The adapters are introduced so that each sub-codebook branch operates on its own adapted feature space, which enables the model to learn more diverse and specialized feature representations before quantization.",
        "relevant_elements": [
            "Adapter 1",
            "Quantization on Sub-Codebook 1"
        ],
        "id": 141,
        "masked_question": "What motivates [mask1] transforming base features prior to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Adapter 1",
            "Quantization on Sub-Codebook 1"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16681v2_figure_2.png",
        "paperid": "2411.16681v2",
        "paper_path": "./papers/2411.16681v2.json",
        "figure_id": "2411.16681v2_figure_2.png",
        "caption": "Figure 2: \nIllustration of the our method.\nThe left part shows FQGAN-Dual, the factorized tokenizer design in an example scenario when k=2𝑘2k=2italic_k = 2.\nThis framework is extendable to factorization of more codebooks.\nThe right part demonstrate how we leverage an additional AR head to accommodate the factorized sub-codes based on standard AR generative transformer.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought:\n\n1.  In the factorized VQGAN setup we start from a single base latent  output by the encoder.  \n2.  We then split this single latent stream into  streams, each of which will be quantized by its own small codebook (the “factorized quantization” stage).  \n3.  If we simply fed the identical  into each sub‐codebook, they would all learn the same codes—losing diversity and reintroducing lookup instability.  \n4.  To avoid that, we first pass  through  lightweight adapters (the red box in the figure), each of which projects  into a distinct feature subspace.  \n5.  Those branch‐specific features are then independently quantized by their respective sub‐codebooks (the blue box).  \n6.  This “adapter → quantizer” ordering is what allows each sub‐codebook to specialize in different aspects of the image (e.g. semantics vs. fine details), stabilizes the lookup process in each smaller codebook, and dramatically increases overall code‐usage and reconstruction quality.\n\nAnswer:\n\nThey introduce per-branch adapters so that each sub-quantizer sees its own transformed version of the shared base feature.  In other words, feature adapters project the common latent into multiple specialized embeddings before quantization on sub-codebooks, ensuring that each small codebook learns diverse, disentangled representations and alleviates lookup instability."
    },
    {
        "question": "How does disentanglement loss encourage distinct feature capture across factorized sub-codebook quantizations?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "To address this issue, we propose a disentanglement regularization mechanism for the factorized sub-codebooks.",
            "We design the disentanglement regularization mechanism as follows: L_{dis} = \\sum_{i,b} (z_1^{(i,b)\\top} z_2^{(i,b)})^2. This regularization mechanism minimizes the squared dot product between the two involved codes.",
            "The dot product directly measures the affinity between the two codes after L2 normalization, ranging from -1/1 indicates negative/positive correlation and 0 denotes orthogonality. Minimizing the squaring function encourages the dot product value to approach 0.",
            "In other words, for each patch, it encourages the involved sub-codes to capture different aspects."
        ],
        "final_answer": "The disentanglement loss penalizes the squared dot product between sub-code vectors for each patch, pushing their dot product toward zero (orthogonality). By minimizing their correlation on a per-patch basis, it forces each sub-codebook to learn distinct, non-overlapping features.",
        "relevant_elements": [
            "Disentangle Loss",
            "Quantization on Sub-Codebook 1"
        ],
        "id": 142,
        "masked_question": "How does [mask1] encourage distinct feature capture across [mask2] quantizations?",
        "masked_number": 2,
        "masked_elements": [
            "Disentangle Loss",
            "Quantization on Sub-Codebook 1"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16681v2_figure_2.png",
        "paperid": "2411.16681v2",
        "paper_path": "./papers/2411.16681v2.json",
        "figure_id": "2411.16681v2_figure_2.png",
        "caption": "Figure 2: \nIllustration of the our method.\nThe left part shows FQGAN-Dual, the factorized tokenizer design in an example scenario when k=2𝑘2k=2italic_k = 2.\nThis framework is extendable to factorization of more codebooks.\nThe right part demonstrate how we leverage an additional AR head to accommodate the factorized sub-codes based on standard AR generative transformer.",
        "qtype": "Design_Rationale",
        "response": "Here’s the core idea, step by step:\n\n1.  What you see in the red‐boxed region (“Disentangle Loss”) is a regularizer that looks at, for each spatial patch, the two (or more) code‐vectors pulled from the different sub-codebooks (the blue-boxed “Quantization on Sub-Codebook 1, 2, …”).  \n2.  It takes those paired vectors, L2-normalizes them, computes their dot‐product (a measure of how aligned/“redundant” they are), squares that dot‐product, and then averages over patches and samples.  \n3.  By minimizing this squared dot‐product, the loss drives the two vectors toward orthogonality (dot ≈ 0).  \n4.  In effect, each sub-codebook is forced to capture information that the others do not— i.e. “disentangled” or distinct features—because if two sub-codes tried to encode the same thing, their dot‐product would be high and the loss would climb.  \n\nSo, the Disentangle Loss explicitly penalizes correlated codes across the factorized quantizers, pushing each sub-quantization branch to learn its own, non-overlapping aspects of the image."
    },
    {
        "question": "How does disentanglement loss enforce orthogonality between sub-codebook outputs per patch?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "This regularization mechanism minimizes the squared dot product between the two involved codes.",
            "The dot product directly measures the affinity between the two codes after L2 normalization, ranging from –1/1, where –1/1 indicates negative/positive correlation and 0 denotes orthogonality. Minimizing the squaring function encourages the dot product value to approach 0.",
            "Note that this regularization does not directly apply to the entire codebook. Instead, it operates on patches of each image instance. In other words, for each patch, it encourages the involved sub-codes to capture different aspects."
        ],
        "final_answer": "The disentanglement loss takes the L2-normalized outputs of the two sub-codebooks for each image patch, computes their dot product, and then minimizes its square. By driving the squared dot product toward zero, it forces the two sub-code outputs to become orthogonal on a per-patch basis.",
        "relevant_elements": [
            "Quantization on Sub-Codebook 1",
            "Quantization on Sub-Codebook 2",
            "Disentangle Loss"
        ],
        "id": 143,
        "masked_question": "How does [mask1] enforce orthogonality between sub-codebook outputs per patch?",
        "masked_number": 1,
        "masked_elements": [
            "Disentangle Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.16681v2_figure_2.png",
        "paperid": "2411.16681v2",
        "paper_path": "./papers/2411.16681v2.json",
        "figure_id": "2411.16681v2_figure_2.png",
        "caption": "Figure 2: \nIllustration of the our method.\nThe left part shows FQGAN-Dual, the factorized tokenizer design in an example scenario when k=2𝑘2k=2italic_k = 2.\nThis framework is extendable to factorization of more codebooks.\nThe right part demonstrate how we leverage an additional AR head to accommodate the factorized sub-codes based on standard AR generative transformer.",
        "qtype": "Implementation_Details",
        "response": "First, note that the little red‐boxed module sitting on top of the two per‐patch code vectors in Figure 2 is exactly the “Disentanglement Loss” that the text describes in §3.2.1.  Concretely, if for patch i you extract two k‐dim vectors zᵢ(1) and zᵢ(2) from the two sub‐codebooks, you L₂‐normalize each and then compute their dot–product.  The disentanglement term is simply the squared dot–product (i.e. the squared cosine similarity) summed over all patches in the batch.  By driving that squared inner–product to zero, you force each pair of sub‐code vectors for the same patch to have zero correlation—i.e. to become orthogonal."
    },
    {
        "question": "How does SEGA iteratively merge client partitions to minimize 2D structural entropy?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Specifically, each client node is initially treated as an individual partition.",
            "New partitions are formed by iteratively merging different partitions.",
            "The changes in the 2DSE before and after merging are observed to identify the partitioning scheme that yields the lowest overall 2DSE and generates the desired partitions.",
            "We leverage the greedy strategy in (Li and Pan, 2016  ###reference_b17###) to minimize 2DSE.",
            "Note that we always merge the two partitions with the smallest SE until all SE , thus obtaining the final partitions ."
        ],
        "final_answer": "SEGA begins with each client as its own partition and then, in a greedy fashion, repeatedly merges the pair of partitions whose union leads to the greatest reduction in the two-dimensional structural entropy. At each step it computes the change in 2DSE for every possible merge, picks the merge with the smallest resulting SE, and continues until no further merges can reduce the entropy, yielding the final client partitions that minimize 2DSE.",
        "relevant_elements": [
            "SEGA",
            "2D Structural Entropy"
        ],
        "id": 145,
        "masked_question": "How does [mask1] iteratively merge client partitions to minimize [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "SEGA",
            "2D Structural Entropy"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.00614v1_figure_1.png",
        "paperid": "2409.00614v1",
        "paper_path": "./papers/2409.00614v1.json",
        "figure_id": "2409.00614v1_figure_1.png",
        "caption": "Figure 1. The overall framework of DAMe.",
        "qtype": "Implementation_Details",
        "response": "Here “[mask1]” is SEGA (“Structural Entropy‐based Global Aggregation”) and “[mask2]” is the two–dimensional structural entropy (2DSE) of the client similarity graph.  SEGA minimizes 2DSE by a simple greedy, agglomerative procedure:\n\n1.  Initialize each client as its own partition.  \n2.  For every pair of current partitions  and , compute the change in 2DSE that would result from merging them.  \n3.  Pick the pair whose merge gives the largest decrease (or smallest increase) in 2DSE—that is, the smallest ΔSE—and merge them into a single partition.  \n4.  Repeat steps 2–3 until no further merge can reduce the overall 2DSE (or until a desired number of partitions is reached).  \n\nBy always choosing the merge that most reduces (or least increases) the 2D structural entropy, SEGA produces a final partitioning that drives the global–aggregation step."
    },
    {
        "question": "How does BOLA combine Expected Improvement and UCB to optimize aggregation weight λ?",
        "relevant_section_ids": [
            "4.2",
            "4.2.3"
        ],
        "relevant_context": [
            "Given the intricate and non-convex nature of the objective function f(λ) (Hoffman et al., 2011), we employ a mixed acquisition strategy of incorporating EI and UCB.",
            "In this work, we apply the Expected Improvement (EI) (Mockus, 1974; Jones et al., 1998) criterion and the Upper Confidence Bound (UCB) (SRINIVAS, 2010) as acquisition functions. EI seeks the next weight with maximal expected improvement under the posterior Gaussian process model, while UCB chooses the weight with the highest upper confidence bound (mean plus a time-dependent multiple of the standard deviation) to encourage exploration."
        ],
        "final_answer": "BOLA uses a mixed acquisition strategy that combines Expected Improvement (EI) and Upper Confidence Bound (UCB). At each step, EI drives exploitation by selecting the λ with the highest expected improvement under the Gaussian‐process posterior, while UCB drives exploration by selecting the λ with the highest upper confidence bound (posterior mean plus a scaled posterior standard deviation). By integrating both criteria, BOLA balances exploring uncertain λ values and exploiting promising ones to efficiently find the optimal aggregation weight.",
        "relevant_elements": [
            "BOLA",
            "Bayesian Optimization"
        ],
        "id": 146,
        "masked_question": "How does [mask1] combine Expected Improvement and UCB to optimize aggregation weight λ?",
        "masked_number": 1,
        "masked_elements": [
            "BOLA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.00614v1_figure_1.png",
        "paperid": "2409.00614v1",
        "paper_path": "./papers/2409.00614v1.json",
        "figure_id": "2409.00614v1_figure_1.png",
        "caption": "Figure 1. The overall framework of DAMe.",
        "qtype": "Implementation_Details",
        "response": "The thing in the red box is nothing more exotic than a small Bayesian‐Optimization (BO) loop wrapped around your usual “local–global” convex combination, θ←λθlocal+(1−λ)θglobal.  What makes it BO rather than a hand‐tuned sweep is that at every round we treat the mapping  \n\n λ↦𝑓(λ):=score(λθlocal+(1−λ)θglobal)\n\nas a black‐box we want to maximize, we put a Gaussian‐process prior on 𝑓, and then we use an acquisition function that is literally a blend of Expected Improvement (EI) and Upper‐Confidence‐Bound (UCB).  \n\nIn practice what happens is:\n\n  1.  You fit a GP to all the (λ,𝑓(λ)) pairs you’ve seen so far.\n  2.  For a dense grid (or some candidate set) of new λ’s you ask the GP for  \n       •  EI(λ) – the expected improvement over your current best 𝑓  \n       •  UCB(λ) – the posterior mean plus a growing multiple of the posterior standard deviation  \n  3.  You then either form a single “mixed” acquisition  \n       α(λ)=EI(λ)+UCB(λ)  (or some slight variant with a time‐varying weight on the UCB term)  \n     or occasionally alternate between purely EI and purely UCB.  \n  4.  You pick the λ that maximizes α(λ), evaluate 𝑓(λ) on your held‐out set, add that data point to your GP, and repeat.  \n\nBy doing this you automatically balance “go where your GP says you’ll get the biggest gain” (EI) against “go where your GP is most uncertain” (UCB), and so you converge quickly to the best local–global mixing weight λ."
    },
    {
        "question": "How does SEGA leverage two-dimensional structural entropy minimization compared to conventional graph partitioning techniques?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "SEGA",
            "2D structural entropy minimization"
        ],
        "id": 147,
        "masked_question": "How does [mask1] leverage [mask2] compared to conventional graph partitioning techniques?",
        "masked_number": 2,
        "masked_elements": [
            "SEGA",
            "2D structural entropy minimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.00614v1_figure_1.png",
        "paperid": "2409.00614v1",
        "paper_path": "./papers/2409.00614v1.json",
        "figure_id": "2409.00614v1_figure_1.png",
        "caption": "Figure 1. The overall framework of DAMe.",
        "qtype": "Literature_Background",
        "response": "In our paper the red‐boxed module is SEGA (Structural Entropy‐based Global Aggregation) and the blue‐boxed part is the little “client–similarity graph + 2D structural‐entropy minimization” engine inside it.  What SEGA does, that ordinary graph cuts or spectral‐clustering routines do not, is\n\n 1.  It constructs a *fully connected* graph over clients, with edge weights equal to the similarity of their model outputs on a shared random graph.  \n 2.  Instead of fixing the number of partitions or tuning a balance parameter, it *greedily merges* pieces of that graph so as to *minimize the two‐dimensional structural entropy* (2D SE).  This step simultaneously tells you how many clusters there really are and which clients belong together.  \n 3.  Finally, SEGA turns each cluster into a *personalized aggregation rule* by renormalizing the edge weights *within* each client’s cluster and using those as mixing coefficients.  \n\nBy leveraging 2D SE in exactly this way, SEGA automatically discovers both the optimal grouping of clients and per‐client aggregation weights—avoiding the need for a user‐supplied k, balance terms, or hand‐tuned cut objectives that conventional partitioners require."
    },
    {
        "question": "How does BOLA incorporate Bayesian Optimization to optimize aggregation weights versus classic weight tuning approaches?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "BOLA",
            "Bayesian Optimization"
        ],
        "id": 148,
        "masked_question": "How does [mask1] incorporate Bayesian Optimization to optimize aggregation weights versus classic weight tuning approaches?",
        "masked_number": 1,
        "masked_elements": [
            "BOLA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.00614v1_figure_1.png",
        "paperid": "2409.00614v1",
        "paper_path": "./papers/2409.00614v1.json",
        "figure_id": "2409.00614v1_figure_1.png",
        "caption": "Figure 1. The overall framework of DAMe.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in (d) of Figure 1 is exactly our BOLA (Bayesian Optimization–based Local Aggregation) routine.  What it does differently from a “classic” fixed‐weight or grid‐search approach is the following:\n\n 1.  It turns the local/global mixing coefficient λ  \n     into a true hyper­parameter and defines as its objective  \n     the actual downstream SED score (e.g. NMI) you get on the client’s own hold-out data after blending the old local  \n     model θᵢ^{r–1} and the current global model θ_g^{r–1}:  \n        f(λ) = performance( λ·θᵢ^{r–1} + (1–λ)·θ_g^{r–1} ).  \n\n 2.  Because each evaluation of f(λ) requires re-training and testing on the client’s data, it is expensive, non-convex, and effectively a black box.  BOLA therefore fits a Gaussian Process (GP) surrogate  \n     to the pairs {λₖ, f(λₖ)} seen so far.  \n\n 3.  It then uses standard BO acquisition rules—here a mixture of Expected Improvement (EI) and Upper Confidence Bound (UCB)—to pick the next λ to try so as to trade off exploration (trying uncertain parts of the λ‐space) and exploitation (focusing where the GP believes performance is high).  \n\n 4.  By iterating this “GP fit → acquisition → true evaluation” loop for only a handful of λ’s, BOLA converges on a near-optimal mixing weight with far fewer model‐retraining runs than an exhaustive grid search or ad-hoc manual tuning.  \n\nIn contrast, a classic weight-tuning approach would pick λ by hand, do a brute-force grid search, or rely on heuristic decay schedules, none of which directly model the true evaluation surface f(λ) nor actively trade off exploration vs. exploitation.  BOLA’s use of a GP surrogate plus EI/UCB acquisition is what makes it both data-efficient and rigorous."
    },
    {
        "question": "What parallels exist between image guardrail optimization and adversarial training methodologies in computer vision?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "We optimize the safety guardrail with respect to unconstrained attack images (Qi et al., 2023), which can be seen as the worst-case scenario an MLLM can encounter in the real world as it is the most effective attack, allowing any pixel values in  after normalization. This optimization ensures robustness against both unconstrained and suboptimal (e.g., constrained) attacks.",
            "Since the additive noise  in Eq. (1  ###reference_###) is continuous and the loss function is differentiable with respect to , we employ the Projected Gradient Descent (PGD) algorithm (Madry et al., 2018  ###reference_b22###; Croce and Hein, 2019  ###reference_b6###) to compute the optimal image safety guardrail .",
            "The hyperparameter  is a distance constraint that controls the noise magnitude."
        ],
        "final_answer": "Image guardrail optimization mirrors adversarial training by explicitly crafting worst-case perturbations under a norm constraint and using Projected Gradient Descent (PGD) to find additive noise that improves robustness against both unconstrained and constrained attacks.",
        "relevant_elements": [
            "Image Guardrail"
        ],
        "id": 149,
        "masked_question": "What parallels exist between [mask1] optimization and adversarial training methodologies in computer vision?",
        "masked_number": 1,
        "masked_elements": [
            "Image Guardrail"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01703v1_figure_1.png",
        "paperid": "2411.01703v1",
        "paper_path": "./papers/2411.01703v1.json",
        "figure_id": "2411.01703v1_figure_1.png",
        "caption": "Figure 1: \\methodrobustifies multimodal large language models (MLLMs) against multimodal jailbreak attacks by using safety guardrails to purify malicious input prompt, ensuring safe responses.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. The red box in the figure surrounds the “Adv. Image + Image Guardrail” block, i.e. the learned additive noise that is added on top of an adversarial image.  \n2. In the text they describe exactly how this noise is optimized:  \n   – They posit a min–max style objective (minimize the model’s likelihood of producing any sentence in a harmful corpus) over adversarial images.  \n   – They constrain the noise by an L∞ (or similar) bound so that it does not destroy image integrity.  \n   – They solve it with PGD (Projected Gradient Descent), sampling different subsets of harmful examples each epoch for scalability.  \n3. This is structurally almost identical to standard adversarial training in vision:  \n   – Adversarial training also casts robustness as a min–max problem (worst‐case perturbations under a norm ball).  \n   – It uses PGD (or another gradient‐based attacker) to generate those worst‐case perturbations.  \n   – The model is then trained (or, here, the guardrail is tuned) on those perturbed examples to immunize against future attacks.  \n\nAnswer  \nThe optimization of the “image guardrail” noise (the red-boxed module) directly mirrors classical adversarial-training in vision. In both cases you:  \n • Define a min–max game over inputs and bounded perturbations  \n • Use PGD (gradient‐based attacks) to find the worst‐case noise under an L∞ (or similar) constraint  \n • Incorporate those perturbed examples into the learning loop (in adversarial training you update model weights; here you update the guardrail)  \n • Sample or cycle through different “attack” examples each epoch for efficiency  \n\nAll these steps make the image‐guardrail procedure mathematically and algorithmically parallel to standard adversarial-training methods used to make classifiers robust against worst-case pixel perturbations."
    },
    {
        "question": "How does text guardrail suffix optimization mirror existing gradient-based discrete token search techniques?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "To ensure full robustness, we jointly optimize a text safety guardrail G_t. Unlike image-based optimization, finding G_t requires discrete optimization. We adapt the gradient-based top-K token search algorithm (Shin et al., 2020; Qi et al., 2023) and begin by initializing G_t with random tokens of a fixed-length L. Subsequently, for each token g_i, we identify the top-K candidate tokens C_i as per reducing the generation probability of harmful content from the MLLM: ... and the gradient is taken with respect to the embedding of the i-th token g_i.",
            "The final step is to replace g_i with a token in C_i one by one and find the best token for a replacement as per reducing the loss. A single optimization step comprises updating all the tokens in G_t, and we repeat this process for multiple epochs (e.g., 50 times). The final G_t is appended at the end of the input text to act as a safety guardrail and robustify the MLLM against the jailbreak attack."
        ],
        "final_answer": "The text guardrail suffix optimization mirrors existing gradient-based discrete token search techniques by adapting the gradient-based top-K token search algorithm: it initializes the suffix as random tokens, uses gradients with respect to each token’s embedding to rank a top-K set of replacement candidates that reduce the probability of harmful outputs, iteratively replaces tokens with the best candidates, and repeats this update over multiple epochs before appending the optimized suffix to the input.",
        "relevant_elements": [
            "Text Guardrail"
        ],
        "id": 150,
        "masked_question": "How does [mask1] suffix optimization mirror existing gradient-based discrete token search techniques?",
        "masked_number": 1,
        "masked_elements": [
            "Text Guardrail"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01703v1_figure_1.png",
        "paperid": "2411.01703v1",
        "paper_path": "./papers/2411.01703v1.json",
        "figure_id": "2411.01703v1_figure_1.png",
        "caption": "Figure 1: \\methodrobustifies multimodal large language models (MLLMs) against multimodal jailbreak attacks by using safety guardrails to purify malicious input prompt, ensuring safe responses.",
        "qtype": "Literature_Background",
        "response": "First, let us pin down what “[mask1]” actually is in the figure.  The red box is around the little block labeled “[TEXT GUARDRAIL]” that gets tacked onto the end of every incoming text prompt.  In the paper this block is denoted Tᵗ, the “text safety guardrail,” and is exactly the discrete‐token suffix that they optimize to neutralize jailbreak attacks.\n\nNow—how do they optimize that suffix?  If you look back at § 3.2 (“Text Safety Guardrail”), they say in plain language:\n\n  1.  Fix a length L for your suffix and initialize it to L random tokens.  \n  2.  Compute a loss—here, the negative log‐probability that the MLLM generates any of the known “harmful” sentences—now viewed as a function of the suffix token embeddings.  \n  3.  Take the gradient of that loss w.r.t. each suffix token’s embedding.  \n  4.  For each position i, rank all possible vocabulary tokens by how well their embeddings line up with the descent direction (i.e. the top-K candidates that would most decrease the harmful‐content score).  \n  5.  Replace the old token at position i with the best of those K candidates.  \n  6.  Repeat steps 2–5 for multiple epochs (e.g. 50) until convergence.\n\nThis is exactly the same recipe used in prior work on gradient‐guided discrete trigger/search (e.g. Shin et al. 2020, Qi et al. 2023): turn your discrete tokens into embeddings, backpropagate to get a “score” for every vocabulary token, pick the top-K, do a discrete replacement, and loop.  Hence, the “suffix optimization” in this paper directly mirrors those existing gradient‐based discrete token search techniques."
    },
    {
        "question": "How does the Image Guardrail mitigate Adv. Image perturbations while preserving original visual features?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Inspired by this method, we aim to find an additive noise (i.e., the safety guardrail) via optimization that, when added to the adversarial image, minimizes the likelihood of generating harmful sentences (e.g., racism or terrorism) of a predefined corpus. These harmful sentences serve as few-shot examples, helping the MLLM recognize jailbreak attacks and making the optimized noise transferable across different attack scenarios.",
            "Formally, the image safety guardrail g is defined as: where g indicates the additive noise applied to the image that satisfies ‖g‖∞ ≤ ε.",
            "Since the additive noise g in Eq. (1) is continuous and the loss function is differentiable with respect to g, we employ the Projected Gradient Descent (PGD) algorithm (Madry et al., 2018; Croce and Hein, 2019) to compute the optimal image safety guardrail g.",
            "Once g is obtained, it can be added to any adversarial input image (e.g., x_adv + g) to neutralize adversarial effects.",
            "Importantly, g does not alter the integrity of images, even when applied to non-adversarial images, as g is upperbounded by ε."
        ],
        "final_answer": "The Image Guardrail is an optimized additive noise g found via Projected Gradient Descent under the constraint ‖g‖∞ ≤ ε. When added to an adversarial image, it neutralizes the attack’s perturbations by minimizing the model’s likelihood of generating harmful content, while the ε‐bound ensures the noise is small enough to preserve the original visual features.",
        "relevant_elements": [
            "Image Guardrail",
            "Adv. Image"
        ],
        "id": 151,
        "masked_question": "How does the [mask1] mitigate [mask2] perturbations while preserving original visual features?",
        "masked_number": 2,
        "masked_elements": [
            "Image Guardrail",
            "Adv. Image"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01703v1_figure_1.png",
        "paperid": "2411.01703v1",
        "paper_path": "./papers/2411.01703v1.json",
        "figure_id": "2411.01703v1_figure_1.png",
        "caption": "Figure 1: \\methodrobustifies multimodal large language models (MLLMs) against multimodal jailbreak attacks by using safety guardrails to purify malicious input prompt, ensuring safe responses.",
        "qtype": "Experimental_Results",
        "response": "Here is how the red-boxed module (the learned image “safety guardrail”) neutralizes the blue-boxed adversarial perturbations without destroying the picture you see:\n\n1. What the red box is doing  \n   • It is not a blurring or random jitter but a small, *learned* additive noise δ.  \n   • δ is found once and for all (offline) by running Projected Gradient Descent against a suite of “harmful” prompts, so that, when δ is added to any adversarial image, the MLLM’s chance of spitting out toxic or unsafe text drops drastically.\n\n2. Why it preserves the original image  \n   • During the PGD optimization, δ is kept within a tight L∞ bound (                  ε) so that its pixels never stray far from the genuine image values.  \n   • That norm-constraint makes δ essentially imperceptible to humans yet sufficiently “anti-adversarial” to cancel out the attack noise.\n\n3. Putting it together  \n   • At inference time you simply take the incoming (possibly adversarially perturbed) image xadv and feed xadv+δ into the MLLM.  \n   • The net effect is that whatever malicious noise was in xadv gets overwhelmed by δ’s counter-signal, but the human-viewable content of the picture remains unchanged."
    },
    {
        "question": "How does the diffusion process conditioned on text prompts improve skeleton-text alignment under noise?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Our framework leverages a conditional denoising diffusion process, not to generate data but to learn a discriminative skeleton latent space by fusing skeleton features with text prompts through the reverse diffusion process.",
            "Guided by our triplet diffusion (TD) loss, the denoising process conditions on text prompts to strengthen the discriminative fusion of skeleton features and their corresponding prompts.",
            "The TD loss encourages correct skeleton-text pairs to be pulled closer in the fused skeleton-text latent space while pushing apart incorrect pairs, enhancing the model’s discriminative power.",
            "The Diffusion Transformer  predicts noise  from noisy feature , conditioned on the global and local text features  and  at given timestep ."
        ],
        "final_answer": "By conditioning the reverse diffusion denoising step on both global and local text features (prompts), the model learns to fuse skeleton and text embeddings in a unified latent space. During training, the triplet diffusion loss pulls correct skeleton-text pairs closer together and pushes incorrect pairs apart under noise perturbations. As a result, even noisy skeleton features are denoised in a way that aligns them more tightly with their corresponding text prompts, improving skeleton-text alignment and discriminative power.",
        "relevant_elements": [
            "Text Encoder",
            "Diffusion Process"
        ],
        "id": 153,
        "masked_question": "How does the [mask1] conditioned on text prompts improve skeleton-text alignment under noise?",
        "masked_number": 1,
        "masked_elements": [
            "Diffusion Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.10745v2_figure_1.png",
        "paperid": "2411.10745v2",
        "paper_path": "./papers/2411.10745v2.json",
        "figure_id": "2411.10745v2_figure_1.png",
        "caption": "Figure 1: Motivation of the proposed TDSM pipeline. Previous methods rely on direct alignment between skeleton and text latent spaces, but suffer from modality gaps that limit generalization. Our TDSM overcomes this challenge by leveraging a reverse diffusion process to embed text prompts into the unified skeleton-text latent space, ensuring more effective cross-modal alignment.",
        "qtype": "Experimental_Results",
        "response": "Here is a step-by-step reasoning of what that red-boxed module is doing and why conditioning it on text prompts closes the skeleton–text gap even when heavy noise is injected:\n\n1. Identification of [mask1]  \n   – In the figure, the red box surrounds the module labeled “Diffusion Process.”  In the text this is the **reverse diffusion process**, implemented by the **Diffusion Transformer** that predicts the noise \\hat ε.  \n\n2. What the Diffusion Process does  \n   – During training we take a clean skeleton embedding zₛ and add Gaussian noise ε to it at a random timestep t (the forward process).  \n   – The reverse (diffusion) process is tasked with “denoising” that noisy zₛ by predicting ε_hat, and thereby recovering a clean skeleton feature.  \n\n3. Conditioning on text prompts  \n   – Crucially, every time we ask the network to predict ε_hat, we feed in not only the noisy skeleton but also the **global** and **local** text embeddings (zₚ) for both the correct label and a negative label.  \n   – These text embeddings enter the Diffusion Transformer via cross-modal attention and FiLM-style modulation in the CrossDiT blocks.  \n\n4. Why this improves alignment under noise  \n   a. Text guidance at every denoising step  \n      – By injecting semantic cues (the prompt embeddings) into each transformer block, the denoiser is forced to sculpt the skeleton latent so that it “looks like” the motion described by the prompt, even though the input is heavily corrupted.  \n   b. Triplet diffusion loss  \n      – We then apply a triplet-style loss on the predicted noises:  \n         • Pull the predicted ε_hat⁺ (with the correct prompt) close to the true noise ε  \n         • Push the predicted ε_hat⁻ (with an incorrect prompt) away by at least a margin  \n      – This explicitly encourages the network to only succeed in denoising when text and skeleton match, and to fail when they do not.  \n\n5. Resulting unified latent space  \n   – Over the course of training the diffusion steps, skeleton features and their matching text prompts are drawn together in the same subregion of latent space, while mismatched pairs are driven apart.  \n   – At test time, when an unseen skeleton comes in, you add the same fixed noise and see which candidate prompt yields the best ε_hat match.  Because the diffusion process learned to tightly couple (and discriminate) text and skeleton under noise, the correct label “lights up” as the best-matched prompt.  \n\nIn short, by **conditioning the reverse diffusion (the “Diffusion Process”) on the text embeddings** and pairing that with a **triplet diffusion loss**, TDSM learns to align skeleton and language features robustly—even when heavy noise is present—closing the modality gap that plagues direct-alignment methods."
    },
    {
        "question": "How did replacing direct alignment with diffusion-based alignment influence zero-shot generalization robustness?",
        "relevant_section_ids": [
            "3.4",
            "4.3"
        ],
        "relevant_context": [
            "Our approach enhances discriminative fusion through the TD loss, which is designed to denoise GT skeleton-text pairs effectively while preventing the fusion of incorrect pairs within the seen dataset. This selective denoising process promotes a robust fusion of skeleton and text features, allowing the model to develop a discriminative feature space that can generalize to unseen action labels.",
            "As shown in Table 1, our TDSM significantly outperforms the very recent state-of-the-art results across all benchmark splits, demonstrating superior generalization and robustness for various splits."
        ],
        "final_answer": "By replacing the previous direct alignment with our diffusion-based (denoising) alignment guided by the triplet diffusion loss, the model learns a more robust, discriminative fusion of skeleton and text features. This selective denoising mechanism yields a latent space that generalizes much better to unseen action classes, translating into significantly improved zero-shot recognition accuracy and overall robustness across multiple benchmark splits.",
        "relevant_elements": [
            "Direct Alignment",
            "Diffusion Process"
        ],
        "id": 154,
        "masked_question": "How did replacing [mask1] with [mask2] influence zero-shot generalization robustness?",
        "masked_number": 2,
        "masked_elements": [
            "Direct Alignment",
            "Diffusion Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10745v2_figure_1.png",
        "paperid": "2411.10745v2",
        "paper_path": "./papers/2411.10745v2.json",
        "figure_id": "2411.10745v2_figure_1.png",
        "caption": "Figure 1: Motivation of the proposed TDSM pipeline. Previous methods rely on direct alignment between skeleton and text latent spaces, but suffer from modality gaps that limit generalization. Our TDSM overcomes this challenge by leveraging a reverse diffusion process to embed text prompts into the unified skeleton-text latent space, ensuring more effective cross-modal alignment.",
        "qtype": "Experimental_Results",
        "response": "By swapping out the old red‐boxed “Direct Alignment” head for our blue‐boxed TDSM diffusion module, we went from a brittle one–to–one pull/push of skeleton and text embeddings to a fully unified skeleton–text latent space learned via conditional reverse diffusion and a triplet‐style loss.  In practice that substitution produces much stronger zero–shot generalization—on NTU-60/120 SynSE and PURLS splits we gain roughly 4–13 percentage‐points of top-1 accuracy over the prior best, and the model is far more stable to random noise at test time."
    },
    {
        "question": "What limitations do modality gaps impose on direct alignment between skeleton and text latent spaces?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "However, achieving the effective alignment between skeleton data and text features entails significant challenges. While skeleton data captures temporal and spatial motion patterns, the text descriptions for action labels carry high-level semantic information. This modality gap makes it difficult to align their corresponding latent spaces effectively, hindering the generalization learning for unseen actions—particularly when distinguishing between semantically similar actions (e.g., “Throw” vs. “Shoot”).",
            "The previous methods [...] aim at directly aligning the different features of skeletons and text descriptions for action labels between the skeleton and text latent spaces, which struggles to generalize due to the inherent differences between these modalities in different latent spaces."
        ],
        "final_answer": "The modality gap—skeleton features encoding temporal and spatial motion patterns versus text features encoding high-level semantics—makes it hard to effectively align their respective latent spaces, which in turn hinders generalization to unseen actions and makes it especially difficult to distinguish semantically similar actions.",
        "relevant_elements": [
            "Direct Alignment",
            "Skeleton Latent",
            "Text Latent"
        ],
        "id": 155,
        "masked_question": "What limitations do modality gaps impose on [mask1] between skeleton and text latent spaces?",
        "masked_number": 1,
        "masked_elements": [
            "Direct Alignment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10745v2_figure_1.png",
        "paperid": "2411.10745v2",
        "paper_path": "./papers/2411.10745v2.json",
        "figure_id": "2411.10745v2_figure_1.png",
        "caption": "Figure 1: Motivation of the proposed TDSM pipeline. Previous methods rely on direct alignment between skeleton and text latent spaces, but suffer from modality gaps that limit generalization. Our TDSM overcomes this challenge by leveraging a reverse diffusion process to embed text prompts into the unified skeleton-text latent space, ensuring more effective cross-modal alignment.",
        "qtype": "Others",
        "response": "Here, the red‐boxed module is the “Direct Alignment” step in prior ZSAR methods – i.e. simply pulling the skeleton embedding zₛ and the text embedding zₚ together in a shared space.  Because skeleton sequences encode low-level spatio-temporal motion patterns while text labels carry high-level, language­centric semantics, there is a large modality gap between them.  That gap makes naïve, one-to-one alignment brittle:  \n•  The two feature sets lie on very different manifolds, so a simple projection or contrastive loss can’t bridge their distributions.  \n•  As a result, embeddings of unseen or semantically similar actions (e.g. “Throw” vs. “Shoot”) tend to collapse or overlap, hurting zero-shot generalization and discriminative power.  \nIn short, the modality gap severely limits the effectiveness of direct alignment between skeleton and text latent spaces."
    },
    {
        "question": "What alternative alignment mechanisms could substitute the diffusion process for skeleton-text latent space fusion?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Most of the existing works focus on aligning the skeleton latent space with the text latent space. These approaches can be categorized broadly into VAE-based methods [47  ###reference_b47###, 16  ###reference_b16###, 30  ###reference_b30###, 32  ###reference_b32###] and contrastive learning-based methods [69  ###reference_b69###, 71  ###reference_b71###, 7  ###reference_b7###, 26  ###reference_b26###, 61  ###reference_b61###].",
            "VAE-based. The previous work, CADA-VAE [47], leverages VAEs [24] to align skeleton and text latent spaces, ensuring that each modality’s decoder can generate useful outputs from the other’s latent representation. SynSE [16] refines this by introducing separate VAEs for verbs and nouns, improving the structure of the text latent space. MSF [30] extends this approach by incorporating action and motion-level descriptions to enhance alignment. SA-DVAE [32] disentangles skeleton features into semantic-relevant and irrelevant components, aligning text features exclusively with relevant skeleton features for improved performance.",
            "Contrastive learning-based. Contrastive learning-based methods align skeleton and text features through positive and negative pairs [5]. SMIE [69] concatenates skeleton and text features and applies contrastive learning by treating masked skeleton features as positive samples and other actions as negatives. PURLS [71] incorporates GPT-3 [1] to generate text descriptions based on body parts and motion evolution, using cross-attention to align text descriptions with skeleton features. STAR [7] extends this idea with GPT-3.5 [1], generating text descriptions for six distinct skeleton groups and introduces learnable prompts to enhance alignment. DVTA [26] introduces a dual alignment strategy, performing direct alignment between skeleton and text features while also generating augmented text features via cross-attention for improved alignment. InfoCPL [61] strengthens contrastive learning by generating 100 unique sentences per action label, enriching the alignment space."
        ],
        "final_answer": "Instead of using the diffusion-based fusion process, one could fall back on direct latent-space alignment techniques, namely: (1) VAE-based alignment (e.g. CADA-VAE, SynSE, MSF, SA-DVAE) and (2) contrastive learning-based alignment (e.g. SMIE, PURLS, STAR, DVTA, InfoCPL).",
        "relevant_elements": [
            "Diffusion Process",
            "Unified Latent"
        ],
        "id": 156,
        "masked_question": "What alternative alignment mechanisms could substitute the [mask1] for skeleton-text [mask2] fusion?",
        "masked_number": 2,
        "masked_elements": [
            "Diffusion Process",
            "Unified Latent"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.10745v2_figure_1.png",
        "paperid": "2411.10745v2",
        "paper_path": "./papers/2411.10745v2.json",
        "figure_id": "2411.10745v2_figure_1.png",
        "caption": "Figure 1: Motivation of the proposed TDSM pipeline. Previous methods rely on direct alignment between skeleton and text latent spaces, but suffer from modality gaps that limit generalization. Our TDSM overcomes this challenge by leveraging a reverse diffusion process to embed text prompts into the unified skeleton-text latent space, ensuring more effective cross-modal alignment.",
        "qtype": "Others",
        "response": "First, let’s be precise about the two “masks” in the figure:\n\n  1. [mask1] is the red‐outlined block in the lower part of the pipeline – namely the “Diffusion Process” (the reverse‐diffusion / denoising U-Net module that takes noisy skeleton embeddings plus text prompts and gradually denoises them into a joint skeleton–text latent).  \n  2. [mask2] is the blue box around the previous state‐of-the-art “Skeleton Latent ↔ Text Latent” fusion – i.e. the *direct alignment* in latent space (often done by minimizing some distance between a skeleton encoder output and a text encoder output, or by contrastive losses that pull matching pairs together and push non‐matches apart).\n\nThe question asks:\n\n  “What alternative alignment mechanisms could substitute the [mask1] for skeleton–text [mask2] fusion?”\n\nIn other words:  “If we did **not** use the diffusion-based fusion block, what other ways of fusing (or aligning) skeleton and text latents might we plug in instead of direct latent‐space alignment?”\n\nHere is a non‐exhaustive list of popular alternatives that have appeared in the skeleton–text literature (or in related multimodal work) which could be dropped in to replace either the diffusion module or the simple direct‐alignment block:\n\n1.  Cross‐Attention Modules  \n    – Implement a two‐stream Transformer (one stream for skeleton tokens, one for text tokens) and insert *cross-attention* layers so that text features attend to skeleton features and vice-versa (as in PURLS [71] or STAR [7]).  \n    – This learns a richer, token-wise fusion than a single vector distance.  \n\n2.  VAE-Based Shared Latent Space  \n    – Train coupled VAEs (one VAE per modality) with a shared prior or a shared latent bottleneck so that both skeletons and text decode from the same latent.  \n    – Examples: CADA-VAE [47], SynSE [16], MSF [30], SA-DVAE [32].  \n\n3.  Contrastive Losses (InfoNCE, Triplet Loss)  \n    – Use an InfoNCE / NT-Xent loss to pull matching skeleton–text pairs together and push apart all others (SMIE [69], InfoCPL [61]).  \n    – Or directly apply a triplet loss on skeleton–text distances (skeleton anchor, positive text, negative text) to impose a margin.  \n\n4.  Adversarial / Domain‐Adversarial Alignment  \n    – Introduce a discriminator that tries to tell skeleton embeddings from text embeddings.  \n    – Train the encoders adversarially so that the discriminator cannot tell them apart, thus forcing the two distributions to align.  \n\n5.  Bilinear or Factorized Multimodal Pooling  \n    – Fuse skeleton and text features via multimodal bilinear pooling (MLB, MFB, MFH) or low-rank factorized pooling to capture second-order interactions before a final projection.  \n\n6.  Canonical Correlation Analysis (CCA) or Maximum Mean Discrepancy (MMD)  \n    – Enforce that the two sets of embeddings have maximal correlation (Deep CCA) or minimal kernel‐distance (MMD) in the shared space.  \n\n7.  Graph‐Based Alignment  \n    – Build a joint graph whose nodes are skeleton joint tokens and text tokens, then run a Graph Neural Network that propagates information across modalities.  \n\n8.  Prompt-Tuning / Adapter Layers  \n    – Keep both encoders frozen and insert small “fusion” adapters or prompt‐tuning vectors that mediate between the two latent spaces, learning to map one into the other.  \n\nAny one of these mechanisms (or a hybrid thereof) could substitute for the diffusion process in the red box, or for the simple direct‐alignment block in the blue box, to achieve skeleton–text fusion."
    },
    {
        "question": "What limitations arise from separating Content Injection and Style Injection steps?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Content Injection",
            "Style Injection"
        ],
        "id": 157,
        "masked_question": "What limitations arise from separating [mask1] and Style Injection steps?",
        "masked_number": 1,
        "masked_elements": [
            "Content Injection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.15007v1_figure_2.png",
        "paperid": "2410.15007v1",
        "paper_path": "./papers/2410.15007v1.json",
        "figure_id": "2410.15007v1_figure_2.png",
        "caption": "Figure 2. Overall framework of DiffuseST. The target branch is to perform style transfer guided by textual and spatial representations of two images. We adopt the BLIP-2 encoder to produce text-aligned features of the style image. We utilize the DDIM inversion technique and extract inner spatial features in the content and style branches, respectively. The content and style spatial injections are separated at different steps in the target branch to achieve balanced stylization.",
        "qtype": "Others",
        "response": "By splitting the U-Net inversion into two disjoint phases—one in which we only inject content features (the “content injection” phase) and a later one in which we only inject style features (the “style injection” phase)—we gain a simple way to bias early timesteps toward faithful layout and late timesteps toward rich texture.  Unfortunately that very separation comes at a cost:\n\n 1.  You now have to pick exactly where to hand off from content to style.  If you turn off style too early, you never get strong stylization; if you turn it on too soon, you wash out your content.  Getting a good balance requires manually tuning a split point (ρ) for each model, sometimes even for each style–content pair.  \n 2.  Because the two injections happen in non-overlapping chunks of the denoising schedule, you lose any ability to let content and style “co-evolve” in those middle timesteps.  In practice this can lead to visible boundary artifacts around the switch-over or to styles that look pasted on rather than organically blended.  \n 3.  It rests on the assumption that “early = structure” and “late = texture” holds universally in the pre-trained denoiser.  In reality, some mid-level features live in the gray zone, so a hard cut can misalign content and style cues that really ought to be fused gradually.  \n\nIn short, the rigid two-stage split gives you control but forces you into a brittle hand-tuning regime and can introduce artifacts whenever content and style really belong to the same denoising timestep."
    },
    {
        "question": "What alternative spatial feature extraction could replace DDIM inversion to improve content fidelity?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "DDIM inversion"
        ],
        "id": 158,
        "masked_question": "What alternative spatial feature extraction could replace [mask1] to improve content fidelity?",
        "masked_number": 1,
        "masked_elements": [
            "DDIM inversion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.15007v1_figure_2.png",
        "paperid": "2410.15007v1",
        "paper_path": "./papers/2410.15007v1.json",
        "figure_id": "2410.15007v1_figure_2.png",
        "caption": "Figure 2. Overall framework of DiffuseST. The target branch is to perform style transfer guided by textual and spatial representations of two images. We adopt the BLIP-2 encoder to produce text-aligned features of the style image. We utilize the DDIM inversion technique and extract inner spatial features in the content and style branches, respectively. The content and style spatial injections are separated at different steps in the target branch to achieve balanced stylization.",
        "qtype": "Others",
        "response": "Here’s the intuition in a nutshell:\n\n1. In the current design (the red‐boxed module), we take the style (or content) image, run a full DDIM inversion through the diffusion U-Net, and then pull out the intermediate feature maps as our “spatial” representation.  \n2. DDIM inversion is expensive and, more importantly, it wasn’t built to preserve fine‐grained content details—it was built to faithfully reproduce whole images under a diffusion prior.  \n3. A much lighter‐weight, content‐preserving alternative is simply to use the diffusion model’s first‐stage VAE encoder (or any off‐the‐shelf CNN encoder like VGG19) to extract spatial feature maps directly.  \n\nSo the short answer is:\n\nReplace the DDIM-inversion block with the latent code (or feature maps) coming straight out of the VAE encoder (or, equivalently, the intermediate convolutional features of a frozen CNN such as VGG19). These encoder features already carry very tight spatial structure from the original image—and incur essentially zero extra inversion cost—so you end up with noticeably sharper, more faithful content in the final stylized output."
    },
    {
        "question": "What is the rationale for separate content and style injection phases in the target branch?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Different steps of pre-trained diffusion models during the reverse process have been observed with varied functions (Meng et al., 2021; Xu et al., 2023; Zhang et al., 2023a), To produce a stylized image with balanced content and style, we are motivated to leverage the step-by-step nature of the diffusion model and separate the content and style injections in the target branch.",
            "Empirically, the early phase of the reverse process for the pre-trained diffusion model influences the overall structure and spatial layout, while the later phase focuses on more detailed visual effects. Therefore, we are inspired to perform the content injection in the early phase and the style injection in the later phase."
        ],
        "final_answer": "Because different denoising steps in diffusion affect different aspects of the image—early steps shape global structure and layout, while later steps add fine visual details—the method injects content in the early phase to preserve structure and injects style in the later phase to emphasize artistic details, achieving a balanced stylization.",
        "relevant_elements": [
            "content injection",
            "style injection",
            "target branch"
        ],
        "id": 159,
        "masked_question": "What is the rationale for separate [mask1] and [mask2] phases in the target branch?",
        "masked_number": 2,
        "masked_elements": [
            "content injection",
            "style injection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.15007v1_figure_2.png",
        "paperid": "2410.15007v1",
        "paper_path": "./papers/2410.15007v1.json",
        "figure_id": "2410.15007v1_figure_2.png",
        "caption": "Figure 2. Overall framework of DiffuseST. The target branch is to perform style transfer guided by textual and spatial representations of two images. We adopt the BLIP-2 encoder to produce text-aligned features of the style image. We utilize the DDIM inversion technique and extract inner spatial features in the content and style branches, respectively. The content and style spatial injections are separated at different steps in the target branch to achieve balanced stylization.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition, step by step:\n\n1.  In a diffusion model’s reverse pass, early timesteps set up the **coarse structure** and global layout of the image, while later timesteps progressively refine **fine details** and textures.\n\n2.  If you were to inject all your “content” and “style” information at once, the two signals would compete: strong style cues injected early can wash out your structural layout, and content cues injected too late may get overwritten by style.\n\n3.  By **splitting** the target‐branch denoising schedule into two phases—with **content injection** ([mask1], red) in the early steps and **style injection** ([mask2], blue) in the later steps—you align each signal with the part of the diffusion process that it is best suited to control:\n    •  Early steps + content injection → lock in structural details and spatial layout.  \n    •  Late steps + style injection → impose the artistic textures and color patterns without disturbing the already‐established content.\n\n4.  This separation yields a **balanced** stylization: the content remains faithful to the source shape and composition, while the style is free to flourish in the texture and detail layers."
    },
    {
        "question": "Why inject residual features for content yet key-value features for style in self-attention modules?",
        "relevant_section_ids": [
            "3.3.1",
            "3.3.2"
        ],
        "relevant_context": [
            "Note that the proposed content injection scheme involves two parts: (1) the residual replacement improves the preservation of high-frequency details, and (2) the attention replacement ensures consistency with the content image for the overall layout.",
            "To inject style, however, replacing key and value elements is more suitable to retain the content and encourage artistic detail in the generated image."
        ],
        "final_answer": "Injecting residual features during content injection helps preserve high-frequency content details and maintain the overall layout, while replacing the key and value in self-attention for style injection best retains the content structure and allows richer artistic (style) details to be infused.",
        "relevant_elements": [
            "residual features",
            "key-value features",
            "self-attention"
        ],
        "id": 160,
        "masked_question": "Why inject residual features for content yet key-value features for style in [mask1] modules?",
        "masked_number": 1,
        "masked_elements": [
            "self-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.15007v1_figure_2.png",
        "paperid": "2410.15007v1",
        "paper_path": "./papers/2410.15007v1.json",
        "figure_id": "2410.15007v1_figure_2.png",
        "caption": "Figure 2. Overall framework of DiffuseST. The target branch is to perform style transfer guided by textual and spatial representations of two images. We adopt the BLIP-2 encoder to produce text-aligned features of the style image. We utilize the DDIM inversion technique and extract inner spatial features in the content and style branches, respectively. The content and style spatial injections are separated at different steps in the target branch to achieve balanced stylization.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition behind the two different “injection” strategies:\n\n1.  Content preservation via residual + Q/K replacement  \n    - In the content‐injection module we want to lock in the exact spatial layout and fine details of the original image.  \n    - The residual connection (Δϕ) carries high‐frequency, pixel-level detail, so we “hook in” the content branch’s residual at each U-Net layer to preserve textures and edges.  \n    - At the same time, in the self-attention block we replace both Query (Q) and Key (K) with their counterparts coming from the content branch.  Since Q⋅Kᵀ literally defines which positions attend to which, forcing them to come from the content image locks the attention map to the original structure.  \n\n2.  Style infusion via K/V replacement  \n    - By the later denoising steps we want to paint in style–brush strokes, color palettes, brushiness–but keep the learned content layout intact.  \n    - In self-attention the Values (V) are what actually get mixed into the next layer’s feature, and the Keys (K) index which “value” gets picked.  By swapping in only the style branch’s K and V, we graft the style feature patterns onto the existing content queries, so that the content layout (encoded in the Query) remains undisturbed but the appearance (encoded in the Values) is replaced with style.  \n\nIn short, residual + Q/K replacement “hard-locks” in content structure, whereas K/V replacement “paints” style textures onto that fixed structure."
    },
    {
        "question": "How does the y2m generator mapping preserve spectral features during young-to-middle Mel-spectrogram conversion?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "CycleGAN-VC3 model introduces a Time-Frequency Adaptive Normalisation (TFAN) module, which is an extension of the traditional instance normalisation, allowing for a more fine-grained tuning of the features in the time-frequency dimension while preserving the information of the source spectrogram.",
            "The generator first converts the input Mel-spectrogram into 128 channels via a 2D convolutional layer, followed by two downsampling layers... During this process, the TFAN module is applied to maintain consistency across time and frequency.",
            "During the upsampling phase, the model restores the spatial resolution and increases the number of channels to 1024, with another application of the TFAN module to ensure feature retention.",
            "The cycle consistency loss ensures that after mapping samples from one domain to another and back, the original data is preserved, enforcing consistency across domain transformations.",
            "The identity loss is used to preserve input characteristics when the input already belongs to the target domain, ensuring that the generator does not alter the data unnecessarily."
        ],
        "final_answer": "The y2m generator uses Time-Frequency Adaptive Normalisation modules in both down- and up-sampling stages to adaptively normalise and re-inject the original spectral details across the time–frequency plane. In addition, cycle consistency loss forces the mapped spectrogram to reconstruct back to its original form, and an identity loss penalises any unnecessary changes when the input already matches the middle-aged domain. Together, these mechanisms ensure that key spectral features are preserved during young-to-middle conversion.",
        "relevant_elements": [
            "y2m generator",
            "Mel-spectrogram"
        ],
        "id": 163,
        "masked_question": "How does the [mask1] mapping preserve spectral features during young-to-middle [mask2] conversion?",
        "masked_number": 2,
        "masked_elements": [
            "y2m generator",
            "Mel-spectrogram"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.11120v1_figure_1.png",
        "paperid": "2410.11120v1",
        "paper_path": "./papers/2410.11120v1.json",
        "figure_id": "2410.11120v1_figure_1.png",
        "caption": "Figure 1: Framework of kinship verification using age voice conversion.",
        "qtype": "Implementation_Details",
        "response": "Here “mask1” is the little box labelled y2m in the red‐outlined Age Conversion stage (i.e. the young→middle generator), and “mask2” is the conversion you perform on the voice (young→middle voice conversion).  In our CycleGAN‐VC3 setup that y2m block preserves the fine spectral structure by means of:  \n1.  A Time–Frequency Adaptive Normalisation (TFAN) module inside each layer that only shifts the high‐level “age style” statistics in the Mel‐spectrogram while leaving the local time–frequency patterns (formants, harmonics, etc.) untouched.  \n2.  A cycle-consistency loss that forces the network, once you map young→middle→young, to recover the exact original, so any changes must be strictly those needed to make the voice sound “middle-aged.”  \n3.  An identity loss on samples already in the middle-aged domain, which discourages the generator from altering any spectral detail that does not contribute to the age shift.  \nTogether, TFAN plus the cycle and identity losses ensure that the y2m mapping only injects the minimal age‐related spectral shifts and otherwise leaves the original voice’s timbre and spectral features intact."
    },
    {
        "question": "How does the HiFiGAN vocoder synthesise waveform fidelity from the generated Mel-spectrogram outputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "HiFiGAN vocoder",
            "generated Mel-spectrogram"
        ],
        "id": 164,
        "masked_question": "How does the [mask1] synthesise waveform fidelity from the [mask2] outputs?",
        "masked_number": 2,
        "masked_elements": [
            "HiFiGAN vocoder",
            "generated Mel-spectrogram"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.11120v1_figure_1.png",
        "paperid": "2410.11120v1",
        "paper_path": "./papers/2410.11120v1.json",
        "figure_id": "2410.11120v1_figure_1.png",
        "caption": "Figure 1: Framework of kinship verification using age voice conversion.",
        "qtype": "Implementation_Details",
        "response": "mask1: HiFi-GAN vocoder  \nmask2: the age‐converted Mel-spectrograms  \n\nChain of thought:  \n1. In the age conversion step (red box) our CycleGAN-VC3 generator produces Mel-spectrograms that lie in the “middle-aged” domain.  \n2. These Mel-spectrograms are not waveforms but time–frequency representations. To get back to a listenable signal we need a neural vocoder.  \n3. The content in the red box is exactly that vocoder: a pre-trained HiFi-GAN. Its role is to turn the spectrogram into a high-quality time-domain waveform.  \n4. It does so by conditioning its generator network on the input spectrogram, using learned up-sampling (transposed convolutions), residual blocks, and adversarial training (multi-scale discriminators) to ensure the reconstructed audio matches both the spectral targets and perceptual realism.  \n\nAnswer:  \nThe HiFi-GAN vocoder takes the age-converted Mel-spectrograms as its input and, via a GAN-based generator with learned up-sampling layers and residual blocks (and judged by multi-scale discriminators), synthesises a time-domain waveform whose spectral detail faithfully reproduces the input spectrogram, yielding high-fidelity speech."
    },
    {
        "question": "How does the Emoji Recommendation Process aggregate outputs from multiple LLMs for a single benchmark dataset entry?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Emoji Recommendation Process",
            "Benchmark Dataset"
        ],
        "id": 165,
        "masked_question": "How does the [mask1] aggregate outputs from multiple LLMs for a single [mask2] entry?",
        "masked_number": 2,
        "masked_elements": [
            "Emoji Recommendation Process",
            "Benchmark Dataset"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.10760v1_figure_2.png",
        "paperid": "2409.10760v1",
        "paper_path": "./papers/2409.10760v1.json",
        "figure_id": "2409.10760v1_figure_2.png",
        "caption": "Figure 2: Overview of the Semantics Preserving Emoji Recommendation Framework. Left side: The Emoji Recommendation Process uses large language models to recommend three emojis for texts from the benchmark dataset. Right side: The Semantics Preserving Evaluation Process compares text + predicted emojis with text + ground truth emojis across 5 selected semantic dimensions, including sentiment, emotion, stance, age, and gender.",
        "qtype": "Implementation_Details",
        "response": "In our pipeline the red‐boxed “Emoji Recommendation Process” simply fans a single benchmark post out to every LLM in turn, pulls back each model’s three suggested emojis, and then lines them up side‐by‐side for that one post. In other words, for one “text + ground-truth” entry we:\n\n 1. Send the same text to each LLM (ChatGPT, Google’s Gemini, Meta’s LLaMa, etc.).  \n 2. Receive each model’s top-3 emoji recommendations.  \n 3. Aggregate those individual 3-emoji sets by concatenating or displaying them in parallel under the original text.  \n\nThat gives us one block of “Text + Recommended Emojis” containing all of the LLMs’ outputs for a single benchmark entry."
    },
    {
        "question": "How does the Semantics Preserving Evaluation Process integrate sentiment and emotion label mismatches into final preservation scoring?",
        "relevant_section_ids": [
            "3",
            "3.3"
        ],
        "relevant_context": [
            "Next, we use the same classifier to assign labels to the modified sentences that combine the original plain text x with the recommended emojis ê. The classification output for the sentence with the recommended emojis is represented as ŷ. For each downstream task i, the semantics preserving capability of the model is evaluated by comparing whether ŷ_i equals the ground truth y_i.",
            "The matching pairs represent the number of labels that are the same across the five downstream tasks, with value ranging from 0 to 5. We use the proportion of correctly matched labels as the downstream task-based semantics preservation score, as shown in Table IV."
        ],
        "final_answer": "Sentiment and emotion are treated as two of the five downstream tasks. If the sentiment or emotion label predicted on the text with recommended emojis does not match the label predicted on the text with the original emojis, that task is counted as a mismatch. The total number of matching labels across all five tasks (including sentiment and emotion) is then divided by five to produce the final semantics preservation score.",
        "relevant_elements": [
            "Semantics Preserving Evaluation Process",
            "Sentiment",
            "Emotion"
        ],
        "id": 166,
        "masked_question": "How does the [mask1] integrate sentiment and emotion label mismatches into final preservation scoring?",
        "masked_number": 1,
        "masked_elements": [
            "Semantics Preserving Evaluation Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.10760v1_figure_2.png",
        "paperid": "2409.10760v1",
        "paper_path": "./papers/2409.10760v1.json",
        "figure_id": "2409.10760v1_figure_2.png",
        "caption": "Figure 2: Overview of the Semantics Preserving Emoji Recommendation Framework. Left side: The Emoji Recommendation Process uses large language models to recommend three emojis for texts from the benchmark dataset. Right side: The Semantics Preserving Evaluation Process compares text + predicted emojis with text + ground truth emojis across 5 selected semantic dimensions, including sentiment, emotion, stance, age, and gender.",
        "qtype": "Implementation_Details",
        "response": "In the red‐boxed “Semantics Preserving Evaluation” module, sentiment and emotion are treated exactly like any of the other five downstream labels:  \n  1.  They run the original text + ground‐truth emojis through the classifier to get a “gold” label for sentiment and for emotion (along with stance, age, gender).  \n  2.  They run the text + recommended emojis through the same classifier to get a “predicted” label for each of the five tasks.  \n  3.  For each task—sentiment, emotion, stance, age, gender—they assign a 1 if the predicted label matches the gold label, and 0 otherwise.  Any mismatch in sentiment or in emotion thus contributes a 0 in that row.  \n  4.  The final semantics‐preservation score for that example is simply the sum of those five match–flags (i.e. the number of “greens”) divided by 5.  \n\nSo whenever the recommended emojis cause the classifier’s sentiment or emotion choice to flip, that dimension is counted as a mismatch (red cell) and directly lowers the overall preservation score."
    },
    {
        "question": "How might the Sentiment module integration within the Semantics Preserving Evaluation Process build upon established sentiment analysis methods?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "For each sentence with the predicted or ground truth emojis, we use a strong pre-trained classifier H(·) to infer labels on five downstream tasks. … For each downstream task t, the semantics preserving capability of the model is evaluated by comparing whether ŷ_t equals the ground truth y_t.",
            "Emojis often serve as indicators of sentiment analysis [26, 27], so maintaining the sentiment label after replacing the original emoji with a recommended one is crucial for semantics preservation. For this task, we use sentiment labels such as positive, negative, and neutral to evaluate consistency."
        ],
        "final_answer": "They treat sentiment analysis as one of the evaluation’s downstream tasks, applying a strong pre‐trained sentiment classifier (e.g. GPT-4o-mini) to assign standard sentiment labels (positive/negative/neutral) to both the original text + ground-truth emojis and the text + recommended emojis. By directly comparing whether those sentiment labels match, the framework builds on established sentiment analysis methods—using conventional sentiment categories and existing classification models—to measure how well the emoji recommendation preserves the original sentiment.",
        "relevant_elements": [
            "Sentiment",
            "Semantics Preserving Evaluation Process"
        ],
        "id": 167,
        "masked_question": "How might the [mask1] module integration within the [mask2] build upon established sentiment analysis methods?",
        "masked_number": 2,
        "masked_elements": [
            "Sentiment",
            "Semantics Preserving Evaluation Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.10760v1_figure_2.png",
        "paperid": "2409.10760v1",
        "paper_path": "./papers/2409.10760v1.json",
        "figure_id": "2409.10760v1_figure_2.png",
        "caption": "Figure 2: Overview of the Semantics Preserving Emoji Recommendation Framework. Left side: The Emoji Recommendation Process uses large language models to recommend three emojis for texts from the benchmark dataset. Right side: The Semantics Preserving Evaluation Process compares text + predicted emojis with text + ground truth emojis across 5 selected semantic dimensions, including sentiment, emotion, stance, age, and gender.",
        "qtype": "Literature_Background",
        "response": "Let’s unpack the two masks first.  The little red box is simply around the Sentiment column in the evaluation table—so [mask1] = “Sentiment.”  The big blue box encloses the entire right‐hand side of Fig. 2, i.e. the Semantics Preserving Evaluation Process—so [mask2] = “Semantics Preserving Evaluation Process.”\n\nChain of thought:  \n1. In our framework we recommend emojis for a piece of text and then want to check that we haven’t changed its meaning.  \n2. One key facet of “meaning” on social media is whether the post’s overall polarity (positive/neutral/negative) remains the same.  That is exactly what the Sentiment module measures.  \n3. To do so, we rely on a strong, pre-trained LLM classifier, H(·), (in practice GPT-4o-mini) which we prompt to label each sentence (plain text + emojis) as positive, neutral, or negative.  \n4. We first run H(·) on the original text + ground-truth emojis to get our “gold” sentiment label, then run H(·) again on text + recommended emojis.  If the two labels match, we count sentiment as preserved.  \n5. In this way we are directly building on decades of work in sentiment analysis—using the familiar three‐way polarity scheme and treating sentiment as a downstream classification task—but updating it by:  \n   • Plugging in a modern LLM as our classifier rather than a small fine-tuned BERT or a lexicon lookup.  \n   • Applying it to mixed text + emoji inputs to explicitly test the impact of emoji substitution on measured sentiment.  \n   • Integrating that single‐number “did the polarity change?” check into a broader, multi‐task semantics‐preservation score.\n\nSo in short: the Sentiment module is exactly the classic positive/neutral/negative sentiment classifier, except that here it’s implemented via a zero-temperature GPT prompt and used as a “consistency check” before and after emoji replacement inside the overall Semantics Preserving Evaluation Process."
    },
    {
        "question": "How does the Stance dimension integration within the Semantics Preserving Evaluation Process relate to attitude detection methodologies?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Stance detection is about identifying the author’s position or attitude towards a topic.",
            "Emojis can modify or reinforce the stance expressed in a sentence, so it is essential that the recommended emojis preserve the stance conveyed by the original text [29].",
            "We classify stance using the labels none, favor, and against."
        ],
        "final_answer": "Within the semantics preserving evaluation, the Stance dimension is implemented via a stance detection task—an attitude detection methodology that assesses whether recommended emojis preserve the author’s attitude or position toward a topic. It does this by classifying each post into one of three attitudinal categories: none, favor, or against.",
        "relevant_elements": [
            "Stance",
            "Semantics Preserving Evaluation Process"
        ],
        "id": 168,
        "masked_question": "How does the [mask1] dimension integration within the [mask2] relate to attitude detection methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Stance",
            "Semantics Preserving Evaluation Process"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.10760v1_figure_2.png",
        "paperid": "2409.10760v1",
        "paper_path": "./papers/2409.10760v1.json",
        "figure_id": "2409.10760v1_figure_2.png",
        "caption": "Figure 2: Overview of the Semantics Preserving Emoji Recommendation Framework. Left side: The Emoji Recommendation Process uses large language models to recommend three emojis for texts from the benchmark dataset. Right side: The Semantics Preserving Evaluation Process compares text + predicted emojis with text + ground truth emojis across 5 selected semantic dimensions, including sentiment, emotion, stance, age, and gender.",
        "qtype": "Literature_Background",
        "response": "Here, the red‐boxed “Stance” column is simply one of the five downstream classifiers in the blue‐boxed Semantics Preserving Evaluation Process. In practice, a pretrained stance detector (i.e. an attitude‐detection model that labels text as none/favor/against) is run on the original text + ground‐truth emojis and again on the text + recommended emojis. By checking that the stance label remains the same, the framework borrows directly from standard attitude‐detection (or stance‐detection) methodologies to ensure the recommended emojis haven’t altered the author’s underlying attitude."
    },
    {
        "question": "How does the Imitation Learning phase extend standard imitation frameworks for multimodal web navigation?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "We utilize GPT-4o along with the WebVoyager paradigm (He et al., 2024) to generate web navigation trajectories corresponding to the above queries. The agent is named WebVoyager-4o and configured to receive observations consisting of the latest  steps, including the accessibility trees and screenshots.",
            "It is worth noting that we preserve the thought and action of each step to maintain the full reasoning process without occupying excessive context.",
            "The collected trajectories fall into three pre-defined categories: unfinished (exceeding the maximum iteration of Navigation), finished & unsuccessful, and finished & successful. In this stage, to better distill knowledge from GPT-4o, we filter out unfinished trajectories, retaining only the other ones for training in Imitation Learning. Meanwhile, we resample the unfinished tasks once to improve the utilization of queries and reduce the problem of navigation failure due to sampling randomness.",
            "We adopt Idefics2 (Laurençón et al., 2024) to learn trajectories collected through WebVoyager-4o. In Idefics2, screenshots are encoded as 64 visual tokens. However, the length of each accessibility tree is typically way longer than 64 tokens.\nConsidering the sequence length issue, we have to further truncate the context and the number of images, retaining the latest  images while keeping only one accessibility tree of the current page.",
            "Through Imitation Learning, the agent has already learned the basic operation logic and response format, so there is no need for the system prompt."
        ],
        "final_answer": "The Imitation Learning phase extends standard imitation frameworks by (1) using a GPT-4o–powered “WebVoyager-4o” to generate multimodal trajectories that include both screenshots and accessibility trees, (2) preserving the full thought-and-action chain at each step to capture reasoning, (3) filtering out only fully executed (successful or failed) trajectories and resampling unfinished ones to maximize data utility, and (4) adapting to the open-source Idefics2 model’s context limits by truncating long accessibility trees and images while discarding the system prompt once the response format is learned.",
        "relevant_elements": [
            "Imitation Learning"
        ],
        "id": 169,
        "masked_question": "How does the [mask1] phase extend standard imitation frameworks for multimodal web navigation?",
        "masked_number": 1,
        "masked_elements": [
            "Imitation Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.19609v1_figure_1.png",
        "paperid": "2410.19609v1",
        "paper_path": "./papers/2410.19609v1.json",
        "figure_id": "2410.19609v1_figure_1.png",
        "caption": "Figure 1: The overall process of OpenWebVoyager, including the Imitation Learning phase and the exploration-feedback-optimization cycles. The agent learns basic multimodal web navigation skills through Imitation Learning and continues to explore real-world web environments. GPT-4o provides feedback on explored multimodal trajectories, leaving successful trajectories for the agent to improve.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed phase in Figure 1 is simply their Imitation Learning (IL) stage – but it is *not* “vanilla” behavior cloning on raw click‐streams.  What makes it an extension over a standard BC framework is:\n\n 1.  Rich, multimodal observations.  \n    –  Instead of just feeding the model DOM‐like trees or sequence‐of-clicks, every step presents the agent with a screenshot *and* its full accessibility tree.  This gives the model both pixel and structured‐text views of the page.  \n\n 2.  Full “thought” trajectories, not just (S,A) pairs.  \n    –  They keep GPT-4o’s entire ReAct‐style reasoning chain – the internal “thought” plus the “action” at each time step – and train on that.  Standard BC on web navigation usually only copies actions.  \n\n 3.  GPT-4o–driven expert trajectories.  \n    –  Instead of collecting human or random synthetic traces, they use WebVoyager-4o (GPT-4o in the loop) to generate a diverse set of finished and successful demos.  They then filter out unfinished paths to focus on high‐quality examples.  \n\n 4.  A vision–language backbone (Idefics2) that can handle high-resolution web screenshots (up to 980×980px) interleaved with tree tokens.  \n    –  They explicitly tailor the IL objective to this architecture, clipping context (latest K images plus the current tree) to work within sequence‐length limits.  \n\nIn short, their IL phase “extends” standard imitation by (a) teaching an LMM to consume *both* screenshots and accessibility trees, (b) training on *chain-of-thought* demonstrations from GPT-4o, and (c) carefully filtering and truncating those trajectories so that the open-source Idefics2 model can learn real multimodal web navigation skills."
    },
    {
        "question": "How does Self-Explore Learning integrate Multimodal Trajectories compared to classic online exploration strategies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Self-Explore Learning",
            "Multimodal Trajectories"
        ],
        "id": 170,
        "masked_question": "How does [mask1] integrate [mask2] compared to classic online exploration strategies?",
        "masked_number": 2,
        "masked_elements": [
            "Self-Explore Learning",
            "Multimodal Trajectories"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.19609v1_figure_1.png",
        "paperid": "2410.19609v1",
        "paper_path": "./papers/2410.19609v1.json",
        "figure_id": "2410.19609v1_figure_1.png",
        "caption": "Figure 1: The overall process of OpenWebVoyager, including the Imitation Learning phase and the exploration-feedback-optimization cycles. The agent learns basic multimodal web navigation skills through Imitation Learning and continues to explore real-world web environments. GPT-4o provides feedback on explored multimodal trajectories, leaving successful trajectories for the agent to improve.",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning, step by step:\n\n1.  Identifying the masks  \n    -  From the figure caption and the red box around the top row, **[mask1]** is the “Self-Explore Learning” phase.  \n    -  From the blue box immediately below it, **[mask2]** is the collection of “Multimodal Trajectories” (i.e. screenshot + accessibility tree observations, thoughts, and actions).\n\n2.  What Self-Explore Learning does  \n    -  After an initial Imitation-Learning pass, the agent enters a loop of  \n       1) synthesizing new queries (via self-instruct)  \n       2) exploring the real web environment with its current policy  \n       3) collecting full **multimodal trajectories** of (observation, thought, action)  \n       4) submitting each trajectory to GPT-4o as an “auto-evaluator”  \n       5) retaining only those trajectories judged “successful”  \n       6) merging those back into the training set to fine-tune the model again  \n\n3.  How classic online exploration differs  \n    -  Traditional exploration in RL or LLM-based agents usually treats exploration separately from learning. You choose actions according to an epsilon-greedy rule, UCB bonus, Thompson sampling, etc., gather scalar reward signals, and then update a value‐ or policy‐based learner.  \n    -  In that paradigm the agent never actually collects or re-reasons over high-fidelity, multimodal state–action–thought traces. You only see “I took action A in state S, got reward R.”\n\n4.  The key contrast  \n    -  In **Self-Explore Learning**, exploration _is_ learning: every time the agent ventures onto a real webpage it produces a rich, multimodal trajectory that is then _directly_ fed back—after GPT-4o validation—into the next training iteration.  \n    -  In classic online exploration you typically only accumulate scalar experiences and use them to update a value function or policy, without ever reusing the full visual + DOM + reasoning trace as supervised training data.\n\nAnswer  \nHow does Self-Explore Learning integrate multimodal trajectories compared to classic online exploration strategies?\n\n“It tightly couples exploration and learning by harvesting entire multimodal trajectories (screenshots + DOM tree + chain-of-thought+action) on each rollout, filtering them via GPT-4o, and then directly fine-tuning the model on those successful episodes. Classic online exploration, by contrast, separates exploration (ε‐greedy, UCB, etc.) from learning and relies only on scalar reward transitions rather than replaying and imitating high-dimensional, multimodal trajectories.”"
    },
    {
        "question": "How does Imitation Learning influence the quality of multimodal trajectories used in Self-Explore Learning?",
        "relevant_section_ids": [
            "3.4",
            "3.5"
        ],
        "relevant_context": [
            "In this stage, to better distill knowledge from GPT-4o, we filter out unfinished trajectories, retaining only the other ones for training in Imitation Learning.",
            "Through Imitation Learning, the agent has already learned the basic operation logic and response format, so there is no need for the system prompt.",
            "After the Imitation Learning phase, the trained agent θ0 will proceed to explore websites and undergo multiple cycles of exploration-feedback-optimization.",
            "At each exploration-feedback-optimization cycle, we employ trajectory-level rejection sampling via GPT-4o to ensure quality trajectories."
        ],
        "final_answer": "By first training on only finished, GPT-4o–generated multimodal trajectories (i.e., filtering out incomplete ones) and internalizing the proper thought–action format, Imitation Learning gives the base agent the basic operation logic and response structure.  As a result, when the agent begins self-exploration, it produces more coherent, correctly structured multimodal trajectories that can be reliably filtered and optimized in subsequent exploration–feedback cycles.",
        "relevant_elements": [
            "Imitation Learning",
            "Multimodal Trajectories",
            "Self-Explore Learning"
        ],
        "id": 171,
        "masked_question": "How does [mask1] influence the quality of [mask2] used in Self-Explore Learning?",
        "masked_number": 2,
        "masked_elements": [
            "Imitation Learning",
            "Multimodal Trajectories"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.19609v1_figure_1.png",
        "paperid": "2410.19609v1",
        "paper_path": "./papers/2410.19609v1.json",
        "figure_id": "2410.19609v1_figure_1.png",
        "caption": "Figure 1: The overall process of OpenWebVoyager, including the Imitation Learning phase and the exploration-feedback-optimization cycles. The agent learns basic multimodal web navigation skills through Imitation Learning and continues to explore real-world web environments. GPT-4o provides feedback on explored multimodal trajectories, leaving successful trajectories for the agent to improve.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step-by-step reasoning:\n\n1.  Identify the masks  \n    -  mask1 (red box) is “Imitation Learning.”  \n    -  mask2 (blue box) is “Multimodal Trajectories.”\n\n2.  What does Imitation Learning do?  \n    -  In the first phase, we use GPT-4o to generate high‐quality demonstration trajectories.  \n    -  We train the base LMM (Idefics2) on these demonstrations so that the agent acquires basic web‐navigation logic, the ReAct thought–action format, and an understanding of the multimodal observation (screenshot + accessibility tree).\n\n3.  How do these initial demonstrations affect later exploration?  \n    -  The weights θIL learned in the IL phase become the starting policy for all subsequent self-exploration cycles.  \n    -  A stronger IL phase (i.e. more diverse, successful, and well-formatted demonstrations) gives the agent better priors  \n       •  it “knows” how to read a page, form coherent thoughts, pick sensible actions, and terminate correctly.  \n       •  it therefore produces cleaner, more successful, and more diverse multimodal trajectories when it goes off on its own.\n\n4.  Conclusion  \n    Imitation Learning directly governs the initial policy quality; this in turn determines how coherent and successful the self-explored multimodal trajectories will be.  \n\nAnswer:  \nImitation Learning provides the seed demonstrations and parameter initialization that the agent builds on. A higher‐quality IL phase (better GPT-4o demos, better filtering) yields a stronger base policy, which in turn generates higher‐quality multimodal trajectories during the later self-exploration stages."
    },
    {
        "question": "How does incorporating screenshot and accessibility tree observations shape thought and action generation?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "In this work, we adopt the vision-language setting that the observation in each step will include an accessibility tree and a screenshot, i.e., o_t = (tree_t, image_t). Let θ represents the parameters of the Large Multimodal Models (LMMs). Following the ReAct paradigm, we derive thoughts and actions using LMMs: (thought_t, action_t) = LMM(θ; system_prompt, o_1, …, o_t), where system_prompt denotes the system prompt, including answer formats, the introduction of web operations and some guidelines.",
            "Similar to the messages fed into GPT-4o, we embed the <image> token at the corresponding position in the context, aligning it with the accessibility tree. The Idefics2-based agent will make a decision based on the observation containing multimodal information."
        ],
        "final_answer": "By treating each web state as a pair of the screenshot and its accessibility tree, the agent feeds both modalities into the LMM under a ReAct-style prompt.  Concretely, the multimodal observation (tree_t, image_t) is passed—alongside the system prompt—into the model, which then jointly reasons over visual and structural cues to generate its internal thought and choose the next action.",
        "relevant_elements": [
            "Screenshot & Accessibility Tree",
            "Thought",
            "Action"
        ],
        "id": 172,
        "masked_question": "How does incorporating [mask1] observations shape [mask2] and action generation?",
        "masked_number": 2,
        "masked_elements": [
            "Screenshot & Accessibility Tree",
            "Thought"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.19609v1_figure_1.png",
        "paperid": "2410.19609v1",
        "paper_path": "./papers/2410.19609v1.json",
        "figure_id": "2410.19609v1_figure_1.png",
        "caption": "Figure 1: The overall process of OpenWebVoyager, including the Imitation Learning phase and the exploration-feedback-optimization cycles. The agent learns basic multimodal web navigation skills through Imitation Learning and continues to explore real-world web environments. GPT-4o provides feedback on explored multimodal trajectories, leaving successful trajectories for the agent to improve.",
        "qtype": "Experimental_Results",
        "response": "Chain of thought  \n1. In the OpenWebVoyager agent loop each step consists of  \n   – an Observation  (what the agent “sees” on the page)  \n   – a Thought      (the agent’s internal reasoning about what to do next)  \n   – an Action       (the click/type operation it issues)  \n\n2. In the paper the red‐boxed observation is the pair “Screenshot & Accessibility Tree” – i.e. the visual snapshot of the page plus its DOM structure in an accessible‐tree form.  \n\n3. That multimodal observation is fed directly into the LMM backbone (Idefics2), so the model’s subsequent Thought (blue‐boxed) is now grounded in both the pixel layout and the structural/textual cues of the page.  \n\n4. Concretely, having the screenshot lets the model see visual affordances (buttons, banners, forms), while the accessibility tree provides explicit element labels, hierarchy, and attributes.  \n\n5. This richer input sculpts the agent’s reasoning in two ways:  \n   a. It narrows down which elements are relevant to the current goal (so the Thought can more precisely reference “click the ‘Add to Cart’ button under the Product Details” rather than issuing a vague “click button”).  \n   b. It enables the model to plan multi‐step strategies (e.g. “first open the menu icon in the header, then select ‘Account & Lists’”) by fusing visual layout and DOM structure.  \n\n6. Once the Thought is generated under this multimodal grounding, parsing that thought in the prescribed ReAct format produces much more accurate Actions (click, type, scroll) on the correct page elements.  \n\nAnswer  \nBy incorporating the red‐boxed “Screenshot & Accessibility Tree” observations, the agent’s subsequent blue‐boxed Thoughts become tightly grounded in both what the page looks like and how its elements are structured. That richer multimodal input lets the LMM produce far more precise internal reasoning (“Thoughts”) about which element to interact with, and when that thought is translated into an Action it leads to more accurate clicks, typing, or navigation."
    },
    {
        "question": "How does applying multi-expert adapters exclusively in the last P blocks influence feature adaptation flexibility?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "By this, the multi-expert mechanism enables the network to develop powerful capabilities to flexibly handle the data from new and old classes.",
            "According to previous research, allocating more adapter experts in higher blocks enhances the effectiveness of models compared with inserting them in the lower blocks. Hence, the MEA structure is incorporated only in the last P blocks out of the total L blocks."
        ],
        "final_answer": "By concentrating the multi-expert adapters in the final P Transformer blocks—where the representations are most abstract—the model gains strong, flexible adaptation capacity. In these deeper layers the branch-wise experts can specialize on old versus new class patterns without disturbing earlier, low-level features, thereby maximizing the flexibility of feature adaptation.",
        "relevant_elements": [
            "multi-expert adapter",
            "last P blocks"
        ],
        "id": 173,
        "masked_question": "How does applying [mask1] exclusively in the last P blocks influence feature adaptation flexibility?",
        "masked_number": 1,
        "masked_elements": [
            "multi-expert adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.21705v1_figure_1.png",
        "paperid": "2410.21705v1",
        "paper_path": "./papers/2410.21705v1.json",
        "figure_id": "2410.21705v1_figure_1.png",
        "caption": "Figure 1: (a) The description of the generalized category discovery (GCD) task. (b) The framework of our proposed AdaptGCD. It contains two critical modules: the multi-expert adapter (MEA) structure and the route assignment constraint. The MEA introduces multiple adapter experts and the route assignment constraint guides the allocation of these experts.",
        "qtype": "Experimental_Results",
        "response": "Here’s the reasoning step‐by‐step:\n\n1.  In the figure the red‐boxed module is the Multi‐Expert Adapter (MEA).  \n2.  The text tells us that, rather than plugging these adapters into every single Transformer block, they only inject them into the top P blocks (out of the total N).  \n3.  All the lower (N–P) blocks remain completely frozen—so their generic visual features learned during pretraining stay intact.  \n4.  By concentrating the only “learnable” bits (the MEA experts) in the final P layers, the model can flexibly tweak just the high-level, semantic representations to suit old versus new classes, without disturbing the low-level, generic features.  \n5.  In practice this means:  \n   •  You get more specialized adaptation capacity exactly where it matters (deep semantic layers)  \n   •  You preserve earlier layers as a stable “backbone” of general vision features  \n   •  You keep parameter growth and computation modest by only fine-tuning a few blocks  \n\nAnswer: By restricting the multi-expert adapters to the last P blocks, the model gains the ability to flexibly adjust high-level, class‐specific features (old vs. new) while leaving lower‐level, general features untouched. This focused placement concentrates adaptation where semantic adjustments are most needed, preserves the pretrained backbone’s generality, and keeps the overall number of trainable parameters low."
    },
    {
        "question": "How does the route assignment constraint balance expert utilization while separating new- and old-class data in AdaptGCD?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "In the multi-expert adapter, the route assignment constraint is required to supervise and control the route distribution. The assignment mainly focuses on two aspects: First, for all data, the load of all experts needs to be balanced to make full use of the resources of experts. Second, for data in old or new classes, the constraint assigns the corresponding experts to them so that the data can be well separated at the routing level. These two aspects correspond to the balanced load loss and the partial balanced load loss, which are introduced in this part.",
            "Balanced Load Loss. The balanced load loss is designed to ensure the maximal usage of diverse experts. … the mean distribution of experts in the l-th layer, averaging the route assignment probabilities across all samples, is aligned to a uniform distribution Uℓ, where Uℓ=1/E. The formulation is depicted in Eq. (5).",
            "Partial Balanced Load Loss. … we propose a partial balanced load loss to separate the new-class and old-class data into different experts and reduce their interference. … We manually specify the expert groups for the old and new classes beforehand and denote the expert groups as Gor_old and Gor_new, respectively. For instance, the first four experts are assigned to Gor_old and the remaining experts are naturally divided into Gor_new. Then the target route distribution probability for the old classes P_old and new classes P_new is established as follows: … As shown in Eq. (9), the Kullback–Leibler divergence is adopted to align Pr_old and Pr_new with the predefined target P_old and P_new.",
            "In the final step, the route assignment loss L_ra for the AdaptGCD is collected as the weighted sum of the two losses, i.e., L_ra = λ_bl·L_bl + λ_pbl·L_pbl, where λ_bl and λ_pbl are the balancing factors."
        ],
        "final_answer": "AdaptGCD’s route assignment constraint implements two complementary losses over the per‐expert routing probabilities.  First, a balanced load loss aligns the mean activation of each expert across all samples to a uniform distribution, ensuring that no expert is under‐ or over‐utilized.  Second, a partial balanced load loss splits the data into old‐ and new‐class subsets (via pseudo‐labels), assigns each subset to a predefined group of experts (e.g. the first half for old classes, the second half for new classes), and then aligns each subset’s routing distribution to its target using KL divergence.  These two losses are combined to both balance overall expert usage and force separation of new‐ vs. old‐class data at the routing level.",
        "relevant_elements": [
            "route assignment constraint",
            "new-class experts",
            "old-class experts"
        ],
        "id": 174,
        "masked_question": "How does the [mask1] balance expert utilization while separating new- and old-class data in AdaptGCD?",
        "masked_number": 1,
        "masked_elements": [
            "route assignment constraint"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.21705v1_figure_1.png",
        "paperid": "2410.21705v1",
        "paper_path": "./papers/2410.21705v1.json",
        "figure_id": "2410.21705v1_figure_1.png",
        "caption": "Figure 1: (a) The description of the generalized category discovery (GCD) task. (b) The framework of our proposed AdaptGCD. It contains two critical modules: the multi-expert adapter (MEA) structure and the route assignment constraint. The MEA introduces multiple adapter experts and the route assignment constraint guides the allocation of these experts.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in (b) is exactly the “route‐assignment constraint,” and it does two things in tandem to both balance expert usage and split old‐ vs. new‐class data onto different experts:  \n1.  Balanced‐load loss (L_bl): it looks at the mean routing-probability vector p̄ over all samples in a minibatch and applies a KL divergence against a uniform distribution over the K experts.  This term forces every expert to get used roughly the same amount, preventing a “winner-takes-all” collapse.  \n2.  Partial balanced‐load loss (L_pbl): it first uses pseudo-labels (old vs. new) to compute separate average routing distributions for old-class samples and for new-class samples.  It then applies KL penalties that push the old-class distribution to match a preassigned “old-class expert” subset and the new-class distribution to match a disjoint “new-class expert” subset.  \n\nIn practice the overall route‐assignment loss is a weighted sum  \n   L_route = α L_bl + β L_pbl,  \nso that at once every expert stays busy and old/new samples are steered toward different groups of experts."
    },
    {
        "question": "What limitations stem from predefining old-class experts and new-class experts in the Multi-Expert Adapter?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Multi-Expert Adapter",
            "old-class experts",
            "new-class experts"
        ],
        "id": 175,
        "masked_question": "What limitations stem from predefining [mask1] and [mask2] in the Multi-Expert Adapter?",
        "masked_number": 2,
        "masked_elements": [
            "old-class experts",
            "new-class experts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.21705v1_figure_1.png",
        "paperid": "2410.21705v1",
        "paper_path": "./papers/2410.21705v1.json",
        "figure_id": "2410.21705v1_figure_1.png",
        "caption": "Figure 1: (a) The description of the generalized category discovery (GCD) task. (b) The framework of our proposed AdaptGCD. It contains two critical modules: the multi-expert adapter (MEA) structure and the route assignment constraint. The MEA introduces multiple adapter experts and the route assignment constraint guides the allocation of these experts.",
        "qtype": "Others",
        "response": "In AdaptGCD’s Multi-Expert Adapter we in fact “hard code” two pools of adapters – one pool reserved exclusively for the old classes (what we drew in red) and one reserved exclusively for the new classes (the blue box).  That rigidity carries two drawbacks:\n\n  1. You must decide ahead of time exactly how many experts to dedicate to the old‐class side and how many to the new‐class side.  If you get that split wrong – say you under-allocate experts to a group that really needs more capacity – the model can’t recover, because there is no mechanism to reassign experts after training starts.\n\n  2. Because the pool assignments are fixed, there is no room for the network to discover or adjust a more nuanced grouping of features.  All “old” inputs are forced through the same sub‐set of experts, all “new” inputs through another, even if the true structure of the data would be better served by a different partition.  \n\nIn short, the very thing that makes AdaptGCD easy to control (pre‐assigning red‐vs.-blue experts) also makes it inflexible: it cannot dynamically rebalance or re‐pool experts if the old/new split is mis‐specified or if the downstream categories shift."
    },
    {
        "question": "What trade-offs occur when reducing dimensions in Down^i and Up^i modules of MEAdaptMLP?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Down^i",
            "Up^i",
            "MEAdaptMLP"
        ],
        "id": 176,
        "masked_question": "What trade-offs occur when reducing dimensions in [mask1] and [mask2] modules of MEAdaptMLP?",
        "masked_number": 2,
        "masked_elements": [
            "Down^i",
            "Up^i"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.21705v1_figure_1.png",
        "paperid": "2410.21705v1",
        "paper_path": "./papers/2410.21705v1.json",
        "figure_id": "2410.21705v1_figure_1.png",
        "caption": "Figure 1: (a) The description of the generalized category discovery (GCD) task. (b) The framework of our proposed AdaptGCD. It contains two critical modules: the multi-expert adapter (MEA) structure and the route assignment constraint. The MEA introduces multiple adapter experts and the route assignment constraint guides the allocation of these experts.",
        "qtype": "Others",
        "response": "In AdaptGCD the little “bottleneck” MLP that lives in the red boxes ([mask1] – the down-projection) and the blue boxes ([mask2] – the up-projection) is where you pay for every extra bit of capacity you add.  If you drive those inner dimensions very low you:\n\n  1. lose parameters (good for storage, compute, and for not disturbing the pre‐trained backbone)  \n  2. improve regularization (harder to overfit your tiny labeled set)  \n  3. but you also throttle the adapter’s ability to reshape the features, so your final accuracy on both old and new classes will eventually suffer.\n\nConversely, if you crank those down- and up-projections wider you gain expressivity (better “fit” to your new categories) at the cost of:\n\n  • more trainable parameters and more FLOPs  \n  • a greater chance of overfitting and of corrupting the frozen backbone’s representations  \n\nHence the familiar adapter trade‐off: narrow bottlenecks are cheap and safe but underpowered, wide bottlenecks are powerful but heavy and prone to overfit."
    },
    {
        "question": "What limitations might stem from confidence mask dependency in progressive rendering material estimation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Confidence Mask",
            "Progressively Render",
            "Material Estimator"
        ],
        "id": 177,
        "masked_question": "What limitations might stem from [mask1] dependency in [mask2] material estimation?",
        "masked_number": 2,
        "masked_elements": [
            "Confidence Mask",
            "Progressively Render"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15138v1_figure_2.png",
        "paperid": "2411.15138v1",
        "paper_path": "./papers/2411.15138v1.json",
        "figure_id": "2411.15138v1_figure_2.png",
        "caption": "Figure 2: Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method [6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.",
        "qtype": "Others",
        "response": "In our pipeline the only “glue” that tells the diffusion‐based estimator when to trust the lighting versus when to ignore it is the red-boxed confidence mask; that mask then drives the blue-boxed progressive, multi‐view material generation.  Because we lean so heavily on that single 1/0 (or high/low) signal, a few failure modes creep in:  \n\n 1.  Mask mis­classification ⇒ error propagation  \n    •  If a region really does have reliable illumination cues but the mask is set low, the model will “regenerate” it from scratch, often losing fine detail or consistency with neighboring views.  \n    •  Conversely, if the mask is set high over a badly lit or heavily shadowed patch, the network will over–trust those spurious highlights and bake them into the PBR maps, then carry that error forward into every subsequent view.  \n\n 2.  Oversimplified confidence  \n    •  Real-world lighting uncertainty is continuous and highly spatially varying; a binary (or overly coarse) mask cannot capture subtle fall-offs in confidence.  Anything that doesn’t neatly fall into “perfectly lit” or “no lighting” will be handled suboptimally.  \n\n 3.  Extra complexity and brittle assumptions  \n    •  Generating a reliable confidence mask itself requires either a separate predictor or hand-tuned heuristics.  If those tools break down (e.g. on wildly novel geometry or truly mixed light sources), the whole multi-view fusion collapses.  \n    •  Because the progressive scheme reuses “known” material latents according to that mask, any upstream mistake simply locks in and is impossible to correct later in UV-space.  \n\nIn short, the single confidence mask is both the key to success and a single point of failure.  Whenever its binary decisions stray from reality, you get either washed-out materials or baked-in lighting artifacts, and those mistakes cascade through the progressive estimator."
    },
    {
        "question": "What alternative inpainting strategies could improve seam handling in UV space material refinement?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "UV Space",
            "Material Refiner"
        ],
        "id": 178,
        "masked_question": "What alternative inpainting strategies could improve seam handling in [mask1] material refinement?",
        "masked_number": 1,
        "masked_elements": [
            "UV Space"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.15138v1_figure_2.png",
        "paperid": "2411.15138v1",
        "paper_path": "./papers/2411.15138v1.json",
        "figure_id": "2411.15138v1_figure_2.png",
        "caption": "Figure 2: Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method [6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.",
        "qtype": "Others",
        "response": "Here is a shortlist of a few orthogonal inpainting ideas that one could plug into the UV-space refiner in place of (or alongside) a vanilla diffusion inpainting head—each of which has been shown in other contexts to dramatically cut down on “zipper” and “bleed” artifacts at hard UV-seams:\n\n1. Partial-Convolution / Gated-Convolution Inpainting  \n   •   Treat the seam “cut” as a hard mask and use the masked-convolution architecture of Liu et al. (“Partial Conv”) or Yu et al. (“Gated Conv”).  \n   •   At training time the network learns to ignore invalid (masked) pixels, which tends to eliminate color bleeding across mask boundaries.  \n\n2. EdgeConnect / Structure-Guided Inpainting  \n   •   First predict a clean “edge map” or seam-contour on the UV tile.  \n   •   Inpaint the missing region in two stages—structure (edges) then texture—so that the model learns to snap fill-regions to the seam geometry.  \n\n3. Poisson / Seamless Cloning Blending  \n   •   After a first pass of coarse hole-filling (e.g. with diffusion), run a Poisson‐blend or seamless‐cloning solver across each seam.  \n   •   This enforces that the normal and albedo gradients line up exactly on either side, removing the visible “step” at UV cuts.  \n\n4. PatchMatch-Style Exemplar Inpainting  \n   •   Use a fast patch-match to copy patches from well-textured neighborhoods into holes.  \n   •   Because patches are drawn from the same UV layout (or even from adjacent charts), you preserve local material statistics and avoid cross-chart color drift.  \n\n5. Mesh- or 3D-Aware Inpainting  \n   •   Operate directly on 3D surface coordinates (or an adjacency graph) rather than 2D UV; fill holes by propagating color/material features along edges of the mesh.  \n   •   Finally re-bake the now–seam‐consistent surface texture into UV space.  \n\n6. Multi-View Reprojection Fill  \n   •   Instead of purely UV inpainting, reproject pixels from the original multi-view renders back into the hole in UV.  \n   •   Wherever several views overlap, blend them (e.g. median or weighted average) before falling back to a learned inpainting model for regions that are truly unobserved.  \n\nAll of these strategies have been shown in the 2D inpainting, texture synthesis, or game-art literature to greatly reduce the “zipper” effects and color mismatches that plague UV seams. In practice you can even compose two or three of them—for example, do a partial-conv inpainting pass followed by a Poisson blend across the borders—to get very tight, seam-free results."
    },
    {
        "question": "What motivates integrating confidence mask into Material Estimator for multi-view consistency under varying illumination?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "To manage inputs with various lighting conditions, we categorize these conditions into two main groups: high confidence (e.g., scanned objects) and low confidence (e.g., no lighting and generated lighting). To guide the model, we introduce a certainty mask that indicates illumination confidence. … Note that, for images with generated lighting, the mask can selectively assign values of 1 in known material regions and 0 elsewhere to enhance multi-view material consistency, as detailed in the progressive material generation (Sec. 3.2).",
            "Consistency is especially challenging in views with generated lighting due to exaggerated highlights and shadows. Therefore, for these views with generated lighting, we additionally utilize the confidence mask introduced in Sec. 3.1 to further enhance consistency between newly generated and known regions. Specifically, we dynamically adjust the mask, setting it to 1 for known regions with higher confidence and to 0 for regions requiring new generation."
        ],
        "final_answer": "The confidence mask is integrated to let the estimator adapt to varying illumination: it tells the model when to trust lighting cues (high confidence) and when to ignore them (low confidence), and it preserves previously generated high-confidence regions while regenerating low-confidence areas, thereby enforcing multi-view consistency under both realistic and unreliable lighting conditions.",
        "relevant_elements": [
            "confidence mask",
            "Material Estimator"
        ],
        "id": 179,
        "masked_question": "What motivates integrating [mask1] into Material Estimator for multi-view consistency under varying illumination?",
        "masked_number": 1,
        "masked_elements": [
            "confidence mask"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15138v1_figure_2.png",
        "paperid": "2411.15138v1",
        "paper_path": "./papers/2411.15138v1.json",
        "figure_id": "2411.15138v1_figure_2.png",
        "caption": "Figure 2: Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method [6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is the per‐pixel “confidence” map that we feed into the diffusion‐based material estimator.  We introduce it because, in a multi‐view pipeline, different views often come with wildly different lighting (scanned objects with faithful illumination versus synthetic or missing lighting setups).  If you simply ran the estimator on each view in isolation, you’d get inconsistent albedo/roughness/metallic/bump outputs wherever the lighting was poor or exaggerated.  By adding a confidence mask that flags which regions of each rendered view are reliable (high confidence) and which are not (low confidence), the network learns to (a) preserve previously estimated material in the trustworthy areas and (b) re-synthesize only the uncertain ones.  This “confidence‐adaptive” conditioning is what lets us maintain seamless material coherence across all views, despite varying illumination."
    },
    {
        "question": "What is rationale for employing a Material Refiner in UV space instead of per-view estimation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To adapt our 2D materials estimator for 3D objects, we propose a progressive generation strategy that dynamically estimates materials across different viewpoints based on the aforementioned confidence mask. We further project the multi-view materials into UV space and apply a refinement diffusion model, which completes occluded regions and refines materials, ensuring seamless and consistent materials maps.",
            "Although coarse UV material maps are generated, issues such as seams (resulting from baking across different views) and texture holes (due to self-occlusion during rendering) remain. We thus refine material maps directly in UV space using a diffusion model."
        ],
        "final_answer": "Per-view material estimation, even when done progressively, still produces seams and holes when the separate view outputs are baked together into a single UV atlas. By switching to a UV-space Refiner that takes the stitched (coarse) UV maps as input, the model can jointly inpaint occluded regions, smooth across seam boundaries, and enforce global consistency in the UV domain—something that is difficult to achieve when refining each view independently.",
        "relevant_elements": [
            "Material Refiner",
            "UV space"
        ],
        "id": 180,
        "masked_question": "What is rationale for employing a [mask1] in UV space instead of per-view estimation?",
        "masked_number": 1,
        "masked_elements": [
            "Material Refiner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15138v1_figure_2.png",
        "paperid": "2411.15138v1",
        "paper_path": "./papers/2411.15138v1.json",
        "figure_id": "2411.15138v1_figure_2.png",
        "caption": "Figure 2: Overview of Material Anything. For texture-less objects, we first generate coarse textures using image diffusion models, similar to the texture generation method [6]. For objects with pre-existing textures, we directly process them. Next, a material estimator progressively estimates materials for each view from a rendered image, normal, and confidence mask. The confidence mask serves as additional guidance for illuminance uncertainty, addressing lighting variations in the input image and enhancing consistency across generated multi-view materials. These materials are then unwrapped into UV space and refined by a material refiner. The final material maps are integrated with the mesh, enabling the object for downstream applications.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. After running the diffusion material estimator on each camera view and baking those per-view material maps into the mesh’s UV atlas, you end up with a single “coarse” UV material map that has:  \n   – Seams where two views met and their estimates didn’t perfectly align  \n   – Holes or missing texels wherever self-occlusion prevented any view from seeing that patch of the surface  \n   – Lighting and highlight mismatches carried over from view-based generation  \n\n2. Trying to go back and fix each view independently risks re-introducing inconsistencies (you’d have to re-render, re-estimate, and then re-bake all over again), and you still wouldn’t address holes that no view ever saw.\n\n3. By contrast, jumping to a UV-space diffusion refiner (the red-boxed module) lets you treat the entire material atlas as one “image.”  \n   – You can inpaint holes in one shot, because the refiner sees the whole map at once.  \n   – You can smooth and blend across UV seams, because the network learns to generate textures that are spatially coherent on that 2D atlas.  \n   – You can incorporate the Canonical Coordinate Map (CCM) to encode 3D adjacency in UV space, further guiding correct geometry-aware inpainting.\n\n4. In short, operating in UV space gives you a single, global canvas on which to remove seams, fill occlusions, and enforce cross-view consistency — something that would be extremely difficult (and compute-intensive) to achieve by re-estimating each view individually.  \n\nAnswer  \nThey refine in UV space because at that stage you have one unified 2D atlas of the entire surface. A UV-space refiner can see and inpaint all seams and holes in a single pass, enforce global consistency, and leverage 3D adjacency (via the CCM) — something you can’t do if you keep estimating materials view by view."
    },
    {
        "question": "What motivates freezing IPA-FR parameters while training the Face Embedding Mapping Model?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "IPA-FR",
            "Face Embedding Mapping Model"
        ],
        "id": 181,
        "masked_question": "What motivates freezing [mask1] parameters while training the Face Embedding Mapping Model?",
        "masked_number": 1,
        "masked_elements": [
            "IPA-FR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18165v1_figure_2.png",
        "paperid": "2411.18165v1",
        "paper_path": "./papers/2411.18165v1.json",
        "figure_id": "2411.18165v1_figure_2.png",
        "caption": "Figure 2: Pipeline of face reconstruction by face embedding mapping.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is the pre‐trained IPA‐FR backbone that produces the “ground-truth” face embedding space for our diffusion model. We freeze its weights during FEM training so that\n\n  1. we have a fixed, stable target embedding distribution to match against (otherwise the mapper could “chase” a moving target),  \n  2. the expensive pre-trained FR network remains intact (no risk of corrupting its identity representation), and  \n  3. all gradient updates go solely into the much smaller mapping network.  \n\nIn short, freezing IPA-FR guarantees a constant reference embedding space and focuses training on learning the mapping layer alone."
    },
    {
        "question": "What warrants multi-term loss optimization between IPA-FR outputs and Face Embedding Mapping Model predictions?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In order to enable target  model to generate realistic target identity face images from IPA-FaceID, the target embedding extracted from  should be close to the corresponding embedding that represents the same face identity.",
            "Therefore, we should minimize the distance between    and , where  and denote FEM and mapped face embedding, respectively.",
            "Our total loss is determined by a linear combination of the aforementioned loss types."
        ],
        "final_answer": "Because the mapped embedding produced by the Face Embedding Mapping model must closely match the embedding extracted by IPA-FR for the same identity, a multi-term loss (combining MSE, pairwise distance, and cosine embedding distance) is used to minimize their distance and preserve identity information.",
        "relevant_elements": [
            "IPA-FR",
            "Face Embedding Mapping Model",
            "Loss Optimization"
        ],
        "id": 182,
        "masked_question": "What warrants multi-term [mask1] between IPA-FR outputs and Face Embedding Mapping Model predictions?",
        "masked_number": 1,
        "masked_elements": [
            "Loss Optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18165v1_figure_2.png",
        "paperid": "2411.18165v1",
        "paper_path": "./papers/2411.18165v1.json",
        "figure_id": "2411.18165v1_figure_2.png",
        "caption": "Figure 2: Pipeline of face reconstruction by face embedding mapping.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. In the top (“Training”) panel, the blue IPA-FR block is frozen and produces an embedding vector for each input face, while the black “Face Embedding Mapping Model” (FEM) learns to predict a corresponding embedding (orange) that would come out of the target FR/PPFR backbone.  \n2. The red box sitting between the two embedding streams is literally labelled “Loss Optimization.” It indicates that during training we are not using a single criterion but a *multi‐term* objective.  \n3. Consulting the text under “Joint Loss,” we see exactly why it must be multi‐term: minimizing only one metric (say plain MSE) tends to miss angular differences or pairwise relationships that matter for identity. So they combine  \n   • Mean-Square-Error (MSE)  \n   • Pairwise (Euclidean) Distance (PD with p=2)  \n   • Cosine Embedding Distance (CED)  \n   into one joint loss so that the mapped embedding is close in magnitude *and* angle to the frozen IPA-FR embedding.  \n\nAnswer  \nThey insert a multi-term joint loss optimization—combining MSE, Euclidean pairwise distance, and cosine-embedding distance—between the frozen IPA-FR outputs and the FEM predictions to ensure the mapped embeddings match the IPA-FR embeddings both in norm and in angular/relational structure."
    },
    {
        "question": "How does Loss Optimization balance contributions from IPA-FR and Target FR embeddings during training?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In order to enable target  model to generate realistic target identity face images from IPA-FaceID, the target embedding extracted from  should be close to the corresponding embedding that represents the same face identity. Therefore, we should minimize the distance between    and , where  and  denote FEM and mapped face embedding, respectively.",
            "Mean Square Error (MSE): To reduce reconstruction difference of the generated embedding, we use MES loss to minimize the square of the reconstruction error:",
            "Pairwise Distance (PD): When p=2, PD computes the pairwise distance between input vectors using the euclidean distance:",
            "Cosine Embedding Distance (CED): CED is used for measuring whether two embedding vectors are similar, it is widely used for comparing face template in FR tasks:",
            "Our total loss is determined by a linear combination of the aforementioned loss types: We empirically determined that the selection of , ,  (value should be set to balance the range of different loss functions) yields the best performance."
        ],
        "final_answer": "During training, the loss module takes the frozen IPA-FR embedding as the target and the FEM-mapped embedding (converted from the Target FR/PPFR output) as the source and computes three distances—Mean Squared Error, Euclidean (pairwise) distance, and Cosine Embedding distance—between them. These three losses are added together in a single joint loss: L = α·L_MSE + β·L_PD + γ·L_CED. The coefficients α, β, γ are chosen empirically to balance the magnitude (and thus the contribution) of each term, ensuring neither the IPA-FR nor the Target FR embedding dominates the optimization.",
        "relevant_elements": [
            "IPA-FR",
            "Target FR/PPFR",
            "Loss Optimization"
        ],
        "id": 183,
        "masked_question": "How does [mask1] balance contributions from IPA-FR and Target FR embeddings during training?",
        "masked_number": 1,
        "masked_elements": [
            "Loss Optimization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18165v1_figure_2.png",
        "paperid": "2411.18165v1",
        "paper_path": "./papers/2411.18165v1.json",
        "figure_id": "2411.18165v1_figure_2.png",
        "caption": "Figure 2: Pipeline of face reconstruction by face embedding mapping.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed block in Figure 2 is nothing mystical – it is simply the “Loss-Optimization” node.  What it does is pull the MLP’s output (the mapped Target-FR embedding) towards the frozen IPA-FR embedding by minimizing a single, joint loss.  Concretely, during training we compute three distances between the IPA-FR “ground‐truth” vector and the MLP’s output:\n\n  1. Mean‐Squared‐Error  \n  2. Pairwise (Euclidean) Distance  \n  3. Cosine Embedding Distance  \n\nWe then form a weighted sum of those three terms,  \n   Ltotal = α·LMSE + β·LPD + γ·LCos,  \nand back-propagate through only the mapping MLP.  By tuning α, β and γ so that each term lives on the same numerical scale, the model automatically balances how much it “cares” about the IPA-FR embedding versus the (mapped) Target-FR embedding at each gradient step."
    },
    {
        "question": "How does the Face Embedding Mapping Model adjust embedding distributions prior to IPA-FaceID generation?",
        "relevant_section_ids": [
            "1",
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "As depicted in Figure 2, we feed training face images to both IPA-FR (default FR of IPA-FaceID) and target FR models. The initial output face embedding from the target FR model is transferred by the Face Embedding Mapping (FEM) model before performing multi-term loss optimization.",
            "In order to enable target $\\mathcal{F}$ model to generate realistic target identity face images from IPA-FaceID, the target embedding extracted from $\\mathcal{F}$ should be close to the corresponding embedding that represents the same face identity. Therefore, we should minimize the distance between $\\mathbf{e}_t$ and $\\mathbf{e}_m$, where $\\mathbf{e}_t$ and $\\mathbf{e}_m$ denote FEM and mapped face embedding, respectively. Mean Square Error (MSE), Pairwise Distance (PD) and Cosine Embedding Distance (CED) are combined into a multi-term loss to align the two embeddings.",
            "Inspired from (Papantoniou et al., 2024) and (Liu et al., 2024), we propose FEM-MLP and FEM-KAN to learn the mapping relation of embedding distributions from different FR backbones. Then trained FEMs can map face embedding from the initial domain into the corresponding target domain of the pre-trained IPA-FaceID diffusion model in order to generate face images."
        ],
        "final_answer": "The Face Embedding Mapping (FEM) model is a small trainable module (implemented either as an MLP or a KAN network) that learns to non-linearly transform embeddings from a target FR or PPFR model into the embedding space expected by IPA-FaceID. During training it minimizes a multi-term loss (MSE, pairwise Euclidean, and cosine distances) between mapped embeddings and the IPA-FR embeddings of the same identities, effectively aligning the two distributions. At inference time, any leaked embedding is passed through this trained FEM to adjust its distribution before feeding it into IPA-FaceID for realistic face generation.",
        "relevant_elements": [
            "Face Embedding Mapping Model",
            "IPA-FaceID"
        ],
        "id": 184,
        "masked_question": "How does the [mask1] adjust embedding distributions prior to [mask2] generation?",
        "masked_number": 2,
        "masked_elements": [
            "Face Embedding Mapping Model",
            "IPA-FaceID"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.18165v1_figure_2.png",
        "paperid": "2411.18165v1",
        "paper_path": "./papers/2411.18165v1.json",
        "figure_id": "2411.18165v1_figure_2.png",
        "caption": "Figure 2: Pipeline of face reconstruction by face embedding mapping.",
        "qtype": "Implementation_Details",
        "response": "Here is the key idea in two steps:\n\n1. The red­-boxed module is the Face Embedding Mapping model (FEM).  At training time it learns a non-linear transform (either an MLP or a KAN-based network) that “warps” any incoming embedding (from a normal FR or a PPFR model) into the embedding‐space that the IPA-FaceID diffusion engine expects.  This warping is trained by minimizing a combination of  \n   • mean-squared‐error on the mapped vectors,  \n   • pairwise Euclidean distances, and  \n   • cosine-similarity losses  \n   against the IPA-FR embeddings extracted from the same faces.  \n\n2. At inference time, you feed the leaked embedding through that learned FEM and thereby align its distribution to the IPA-FR domain before handing it off to IPA-FaceID.  The result is that the diffusion model can generate crisp, identity-preserving face images (the blue-circled “generated identities”)."
    },
    {
        "question": "How does Conditional Referring Module incorporate target-related cues with visual features for embedding refinement?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Considering the situation at stage t, we first concatenate one target-related cue Q_t and the M negative text cues obtained from other images, to form Q_concat. We then fuse the visual features V_t with Q_concat through a vision-to-text cross-attention, to obtain vision-attended cue features F_v2t.",
            "Using the vision-attended cue features F_v2t, we then enrich the global textual features Q_global into cue-enhanced textual features Q_tilde through another text-to-text cross-attention.",
            "To compute the response map, we first update the visual features V_t to V_{t+1} by integrating them with the updated referring text embedding Q_tilde using a text-to-visual cross-attention, thereby reducing the cross-modality discrepancy."
        ],
        "final_answer": "At each stage the CRM first combines the current short–phrase cue with negative cues and attends over the image features via a vision-to-text cross-attention to yield vision-attended cue features. Those features then inform a text-to-text cross-attention that refines the global referring embedding into a cue-enhanced text embedding. Finally, a text-to-visual cross-attention uses that refined text embedding to update (refine) the visual features and produce the stage’s response map.",
        "relevant_elements": [
            "Conditional Referring Module",
            "target-related cues",
            "visual features"
        ],
        "id": 185,
        "masked_question": "How does [mask1] incorporate target-related cues with [mask2] for embedding refinement?",
        "masked_number": 2,
        "masked_elements": [
            "Conditional Referring Module",
            "visual features"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.01544v2_figure_2.png",
        "paperid": "2410.01544v2",
        "paper_path": "./papers/2410.01544v2.json",
        "figure_id": "2410.01544v2_figure_2.png",
        "caption": "Figure 2: The pipeline of PCNet.\nGiven a pair of image-text as input,\nPCNet enhances the visual-linguistic alignment\nby progressively comprehending the target-related textual nuances in the text description. It starts with using a LLM to decompose the input description into several target-related short phrases as target-related textual cues. The proposed Conditional Referring Module (CRM) then processes these cues to update the linguistic embeddings across multiple stages.\nTwo novel loss functions, Region-aware Shrinking (RaS) and Instance-aware Disambiguation (IaD), are also proposed to supervise the progressive comprehension process.",
        "qtype": "Implementation_Details",
        "response": "Within our PCNet, the red‐boxed module is the Conditional Referring Module’s Referring‐Modulation block, and the blue‐boxed signal is the updated visual embedding (V₁).  At each stage t (stage 0 in your example), that Referring‐Modulation block:\n\n 1. Takes the current cue embedding qₜ (plus a small set of “negative” cues) and the existing visual feature Vₜ,  \n 2. Applies a vision→text cross‐attention so that the image features Vₜ “attend to” and contextualize the cue, yielding vision‐attended cue features,  \n 3. Uses a text→text cross‐attention to combine those vision‐attended cues with the global referring embedding qₜ to produce a refined referring embedding qₜ₊₁,  \n 4. Finally, re‐injects qₜ₊₁ back into Vₜ via a text→visual cross‐attention, producing the next‐stage visual embedding Vₜ₊₁ (the blue box).\n\nIn this way, the red‐boxed Referring‐Modulation fuses the target‐related text cue and the current visual embedding together, and then “pushes” the resulting, cue‐aware linguistic embedding back into the image feature stream to produce a refined visual embedding (V₁)."
    },
    {
        "question": "How does Region-aware Shrinking loss leverage mask proposals to refine foreground activation and suppress background?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, at stage t, we first employ a pretrained proposal generator to obtain a set of mask proposals, 𝓜ₜ = {Mₜᵏ}ₖ₌₁ᴺ, where each proposal Mₜᵏ ∈ [0,1] and N is the total number of segment proposals.",
            "We then compute a alignment score between the response map Rₜ and each proposal Mₜᵏ as: sₜ,ₖ = ∑ (Rₜ ⊙ Mₜᵏ). The proposal with the highest score is then treated as the target foreground region, while the combination of other proposals is regarded as non-target background regions.",
            "By introducing region-wise ambiguity, L_{RaS} can direct non-target regions to converge towards attenuation while maintaining and improving the quality of the response map in the target region. Additionally, the mask proposals can also provide a shape prior to the target region, which helps to further enhance the accuracy of the target object localization."
        ],
        "final_answer": "Region-aware Shrinking loss first uses a pretrained proposal generator to obtain multiple mask proposals. It measures how well each proposal overlaps the current response map and picks the highest‐scoring mask as the foreground, treating all others as background. By defining a region-wise ambiguity score (the ratio of background activation vs. total activation) and penalizing increases in that ambiguity across stages, RaS forces background activations to shrink while preserving and sharpening the true foreground response. The mask proposals thus serve both to separate foreground from background and to provide a shape prior that refines target activation.",
        "relevant_elements": [
            "Region-aware Shrinking loss",
            "mask proposals",
            "foreground activation"
        ],
        "id": 186,
        "masked_question": "How does [mask1] leverage [mask2] to refine foreground activation and suppress background?",
        "masked_number": 2,
        "masked_elements": [
            "Region-aware Shrinking loss",
            "mask proposals"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.01544v2_figure_2.png",
        "paperid": "2410.01544v2",
        "paper_path": "./papers/2410.01544v2.json",
        "figure_id": "2410.01544v2_figure_2.png",
        "caption": "Figure 2: The pipeline of PCNet.\nGiven a pair of image-text as input,\nPCNet enhances the visual-linguistic alignment\nby progressively comprehending the target-related textual nuances in the text description. It starts with using a LLM to decompose the input description into several target-related short phrases as target-related textual cues. The proposed Conditional Referring Module (CRM) then processes these cues to update the linguistic embeddings across multiple stages.\nTwo novel loss functions, Region-aware Shrinking (RaS) and Instance-aware Disambiguation (IaD), are also proposed to supervise the progressive comprehension process.",
        "qtype": "Implementation_Details",
        "response": "First, let us map the two “masks” in the question back onto the paper’s notation:\n\n  1. [mask1]  is the little red‐outlined box in the top–right of Fig. 2, i.e. the Region‐aware Shrinking (RaS) loss.  \n  2. [mask2]  is everything inside the big blue box, i.e. the branch of the network that (a) runs the proposal generator, (b) produces the stage‐wise response maps R₀, R₁, …, R_{N–1}, and (c) feeds those maps into RaS (and IaD).\n\nThe question asks:\n\n  “How does RaS leverage the proposal-generator branch (and its mask proposals) to refine foreground activation and suppress background?”\n\nChain of thought:\n\n  •  At each stage t, the network already has a response map R_t over the image (this lives in the blue region).  \n  •  Simultaneously the pretrained proposal generator (also in that blue box) spits out a set of K mask proposals {Mₜ¹, …, Mₜᵏ} that cover different candidate regions in the image.  \n  •  RaS computes, for each proposal Mₜᵢ, an alignment score sᵢ = ⟨Rₜ, Mₜᵢ⟩  (pixelwise Hadamard product plus sum).  \n  •  It picks the proposal Mₜ^{*} with the highest score as the “foreground” mask and treats the union of the remaining proposals as “background.”  \n  •  It then defines an ambiguity measure A(Rₜ, Mₜ^{*}) = 1 – IoU(Rₜ, Mₜ^{*}), which is high when Rₜ “leaks” outside the chosen proposal and low when it is tightly packed inside.  \n  •  Finally, RaS enforces that this ambiguity must *decrease* from stage t to t+1.  Concretely it adds a loss term  \n       L_{RaS} = ∑_{t=0}^{N–2} [ A(R_{t+1}, M_{t+1}^{*})  –  A(R_t, M_t^{*}) ]  \n    which drives A(R_{t+1}) < A(R_t).  \n\nEffect of that constraint:\n\n  –  Since A measures “how much of R leaks outside the chosen mask,” driving ambiguity down means packing R more tightly inside the single best proposal → stronger *foreground* activation.  \n  –  Any activation that lies outside the winning proposal is penalized, so background activations get pushed down.  \n\nIn this way, RaS “shrinks” the activated region stage by stage, refining the true object mask and suppressing everything else."
    },
    {
        "question": "How does LLM decomposition influence CRM stage-wise refinement compared to fixed-text embedding methods?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "However, these methods encode the entire referring text as a single language embedding. They can easily overlook some critical cues related to the target object in the text description, leading to localization ambiguity and even errors.",
            "Inspired by the human comprehension process, we propose in this paper a novel Progressive Comprehension Network (PCNet) for WRIS. We first employ a Large Language Model (LLM) to dissect the input text description into multiple short phrases. These decomposed phrases are considered as target-related cues and fed into a novel Conditional Referring Module (CRM), which helps update the global referring embedding and enhance target localization in a multi-stage manner.",
            "To do this, we leverage the strong in-context capability of the LLM to decompose the text description. ... In this way, phrases generated by LLM are related to the target object and align closely with our objective.",
            "Given the decomposed phrases (i.e., target-related cues), we propose a CRM to enhance the discriminative ability on the target object region conditioned on these phrases, thereby improving localization accuracy. As shown in Fig. 2, the CRM operates across K consecutive stages. At each stage, it first utilizes a different target-related cue to modulate the global referring embedding via a referring modulation block and then produces the image-to-text response map through a response map generation block."
        ],
        "final_answer": "By using an LLM to split the referring text into multiple short phrases, PCNet feeds a different cue into each CRM stage. At each stage the CRM modulates the global referring embedding with that stage’s phrase and refines the response map. In contrast, fixed-text embedding methods collapse the entire description into one embedding and perform only a single, coarse alignment—whereas LLM decomposition enables a progressive, multi-stage refinement that better captures fine-grained cues and reduces ambiguity.",
        "relevant_elements": [
            "LLM",
            "CRM"
        ],
        "id": 187,
        "masked_question": "How does [mask1] decomposition influence [mask2] stage-wise refinement compared to fixed-text embedding methods?",
        "masked_number": 2,
        "masked_elements": [
            "LLM",
            "CRM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.01544v2_figure_2.png",
        "paperid": "2410.01544v2",
        "paper_path": "./papers/2410.01544v2.json",
        "figure_id": "2410.01544v2_figure_2.png",
        "caption": "Figure 2: The pipeline of PCNet.\nGiven a pair of image-text as input,\nPCNet enhances the visual-linguistic alignment\nby progressively comprehending the target-related textual nuances in the text description. It starts with using a LLM to decompose the input description into several target-related short phrases as target-related textual cues. The proposed Conditional Referring Module (CRM) then processes these cues to update the linguistic embeddings across multiple stages.\nTwo novel loss functions, Region-aware Shrinking (RaS) and Instance-aware Disambiguation (IaD), are also proposed to supervise the progressive comprehension process.",
        "qtype": "Literature_Background",
        "response": "Here’s the high-level intuition in two steps:\n\n1.  Mask₁ (the red-boxed “LLM decomposition”)  \n    • Instead of encoding the entire referring sentence into one monolithic vector, PCNet asks a large language model to break it into a small ordered list of short, target-related cues (e.g. “a player” → “blue/gray uniform” → “catches a ball”).  \n    • Each cue is thus isolated and guaranteed to carry a single piece of discriminative information about the referent.\n\n2.  Mask₂ (the blue-boxed Conditional Referring Module, i.e. stage-wise refinement)  \n    • At stage 0 it takes cue₀ (“a player”) and produces a very coarse response map (all people).  \n    • At stage 1 it then conditions on cue₁ (“blue/gray uniform”) to modulate that same embedding, yielding a tighter map (only the blue/gray player).  \n    • At stage 2 it uses cue₂ (“catches a ball”) to shrink it again to the final correct player.\n\nContrast with fixed-text embedding methods: they collapse all of “a player wearing a blue and gray uniform catches a ball” into one vector and attend only once. There is no “first find all players → then focus on color → then focus on action” process. As a result, they often over-activate irrelevant regions (e.g. every player), miss fine details, and cannot progressively refine the mask.\n\nBy using LLM-powered decomposition, PCNet’s CRM can do a true coarse-to-fine, stage-by-stage attention and shrinking—leading to much cleaner, more accurate masks than one-shot, fixed-text approaches."
    },
    {
        "question": "How does CRM-conditioned response map facilitate RaS loss improvement over Cls-only supervision?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Section 3.2: “At each stage, [the CRM] first utilizes a different target-related cue to modulate the global referring embedding via a referring modulation block and then produces the image-to-text response map through a response map generation block. … To achieve global visual-linguistic alignment, we adopt classification loss in [30] to optimize the generation of the response map at each stage.”",
            "Section 3.3: “Despite modulating the referring attention with the target-related cues stage-by-stage, image-text classification often activates irrelevant background objects due to its reliance on global and coarse response map constraints. Ideally, as the number of target-related cues used increases across each stage, the response map should become more compact and accurate. … We propose a novel region-aware shrinking (RaS) loss, which segments the response map into foreground (target) and background (non-target) regions. Through contrastive enhancement between these regions, our method gradually reduces the background interference while refining the foreground activation in the response map.”"
        ],
        "final_answer": "By conditioning each stage’s response map on progressively finer, cue-specific embeddings (via the CRM), the model produces multi-stage activations that are increasingly focused on the true target and less on background clutter. RaS loss then leverages these CRM-refined maps—by splitting them into foreground and background regions and applying a contrastive ‘shrinking’ constraint—to drive background activations down while preserving and sharpening the foreground. In contrast, Cls-only supervision treats the map globally and remains prone to coarse, background‐biased activations.",
        "relevant_elements": [
            "CRM",
            "RaS",
            "Cls"
        ],
        "id": 188,
        "masked_question": "How does [mask1]-conditioned response map facilitate [mask2] loss improvement over Cls-only supervision?",
        "masked_number": 2,
        "masked_elements": [
            "CRM",
            "RaS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.01544v2_figure_2.png",
        "paperid": "2410.01544v2",
        "paper_path": "./papers/2410.01544v2.json",
        "figure_id": "2410.01544v2_figure_2.png",
        "caption": "Figure 2: The pipeline of PCNet.\nGiven a pair of image-text as input,\nPCNet enhances the visual-linguistic alignment\nby progressively comprehending the target-related textual nuances in the text description. It starts with using a LLM to decompose the input description into several target-related short phrases as target-related textual cues. The proposed Conditional Referring Module (CRM) then processes these cues to update the linguistic embeddings across multiple stages.\nTwo novel loss functions, Region-aware Shrinking (RaS) and Instance-aware Disambiguation (IaD), are also proposed to supervise the progressive comprehension process.",
        "qtype": "Literature_Background",
        "response": "First, let us be explicit about what the two masks refer to:  \n•  [​mask1​] is the little red‐boxed module in Fig. 2 – namely the “Response Map Generation” inside each Conditional Referring Module, i.e. the *cue-conditioned* response map.  \n•  [​mask2​] is the blue box at the top – i.e. our *Region-aware Shrinking (RaS)* loss (we could equally have said “the RaS + IaD losses,” but RaS is the one that most directly builds on the intermediate maps).  \n\n– Chain of Thought –  \n1.  A Cls-only loss (the grey “Cls” boxes) simply pushes the network to produce *some* heatmap that, in aggregate, scores higher on the correct text than on negative text.  It does not tell the model *where* in the image it should be tightening or suppressing activation.  \n2.  In PCNet, at each stage we first inject a *single* short phrase (e.g. “wearing blue/gray”) via the Referring Modulation block, update the global text embedding, and then produce a *cue-conditioned* response map Rₖ (this is [mask1]).  Because each phrase is very local (attributes, relations, etc.), Rₖ already tends to focus on the right region for that cue.  \n3.  The RaS loss ( [mask2] ) now takes that cue-conditioned map Rₖ, aligns it against a set of precomputed mask proposals, picks one as “foreground” and the rest as “background,” and then (a) *presses down* the background regions and (b) *keeps or sharpens* the foreground.  \n4.  By chaining this across stages—i.e. requiring R₁ to be even more compact than R₀, R₂ more compact than R₁, etc.—RaS *gradually shrinks away* spurious activations while preserving the true object shape.  \n5.  In contrast to a pure Cls loss, which only knows “correct heatmap vs. incorrect heatmap” globally, the *cue-conditioned maps* give RaS the *spatial structure* it needs to do a fine-grained, proposal-based contrast between target and non-target regions.  That is exactly why RaS turns *much* sharper, more accurate response maps than Cls alone can.  \n\nTo summarize in one sentence:  \nBecause each CRM stage already produces a *cue-aware* response map (the red-boxed module), RaS can explicitly split that map into a “best” proposal and the rest, and then enforce a *coarse-to-fine shrinking* of non-target activations across stages—something that a global Cls loss by itself cannot do."
    },
    {
        "question": "How do the VAE Encoder and VAE Decoder modules reflect Stable Diffusion's autoencoding design for quality preservation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Our network structure is inspired by the Latent Diffusion Model (LDM) (Rombach et al., 2022), which employs a pretrained autoencoder (VQ-VAE) to map images from pixel space to latent space, where the diffusion process is performed. As demonstrated in (Rombach et al., 2022), operating in latent space promotes local realism and avoids the blurriness that often arises from pixel-space losses, such as L1 or L2 objectives.",
            "As shown in Fig. 1, an occluded lower half of the target image and a reference identity image at time t are each passed through a pre-trained VAE encoder. The resulting outputs are concatenated along the channel dimension to create a comprehensive latent feature representation. Ultimately, the fused feature is fed into a pre-trained VAE decoder to generate the final result. This refined process ensures that the generated video not only aligns perfectly with the audio track but also maintains the visual coherence and quality expected in state-of-the-art audio-vision synchronization systems."
        ],
        "final_answer": "MuseTalk borrows Stable Diffusion’s two-stage autoencoding design by freezing a high-capacity VAE encoder and decoder around its U-Net fusion backbone. Images are first encoded into a compact latent space—just as in LDM/VQ-VAE—where multi-scale cross-attention mixing of audio and visual features preserves local detail and avoids pixel-level blur. After fusion, the pre-trained VAE decoder reconstructs high-quality frames, thereby inheriting the stability and realism benefits of Stable Diffusion’s latent autoencoding architecture.",
        "relevant_elements": [
            "VAE Encoder",
            "VAE Decoder"
        ],
        "id": 189,
        "masked_question": "How do the [mask1] and [mask2] modules reflect Stable Diffusion's autoencoding design for quality preservation?",
        "masked_number": 2,
        "masked_elements": [
            "VAE Encoder",
            "VAE Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.10122v2_figure_1.png",
        "paperid": "2410.10122v2",
        "paper_path": "./papers/2410.10122v2.json",
        "figure_id": "2410.10122v2_figure_1.png",
        "caption": "Figure 1: Illustration of our MuseTalk framework. We first encodes a reference facial image and a occluded lower half target image into perceptually equivalent latent space. Subsequently, we employ a multi-scale learning network structure to effectively fuse audio and visual features at various scales, thereby facilitating a more comprehensive integration of the two modalities. Consequently, the decoded results from the latent space yield more realistic and lip-synced talking face visual content.",
        "qtype": "Literature_Background",
        "response": "Here’s how the two masked blocks mirror Stable Diffusion’s own auto‐encoding strategy and thus help preserve high‐fidelity detail:\n\n1. Mask 1 (red box): this is nothing more than the pre-trained VAE encoder from Stable Diffusion (or its LDM backbone).  Both your reference image and the occluded source image are passed through this frozen encoder to be projected into a compact, perceptually-aware latent space (size w×h×c).  By re-using the exact same encoder weights that Stable Diffusion was trained with, you inherit all of its learned image priors—edges, textures, identity cues—right at the front end and avoid having to learn them from scratch in pixel space.\n\n2. Mask 2 (blue box): this is the corresponding pre-trained VAE decoder, again carried over verbatim from Stable Diffusion.  After your multi‐scale fusion U-Net has merged and manipulated those image latents together with audio‐derived condition tokens, the decoder “lifts” the resulting latent back into pixel space.  Because you’re decoding with the very same weights that were trained to invert the LDM encoder, you retain the sharpness, color consistency, and fine texture detail that a purely pixel‐space generator would struggle to match.\n\nIn short, by sandwiching your trainable U-Net between the frozen VAE encoder (mask 1) and the frozen VAE decoder (mask 2), you exactly reproduce the “encode → denoise/fuse → decode” loop at the heart of Stable Diffusion.  That is the simplest way to preserve the high‐quality, locally realistic image statistics learned by the SD autoencoder."
    },
    {
        "question": "How does the Whisper Encoder collaborate with audio attn. modules for multi-scale U-Net fusion similar to cross-attention methods?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For the audio data, we leverage a pre-trained Whisper (Radford et al., 2023) encoder to extract features from a sequence audio segment. The length of the audio segment is set to T, centered at time t. This segment is first re-sampled to 16,000 Hz and then transformed into an 80-channel log magnitude Mel spectrogram, which serves as the input A^{t}_{mel}. The output audio feature shape is a^{T×d}.",
            "Our network structure is inspired by the Latent Diffusion Model (LDM) (Rombach et al., 2022), which employs a pretrained autoencoder (VQ-VAE) to map images into latent space. The training objective is formulated as L, where f represents the backbone denoising network, including several condition-related cross attention modules.",
            "The U-Net’s multi-scale learning network architecture adeptly fuses audio and visual features across various scales with the capability of modeling conditional distributions of the form p(v|a), enabling a more nuanced and holistic integration of the two modalities."
        ],
        "final_answer": "The frozen Whisper encoder first processes each synchronized audio segment into a sequence of d-dimensional embeddings (shape T×d). These audio embeddings are then injected as conditioning inputs into multiple “audio attn.” modules—i.e. cross-attention layers—at different resolutions within the U-Net backbone. At each scale, the U-Net’s cross-attention (audio attn.) layer attends to the Whisper embeddings, allowing the network to align and fuse audio cues with the visual latent features in a multi-scale manner.",
        "relevant_elements": [
            "Whisper Encoder",
            "Audio attn."
        ],
        "id": 190,
        "masked_question": "How does the [mask1] collaborate with [mask2] modules for multi-scale U-Net fusion similar to cross-attention methods?",
        "masked_number": 2,
        "masked_elements": [
            "Whisper Encoder",
            "Audio attn."
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.10122v2_figure_1.png",
        "paperid": "2410.10122v2",
        "paper_path": "./papers/2410.10122v2.json",
        "figure_id": "2410.10122v2_figure_1.png",
        "caption": "Figure 1: Illustration of our MuseTalk framework. We first encodes a reference facial image and a occluded lower half target image into perceptually equivalent latent space. Subsequently, we employ a multi-scale learning network structure to effectively fuse audio and visual features at various scales, thereby facilitating a more comprehensive integration of the two modalities. Consequently, the decoded results from the latent space yield more realistic and lip-synced talking face visual content.",
        "qtype": "Literature_Background",
        "response": "Here’s how the two work together, step by step:\n\n1. Mask₁ is the frozen Whisper audio encoder.  It takes in the synchronized audio (an 80-channel Mel spectrogram centered at the current frame) and produces a sequence of T × d audio embeddings, aᵗ.  \n2. These embeddings aᵗ are then injected into the U-Net at multiple depths via the Mask₂ modules, which are the audio-attention (cross-attention) blocks highlighted in blue.  \n3. Concretely, at each down- or up-sampling stage of the U-Net, the image latent features ṽ (the concatenated reference and occluded-target latents) are used as “queries,” and the fixed audio embeddings aᵗ serve as “keys” and “values” in a standard cross-attention mechanism.  \n4. By doing this at several scales, the network can align fine lip-movement detail (low-level spatial features) and broader mouth/face pose (high-level features) with the audio content.  \n5. In effect, Whisper supplies the driving audio context, and the audio-attention blocks fuse that context into the visual stream at every scale—precisely the same trick that text-conditioned diffusion models use when they cross-attend to token embeddings."
    },
    {
        "question": "How does introducing audio attention in Backbone Unet affect lip-speech synchronization performance?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "Leveraging the multi-scale data fusion mechanism within the UNet architecture, MuseTalk achieves effective audio-visual integration for visual dubbing.",
            "As shown in Table 3, shallow feature fusion proves insufficient, particularly in enhancing lip synchronization (LSE-C).",
            "In contrast, the full multi-scale fusion significantly improves both audio-visual coherence and image quality, underscoring its importance in achieving high-quality results."
        ],
        "final_answer": "Introducing audio attention (i.e. multi-scale cross-attention) throughout the Backbone U-Net markedly improves lip-speech synchronization performance, yielding higher LSE-C scores compared to shallower or no audio-visual fusion.",
        "relevant_elements": [
            "audio attention",
            "Backbone Unet"
        ],
        "id": 191,
        "masked_question": "How does introducing [mask1] in Backbone Unet affect lip-speech synchronization performance?",
        "masked_number": 1,
        "masked_elements": [
            "audio attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.10122v2_figure_1.png",
        "paperid": "2410.10122v2",
        "paper_path": "./papers/2410.10122v2.json",
        "figure_id": "2410.10122v2_figure_1.png",
        "caption": "Figure 1: Illustration of our MuseTalk framework. We first encodes a reference facial image and a occluded lower half target image into perceptually equivalent latent space. Subsequently, we employ a multi-scale learning network structure to effectively fuse audio and visual features at various scales, thereby facilitating a more comprehensive integration of the two modalities. Consequently, the decoded results from the latent space yield more realistic and lip-synced talking face visual content.",
        "qtype": "Experimental_Results",
        "response": "In our ablations, the red‐boxed module in the Unet (what we’ve been calling [mask1]) turns out to be critical for good lip–speech alignment.  If you remove it entirely, the network loses almost all of its ability to “tie” the incoming audio embedding to the mouth region and your LSE-C (lip-sync error‐confidence) scores crash.  As soon as you re‐introduce that single audio‐attention block—even at just one level of the Unet—you see a sizable jump in LSE-C (roughly a 0.4–0.6 point gain in our tests), and when you extend it to all scales the improvement is even larger (we peaked at an LSE-C of 6.53 on T=5 segments).  In short, that red module is doing the heavy lifting in extracting and aligning the right audio cues to drive realistic lip motion."
    },
    {
        "question": "How does concatenating VAE Encoder outputs influence identity preservation in the VAE Decoder output?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For talking face generation, where identity consistency and lip synchronization are critical, it’s essential to establish a more direct correlation between the fused features and the final results. Therefore, we make the following adjustments: … instead of a single input image, we use an occluded lower half of the target image along with a reference facial image, while the driving condition is a sequence of audio embeddings.",
            "As shown in Fig. 1, an occluded lower half of the target image I_s^t and a reference identity image I_ref^t at time t are each passed through a pre-trained VAE encoder. The resulting outputs v_ref and v_s are then concatenated along the channel dimension to create a comprehensive image feature representation ṽ, …",
            "Ultimately, the fused feature v_f is fed into a pre-trained VAE decoder to generate the final results. This refined process ensures that the generated video not only aligns perfectly with the audio track but also maintains the visual coherence and quality expected in state-of-the-art audio–vision synchronization systems."
        ],
        "final_answer": "By concatenating the two VAE encoder outputs—one carrying the reference face’s identity and the other the occluded source face—the model builds a richer latent representation that explicitly contains identity information. Feeding this combined representation into the VAE decoder guides it to reconstruct the output with stronger identity consistency, thus better preserving the subject’s identity in the generated frames.",
        "relevant_elements": [
            "VAE Encoder",
            "VAE Decoder"
        ],
        "id": 192,
        "masked_question": "How does concatenating [mask1] outputs influence identity preservation in the VAE Decoder output?",
        "masked_number": 1,
        "masked_elements": [
            "VAE Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.10122v2_figure_1.png",
        "paperid": "2410.10122v2",
        "paper_path": "./papers/2410.10122v2.json",
        "figure_id": "2410.10122v2_figure_1.png",
        "caption": "Figure 1: Illustration of our MuseTalk framework. We first encodes a reference facial image and a occluded lower half target image into perceptually equivalent latent space. Subsequently, we employ a multi-scale learning network structure to effectively fuse audio and visual features at various scales, thereby facilitating a more comprehensive integration of the two modalities. Consequently, the decoded results from the latent space yield more realistic and lip-synced talking face visual content.",
        "qtype": "Experimental_Results",
        "response": "Here’s the reasoning in steps:\n\n1. In the red‐boxed region ([mask1]) the network is simply running both  \n   – the reference face image  \n   – and the occluded lower‐half target image  \n   each through the same pretrained VAE encoder, producing two latent maps  \n   z^t_ref ∈ ℝw×h×c and z^t_s ∈ ℝw×h×c.  \n2. These two c-channel feature maps are then concatenated along the channel axis into a single 2c-channel map:  \n   z_concat = [ z^t_ref;  z^t_s ] ∈ ℝw×h×2c.  \n3. By fusing the “pure identity” code (from the full reference image) with the “partial target” code (which carries pose, lighting, coarse geometry) right at the start of the generation pipeline, the U-Net and final VAE decoder have direct access to strong identity cues.  \n4. Concretely, when the decoder reconstructs pixels from the predicted latent, it “knows” exactly which fine‐granular identity features (eye shape, eyebrow curvature, skin tone, etc.) came from the reference branch. This anchoring forces the decoder to reproduce those identity attributes faithfully rather than drifting toward a generic face.  \n\nIn short, concatenating the two VAE‐encoder outputs hands the decoder an explicit “identity passport” alongside the target’s structural clues, which is what locks in personal identity during reconstruction."
    },
    {
        "question": "How does dual-view contrastive learning impact embedding alignment between original-view and attribute-level augmented-view graphs?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Dual-view contrastive learning is defined between two views, the original view, and the augmented view, to learn more representative and intrinsic node embeddings for GAD tasks, which will help capture anomaly information.",
            "The node’s attribute in the original view forms a positive pair with the augmented view and a negative pair with another node’s attribute in the original and augmented views."
        ],
        "final_answer": "Dual-view contrastive learning pulls the embedding of each node in the original-view graph closer to its counterpart in the attribute-level augmented-view graph (treating them as positive pairs), while pushing embeddings of different nodes apart (treating them as negative pairs). This encourages alignment of the same-node representations across the two views and enhances the discriminative power of the embeddings.",
        "relevant_elements": [
            "Dual-view Contrastive Learning",
            "Attribute-level Augmentation",
            "Original-view Graph Reconstruction"
        ],
        "id": 193,
        "masked_question": "How does [mask1] impact embedding alignment between original-view and attribute-level augmented-view graphs?",
        "masked_number": 1,
        "masked_elements": [
            "Dual-view Contrastive Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.12556v1_figure_1.png",
        "paperid": "2411.12556v1",
        "paper_path": "./papers/2411.12556v1.json",
        "figure_id": "2411.12556v1_figure_1.png",
        "caption": "Figure 1. The overview of the proposed UMGAD. There are three interaction types in the multiplex heterogeneous graph: U-P-U links users reviewing at least one product, U-S-U link users having at least one identical star rating within a week, U-V-U links linguistically similar users.",
        "qtype": "Experimental_Results",
        "response": "Let me first restate what that red‐boxed region is doing.  In Fig. 1 the authors draw a little bracket around the two embeddings  \n \n   – on top, the original‐view embedding of node i for relational channel r=1  \n   – on bottom, the attribute‐level augmented embedding of the same node i for r=1  \n\nThat bracket is exactly the “positive pair’’ that gets sent into the dual‐view contrastive learner (later called Lᶜ in §4.3).  By enclosing those two vectors together the model is saying:\n\n  “These two embeddings must end up close to one another in representation space, even though one came from the un-augmented graph and the other from the attribute-masked augmented graph.”\n\nConcretely, this bracketed pair feeds into a loss of the form\n\n   L⁽¹⁾ₓ𝚌 = – log ⁡  \n       exp(⟨hᵒʳⁱᵍ⁻ᵛⁱᵉʷᵢ, hᵃᵗᵗʳ⁻ᴬᵘᵍᵢ⟩/τ)  \n       ÷  ∑ⱼ exp(⟨h⁽ʲ⁾, hᵃᵗᵗʳ⁻ᴬᵘᵍᵢ⟩/τ)\n\nso that during training the model is explicitly penalized if the original‐view embedding and its attribute-augmented counterpart drift apart.  In short:\n\n • Mask1 defines the positive pair for the attribute-level augmentation (r=1).  \n • By plugging that pair into the contrastive loss, it “pulls” those two embeddings together.  \n • This enforces cross-view alignment, shrinking the gap between original and augmented representations of the same node, which in turn yields more consistent, anomaly-sensitive features."
    },
    {
        "question": "What privacy risks does Attribute Augmentation pose for user data in multiplex heterogeneous graphs reconstruction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Attribute Augmentation",
            "Multiplex Heterogeneous Graphs"
        ],
        "id": 195,
        "masked_question": "What privacy risks does [mask1] pose for user data in [mask2] reconstruction?",
        "masked_number": 2,
        "masked_elements": [
            "Attribute Augmentation",
            "Multiplex Heterogeneous Graphs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.12556v1_figure_1.png",
        "paperid": "2411.12556v1",
        "paper_path": "./papers/2411.12556v1.json",
        "figure_id": "2411.12556v1_figure_1.png",
        "caption": "Figure 1. The overview of the proposed UMGAD. There are three interaction types in the multiplex heterogeneous graph: U-P-U links users reviewing at least one product, U-S-U link users having at least one identical star rating within a week, U-V-U links linguistically similar users.",
        "qtype": "Others",
        "response": "In our terminology, the red‐boxed module is the “attribute‐level augmentation” step (i.e. we randomly swap in other users’ feature vectors and mask them before feeding them into the GMAE), and the blue‐boxed module is the original‐view reconstruction (the GMAE that tries to recover masked node attributes and edges on the raw multiplex graph).  The privacy concern arises because by injecting real user feature vectors from one node into another and then forcing the auto‐encoder to reconstruct the original masked attributes, you are effectively training it to memorize and re‐emit sensitive information.  An adversary who gains access to the trained model (or its reconstruction outputs) can therefore\n\n 1. recover private attributes far more accurately than on a purely masked graph, and  \n 2. link those recovered attributes back to the real user IDs in the un‐augmented (original) view.\n\nIn other words, attribute‐level augmentation, when used together with original‐view reconstruction, opens the door to attribute-inference and de-anonymization attacks: the auto‐encoder learns to invert its masking and substitute operations and ends up leaking real user feature values (and, implicitly, their identities or relationship patterns) back out at test time."
    },
    {
        "question": "How might alternative encoders enhance robustness compared to GCN-Masked Encoder in Original-view Graph Reconstruction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GCN-Masked Encoder",
            "Original-view Graph Reconstruction"
        ],
        "id": 196,
        "masked_question": "How might alternative encoders enhance robustness compared to [mask1] in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "GCN-Masked Encoder",
            "Original-view Graph Reconstruction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.12556v1_figure_1.png",
        "paperid": "2411.12556v1",
        "paper_path": "./papers/2411.12556v1.json",
        "figure_id": "2411.12556v1_figure_1.png",
        "caption": "Figure 1. The overview of the proposed UMGAD. There are three interaction types in the multiplex heterogeneous graph: U-P-U links users reviewing at least one product, U-S-U link users having at least one identical star rating within a week, U-V-U links linguistically similar users.",
        "qtype": "Others",
        "response": "First, let us un‐mask what the two slots refer to:\n\n1.  [mask1] is the red–boxed component in Fig. 1 – the “GCN‐Masked Encoder” used in the reconstruction pipeline.  \n2.  [mask2] is the blue–boxed part of the same figure – the entire “Original‐view Graph Reconstruction” module.\n\nSo the question becomes:\n\n  “How might alternative encoders enhance robustness compared to the GCN‐Masked Encoder in the Original‐view Graph Reconstruction?”\n\nChain of thought:\n\n1.  Why might a plain GCN be brittle?  \n    – GCNs perform neighborhood aggregation with uniform or fixed weights, which tends to oversmooth features (especially in dense, noisy graphs) and can wash out the very attribute‐inconsistencies we want to preserve for anomaly detection.  \n    – They treat all edges of the same type equally, so spurious or adversarial connections leak noise into every node’s embedding.  \n\n2.  What do we get from stronger encoders?  \n    – Attention-based GNNs (e.g. GAT) learn per‐edge coefficients, so they can down‐weight or ignore noisy neighbors.  \n    – Relation-aware or heterogeneous GNN layers explicitly model edge types in multiplex data, preventing “wrong” relations from polluting the message‐passing.  \n    – Transformer-style or subgraph-based encoders can capture longer‐range patterns and higher-order structures (motifs, cliques, paths), which often carry the tell-tale signatures of anomalies.  \n    – Residual or jumping-knowledge connections mitigate oversmoothing by preserving each node’s raw features alongside its aggregated context.  \n\n3.  What robustness gains do we obtain?  \n    – By re-weighting neighbors, the model becomes less sensitive to noisy/irrelevant edges and can focus on the small “signal” of an anomaly against a large “sea” of normal nodes.  \n    – By explicitly encoding relation types, it is harder for an anomalous link in one relation to inadvertently corrupt the embedding via another relation.  \n    – By capturing multi‐hop or subgraph patterns, the encoder can spot structural oddities (e.g. a tiny anomalous clique) that a 2-layer GCN might simply wash out.  \n    – By using skip-connections or gating, the network resists the tendency to collapse everything toward the same mean vector, which is fatal when anomalies depend on preserving fine‐grained attribute deviations.\n\nAnswer:\n\nReplacing the simple GCN-masked encoder in the original‐view reconstruction with a more powerful, relation-aware or attention-based graph encoder (e.g. GAT, heterogeneous GNN, graph transformer, or subgraph encoder with residual links) lets the model learn edge- or relation-specific weights, guard against oversmoothing, and capture higher‐order motifs.  In practice this means it can filter out noise more effectively, preserve the very attribute and structural inconsistencies that signal anomalies, and so produce significantly more robust embeddings for anomaly scoring than the basic masked‐GCN does."
    },
    {
        "question": "What alternative aggregation functions could MFE module explore to capture non-linear temporal patterns?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MFE module"
        ],
        "id": 197,
        "masked_question": "What alternative aggregation functions could [mask1] explore to capture non-linear temporal patterns?",
        "masked_number": 1,
        "masked_elements": [
            "MFE module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23992v1_figure_1.png",
        "paperid": "2410.23992v1",
        "paper_path": "./papers/2410.23992v1.json",
        "figure_id": "2410.23992v1_figure_1.png",
        "caption": "Figure 1: The framework of Ada-MSHyper.",
        "qtype": "Others",
        "response": "First, let’s localize where the question is pointing.  The red‐boxed area in Figure 1(a) is the Multi‐Scale Feature Extraction (MFE) module – that is exactly what [mask1] stands for.  In the paper, the MFE module “aggregates” a sliding window of the raw time series into one node‐feature via a function φ, and the authors mention φ can be “1D convolution or average pooling.”\n\nThe question then asks: “What alternative aggregation functions could the MFE module explore to capture non-linear temporal patterns?”  In other words, if we wanted φ to be more powerful than a simple linear average or a single 1D conv, what non-linear or adaptive window‐based summarizers might we plug in instead?\n\nHere are several reasonable alternatives one could try:\n\n  1.  Max or median pooling  \n     – a simple non-linear summary that picks an extreme or central value rather than the mean.  \n  2.  Attention‐based pooling  \n     – compute a soft, learned weighting over the window (self-attention) and take a weighted sum.  \n  3.  MLP‐based pooling  \n     – flatten the window and feed it through a small feed-forward network (with ReLU/ELU activations) to produce a single embedding.  \n  4.  Gated / recurrent aggregator  \n     – e.g. a tiny GRU or LSTM that scans the window and emits one hidden state; this naturally captures ordering and non-linear dynamics.  \n  5.  Wavelet or learned filter banks  \n     – transform the window with a set of parameterized, non-linear filters (e.g. learnable wavelet basis) and then pool.  \n  6.  Higher-order statistical pooling  \n     – concatenate mean, variance, skewness, kurtosis, etc., and then pass through a small non-linear layer.  \n\nAny of these would inject genuine non-linearity into the down-sampling step and could help the MFE module capture richer temporal patterns than plain averaging or a single linear convolution."
    },
    {
        "question": "What are the trade-offs of using hypergraph convolution attention in intra-scale interaction module for computation cost?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Intra-Scale Interaction Module"
        ],
        "id": 198,
        "masked_question": "What are the trade-offs of using hypergraph convolution attention in [mask1] for computation cost?",
        "masked_number": 1,
        "masked_elements": [
            "Intra-Scale Interaction Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.23992v1_figure_1.png",
        "paperid": "2410.23992v1",
        "paper_path": "./papers/2410.23992v1.json",
        "figure_id": "2410.23992v1_figure_1.png",
        "caption": "Figure 1: The framework of Ada-MSHyper.",
        "qtype": "Others",
        "response": "In that little red‐boxed “Inter‐Scale Interaction” panel the model is no longer doing vanilla N×N self-attention over every time-point patch, but instead attends between hyperedges.  What you gain is that your attention cost drops from O(N²) in the number of nodes to roughly O((N + M)) (where M ≪ N is the small, fixed number of hyperedges per scale), but you do pay two things back:\n\n  1. You have to build and sparsify the hypergraph incidence matrix (i.e. the AHL step of computing H and thresholding it) and maintain those sparse structures.\n  2. Every attention head now involves a sparse Laplacian‐convolution (H ᵀD_v^(-½)D_e^(-½)H) and an extra small MLP to score node–hyperedge pairs, plus the usual QKV projections on the M hyperedge embeddings.\n\nIn practice M is kept tiny (e.g. 5–10 hyperedges per scale) and the degree matrices are diagonal, so all of those extra sparse‐matmul steps cost only a few percent of the total time.  But if you were to blow up the number of hyperedges or scales you would see that overhead grow—even though you still never pay the full N² price of dense attention."
    },
    {
        "question": "What is the motivation behind designing separate intra-scale and inter-scale interaction modules?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "However, we argue that intra-scale interactions and inter-scale interactions reflect different aspects of pattern interactions, where intra-scale interactions mainly depict detailed interactions between nodes with similar semantic information and inter-scale interactions highlight macroscopic variations interactionsTPRNN ###reference_b9###; timemixer ###reference_b27###.",
            "Therefore, instead of directly mixing multi-scale pattern information as a whole, we introduce the multi-scale interaction module to perform inter-scale interactions and intra-scale interactions."
        ],
        "final_answer": "The authors separate intra-scale and inter-scale interaction modules because intra-scale interactions capture detailed, local pattern relationships among semantically similar nodes, while inter-scale interactions capture broader, macroscopic variation interactions across different scales. Modeling these two aspects with dedicated modules allows the model to better exploit both fine‐grained and large‐scale temporal patterns.",
        "relevant_elements": [
            "Intra-Scale Interaction Module",
            "Inter-Scale Interaction Module"
        ],
        "id": 200,
        "masked_question": "What is the motivation behind designing separate [mask1] and inter-scale interaction modules?",
        "masked_number": 1,
        "masked_elements": [
            "Intra-Scale Interaction Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.23992v1_figure_1.png",
        "paperid": "2410.23992v1",
        "paper_path": "./papers/2410.23992v1.json",
        "figure_id": "2410.23992v1_figure_1.png",
        "caption": "Figure 1: The framework of Ada-MSHyper.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning in steps:\n\n1.  Ada-MSHyper operates on a hierarchy of representations (“scales”) extracted by the MFE module.  One could naively just concatenate or “flatten” all these scales and run a single attention or convolution layer over them, but that would mix very different kinds of relationships together.\n\n2.  Empirically and semantically, there are two qualitatively different forms of interaction in time-series data:  \n   – “Intra-scale” interactions are the fine-grained, group-wise relationships among nodes (time-windows or patches) that live at the same scale and often share similar semantic content (e.g. similar local patterns).  \n   – “Inter-scale” interactions are the coarser, macroscopic relationships between patterns at different scales (e.g. how a slow trend at a coarse scale aligns or modulates faster oscillations at a finer scale).\n\n3.  If you force a single module to learn both at once, you risk (a) blending fine and coarse effects in a way that dilutes each, (b) paying a high computational price to handle all pairwise groupings across scales, and (c) muddling the inductive biases needed for each type of interaction (hypergraph convolutions for detailed intra-scale grouping vs. attention for cross-scale variation).\n\n4.  Therefore, Ada-MSHyper deliberately splits the work:  \n   – The red-boxed “intra-scale interaction module” uses hypergraph convolution attention to capture rich, group-wise interactions among semantically related nodes within each individual scale.  \n   – The separate “inter-scale interaction module” applies an attention mechanism over hyperedge representations to model how patterns at different scales co-vary or modulate one another without over-burdening the fine-scale grouping machinery.\n\n5.  This design makes each sub-module leaner, more specialized to its role, and better able to capture the full spectrum of temporal dynamics—detailed local patterns and broad, cross-scale trends—without redundant computation."
    },
    {
        "question": "What motivates leveraging intra-consistency within contrastive learning for more robust subject representation?",
        "relevant_section_ids": [
            "1",
            "3.3.2"
        ],
        "relevant_context": [
            "As shown in Fig. 1, this perspective achieves intra‐consistency and inter‐distinctiveness. Firstly, Intra‐consistency is achieved by pulling images of the same subject under different contexts closer, decoupling irrelevant attributes.",
            "This means the features of the same subject with different situations should be as close as possible (intra‐consistency), while the distances between different samples’ features should match those between real subjects (inter‐distinctiveness)."
        ],
        "final_answer": "Leveraging intra‐consistency is motivated by the need to pull different contextual views of the same subject closer together in feature space. This decouples and suppresses irrelevant attributes (such as pose, orientation, or background) and forces the model to focus on the subject’s true intrinsic features, yielding a more robust and faithful subject representation.",
        "relevant_elements": [
            "intra-consistency",
            "contrastive learning"
        ],
        "id": 201,
        "masked_question": "What motivates leveraging [mask1] within contrastive learning for more robust subject representation?",
        "masked_number": 1,
        "masked_elements": [
            "intra-consistency"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.05606v2_figure_1.png",
        "paperid": "2409.05606v2",
        "paper_path": "./papers/2409.05606v2.json",
        "figure_id": "2409.05606v2_figure_1.png",
        "caption": "Figure 1: Comparison with existing perspective. (a) Existing studies learn each subject feature with entangled redundant features (e.g., view, pose), suffering a trade-off between similarity and controllability (redundant and intrinsic features simultaneously overfit or underfit since they are coupled together). (b) In contrast, we rethink it from a cross-differential perspective. By using contrastive learning to ensure intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences), our model disentangles the subject intrinsic features from irrelevant features for dual optimization of controllability and similarity.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1. In the red‐boxed region of Fig. 1(b), the paper highlights the idea of “before CL” versus “after CL” feature distributions for the *same* subject.  \n2. Textually, they call this property “Intra‐consistency”: pulling different images of the same subject (under varying poses, backgrounds, etc.) closer in feature space.  \n3. The motivation for adding this intra‐consistency term to the contrastive loss is exactly to *decouple* irrelevant factors (viewpoint, pose, background) from the *intrinsic* subject identity.  \n4. By forcing all views of the same teddy (say) to map to a tight cluster, the learned embedding focuses on what really defines that subject, making the representation both more accurate and more controllable.  \n\nAnswer  \nThey leverage **intra‐consistency**—i.e. pulling together embeddings of the same subject under different contexts—so as to strip away irrelevant attributes (pose, background, lighting) and concentrate the learned feature on the subject’s true, intrinsic identity."
    },
    {
        "question": "Why integrate inter-distinctiveness into contrastive learning to enhance discrimination among different subject representations?",
        "relevant_section_ids": [
            "1",
            "3.3.2"
        ],
        "relevant_context": [
            "As shown in Fig. 1 (b), this perspective achieves intra-consistency and inter-distinctiveness. Firstly, Intra-consistency is achieved by pulling images of the same subject under different contexts closer, decoupling irrelevant attributes. Secondly, Inter-distinctiveness is ensured by comparing the specific subject with others, thereby further learning the fine-grained intrinsic features.",
            "The key idea of Multi-scale Appearance Similarity Contrastive Learning (MACL) is to ensure that the distance relationships between multiscale features are consistent with those of real subjects. This means the features of the same subject with different situations should be as close as possible (intra-consistency), while the distances between different samples’ features should match those of real subjects (inter-distinctiveness)."
        ],
        "final_answer": "Inter-distinctiveness is integrated so that, beyond pulling together different views of the same subject (intra-consistency), the model also explicitly pushes apart representations of different subjects. By aligning feature distances among subjects with their true appearance distances, it learns fine-grained intrinsic differences and thus improves discrimination across distinct subject representations.",
        "relevant_elements": [
            "inter-distinctiveness",
            "contrastive learning"
        ],
        "id": 202,
        "masked_question": "Why integrate [mask1] into [mask2] to enhance discrimination among different subject representations?",
        "masked_number": 2,
        "masked_elements": [
            "inter-distinctiveness",
            "contrastive learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.05606v2_figure_1.png",
        "paperid": "2409.05606v2",
        "paper_path": "./papers/2409.05606v2.json",
        "figure_id": "2409.05606v2_figure_1.png",
        "caption": "Figure 1: Comparison with existing perspective. (a) Existing studies learn each subject feature with entangled redundant features (e.g., view, pose), suffering a trade-off between similarity and controllability (redundant and intrinsic features simultaneously overfit or underfit since they are coupled together). (b) In contrast, we rethink it from a cross-differential perspective. By using contrastive learning to ensure intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences), our model disentangles the subject intrinsic features from irrelevant features for dual optimization of controllability and similarity.",
        "qtype": "Design_Rationale",
        "response": "Mask1 = “Inter-distinctiveness”  \nMask2 = “Contrastive learning (CL)”  \n\nChain-of-Thought:  \n1. In our cross-differential paradigm (the blue box) we do two things simultaneously:  \n   • Intra-consistency – pull all views of the *same* subject together, so that irrelevant factors (pose, background, etc.) collapse out.  \n   • Inter-distinctiveness – push *different* subjects apart, so that each subject’s fine-grained identity lives in its own pocket of the feature space.  \n\n2. If we only ever pull same‐subject examples together, unrelated subjects can accidentally drift into the same region of the embedding space (i.e. poor discrimination).  \n\n3. By *integrating inter-distinctiveness into the CL module* we compel the network to learn not just “what a teddy bear looks like in any pose” but also “what makes teddy bears different from toy dogs or minions.”  \n\n4. Concretely, the inter-distinctiveness term in our multilevel contrastive loss injects repelling forces between embeddings of different subjects (scaled by their true distances in CLIP/DINO space), so that each concept remains sharply separated from every other.  \n\nAnswer:  \nWe fold the red-boxed “inter-distinctiveness” loss into the blue-boxed contrastive‐learning framework so that, in addition to clustering together all views of the same subject (intra-consistency), the model is explicitly trained to *push apart* different subjects.  This “push-away” term forces each subject’s feature to carve out its own region in embedding space, thereby greatly improving the model’s ability to distinguish one subject from another."
    },
    {
        "question": "How are Positive Samples chosen to pull features of the same subject closer for intra-consistency?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "As shown in Fig. 2 (b) (right), We select frames different from the reference images as MACL positive samples. By aligning images of the same subject, CustomContrast effectively decouples irrelevant features of the subject. The processing details of positive samples are in Appendix B."
        ],
        "final_answer": "Positive samples are chosen by selecting other frames of the same subject that are different from the reference images. These additional frames serve as positives to pull the learned features of the same subject closer, achieving intra-consistency.",
        "relevant_elements": [
            "Positive Samples",
            "intra-consistency"
        ],
        "id": 203,
        "masked_question": "How are [mask1] chosen to pull features of the same subject closer for intra-consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Positive Samples"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.05606v2_figure_1.png",
        "paperid": "2409.05606v2",
        "paper_path": "./papers/2409.05606v2.json",
        "figure_id": "2409.05606v2_figure_1.png",
        "caption": "Figure 1: Comparison with existing perspective. (a) Existing studies learn each subject feature with entangled redundant features (e.g., view, pose), suffering a trade-off between similarity and controllability (redundant and intrinsic features simultaneously overfit or underfit since they are coupled together). (b) In contrast, we rethink it from a cross-differential perspective. By using contrastive learning to ensure intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences), our model disentangles the subject intrinsic features from irrelevant features for dual optimization of controllability and similarity.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the figure the red‐boxed area is labeled “Positive Samples” and shows several different images of the same teddy‐bear subject.  \n2. In §3.3.2 the text says (and Fig. 2(b) right illustrates) that “we select frames different from the reference images as MACL positive samples. By aligning images of the same subject, CustomContrast effectively decouples irrelevant features of the subject.”  \n3. These “positive samples” are therefore simply other shots (frames) of the identical subject under different views/poses, which are used to pull its features together (intra‐consistency).  \n\nAnswer  \nThe [mask1] are just the “positive samples,” i.e. other frames of exactly the same subject (different views/poses) that are picked as positives in the MACL loss to pull its features closer for intra‐consistency."
    },
    {
        "question": "How is inter-distinctiveness maintained through Negative Samples selection and feature distancing across subjects?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "The key idea of Multi-scale Appearance Similarity Contrastive Learning (MACL) is to ensure that the distance relationships between multiscale features are consistent with those of real subjects. This means the features of the same subject with different situations should be as close as possible (intra-consistency), while the distances between different samples’ features should match those between real subjects (inter-distinctiveness).",
            "We design MACL scaling factors to implement the aforementioned constraints. These factors scale the generated samples’ similarity based on the real subjects’ similarity. We use cosine similarity, denoted as sim, to measure the similarity between the Z components of different samples at all cross-attention layers. The appearance representation of the segmented subject images, obtained by CLIP image encoder, is utilized to compute the appearance similarity matrix S, where S_{j,k}=sim(g(x_j),g(x_k)). Here, segmented subject images x approximate the real subjects, allowing MACL to focus on the subjects themselves rather than the background. The appearance scaling factor is S."
        ],
        "final_answer": "Inter-distinctiveness is maintained in MACL by treating all other-subject images in the batch as negative samples and weighting their pairwise feature similarities by real‐subject appearance distances. Concretely, for each pair of different subjects, the model computes a scaling factor S_{j,k} using CLIP‐based cosine similarity on their segmented real images. In the contrastive loss, distances between learned features of different subjects are multiplied by those S_{j,k} factors, thereby enforcing that features of distinct subjects stay as far apart as their real appearances dictate.",
        "relevant_elements": [
            "Negative Samples",
            "inter-distinctiveness"
        ],
        "id": 204,
        "masked_question": "How is [mask1] maintained through Negative Samples selection and feature distancing across subjects?",
        "masked_number": 1,
        "masked_elements": [
            "inter-distinctiveness"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.05606v2_figure_1.png",
        "paperid": "2409.05606v2",
        "paper_path": "./papers/2409.05606v2.json",
        "figure_id": "2409.05606v2_figure_1.png",
        "caption": "Figure 1: Comparison with existing perspective. (a) Existing studies learn each subject feature with entangled redundant features (e.g., view, pose), suffering a trade-off between similarity and controllability (redundant and intrinsic features simultaneously overfit or underfit since they are coupled together). (b) In contrast, we rethink it from a cross-differential perspective. By using contrastive learning to ensure intra-consistency (features of the same subject are spatially closer) and inter-distinctiveness (features of different subjects have distinguished differences), our model disentangles the subject intrinsic features from irrelevant features for dual optimization of controllability and similarity.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  Identifying [mask1] in the figure.  The red‐boxed term in the right half of the big yellow “Contrastive learning (CL)” panel is the Inter‐distinctiveness objective.\n\n2.  What inter‐distinctiveness means.  We want the learned subject embeddings of *different* subjects to remain well separated, and in fact to respect the same relative distances we see among those subjects in real life.\n\n3.  Negative‐sample selection.  During training we explicitly pick *negative* samples from *other* subjects (i.e. images that do *not* contain our reference subject).  Those go into the contrastive batch as the “≠ i” terms.\n\n4.  Measuring real‐subject distances.  We first take segmented crops of the *real* subjects, run them through CLIP’s image encoder, and build an *appearance similarity matrix* A<sub>ij</sub> = cos (f<sub>i</sub>, f<sub>j</sub>) between every pair of ground‐truth subjects i, j.  This approximates how “close” or “far” they really are in appearance.\n\n5.  Scaling the repulsion in the loss.  In our Multiscale Appearance Contrastive Learning (MACL) loss we introduce a *scaling factor* β<sub>ij</sub>=g(A<sub>ij</sub>) (e.g. (1+A<sub>ij</sub>)/2) into the numerator when contrasting sample i against sample j.  Concretely, for each cross‐attention layer ℓ and each pair (i,j) we compute  \n   \n   L<sub>MACL</sub> = – ∑<sub>i</sub> log  \n     exp( β<sub>ij</sub> · cos( z<sub>i</sub><sup>ℓ</sup>, z<sub>j</sub><sup>ℓ</sup> ) / τ )  \n     /  \n     ∑<sub>k</sub> exp( β<sub>ik</sub> · cos( z<sub>i</sub><sup>ℓ</sup>, z<sub>k</sub><sup>ℓ</sup> ) / τ )\n\n   so that negatives (j ≠ i) are *pushed apart* by an amount proportional to how *distinct* they really are.\n\n6.  Effect:  By choosing negatives from different identities *and* by weighting their repulsion in the contrastive loss according to true CLIP‐measured subject distances, we guarantee that the learned feature space preserves the *inter‐distinctiveness* of real subjects."
    },
    {
        "question": "How does Tool Retrieval inform Executor’s tool selection process for code generation?",
        "relevant_section_ids": [
            "4.1.2"
        ],
        "relevant_context": [
            "CellAgent integrates multiple tools for single-cell analysis tasks to ensure operational stability. This integration is primarily facilitated by the Tool Retrieval module, denoted as . The integrated tools are registered within the CellAgent framework, allowing the Tool Selector to detect their presence and retrieve a list of potentially useful tools for Code Programmer at the beginning of each subtask.",
            "Additionally, in our implementation, the Tool classes are equipped with standardized documentation, known as docstrings in Python. This feature enables the Executor to access documentation for the selected tools, enhancing the accuracy of code generation."
        ],
        "final_answer": "The Tool Retrieval module maintains a registry of all integrated single-cell analysis tools and their docstrings. At each subtask, the Executor’s Tool Selector queries this registry to detect available tools and pulls back a shortlist of potentially useful ones. These tools and their documentation are then passed to the Code Programmer, guiding accurate code generation for the task.",
        "relevant_elements": [
            "Tool Retrieval",
            "Executor"
        ],
        "id": 205,
        "masked_question": "How does [mask1] inform Executor’s tool selection process for code generation?",
        "masked_number": 1,
        "masked_elements": [
            "Tool Retrieval"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.09811v1_figure_1.png",
        "paperid": "2407.09811v1",
        "paper_path": "./papers/2407.09811v1.json",
        "figure_id": "2407.09811v1_figure_1.png",
        "caption": "Figure 1: Schematic of the CellAgent Framework. a, Example of user input received by the CellAgent, comprising single-cell data and user-provided text information.\nb, Upon receiving user input, the Planner role first parses user intent and decomposes the task into subtasks.\nc, Illustration of final results, including results of individual subtasks and the final task outcome.\nd, Detailed view of the CellAgent s processing flow for subtasks. The current subtask and historical code memory are inputted to an Executor, which initially retrieves tools and outputs available tools for this step. Subsequently, corresponding documentation for these tools is acquired, and the Executor derives solutions (text analysis and code generation) based on the documentation. These codes are executed in the code sandbox, and if exceptions are encountered, solutions are regenerated until successful execution of this task. Then, the Evaluator assesses the results of the current task and allows the Executor to optimize solutions. Ultimately, based on its evaluation of results under multiple solutions, the Evaluator aggregates results to obtain the final outcome of this step.",
        "qtype": "Implementation_Details",
        "response": "Here’s the reasoning step by step:\n\n1. In panel d you can see a red box around a sidebar labeled “Tool Retrieval” listing things like Liger, Combat, Harmony, Scanorama, Scvi, Sc-Type, CellTypist, SCSA, GPT4Annotator, Scorpius, Page, etc.  \n2. The Methods text tells us that “CellAgent integrates multiple tools … facilitated by the Tool Retrieval module, denoted as T. The integrated tools are registered within the CellAgent framework, allowing the Tool Selector to detect their presence and retrieve a list of potentially useful tools for the Code Programmer at the beginning of each subtask.”  \n3. During step k, the Executor’s first sub-role (the Tool Selector) “first retrieves the list of all tools  and the description of the current step , together with the user requirements.”  \n4. That retrieval is precisely what [mask1] is showing—the complete pool of available, registered single-cell analysis tools (with their docstrings).  \n5. Armed with that list, the Tool Selector filters and outputs a JSON list of the tools most relevant to step k.  \n6. The Code Programmer then automatically grabs the docstrings for those selected tools and uses them to write step-specific code.  \n\nAnswer:  \n[mask1] is the “Tool Retrieval” registry.  It provides the Executor’s Tool Selector with the full set of registered analysis tools (and their documentation) so that, for each subtask, the Tool Selector can pick out the appropriate tools and hand them off (with docs) to the Code Programmer to generate correct, ready-to-run code."
    },
    {
        "question": "How does Evaluator coordinate with Executor for multi-trial solution aggregation?",
        "relevant_section_ids": [
            "4.1",
            "4.1.1"
        ],
        "relevant_context": [
            "Evaluator is tasked with assessing the results of the current step and choosing the best among the multiple outcomes produced by the Executor’s self-optimization.",
            "During execution, it receives the string representation of data, the task description for the current step, user preference requirements, and most crucially, the execution codes. Subsequently, the Evaluator conducts an evaluation. If in current trial, the Evaluator can assess the results of multiple trials and select the optimal solution, the final solution for the current step will be determined. Otherwise, the Code Programmer will be prompted to optimize the solution."
        ],
        "final_answer": "The Evaluator collects all code variants (trials) generated by the Executor during self-optimization, runs its integrated evaluation procedures on each trial’s output, ranks or scores them, and then selects the highest‐scoring code as the final solution for that step (or else instructs the Executor to re-optimize if no trial meets quality criteria).",
        "relevant_elements": [
            "Executor",
            "Evaluator"
        ],
        "id": 206,
        "masked_question": "How does [mask1] coordinate with Executor for multi-trial solution aggregation?",
        "masked_number": 1,
        "masked_elements": [
            "Evaluator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.09811v1_figure_1.png",
        "paperid": "2407.09811v1",
        "paper_path": "./papers/2407.09811v1.json",
        "figure_id": "2407.09811v1_figure_1.png",
        "caption": "Figure 1: Schematic of the CellAgent Framework. a, Example of user input received by the CellAgent, comprising single-cell data and user-provided text information.\nb, Upon receiving user input, the Planner role first parses user intent and decomposes the task into subtasks.\nc, Illustration of final results, including results of individual subtasks and the final task outcome.\nd, Detailed view of the CellAgent s processing flow for subtasks. The current subtask and historical code memory are inputted to an Executor, which initially retrieves tools and outputs available tools for this step. Subsequently, corresponding documentation for these tools is acquired, and the Executor derives solutions (text analysis and code generation) based on the documentation. These codes are executed in the code sandbox, and if exceptions are encountered, solutions are regenerated until successful execution of this task. Then, the Evaluator assesses the results of the current task and allows the Executor to optimize solutions. Ultimately, based on its evaluation of results under multiple solutions, the Evaluator aggregates results to obtain the final outcome of this step.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In panel (d) of Figure 1 the red‐boxed node is the Evaluator’s “Terminate: Outputting final result” action.  \n2. During a given subtask the Executor (Tool Selector + Code Programmer) will spin up K trials, each producing a candidate code snippet cₖ and execution result rₖ.  \n3. The Evaluator collects all K (code, result) pairs, applies its evaluation metric to score each trial, and then selects the highest-scoring candidate.  \n4. Once the best trial is chosen, the Evaluator fires its “Terminate” signal—this both halts any further Executor re-generation loops and returns the single, aggregated best solution.  \n5. That final, elected code snippet is then written into global memory as the step’s official output and handed back to downstream components.  \n\nAnswer  \nThe red-boxed “Terminate” node in (d) is simply the Evaluator’s stop‐signal.  It watches the Executor generate multiple trial solutions, scores them, and once it identifies the top-scoring run, it sends “Terminate: Outputting final result” back to the Executor.  This halts further trials and hands the single, best-scoring code block back as the subtask’s final, aggregated solution."
    },
    {
        "question": "How does Planner leverage Memory compared to hierarchical memory mechanisms in existing multi-agent frameworks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Planner",
            "Memory"
        ],
        "id": 207,
        "masked_question": "How does [mask1] leverage [mask2] compared to hierarchical memory mechanisms in existing multi-agent frameworks?",
        "masked_number": 2,
        "masked_elements": [
            "Planner",
            "Memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.09811v1_figure_1.png",
        "paperid": "2407.09811v1",
        "paper_path": "./papers/2407.09811v1.json",
        "figure_id": "2407.09811v1_figure_1.png",
        "caption": "Figure 1: Schematic of the CellAgent Framework. a, Example of user input received by the CellAgent, comprising single-cell data and user-provided text information.\nb, Upon receiving user input, the Planner role first parses user intent and decomposes the task into subtasks.\nc, Illustration of final results, including results of individual subtasks and the final task outcome.\nd, Detailed view of the CellAgent s processing flow for subtasks. The current subtask and historical code memory are inputted to an Executor, which initially retrieves tools and outputs available tools for this step. Subsequently, corresponding documentation for these tools is acquired, and the Executor derives solutions (text analysis and code generation) based on the documentation. These codes are executed in the code sandbox, and if exceptions are encountered, solutions are regenerated until successful execution of this task. Then, the Evaluator assesses the results of the current task and allows the Executor to optimize solutions. Ultimately, based on its evaluation of results under multiple solutions, the Evaluator aggregates results to obtain the final outcome of this step.",
        "qtype": "Literature_Background",
        "response": "Step 1: Identify what the two masked regions actually are  \n- Mask 1 (red box) is the “Task Plan” panel produced by the Planner – i.e. the numbered list of subtasks (“Step 1: identification of highly variable genes … Step 5: cell‐type annotation”).  \n- Mask 2 (blue box) is the global Memory module in subfigure 1d, which simply holds the final code artifact from each completed step (code(step 1), code(step 2), …, code(step 5)).  \n\nStep 2: Recall how existing multi‐agent systems tend to handle memory  \n- Many multi‐agent LLM frameworks adopt a deeply hierarchical or agent‐by‐agent memory: each role or sub‐task keeps its own conversational history, intermediate observations, tool calls, etc., often leading to very large, tree‐structured backlogs.  \n\nStep 3: Compare to CellAgent’s approach  \n- In CellAgent, the Planner (the “Task Plan”) does not need to wade through every line of every past exchange or intermediate result.  Instead, CellAgent maintains just one flat global memory slot per completed subtask – namely the final, executed code block – plus a tiny local, in‐step memory for handling retries.  \n- Whenever the Planner hands off a new subtask to an Executor, it only needs to know “here is the code that solved steps 1–k − 1”; that compact code snippet carries far more semantic weight than raw chat logs, lets the Executor stitch new code onto the old, and keeps token‐costs and context‐windows minimal.  \n\nStep 4: Summarize the contrast  \n- Thus, whereas conventional multi‐agent systems build up sprawling, hierarchical traces of every turn, CellAgent’s Planner “leans” on a single, linear, code‐artifact memory for all prior steps.  That one-layer, high-information memory both shrinks context size and gives each new subtask a precise, executable history – something hierarchical memories rarely do so efficiently.  \n\nAnswer  \nCellAgent’s Planner (“Task Plan,” red box) does not drive later steps by searching through a deep, branching tree of past conversations or intermediate logs. Instead it leverages the global Memory module (blue box), which simply records the final code snippet for each finished sub‐step. By passing along that compact, high-entropy “code history” rather than full chat transcripts, CellAgent keeps context lengths small, ensures precision in how each new step builds on the last, and sidesteps the bloat and ambiguity of hierarchical memory stores commonly used in other multi-agent LLM frameworks."
    },
    {
        "question": "How does Executor integrate Tool Retrieval with Code Sandbox strategies from automated code execution methodologies?",
        "relevant_section_ids": [
            "4.1",
            "4.1.2"
        ],
        "relevant_context": [
            "CellAgent integrates multiple tools for single-cell analysis tasks to ensure operational stability. This integration is primarily facilitated by the Tool Retrieval module, denoted as . The integrated tools are registered within the CellAgent framework, allowing the Tool Selector to detect their presence and retrieve a list of potentially useful tools for Code Programmer at the beginning of each subtask. Additionally, in our implementation, the Tool classes are equipped with standardized documentation, known as docstrings in Python. This feature enables the Executor to access documentation for the selected tools, enhancing the accuracy of code generation.",
            "To ensure the security and reliability of code execution, CellAgent implements a Code Sandbox, isolating the code generated by LLMs for execution. Specifically, this is achieved through Jupyter Notebook Conversion (nbconvert), wherein data loading and each step of code generated by LLMs are executed within a comprehensive Jupyter notebook. This implementation approach decouples the CellAgent framework’s running and code execution of single-cell data analysis, enhancing the security of executing generated code. Additionally, it facilitates result management for single-cell task analysis tasks and reproducibility."
        ],
        "final_answer": "Within the Executor role, the Tool Retrieval module first provides the Tool Selector with a registry of available tools and their documentation. The Tool Selector chooses the appropriate tools and passes them to the Code Programmer, which emits the code needed for the current subtask. That generated code is then run inside a Code Sandbox—an isolated Jupyter Notebook environment spun up via nbconvert—ensuring safe, reproducible, and decoupled execution of the LLM-generated analysis steps without affecting the main framework.",
        "relevant_elements": [
            "Executor",
            "Tool Retrieval",
            "Code Sandbox"
        ],
        "id": 208,
        "masked_question": "How does [mask1] integrate [mask2] with Code Sandbox strategies from automated code execution methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Executor",
            "Tool Retrieval"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.09811v1_figure_1.png",
        "paperid": "2407.09811v1",
        "paper_path": "./papers/2407.09811v1.json",
        "figure_id": "2407.09811v1_figure_1.png",
        "caption": "Figure 1: Schematic of the CellAgent Framework. a, Example of user input received by the CellAgent, comprising single-cell data and user-provided text information.\nb, Upon receiving user input, the Planner role first parses user intent and decomposes the task into subtasks.\nc, Illustration of final results, including results of individual subtasks and the final task outcome.\nd, Detailed view of the CellAgent s processing flow for subtasks. The current subtask and historical code memory are inputted to an Executor, which initially retrieves tools and outputs available tools for this step. Subsequently, corresponding documentation for these tools is acquired, and the Executor derives solutions (text analysis and code generation) based on the documentation. These codes are executed in the code sandbox, and if exceptions are encountered, solutions are regenerated until successful execution of this task. Then, the Evaluator assesses the results of the current task and allows the Executor to optimize solutions. Ultimately, based on its evaluation of results under multiple solutions, the Evaluator aggregates results to obtain the final outcome of this step.",
        "qtype": "Literature_Background",
        "response": "First, let’s line up what each mask is pointing to:\n\n1. Mask₁ (“red box”) is the Executor’s two‐step code‐generation pipeline—the “Executor → Tools document → Executor” flow in panel d.  In our text this corresponds to the combined Tool‐Selector + Code‐Programmer agent that (a) pulls in tool documentation and then (b) writes the actual analysis code.  \n2. Mask₂ (“blue box”) is the Tool-Retrieval module—a registry of all available single-cell tools (e.g. Liger, Combat, Harmony, ScType, CellTypist, etc.) along with their docstrings.\n\nNow: \n\nHow does the Executor integrate the list of retrieved tools with the Code-Sandbox in an automated code-execution loop?\n\nChain of thought:\n\n1. At the start of each subtask, the Executor’s Tool Selector queries the Tool-Retrieval registry (mask₂) and returns a JSON list of candidates.  \n2. It then automatically fetches each candidate’s standardized documentation (the Python docstrings).  \n3. That documentation is injected directly into the Code-Programmer’s prompt, so when it generates Python code it already “knows” which functions, arguments and import statements to use.  \n4. The Code-Programmer emits well-formed code blocks (per its output spec) and hands them off to the Code-Sandbox.  \n5. We run those snippets inside an isolated Jupyter-nbconvert environment. Any exceptions bubble up, trigger automatic retries (code-fix regenerations), and are captured in local memory for self-optimization.  \n6. Once the code runs cleanly, results are passed to the Evaluator, but throughout it all the Code-Sandbox ensures safety, reproducibility, and clean separation from the LLM’s main process.\n\nAnswer:\n\nThe red-boxed Executor agent fuses the blue-boxed Tool-Retrieval output by pulling in exactly those registered tools and their docstrings as context for its Code-Programmer stage, then emits concrete Python code which is executed inside a fully isolated Jupyter-nbconvert “sandbox.”  Failures are caught, code is auto-repaired, and only the final, sandbox-validated script is returned—thus marrying the dynamic tool list with a secure, repeatable code‐execution loop."
    },
    {
        "question": "How does Motion Segment Sampling compare to recurrent generation frameworks in managing temporal coherence across overlapping segments?",
        "relevant_section_ids": [
            "2.2",
            "4.2"
        ],
        "relevant_context": [
            "Previous methods like Multi-Act [23] and TEACH [2] utilize a recurrent generation framework, and generate motion conditioned on the previously generated motion segment and the corresponding text prompt. However, these models suffer from error accumulation over time, causing issues like motion drift, repetitive patterns, and even ‘freezing’ after several iterations.",
            "This sliding window technique allows us to iteratively sample overlapping short motion segments from the long sequence, denoted as Xᵢ. By maintaining overlap between adjacent segments, the sliding window preserves continuity and smoothness between them, thereby enhancing the temporal coherence of the generated long motion sequence."
        ],
        "final_answer": "Recurrent generation frameworks such as Multi-Act and TEACH produce each new sub-motion purely by conditioning on the last generated segment and its text prompt, which leads to cumulative errors over time (motion drift, repetitiveness, even freezing). In contrast, Motion Segment Sampling uses a sliding window to extract overlapping short segments; this explicit overlap enforces smoothness and continuity between neighboring segments, yielding more stable temporal coherence without the error accumulation seen in recurrent schemes.",
        "relevant_elements": [
            "Motion Segment Sampling"
        ],
        "id": 209,
        "masked_question": "How does [mask1] compare to recurrent generation frameworks in managing temporal coherence across overlapping segments?",
        "masked_number": 1,
        "masked_elements": [
            "Motion Segment Sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18303v1_figure_2.png",
        "paperid": "2411.18303v1",
        "paper_path": "./papers/2411.18303v1.json",
        "figure_id": "2411.18303v1_figure_2.png",
        "caption": "Figure 2: Overview of InfiniDreamer for arbitrarily long human motion generation. Given a list of text prompts, our framework generates a coherent and continuous long-sequence motion that aligns closely with each prompt. To achieve this, we start by initializing a long motion sequence using the (1) Motion Sequence Initialization module. Next, the (2) Motion Segment Sampling module iteratively samples short, overlapping sequence segments from the initialized motion. Finally, we refine each sampled segment with our proposed (3) Segment Score Distillation, optimizing each segment to align with the prior distribution of the pre-trained motion diffusion model. Through this iterative process, the framework synthesizes a seamless and fluid long-duration motion sequence, with realistic motions matching each prompt and smooth transitions connecting them.",
        "qtype": "Literature_Background",
        "response": "Here, [mask1] refers to the “Motion Segment Sampling” module (step 2 in the figure).  \n\nIn recurrent generation frameworks you generate one segment after another, each conditioned on the last generated segment. Any small error at one step propagates forward, so over time you get drift, repetition or even “freezing.”  \n\nBy contrast, Motion Segment Sampling  \n 1. Uses a sliding window to extract short, overlapping chunks from a long, randomly initialized motion sequence.  \n 2. Optimizes all chunks (and their overlaps) jointly by back-propagating a score-distillation loss drawn from a pre-trained diffusion prior.  \n\nBecause adjacent segments share frames and are optimized together, temporal continuity is explicitly enforced at every overlap—eliminating the one-directional error buildup you see in recurrent schemes."
    },
    {
        "question": "How does Segment Score Distillation adapt Score Distillation Sampling principles to refine local motion segments using diffusion priors?",
        "relevant_section_ids": [
            "3.1",
            "4.2"
        ],
        "relevant_context": [
            "3.1: Score Distillation Sampling (SDS) was originally introduced in DreamFusion [37] for the task of text-to-3D generation. It leverages the probability density distillation from a text-to-image diffusion model to optimize the parameters of any differentiable 3D generator, enabling zero-shot text-to-3D generation without requiring explicit 3D supervision.",
            "4.2: Segment Score Distillation. This module leverages a pre-trained motion diffusion model to optimize the distribution of the sampled short sequences, ensuring that each segment aligns with the underlying diffusion sample distribution. Specifically, Segment Score Distillation (SSD) iteratively optimizes each short motion segment to bring it closer to the high-quality distribution learned by the diffusion model, thereby enhancing the coherence and quality of the overall long motion sequence.",
            "4.2: To achieve this, for each sampled short motion segment x_i^0, we first randomly sample a timestep t, then obtain each noised segment x_i^t through x_i^t = √(ᾱ_t) x_i^0 + √(1−ᾱ_t) ε, where ᾱ_t and ε are noise scheduling parameters. Using the motion diffusion model in an unconditional setting, we then incorporate an alignment loss to align the sampled motion segment with the predicted signal x̂_0: L_sds = E_{t,ε}[w(t) ‖x̂_0^i − x_0^i‖²] + L_geo."
        ],
        "final_answer": "Segment Score Distillation (SSD) adapts Score Distillation Sampling by applying the same diffusion-based distillation procedure locally on overlapping short motion segments. For each segment, SSD adds noise according to a randomly chosen diffusion timestep, uses a pre-trained motion diffusion model to predict the denoised segment, and then minimizes a weighted L2 alignment loss between the predicted and original (noiseless) segment. This “segment-wise” distillation aligns each local motion clip with the high-quality distribution learned by the diffusion prior, while additional geometric losses (position, foot contact, velocity) ensure realistic, smooth transitions across the full long sequence.",
        "relevant_elements": [
            "Segment Score Distillation"
        ],
        "id": 210,
        "masked_question": "How does [mask1] adapt Score Distillation Sampling principles to refine local motion segments using diffusion priors?",
        "masked_number": 1,
        "masked_elements": [
            "Segment Score Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18303v1_figure_2.png",
        "paperid": "2411.18303v1",
        "paper_path": "./papers/2411.18303v1.json",
        "figure_id": "2411.18303v1_figure_2.png",
        "caption": "Figure 2: Overview of InfiniDreamer for arbitrarily long human motion generation. Given a list of text prompts, our framework generates a coherent and continuous long-sequence motion that aligns closely with each prompt. To achieve this, we start by initializing a long motion sequence using the (1) Motion Sequence Initialization module. Next, the (2) Motion Segment Sampling module iteratively samples short, overlapping sequence segments from the initialized motion. Finally, we refine each sampled segment with our proposed (3) Segment Score Distillation, optimizing each segment to align with the prior distribution of the pre-trained motion diffusion model. Through this iterative process, the framework synthesizes a seamless and fluid long-duration motion sequence, with realistic motions matching each prompt and smooth transitions connecting them.",
        "qtype": "Literature_Background",
        "response": "Here, [mask1] is the “Segment Score Distillation” block (the red‐boxed module in Fig. 2).  It takes exactly the same core idea as Score Distillation Sampling in DreamFusion—“add noise → denoise with a frozen diffusion model → turn the denoising error into a loss and back‐propagate”—and applies it to short motion clips instead of rendered images.  Concretely:\n\n1.  From the current long sequence M, extract a short overlapping segment x0ᶦ via a sliding window.  \n2.  Sample a random diffusion timestep t and corrupt x0ᶦ with Gaussian noise to obtain xₜᶦ (using the same noise schedule βₜ as in the pre-trained motion diffusion model).  \n3.  Run the noised segment xₜᶦ through the frozen motion diffusion U-Net to predict its denoised estimate \\hat x₀ᶦ.  \n4.  Define a distillation loss L_ssd = E_{t,ε}[w(t)‖\\hat x₀ᶦ – x₀ᶦ‖²] plus lightweight geometric terms (forward‐kinematic position deviation, foot‐contact stability, velocity smoothness).  \n5.  Back-propagate this loss into the underlying long sequence M, nudging that local segment toward high-likelihood motions under the diffusion prior.\n\nIn this way, each short clip is “pulled” into the manifold of realistic human motions learned by the diffusion model, exactly mirroring SDS’s add-noise/denoise-and-distill recipe but at the level of joint rotations and poses rather than pixels."
    },
    {
        "question": "How does integrating DDIM sampling in Motion Sequence Initialization facilitate subsequent Segment Score Distillation?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "To create this initial sequence, we start by randomly initializing the entire long motion sequence M, which provides a rough, unsmoothed outline of the target motion. Then, we employ a pre-trained Motion Diffusion Model (MDM) [49] with DDIM sampling [46] to generate each motion segment s_i within the sequence. Each segment s_i is conditioned on the respective text prompt c_i, ensuring that the generated motion aligns semantically with the desired motion described in the prompt.",
            "Segment Score Distillation. This module leverages a pre-trained motion diffusion model Φ to optimize the distribution of the sampled short sequences, ensuring that each segment aligns with the underlying diffusion sample distribution."
        ],
        "final_answer": "By using DDIM sampling during Motion Sequence Initialization, each segment is already generated under the same diffusion prior and conditioned on its text prompt, giving it a rough but semantically and statistically plausible shape. This head start means that when Segment Score Distillation begins, it can focus on fine‐grained alignment to the learned diffusion distribution and the addition of geometric constraints, rather than having to pull each segment out of random noise, thereby improving stability, coherence, and convergence speed.",
        "relevant_elements": [
            "Motion Sequence Initialization",
            "Segment Score Distillation"
        ],
        "id": 211,
        "masked_question": "How does integrating DDIM sampling in [mask1] facilitate subsequent [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Motion Sequence Initialization",
            "Segment Score Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.18303v1_figure_2.png",
        "paperid": "2411.18303v1",
        "paper_path": "./papers/2411.18303v1.json",
        "figure_id": "2411.18303v1_figure_2.png",
        "caption": "Figure 2: Overview of InfiniDreamer for arbitrarily long human motion generation. Given a list of text prompts, our framework generates a coherent and continuous long-sequence motion that aligns closely with each prompt. To achieve this, we start by initializing a long motion sequence using the (1) Motion Sequence Initialization module. Next, the (2) Motion Segment Sampling module iteratively samples short, overlapping sequence segments from the initialized motion. Finally, we refine each sampled segment with our proposed (3) Segment Score Distillation, optimizing each segment to align with the prior distribution of the pre-trained motion diffusion model. Through this iterative process, the framework synthesizes a seamless and fluid long-duration motion sequence, with realistic motions matching each prompt and smooth transitions connecting them.",
        "qtype": "Experimental_Results",
        "response": "Here is the high‐level intuition, step by step:\n\n1. What happens in [mask1]?  \n   In the red‐boxed “Motion Sequence Initialization” stage we take each text prompt, feed it into a pretrained Motion Diffusion Model (MDM), and run a few steps of DDIM sampling to produce a rough, long motion sequence.  Because DDIM is a deterministic, non‐Markovian sampler it tends to give you a sharp, coherent motion segment that already lives on the diffusion model’s learned manifold and roughly matches your text.\n\n2. Why does that help [mask2]?  \n   The blue-boxed “Segment Score Distillation” stage is essentially a fine‐tuning loop in which we  \n     a. cut out short overlapping windows of the long sequence,  \n     b. add synthetic noise at a random time step,  \n     c. ask the same diffusion prior to predict the clean motion, and  \n     d. backpropagate alignment plus geometry losses to pull your segment closer to what the diffusion prior “expects.”  \n\n   If your initialization had been random white‐noise or badly structured, the denoiser in step (c) would struggle to produce reliable score signals, and the alignment losses would fight a huge gap between your current motion and the learned data manifold.\n\n3. The key payoff:  \n   By seeding each text segment with a DDIM‐drawn sample, you start already near the diffusion model’s high‐density region.  As a result, when you later add noise and compute the denoiser’s score, you get strong, meaningful gradients that the SSD step can use to  \n     • tighten up semantic alignment,  \n     • enforce smooth foot contacts and velocities,  \n     • stitch neighboring segments without popping or sliding.  \n\nIn short, DDIM sampling in the initialization gives you a “good enough” motion to begin with—one that both respects the text and sits on the diffusion manifold—so that the subsequent score‐distillation loop can quickly and stably refine each window into a high-fidelity, coherent long‐range motion."
    },
    {
        "question": "How does sliding window size in Motion Segment Sampling influence coherence during Segment Score Distillation?",
        "relevant_section_ids": [
            "4.2",
            "6"
        ],
        "relevant_context": [
            "To achieve this, we employ a sliding window of size s, which moves along the long motion sequence with a stride size p. This sliding window technique allows us to iteratively sample overlapping short motion segments from the long sequence, denoted as Mi. By maintaining overlap between adjacent segments, the sliding window preserves continuity and smoothness between them, thereby enhancing the temporal coherence of the generated long motion sequence.",
            "Ablation on Sliding Window Size s. In Tab. 3, we present the impact of the hyper-parameter Sliding Window Size s on model performance. s controls the size of each sampled segment, whereas a larger s allows the model to incorporate more contextual information. We observe that with a very small s, the performance of transition segments declines sharply. However, as s increases, the transition quality exhibits fluctuating declines. This suggests that a moderate context length is beneficial for transition generation, whereas an overly extended context introduces interference. In terms of motion segment generation, performance consistently decreases as s grows. We speculate this is due both to MDM’s limitations in handling long sequences and to the interference in semantic alignment caused by excessive context length."
        ],
        "final_answer": "The sliding window size s governs how much temporal context is seen by each sampled segment during Segment Score Distillation. Small window sizes break the overlap needed for smooth transitions and sharply degrade coherence. Increasing s initially improves transition coherence by providing more context, but making s too large introduces interference between distant frames and again harms smoothness. Thus, a moderate sliding window size best balances coherence in the distilled segments.",
        "relevant_elements": [
            "Motion Segment Sampling",
            "Segment Score Distillation"
        ],
        "id": 212,
        "masked_question": "How does sliding window size in [mask1] influence coherence during Segment Score Distillation?",
        "masked_number": 1,
        "masked_elements": [
            "Motion Segment Sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18303v1_figure_2.png",
        "paperid": "2411.18303v1",
        "paper_path": "./papers/2411.18303v1.json",
        "figure_id": "2411.18303v1_figure_2.png",
        "caption": "Figure 2: Overview of InfiniDreamer for arbitrarily long human motion generation. Given a list of text prompts, our framework generates a coherent and continuous long-sequence motion that aligns closely with each prompt. To achieve this, we start by initializing a long motion sequence using the (1) Motion Sequence Initialization module. Next, the (2) Motion Segment Sampling module iteratively samples short, overlapping sequence segments from the initialized motion. Finally, we refine each sampled segment with our proposed (3) Segment Score Distillation, optimizing each segment to align with the prior distribution of the pre-trained motion diffusion model. Through this iterative process, the framework synthesizes a seamless and fluid long-duration motion sequence, with realistic motions matching each prompt and smooth transitions connecting them.",
        "qtype": "Experimental_Results",
        "response": "Let’s walk through what that red‐boxed module is doing and how its window size w feeds into the SSD step:\n\n1.  In the “Motion Segment Sampling” stage (the red box), we sweep a fixed-length window of size w (with some overlap set by the stride) over the entire, initially generated long motion M.  Each windowed chunk is one “short motion segment” that will later be refined by Segment Score Distillation.\n\n2.  Because SSD optimizes each segment independently but then stitches them back together, the amount of overlap (and hence the window size) determines how much shared context each neighbouring segment carries.  That shared context is what enforces temporal coherence across segment boundaries once they’re all reassembled.\n\n3.  From the ablations (Table 3 in the paper):\n   - If w is too small, you simply don’t have enough overlap—adjacent windows barely see each other’s frames—so transitions become ragged and incoherent.\n   - If w grows moderately, you get enough context in each window to produce smooth, natural hand-offs between segments, and transition quality peaks.\n   - If w grows beyond that sweet spot, you actually start to hurt both transition and per-segment quality.  The diffusion prior struggles with very long chunks (semantic alignment degrades), and the extra “stale” context starts to confuse the model during score distillation.\n\n4.  In short, sliding-window size w directly trades off:\n   – Too small → not enough overlap → poor boundary coherence  \n   – Too large → model interference and semantic drift → again worse coherence  \n   – Moderate → best smoothing of transitions during SSD  \n\nSo the sliding window size influences coherence by governing how much shared context each SSD-refined segment has: too little context fragments your motion, too much context adds noise and misalignment, and a middle‐of‐the‐road window size yields the smoothest, most coherent long motion."
    },
    {
        "question": "How does lower Hessian Lipschitz constant accelerate convergence of Newton iterative solver?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "On the other hand, the convergence of the solver (i.e., the number of iterations) highly depends on the properties of the problem. In particular when optimizing P with Lipschitz continuous Hessian L using Newton’s method, we have a quadratic convergence rate (Nocedal and Wright, 2006) written as: … It can be seen that, the number of iterations required to reach a certain error threshold scales with the Hessian’s Lipschitz constant.",
            "In this work, we focus on accelerating the simulation by reducing the number of iterations k through the use of our Lipschitz regularization in subspace construction."
        ],
        "final_answer": "When the Hessian of the objective has a smaller Lipschitz constant, Newton’s method still enjoys quadratic convergence but requires fewer iterations to reach a given error tolerance. In other words, reducing the Hessian Lipschitz constant directly lowers the number of Newton steps needed for convergence, thereby accelerating the iterative solve.",
        "relevant_elements": [
            "Hessian Lipschitz constant",
            "Newton iterative solver"
        ],
        "id": 214,
        "masked_question": "How does lower [mask1] accelerate convergence of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Hessian Lipschitz constant",
            "Newton iterative solver"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.03807v1_figure_1.png",
        "paperid": "2409.03807v1",
        "paper_path": "./papers/2409.03807v1.json",
        "figure_id": "2409.03807v1_figure_1.png",
        "caption": "Figure 1. We propose a Lipschitz optimization method that can significantly accelerate the convergence speed of reduced-order simulations driven by neural-network-based approaches. (a) The deformation process can be formulated as a path through a configuration manifold ℳ⊆ℝnℳsuperscriptℝ𝑛\\mathcal{M}\\subseteq\\mathbb{R}^{n}caligraphic_M ⊆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, where reduced-order solvers tend to find a mapping 𝒇θ (𝒛)subscript𝒇𝜃𝒛\\bm{f}_{\\theta}(\\bm{z})bold_italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) that maps a low-dimensional subspace ℝrsuperscriptℝ𝑟\\mathbb{R}^{r}blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT to the manifold. (b) Our method enhances the objective landscape in the neural subspace by minimizing the second-order Lipschitz regularization energy, which substantially improves convergence speed when using iterative solvers like Newton s method. (c, d) Compared to conventional linear subspace methods (driven by PCA) and direct neural subspace constructions, our method achieves faster convergence and maintains quality when using the same subspace dimension.",
        "qtype": "Experimental_Results",
        "response": "First, let’s be very explicit about what each mask is pointing to:\n\n  • [mask1] is the red‐boxed quantity “Lip[∇²ₓ P]” – in other words, the second–order Lipschitz constant of the reduced‐space energy’s Hessian.  \n  • [mask2] is the blue‐boxed process “iterative solver convergence” – namely the number of Newton (or quasi‐Newton) steps needed to solve the reduced‐order optimization.\n\nWhy does lowering Lip[∇²ₓ P] speed up Newton’s method?  Recall from standard convergence theory (Nocedal & Wright, 2006) that, in the neighbourhood of the solution, the Newton iteration error satisfies\n\n  ∥xₖ₊₁ − x*∥ ≤ (L/2μ) ∥xₖ − x*∥²,\n\nwhere L is the Lipschitz constant of the Hessian (∇²P) and μ is the strong–convexity constant.  Equivalently, the number of iterations N to reach a given accuracy ε grows (at worst) like\n\n  N = O( L · log(1/ε) ).\n\nHence, if we can train our subspace so that Lip[∇²ₓ P] is small, then:\n\n  1. The Hessian varies more slowly across the subspace (the energy landscape is “smoother”).  \n  2. Each Newton step is closer to the true quadratic step, so the method exhibits its textbook quadratic convergence in fewer iterations.  \n  3. In practice this directly translates into a much smaller N – i.e. faster convergence of the reduced‐order solver.  \n\nIn short: by regularizing the network so that the Hessian’s Lipschitz constant is low, the curvature of the energy becomes better behaved, and Newton’s method requires far fewer steps to converge."
    },
    {
        "question": "What alternative regularization could augment Lipschitz regularization energy for faster convergence with iterative solvers?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "iterative solvers",
            "Lipschitz regularization energy"
        ],
        "id": 216,
        "masked_question": "What alternative regularization could augment [mask1] for faster convergence with iterative solvers?",
        "masked_number": 1,
        "masked_elements": [
            "Lipschitz regularization energy"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.03807v1_figure_1.png",
        "paperid": "2409.03807v1",
        "paper_path": "./papers/2409.03807v1.json",
        "figure_id": "2409.03807v1_figure_1.png",
        "caption": "Figure 1. We propose a Lipschitz optimization method that can significantly accelerate the convergence speed of reduced-order simulations driven by neural-network-based approaches. (a) The deformation process can be formulated as a path through a configuration manifold ℳ⊆ℝnℳsuperscriptℝ𝑛\\mathcal{M}\\subseteq\\mathbb{R}^{n}caligraphic_M ⊆ blackboard_R start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT, where reduced-order solvers tend to find a mapping 𝒇θ (𝒛)subscript𝒇𝜃𝒛\\bm{f}_{\\theta}(\\bm{z})bold_italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( bold_italic_z ) that maps a low-dimensional subspace ℝrsuperscriptℝ𝑟\\mathbb{R}^{r}blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT to the manifold. (b) Our method enhances the objective landscape in the neural subspace by minimizing the second-order Lipschitz regularization energy, which substantially improves convergence speed when using iterative solvers like Newton s method. (c, d) Compared to conventional linear subspace methods (driven by PCA) and direct neural subspace constructions, our method achieves faster convergence and maintains quality when using the same subspace dimension.",
        "qtype": "Others",
        "response": "In our work (the red‐boxed part in Fig. 1b) we focus on “second‐order” Lipschitz regularization – i.e. directly penalizing the variation of the subspace‐objective Hessian.  In the paper (Sec 5.4) we point out that for fully dynamic sims you could likewise add a “first‐order” Lipschitz penalty on the network itself – in other words, bound or regularize the input-output Jacobian of your decoder (via spectral‐norm or gradient‐penalty schemes like in Gulrajani et al. 2017, Gouk et al. 2021, Liu et al. 2022).  That Jacobian‐norm regularizer can further tame the inertia term’s Hessian and yield yet faster convergence in Newton– or quasi-Newton–style solvers."
    },
    {
        "question": "What limitations might learned positional embedding E_PE face with varied map prior instances in complex intersections?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "E_PE",
            "Map Prior Instances"
        ],
        "id": 217,
        "masked_question": "What limitations might learned positional embedding [mask1] face with varied map prior instances in complex intersections?",
        "masked_number": 1,
        "masked_elements": [
            "E_PE"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10316v1_figure_1.png",
        "paperid": "2411.10316v1",
        "paper_path": "./papers/2411.10316v1.json",
        "figure_id": "2411.10316v1_figure_1.png",
        "caption": "Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT to create a map prior ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT. Using ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT, we try to reconstruct ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5.",
        "qtype": "Others",
        "response": "Here is the reasoning step by step:\n\n1.  In design C (the red‐boxed variant) each query point carries  \n    - a learned point embedding (E_pt)  \n    - a learned positional embedding (E_PE)  \n    - plus a reference point P_ref derived from the raw prior point through an affine transform T_ref.  \n\n2.  Those learned positional embeddings E_PE were trained on whatever intersection geometries and point‐sampling patterns were seen during training. They end up encoding “typical” relative offsets under those conditions.\n\n3.  In a simple straight‐road scenario or a lightly branched junction, the fixed learned offsets can still line up cross‐attention sampling well.  \n\n4.  But in truly complex intersections you can get:  \n    • a wide variety of junction branching angles (e.g. 5-way crossings, roundabouts, skewed entries)  \n    • very non‐uniform point spacings (some lanes sampled every 0.5 m, others every 2 m)  \n    • different overall polyline lengths (one branch might be 20 m long, another 200 m)  \n    • arbitrary rotations or flips w.r.t. the BEV grid  \n\n5.  A single learned positional embedding per index cannot easily stretch, shrink or rotate to accommodate all of these variants. In effect it:  \n    – Assumes a canonical ordering, spacing and orientation of points.  \n    – Overfits to the “average” geometry seen at train time.  \n    – Lacks an explicit mechanism to rescale or reorient itself when the local topology changes drastically.  \n\n6.  Consequences in complex intersections:  \n    – The cross‐attention heads sample features at the wrong offsets and thus pick up misleading BEV information.  \n    – Corner cases (unseen branching patterns, asymmetric roundabouts) will be poorly represented.  \n    – The model may simply ignore fine junction detail or hallucinate incorrect lane continuations.  \n\n7.  In short, learned positional embeddings in variant C are brittle whenever the prior instance departs strongly from the training distribution’s geometry—precisely the regime of highly complex, irregular intersections.  \n\nAnswer:  \nLearned positional embeddings in design C tend to “bake in” the spacing, ordering and orientation of points seen during training. As soon as you feed them a prior with very different branch counts, segment lengths or sampling densities (as you find in complex multi-arm intersections or roundabouts), those fixed offsets no longer line up correctly, the deformable attention samples at the wrong places, and the model’s completion performance degrades."
    },
    {
        "question": "How could alternative reference point generation mitigate errors in P_ref under occluded sensor observations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "T_ref",
            "P_ref"
        ],
        "id": 218,
        "masked_question": "How could alternative reference point generation mitigate errors in [mask1] under occluded sensor observations?",
        "masked_number": 1,
        "masked_elements": [
            "P_ref"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10316v1_figure_1.png",
        "paperid": "2411.10316v1",
        "paper_path": "./papers/2411.10316v1.json",
        "figure_id": "2411.10316v1_figure_1.png",
        "caption": "Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT to create a map prior ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT. Using ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT, we try to reconstruct ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5.",
        "qtype": "Others",
        "response": "In our diagram the little red‐boxed sub-figure (we call it design “C”) is exactly where the reference point Pₙᵢ for each query is being produced from the map-prior point coordinates, rather than by simply linearly projecting a learned positional embedding.  What we found is that as soon as part of a lane or divider is occluded in the camera (or LiDAR) view, the purely embedding-based projection (designs A and B) can “drift” – it hangs off whatever bits of the positional embedding survived the occlusion and the decoder then attends to the wrong place in the BEV.  \n\nBy contrast, if you generate your Pₙᵢ directly from the *actual* map prior coordinates – for example:\n\n • take the centroid (average) of all the known (non-occluded) polyline points,  \n • or take the midpoint of the end–points of that prior segment,  \n • or feed the concatenated x,y coordinates of every prior point into a small MLP –  \n\nthen your reference point sits stably at the true geometric centre of that instance, irrespective of which bits got masked out by occlusion.  In practice this “shape-aware” or “direct-coordinate” Pₙᵢ computation (i.e. design C) cuts down on decoder localization errors whenever the raw sensor inputs are missing or noisy."
    },
    {
        "question": "What motivates masking ground truth maps to supply map prior instances instead of full maps?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "Map priors M_p are derived from the complete ground truth map M using the scenario specific prior generator g which masks out or selects only specific map elements.",
            "However, transformer models quickly learn to pass through prior elements almost identically and, if known as prior, any downstream application would prefer the map prior over the corresponding, but possibly noisy prediction. Hence, we propose to focus on exactly those map elements which are unknown to the model at inference time.",
            "This exploits the entire combinatorial variety of dataset diversity and map prior categories and leads to an M-fold increase in training data, promising greater generalization performance."
        ],
        "final_answer": "Masking the ground truth maps to provide only partial map prior instances serves two key purposes: first, it prevents the model from simply ‘‘copying through’’ all elements of a full prior (which it would do when given complete maps), and instead forces it to focus on and learn to complete only the missing or changed elements. Second, by generating multiple masked versions of each scene, it acts as an augmentation that multiplies the training data and thereby improves generalization.",
        "relevant_elements": [
            "Masking of Map Instances",
            "Map Prior Instances"
        ],
        "id": 219,
        "masked_question": "What motivates [mask1] to supply map prior instances instead of full maps?",
        "masked_number": 1,
        "masked_elements": [
            "Masking of Map Instances"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.10316v1_figure_1.png",
        "paperid": "2411.10316v1",
        "paper_path": "./papers/2411.10316v1.json",
        "figure_id": "2411.10316v1_figure_1.png",
        "caption": "Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT to create a map prior ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT. Using ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT, we try to reconstruct ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5.",
        "qtype": "Design_Rationale",
        "response": "In our diagram the red‐boxed module is simply “Masking of Map Instances” – in other words, instead of feeding the network the full, unaltered ground‐truth map we deliberately strip out (i.e. mask) everything the model is supposed to re-predict.  The reason for doing exactly that is two-fold:\n\n 1. if you hand the model the complete map as a prior it will learn to “copy through” those elements almost perfectly (and any downstream system would simply trust the prior rather than your network’s output),  \n 2. by masking out only the parts we want it to reconstruct you force the network to actually learn to *complete* the missing geometry, and you can then measure its true completion performance on just those elements that weren’t given."
    },
    {
        "question": "What motivates combining zero-padded point information with learned prior embeddings in the Point Query Encoder?",
        "relevant_section_ids": [
            "3.5.x"
        ],
        "relevant_context": [
            "While in A, the baseline proposed in MapEX [22], the zero-padded point information is directly used as point embedding Q_pt, we propose to combine it with a learned prior embedding E_pt which is added to it in B and C.",
            "This makes use of the prior information, but provides a learnable degree of freedom for the model."
        ],
        "final_answer": "They combine zero-padded point information with learned prior embeddings in order to leverage the available prior map information while still giving the model a learnable degree of freedom to adapt and improve map completion.",
        "relevant_elements": [
            "Point Query Encoder",
            "learned prior embeddings"
        ],
        "id": 220,
        "masked_question": "What motivates combining zero-padded point information with [mask1] in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "learned prior embeddings",
            "Point Query Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.10316v1_figure_1.png",
        "paperid": "2411.10316v1",
        "paper_path": "./papers/2411.10316v1.json",
        "figure_id": "2411.10316v1_figure_1.png",
        "caption": "Figure 1: Overview of the model architecture of M3TR and the investigated point query encoder designs. For our evaluated task of HD map completion, we mask out instances from the ground truth map ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT to create a map prior ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT. Using ℳPsubscriptℳP\\mathcal{M_{\\mathrm{P}}}caligraphic_M start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT, we try to reconstruct ℳGTsubscriptℳGT\\mathcal{M_{\\mathrm{GT}}}caligraphic_M start_POSTSUBSCRIPT roman_GT end_POSTSUBSCRIPT. The map prior instances are supplied to the model as queries, influenced by the shown point query encoder and the detection query set design which is further illustrated in Fig. 5.",
        "qtype": "Design_Rationale",
        "response": "The big-picture idea is simply that “pure” zero-padded point coordinates carry the prior geometry but no way for the network to adapt or correct them during training.  By adding (i.e. summing with) a small, learned prior embedding in the point‐query encoder, the model can still see exactly where the prior says the points lie, but it also gets a trainable “knob” to tweak or override that prior if the visual evidence suggests the map has changed.  In short, fusing raw, zero-padded coordinates with a learned embedding injects prior knowledge while preserving a learnable degree of freedom, and empirically yields better completion performance."
    },
    {
        "question": "What motivates freezing SAM when training a DETR head for in-context segmentation?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "To adapt SAM for in-context learning, we propose an architecture that largely reuses the base knowledge already present in the pretrained model.",
            "Specifically, we freeze both encoder and decoder and solely train a DETR [4  ###reference_b4###] decoder head to bridge the semantics of the reference image with the target images.",
            "This way, we hope to minimise visual feature discrepancy from reference and target since both representations come from the same frozen model."
        ],
        "final_answer": "They freeze SAM to leverage its pretrained segmentation knowledge and ensure that the reference and target image features come from the same frozen model, thereby minimising visual feature discrepancies and allowing a lightweight DETR head to focus solely on learning the semantic mapping.",
        "relevant_elements": [
            "SAM",
            "DETR head"
        ],
        "id": 221,
        "masked_question": "What motivates freezing [mask1] when training a DETR head for in-context segmentation?",
        "masked_number": 1,
        "masked_elements": [
            "SAM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15288v1_figure_1.png",
        "paperid": "2411.15288v1",
        "paper_path": "./papers/2411.15288v1.json",
        "figure_id": "2411.15288v1_figure_1.png",
        "caption": "Figure 1: Exploring SAM’s Semantic Gap for Image Understanding. (1) Quantifying SAM’s Semantic Understanding: Despite training on a very large dataset, SAM lacks inherent semantics, as shown by its lower ImageNet1K classification accuracy compared to CLIP and DINOv2 models. (2) Recovering Semantics with Fine-tuning: SAM’s ability to generalise remains limited; it can identify classes in the training set but struggles with unseen classes even with in-context learning through DETR. (3) Injecting Semantics from External Models: By integrating semantic-rich representations from models like DINOv2, we can enhance SAM’s ability to match semantics and improve its understanding of segmented regions.",
        "qtype": "Design_Rationale",
        "response": "Here’s how I arrive at the answer:\n\n1. Image-Text Alignment  \n  – In the diagram (Section 2), the blue “SAM” block is decorated with a snowflake icon, indicating it is frozen.  \n  – The red box highlights exactly that frozen SAM module, whose outputs feed into the newly added DETR head.  \n  – The text immediately beneath “Adapting SAM” says:  \n    “We freeze both encoder and decoder and solely train a DETR decoder head … This way, we hope to minimise visual feature discrepancy from reference and target since both representations come from the same frozen model.”\n\n2. What is [mask1]?  \n  – [mask1] corresponds to the frozen SAM encoder + decoder module.\n\n3. Why freeze it?  \n  – To preserve the rich, pre-learned representations in SAM without corrupting them.  \n  – By not updating the huge pretrained backbone, both reference and target images are encoded into exactly the same feature space (no drift), which is crucial for an in-context setting.  \n  – It also keeps the number of trainable parameters small—only the DETR head must learn—making the adaptation lightweight and focused solely on mapping from SAM’s fixed embeddings to the correct semantic instances.\n\nAnswer: They freeze the entire SAM encoder + decoder so as to preserve its pretrained visual representations and ensure that reference and target images are mapped into the same feature space, minimizing feature‐discrepancy and allowing efficient, lightweight fine-tuning of only the DETR head."
    },
    {
        "question": "What is the reasoning for matching DINOv2 features with SAM regions via cosine similarity for instance detection?",
        "relevant_section_ids": [
            "6"
        ],
        "relevant_context": [
            "Then, we use DINOv2 to extract the latent feature representations for all reference images, and the target image, respectively. Lastly, we match the DINOv2 features of the target image with all the encoded reference images features via cosine similarity.",
            "By adopting this approach, our baseline achieves semantic matching at instance level through feature-based similarity rather than extensive training.",
            "Furthermore, being a fully training-free method makes it naturally adaptable to novel classes (without any risk of overfitting to seen categories)."
        ],
        "final_answer": "They match DINOv2 features with SAM regions via cosine similarity to exploit DINOv2’s semantically rich embeddings, directly comparing reference and target region feature vectors to detect instances. This training-free, feature-based semantic matching enables instance-level detection and naturally generalises to novel classes without overfitting.",
        "relevant_elements": [
            "DINOv2 features",
            "SAM regions"
        ],
        "id": 222,
        "masked_question": "What is the reasoning for matching [mask1] with SAM regions via cosine similarity for instance detection?",
        "masked_number": 1,
        "masked_elements": [
            "DINOv2 features"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15288v1_figure_1.png",
        "paperid": "2411.15288v1",
        "paper_path": "./papers/2411.15288v1.json",
        "figure_id": "2411.15288v1_figure_1.png",
        "caption": "Figure 1: Exploring SAM’s Semantic Gap for Image Understanding. (1) Quantifying SAM’s Semantic Understanding: Despite training on a very large dataset, SAM lacks inherent semantics, as shown by its lower ImageNet1K classification accuracy compared to CLIP and DINOv2 models. (2) Recovering Semantics with Fine-tuning: SAM’s ability to generalise remains limited; it can identify classes in the training set but struggles with unseen classes even with in-context learning through DETR. (3) Injecting Semantics from External Models: By integrating semantic-rich representations from models like DINOv2, we can enhance SAM’s ability to match semantics and improve its understanding of segmented regions.",
        "qtype": "Design_Rationale",
        "response": "Here is a step-by-step rationale for why, in our “injecting semantics” pipeline, we take the DINOv2 feature vectors (what we have marked as [mask1]) and match them against each SAM-proposed region with cosine similarity:\n\n1.  SAM alone only gives you *where* things are (masks) but not *what* they are.  \n2.  DINOv2, on the other hand, is a self-supervised model whose internal embeddings carry rich category-level information.  \n3.  We first extract a fixed DINOv2 feature vector for each *reference* image of our target class (this is the [mask1] in the diagram).  \n4.  We also extract DINOv2 features—region by region—for every mask that SAM proposes in the *target* image.  \n5.  We then compare (via cosine similarity) each reference feature vector against each region’s feature vector.  Cosine similarity is the standard, training-free way to gauge the angle (i.e. semantic alignment) between two high-dimensional embeddings.  \n6.  A high cosine score tells us that “this SAM mask looks very much like my reference class in feature space,” so we pick it as an instance of that class.  \n7.  Finally, we do some simple non-max suppression to remove overlapping duplicates.  \n\nBy matching DINOv2’s *semantic* embeddings to SAM’s *spatial* masks via cosine similarity, we effectively “label” each mask with the most likely class—achieving instance detection without any extra training or fine-tuning."
    },
    {
        "question": "How does the DETR decoder integrate SAM decoder outputs with reference image embeddings?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "As shown in Figure 2, the output embeddings from the SAM decoder for the reference image are added to the DETR query tokens.",
            "Thus, the DETR head learns to predict object instances in the target image given the reference image query tokens."
        ],
        "final_answer": "The DETR decoder takes the embeddings produced by the frozen SAM mask decoder on the reference image and adds them to its own query tokens, allowing the DETR head to use those reference-image–conditioned queries to predict object instances in the target image.",
        "relevant_elements": [
            "DETR decoder",
            "SAM decoder"
        ],
        "id": 223,
        "masked_question": "How does the [mask1] integrate [mask2] outputs with reference image embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "DETR decoder",
            "SAM decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15288v1_figure_1.png",
        "paperid": "2411.15288v1",
        "paper_path": "./papers/2411.15288v1.json",
        "figure_id": "2411.15288v1_figure_1.png",
        "caption": "Figure 1: Exploring SAM’s Semantic Gap for Image Understanding. (1) Quantifying SAM’s Semantic Understanding: Despite training on a very large dataset, SAM lacks inherent semantics, as shown by its lower ImageNet1K classification accuracy compared to CLIP and DINOv2 models. (2) Recovering Semantics with Fine-tuning: SAM’s ability to generalise remains limited; it can identify classes in the training set but struggles with unseen classes even with in-context learning through DETR. (3) Injecting Semantics from External Models: By integrating semantic-rich representations from models like DINOv2, we can enhance SAM’s ability to match semantics and improve its understanding of segmented regions.",
        "qtype": "Implementation_Details",
        "response": "Here’s how I worked it out step by step:\n\n1.  In the text under “Adapting SAM” we are told that we keep the SAM encoder and decoder frozen and “solely train a DETR decoder head to bridge the semantics of the reference image with the target images.”  \n2.  The paragraph then says:  \n    “As shown in Figure 2, the output embeddings from the SAM decoder for the reference image are added to the DETR query tokens.  Thus, the DETR head learns to predict object instances in the target image given the reference image query tokens.”  \n3.  In the figure, the blue‐boxed element ([mask2]) is precisely those “output embeddings from the SAM decoder for the reference image.”  The red‐boxed element ([mask1]) is the DETR decoder head that takes those embeddings and the DETR’s own learnable queries and produces box predictions.  \n4.  Hence, the way the DETR head ([mask1]) “integrates” the SAM decoder’s reference embeddings ([mask2]) is by simply adding them to its query vectors.  This fused query then attends over the target‐image features to produce the final object proposals, which are later turned into masks by SAM.\n\nAnswer:\n\nThe DETR head takes the SAM reference‐image decoder outputs and injects them into its own query tokens via vector addition.  Those “augmented” queries then go through the DETR decoder, conditioning its box predictions on the semantic cues extracted by SAM from the reference image."
    },
    {
        "question": "How does cosine similarity operate on DINOv2 and SAM features to match target and reference regions?",
        "relevant_section_ids": [
            "6"
        ],
        "relevant_context": [
            "First, we generate object proposals for all regions in the test image by prompting SAM with a dense grid of points.",
            "Then, we use DINOv2 to extract the latent feature representations for all reference images, and the target image, respectively.",
            "Lastly, we match the DINOv2 features of the target image with all the encoded reference images features via cosine similarity."
        ],
        "final_answer": "For each candidate region proposed by SAM in the target image, we extract its latent feature vector using a pretrained DINOv2 encoder. We do the same for each reference image (or region) to obtain their DINOv2 feature vectors. We then compute the cosine similarity between every target-region feature and every reference-region feature. Regions are matched (and hence assigned to a reference category) based on which reference feature yields the highest cosine similarity, after which simple post-processing removes duplicate detections.",
        "relevant_elements": [
            "DINOv2",
            "SAM"
        ],
        "id": 224,
        "masked_question": "How does cosine similarity operate on [mask1] and [mask2] features to match target and reference regions?",
        "masked_number": 2,
        "masked_elements": [
            "DINOv2",
            "SAM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.15288v1_figure_1.png",
        "paperid": "2411.15288v1",
        "paper_path": "./papers/2411.15288v1.json",
        "figure_id": "2411.15288v1_figure_1.png",
        "caption": "Figure 1: Exploring SAM’s Semantic Gap for Image Understanding. (1) Quantifying SAM’s Semantic Understanding: Despite training on a very large dataset, SAM lacks inherent semantics, as shown by its lower ImageNet1K classification accuracy compared to CLIP and DINOv2 models. (2) Recovering Semantics with Fine-tuning: SAM’s ability to generalise remains limited; it can identify classes in the training set but struggles with unseen classes even with in-context learning through DETR. (3) Injecting Semantics from External Models: By integrating semantic-rich representations from models like DINOv2, we can enhance SAM’s ability to match semantics and improve its understanding of segmented regions.",
        "qtype": "Implementation_Details",
        "response": "Here is how the “cosine‐matching” step works in our simple, training‐free pipeline:\n\n1.  Run the target image through SAM to get N proposed regions (we’ll call these mask₁ regions).  \n2.  Run each reference exemplar (the few shots you have of each class) through DINOv2 to get M reference embeddings (mask₂).  \n3.  For every pair (i, j)—where i indexes one of the N SAM regions and j indexes one of the M reference exemplars—you take the two DINOv2 feature vectors fᵗᵃʳᵍᵉᵗᵢ and fʳᵉᶠʳⱼ, L₂-normalize each, and compute  \n     \n     similarity(i,j) =  \n       ( fᵗᵃʳᵍᵉᵗᵢ · fʳᵉᶠʳⱼ )  \n       / ( ‖fᵗᵃʳᵍᵉᵗᵢ‖ · ‖fʳᵉᶠʳⱼ‖ )  \n\n4.  This yields an N×M matrix of cosine scores.  For each target region i you pick the reference j with the highest score (above some threshold) and assign that class label to region i.  \n5.  Finally, you do simple deduplication (e.g. non-max suppression) on overlapping boxes to get the final set of detections.\n\nIn short, you extract DINOv2 vectors for (1) each SAM mask in the target and (2) each reference shot, L₂-normalize them, and then use their dot‐product (i.e. cosine similarity) to decide which reference class each target mask most closely matches."
    },
    {
        "question": "How does the framework transfer adaptation dynamics features to accurately reconstruct deployment dynamics?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Our key innovation is training a hybrid machine-learning framework in a laboratory environment using a variety of synthetic dynamical systems other than data from the target system itself, and deploy the trained architecture to reconstruct the dynamics of the target system from one-time sparse observations.",
            "More specifically, we exploit the machine-learning framework of transformers with training data not from the target system but from a number of known, synthetic systems that show qualitatively similar dynamical behaviors to those of the target system, for which complete data are available. The training process can thus be regarded as a “laboratory-calibration” process during which the transformer learns the dynamical rules generating the synthetic but complete data. The so-trained transformer is then deployed to the real application with the random and sparse data, and is expected to adapt to the unseen data and reconstruct the underlying dynamics."
        ],
        "final_answer": "By pre-training a transformer on a diverse set of fully observed synthetic systems (the “adaptation” dynamics), the model learns a general feature extractor for underlying dynamical rules.  At deployment, these learned features are applied to the sparse, one-time observations of the target system, enabling the transformer (and the downstream reservoir computer) to fill in the missing points and faithfully reconstruct the full “deployment” dynamics without ever having seen target-system training data.",
        "relevant_elements": [
            "adaptation dynamics",
            "deployment dynamics"
        ],
        "id": 225,
        "masked_question": "How does the framework transfer [mask1] features to accurately reconstruct deployment dynamics?",
        "masked_number": 1,
        "masked_elements": [
            "adaptation dynamics"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.21222v1_figure_1.png",
        "paperid": "2410.21222v1",
        "paper_path": "./papers/2410.21222v1.json",
        "figure_id": "2410.21222v1_figure_1.png",
        "caption": "Figure 1: Dynamics reconstruction from random and sparse data.\n(a) The textbook case of a random time series\nsampled at a frequency higher than the Nyquist frequency, where the dynamical data\ncan be faithfully reconstructed. (b) Training data from the target system (left)\nand a segment of time series of six data points in a time interval containing\napproximately two cycles of oscillation. According to the Nyquist criterion, the\nsignal can be faithfully reconstructed with more than 20 uniformly sampled data\npoints (see text). When the data points are far fewer than 20 and even worse, they\nare randomly sampled, reconstruction becomes challenging. However, if training data\nfrom the same target system are available, existing machine-learning methods can\nbe used to reconstruct the dynamics from the sparse data [33]. (c) If\nno training data from the target system are available, hybrid machine learning\nproposed here provides a viable solution to reconstructing the dynamics from sparse\ndata. (d) Problem statement. Given random and sparse data, the goal is to reconstruct\nthe dynamics of the target system governed by d⁢𝐱/d⁢t=f⁢(𝐱,t)𝑑𝐱𝑑𝑡𝑓𝐱𝑡d\\mathbf{x}/dt=f(\\mathbf{x},t)italic_d bold_x / italic_d italic_t = italic_f ( bold_x , italic_t ). A\nhurdle that needs to be overcome is that, for any given three points, there exist\ninfinitely many ways to fit the data, as illustrated on the right side. (e) Training\nof the machine-learning framework is done using complete data from a large number of\nsynthetic dynamical systems [𝐡1,𝐡2,⋯,𝐡k]subscript𝐡1subscript𝐡2⋯subscript𝐡𝑘[\\mathbf{h}_{1},\\mathbf{h}_{2},\\cdots,\\mathbf{h}_{k}][ bold_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , bold_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ]. The\nframework is then adapted to reconstruct and predict the dynamics of the target\nsystems [𝐟1,⋯,𝐟m]subscript𝐟1⋯subscript𝐟𝑚[\\mathbf{f}_{1},\\cdots,\\mathbf{f}_{m}][ bold_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , bold_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]. (f) An example: in the testing\n(deployment) phase, sparse observations are provided to the trained neural network\nfor dynamics reconstruction.",
        "qtype": "Implementation_Details",
        "response": "The key is that nothing “mystical” is hiding behind the red box in panel (e) – it is simply the bank of adaptation-dynamics embeddings that the transformer has learned during its off-line “laboratory” training.  Concretely:\n\n  1.  In the training phase the transformer digests complete, regularly sampled trajectories from k known prototype systems and distils each of them into a fixed‐length vector h₁,…,hₖ (the curves in the red box).  These h’s form a small library of generic dynamical motifs.\n\n  2.  At deployment time you feed in your one-shot, sparse observations of the new, unseen system.  The transformer uses its attention layers to “match” those few data‐points against the closest h-vectors in its library, effectively projecting the new measurements into the same latent space it built during training.\n\n  3.  Finally, the decoder branches off that latent code and “rewinds＋forwards” to fill in all of the missing time steps – synthesizing a high–resolution, Nyquist-compliant time series whose short‐time behavior is a weighted combination of the pre‐learned adaptation dynamics.\n\n  4.  That reconstructed dense trajectory can then be handed off to the reservoir computer for long-term prediction.  \n\nIn this way the transformer never sees a full time series from the target system in training, but it can still “transfer” the generic h–features it learned from the synthetic systems to accurately interpolate and reconstruct the continuous‐time dynamics of the new, sparsely observed system."
    },
    {
        "question": "How does the transformer adapt synthetic training to handle sparse observations without target-system data?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "Our key innovation is training a hybrid machine-learning framework in a laboratory environment using a variety of synthetic dynamical systems other than data from the target system itself, and deploy the trained architecture to reconstruct the dynamics of the target system from one-time sparse observations.",
            "More specifically, we exploit the machine-learning framework of transformers with training data not from the target system but from a number of known, synthetic systems that show qualitatively similar dynamical behaviors to those of the target system, for which complete data are available. The training process can thus be regarded as a “laboratory-calibration” process during which the transformer learns the dynamical rules generating the synthetic but complete data. The so-trained transformer is then deployed to the real application with the random and sparse data, and is expected to adapt to the unseen data and reconstruct the underlying dynamics.",
            "Selecting an appropriate neural network architecture for reconstructing dynamics from sparse data requires meeting two fundamental requirements: (1) dynamical memory to capture long-range dependencies in the sparse data, and (2) flexibility to handle input sequences of varying lengths. Transformers, originally developed for natural language processing, satisfy these requirements and have proven effective for time series analysis.",
            "To evaluate the reliability of the generated output, we minimize a combined loss function with two components: (1) a mean squared error (MSE) loss that measures absolute error between the output and ground truth, and (2) a smoothness loss that ensures the output maintains appropriate continuity."
        ],
        "final_answer": "The transformer is first laboratory-calibrated on complete trajectories from a set of synthetic dynamical systems (rather than on the target system), allowing it to learn general update rules for nonlinear time series.  By using positional encoding to handle irregular time stamps, multi-head self-attention to capture long-range dependencies, and a combined MSE plus smoothness loss to enforce both accuracy and continuity, the so-trained transformer can be deployed directly on one-time, sparse observations from an unseen target system and automatically fill in missing data to reconstruct the underlying dynamics.",
        "relevant_elements": [
            "transformer",
            "sparse observations"
        ],
        "id": 226,
        "masked_question": "How does the [mask1] adapt synthetic training to handle sparse observations without target-system data?",
        "masked_number": 1,
        "masked_elements": [
            "transformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.21222v1_figure_1.png",
        "paperid": "2410.21222v1",
        "paper_path": "./papers/2410.21222v1.json",
        "figure_id": "2410.21222v1_figure_1.png",
        "caption": "Figure 1: Dynamics reconstruction from random and sparse data.\n(a) The textbook case of a random time series\nsampled at a frequency higher than the Nyquist frequency, where the dynamical data\ncan be faithfully reconstructed. (b) Training data from the target system (left)\nand a segment of time series of six data points in a time interval containing\napproximately two cycles of oscillation. According to the Nyquist criterion, the\nsignal can be faithfully reconstructed with more than 20 uniformly sampled data\npoints (see text). When the data points are far fewer than 20 and even worse, they\nare randomly sampled, reconstruction becomes challenging. However, if training data\nfrom the same target system are available, existing machine-learning methods can\nbe used to reconstruct the dynamics from the sparse data [33]. (c) If\nno training data from the target system are available, hybrid machine learning\nproposed here provides a viable solution to reconstructing the dynamics from sparse\ndata. (d) Problem statement. Given random and sparse data, the goal is to reconstruct\nthe dynamics of the target system governed by d⁢𝐱/d⁢t=f⁢(𝐱,t)𝑑𝐱𝑑𝑡𝑓𝐱𝑡d\\mathbf{x}/dt=f(\\mathbf{x},t)italic_d bold_x / italic_d italic_t = italic_f ( bold_x , italic_t ). A\nhurdle that needs to be overcome is that, for any given three points, there exist\ninfinitely many ways to fit the data, as illustrated on the right side. (e) Training\nof the machine-learning framework is done using complete data from a large number of\nsynthetic dynamical systems [𝐡1,𝐡2,⋯,𝐡k]subscript𝐡1subscript𝐡2⋯subscript𝐡𝑘[\\mathbf{h}_{1},\\mathbf{h}_{2},\\cdots,\\mathbf{h}_{k}][ bold_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , bold_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ]. The\nframework is then adapted to reconstruct and predict the dynamics of the target\nsystems [𝐟1,⋯,𝐟m]subscript𝐟1⋯subscript𝐟𝑚[\\mathbf{f}_{1},\\cdots,\\mathbf{f}_{m}][ bold_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , bold_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]. (f) An example: in the testing\n(deployment) phase, sparse observations are provided to the trained neural network\nfor dynamics reconstruction.",
        "qtype": "Implementation_Details",
        "response": "In our architecture the red‐boxed module is simply the Transformer itself.  What we do is first “laboratory–calibrate” that Transformer on a large library of fully sampled, synthetic dynamical systems.  In that training phase we randomly mask–out time points in the synthetic series and force the network to reconstruct the missing samples.  Because the Transformer’s self‐attention layers and positional encodings are designed to ingest inputs of arbitrary length and irregular timestamps and to learn long-range dependencies, it literally learns a “universal interpolator” for continuous dynamical trajectories.  \n\nAt deployment time, we never fine-tune on any real target‐system data—instead we feed the one‐off, sparse, irregular measurements into the trained Transformer (using the same positional‐encoding scheme), and the network “knows” how to fill in the gaps.  In other words, by training on many different synthetic systems with random masking, the Transformer has learned to adapt zero–shot to any new sparse observations without ever having seen that particular system before."
    },
    {
        "question": "How does adaptation dynamics [h1,h2,...,hk] training diverge from Nyquist criterion–based interpolation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "adaptation dynamics [h1,h2,...,hk]",
            "Nyquist criterion"
        ],
        "id": 228,
        "masked_question": "How does [mask1] training diverge from [mask2]–based interpolation?",
        "masked_number": 2,
        "masked_elements": [
            "adaptation dynamics [h1,h2,...,hk]",
            "Nyquist criterion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.21222v1_figure_1.png",
        "paperid": "2410.21222v1",
        "paper_path": "./papers/2410.21222v1.json",
        "figure_id": "2410.21222v1_figure_1.png",
        "caption": "Figure 1: Dynamics reconstruction from random and sparse data.\n(a) The textbook case of a random time series\nsampled at a frequency higher than the Nyquist frequency, where the dynamical data\ncan be faithfully reconstructed. (b) Training data from the target system (left)\nand a segment of time series of six data points in a time interval containing\napproximately two cycles of oscillation. According to the Nyquist criterion, the\nsignal can be faithfully reconstructed with more than 20 uniformly sampled data\npoints (see text). When the data points are far fewer than 20 and even worse, they\nare randomly sampled, reconstruction becomes challenging. However, if training data\nfrom the same target system are available, existing machine-learning methods can\nbe used to reconstruct the dynamics from the sparse data [33]. (c) If\nno training data from the target system are available, hybrid machine learning\nproposed here provides a viable solution to reconstructing the dynamics from sparse\ndata. (d) Problem statement. Given random and sparse data, the goal is to reconstruct\nthe dynamics of the target system governed by d⁢𝐱/d⁢t=f⁢(𝐱,t)𝑑𝐱𝑑𝑡𝑓𝐱𝑡d\\mathbf{x}/dt=f(\\mathbf{x},t)italic_d bold_x / italic_d italic_t = italic_f ( bold_x , italic_t ). A\nhurdle that needs to be overcome is that, for any given three points, there exist\ninfinitely many ways to fit the data, as illustrated on the right side. (e) Training\nof the machine-learning framework is done using complete data from a large number of\nsynthetic dynamical systems [𝐡1,𝐡2,⋯,𝐡k]subscript𝐡1subscript𝐡2⋯subscript𝐡𝑘[\\mathbf{h}_{1},\\mathbf{h}_{2},\\cdots,\\mathbf{h}_{k}][ bold_h start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_h start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ⋯ , bold_h start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ]. The\nframework is then adapted to reconstruct and predict the dynamics of the target\nsystems [𝐟1,⋯,𝐟m]subscript𝐟1⋯subscript𝐟𝑚[\\mathbf{f}_{1},\\cdots,\\mathbf{f}_{m}][ bold_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , bold_f start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT ]. (f) An example: in the testing\n(deployment) phase, sparse observations are provided to the trained neural network\nfor dynamics reconstruction.",
        "qtype": "Literature_Background",
        "response": "Mask 1: “Adaptation dynamics [h₁,h₂,…,hₖ]”  \nMask 2: “Traditional method (Nyquist criterion)”  \n\nIn a nutshell, the two approaches part company as follows:\n\n1. Nyquist‐based interpolation (mask 2)  \n   • Assumes the signal is strictly band‐limited and has been sampled at or above its Nyquist rate.  \n   • Fills in missing points by an analytic formula (e.g. sinc or spline interpolation) that is guaranteed to recover the original continuous waveform, but only so long as the uniform‐sampling and band‐limit hypotheses hold.  \n   • Relies entirely on the mathematics of Fourier‐theory and does not “learn” anything beyond those linear, frequency‐domain constraints.\n\n2. Adaptation‐dynamics training (mask 1)  \n   • Treats each new, sparsely sampled series as coming from an unknown dynamical system.  \n   • During a preceding “laboratory” phase it meta‐trains a transformer on many different synthetic chaotic or nonlinear data sets, extracting a set of latent adaptation vectors h₁,…,hₖ that capture generic dynamical motifs.  \n   • At deployment it uses those learned features to infer—i.e. to “adapt to”—an unseen system’s underlying continuous trajectory from irregular, sub‐Nyquist observations, without ever invoking the band‐limit assumption or uniform‐sampling theorem.  \n\nIn short, Nyquist‐interpolation is a direct, model‐free reconstruction under a linear band‐limit hypothesis, whereas adaptation‐dynamics training is a learned, data‐driven strategy that generalizes across families of nonlinear systems and can recover trajectories when the Nyquist criteria fail."
    },
    {
        "question": "How does Attribute normalization & expansion extend SNOMED CT-based ontology normalization techniques in retrieval?",
        "relevant_section_ids": [
            "3.3",
            "3.3.1",
            "3.3.2"
        ],
        "relevant_context": [
            "In this section, we formalize a similarity-based approach to first normalize the attribute values with respect to a domain reference, and then to expand them, when relevant, to a set of candidate values, such that to enable and maximize downstream matching likelihood. In this process, we focus on patient Diagnosis and trial targeted Condition, as these attributes bear the significant alignment signal.",
            "Given , the set of diagnosis values for a patient , and , the set of condition values of a CTR , for each  and  we define their normalized variants  and , where  is a reference ontology for the medical domain (e.g., Systematized Nomenclature of Medicine-Clinical Terms (SNOMED CT) Donnelly et al. (2006)).  consists of standardized and universal representation of concepts (denoted by ), properties, and relationships between concepts within the domain, organized in a taxonomic structure.  denotes a similarity function (e.g., Jaccard) between an attribute value and some ontology concept . In practice, given the potential size of the concept-set in the ontology, a nearest-neighbour search algorithm (e.g., Locality Sensitive Hashing (LSH) Datar et al. (2004)) could be employed to efficiently normalize each noun-phrase or constant  to their most similar terms . Finally, the normalized  and  are defined by:  and , respectively.",
            "In addition to normalization, the domain ontology opens the possibility for leveraging its defined properties and hierarchical concept relationships to expand some of the normalized attribute values with their relevant ontological neighborhood. We apply the predicate expansion method to a patient normalized diagnosis, , to maximize its match against a trial's targeted conditions. Thus, the expansion of normalized  is defined by ."
        ],
        "final_answer": "Attribute normalization & expansion extends standard SNOMED CT–based ontology normalization purely by mapping terms to their closest SNOMED CT concepts, by (1) using a similarity function (e.g., Jaccard) and approximate nearest-neighbor search (e.g., LSH) to normalize raw patient and trial attribute values into SNOMED CT concepts, and then (2) leveraging the SNOMED CT IS-A hierarchy to expand each normalized concept into its n-level ontological neighborhood. This two-step process not only grounds attributes in SNOMED CT codes but also enriches retrieval by including semantically related terms, boosting recall and alignment between patient diagnoses and trial conditions.",
        "relevant_elements": [
            "Attribute normalization & expansion",
            "SNOMED CT"
        ],
        "id": 229,
        "masked_question": "How does [mask1] extend [mask2]-based ontology normalization techniques in retrieval?",
        "masked_number": 2,
        "masked_elements": [
            "Attribute normalization & expansion",
            "SNOMED CT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.18998v1_figure_1.png",
        "paperid": "2409.18998v1",
        "paper_path": "./papers/2409.18998v1.json",
        "figure_id": "2409.18998v1_figure_1.png",
        "caption": "Figure 1: End-to-end depiction of the set-reasoning guided patient-trial matching.",
        "qtype": "Literature_Background",
        "response": "Here’s the intuition in two steps:\n\n1. SNOMED CT alone (the blue‐boxed component) gives you a huge, rigid taxonomy of clinical concepts – you can map a free‐text term to a SNOMED CT code, but if the trial is indexed under a slightly different but related SNOMED concept (say a parent or child in the hierarchy), a vanilla “exact‐match” normalization will miss it.\n\n2. The red‐boxed step (“Attribute normalization & expansion”) builds on that by:\n   • Taking each extracted noun‐phrase (e.g. a patient’s diagnosis or a trial’s condition) and finding its closest SNOMED CT concept via a similarity measure (e.g. Jaccard over tokens plus an LSH‐accelerated nearest‐neighbor search).  \n   • Once you’ve identified the best‐matching SNOMED CT concept, you don’t stop there – you “expand” it by grabbing its one‐hop IS-A neighbors (parents and children in the SNOMED hierarchy).  \n   • Finally, you retrieve all trials whose indexed conditions lie anywhere in that expanded set.  \n\nBy doing this, you go beyond mere SNOMED CT code lookup and gain recall on semantically related but non-identical concepts, so clinically relevant trials aren’t dropped just because they used a closely related SNOMED term."
    },
    {
        "question": "How does Retrieval & Filtering leverage Demographic Filter to enhance initial trial selection methodologies?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Additionally, the approach proposed in this work allows for additional, albeit weaker, relevance types, viz., age-relevance (R_age) and gender-relevance (R_gender). Specifically, given a patient note t with associated age and gender attribute-sets, A_P and G_P, and a CTR c with its age and gender attribute-sets, A_c, G_c, where ⊤ denotes any case variation thereof. In practice, R_age and R_gender can be used as a demographic filter (DF), applied on condition-relevant candidates.",
            "Having defined the age-/gender-/condition-relevance notions of a CTR, given a patient note, we treat the concept of condition relevance as a means for initial retrieval of clinical trials, followed by age and gender filtering, as defined above."
        ],
        "final_answer": "The system first retrieves trials whose Condition attributes match the patient’s diagnosis (condition‐relevance). It then applies the Demographic Filter (DF) by requiring that the patient’s age and gender values fall within each trial’s Age and Gender attribute‐sets, respectively. By pruning out any trial that fails these demographic checks, the initial retrieval is refined to trials more appropriate for the patient’s specific age and gender.",
        "relevant_elements": [
            "Retrieval & Filtering",
            "Demographic Filter"
        ],
        "id": 230,
        "masked_question": "How does [mask1] leverage [mask2] to enhance initial trial selection methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Retrieval & Filtering",
            "Demographic Filter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.18998v1_figure_1.png",
        "paperid": "2409.18998v1",
        "paper_path": "./papers/2409.18998v1.json",
        "figure_id": "2409.18998v1_figure_1.png",
        "caption": "Figure 1: End-to-end depiction of the set-reasoning guided patient-trial matching.",
        "qtype": "Literature_Background",
        "response": "Mask1 in the red box is the “Retrieval & Filtering” stage of the pipeline.  Mask2 in the blue box is the simple Demographic Filter:\n\n  • Patient.Age ∈ CTR.Age  \n  • Patient.Gender ∈ CTR.Gender  \n\nHere’s how “Retrieval & Filtering” uses that filter to improve initial trial selection:\n\n1.  Condition‐driven recall  \n    – First, it retrieves all trials whose normalized Condition attributes overlap with the patient’s diagnosis (the 1-level retrieval set).  \n2.  Demographic pruning  \n    – From that pool it then applies the demographic filter (mask2), dropping any trial whose age or gender criteria do not match the patient.  \n3.  Refined candidate set  \n    – The remaining trials are both condition‐relevant and demographically eligible, yielding a much tighter, more precise set of candidates for the downstream ranking stages."
    },
    {
        "question": "How does 1 Level Retrieval optimize the demographic filter's selection of CTR candidates?",
        "relevant_section_ids": [
            "3.3.2",
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "In addition to normalization, the domain ontology opens the possibility for leveraging its defined properties and hierarchical concept relationships to expand some of the normalized attribute values with their relevant ontological neighborhood. … We apply the predicate expansion method to a patient normalized diagnosis, \\hat{d}_1, to maximize its match against a trial’s targeted conditions. Thus, the expansion of normalized \\hat{d}_1 is defined by D_1. (Section 3.3.2)",
            "Additionally, the approach proposed in this work allows for additional, albeit weaker, relevance types, viz., age-relevance (φ_age) and gender-relevance (φ_gender). … In practice, φ_age and φ_gender can be used as a demographic filter (DF), applied on condition-relevant candidates. (Section 4.2)",
            "Having defined the age-/gender-/condition-relevance notions of a CTR, given a patient note, we treat the concept of condition relevance as a means for initial retrieval of clinical trials, followed by age and gender filtering, as defined above. … In other words, the higher the overlap between a CTR’s condition and a patient’s diagnosis, the more relevant the trial would be. (Section 4.3)"
        ],
        "final_answer": "By using 1 Level Retrieval, the patient’s normalized diagnosis is first expanded to include only those concepts one taxonomic hop away in the ontology. This yields a focused set of condition‐relevant trials. The demographic filter (matching age and gender) is then applied exclusively to this narrowed, semantically relevant pool. As a result, the filter operates on far fewer, but more appropriate, CTRs—improving both efficiency and precision of candidate selection.",
        "relevant_elements": [
            "1 Level Retrieval",
            "Demographic Filter"
        ],
        "id": 231,
        "masked_question": "How does [mask1] optimize the [mask2]'s selection of CTR candidates?",
        "masked_number": 2,
        "masked_elements": [
            "1 Level Retrieval",
            "Demographic Filter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.18998v1_figure_1.png",
        "paperid": "2409.18998v1",
        "paper_path": "./papers/2409.18998v1.json",
        "figure_id": "2409.18998v1_figure_1.png",
        "caption": "Figure 1: End-to-end depiction of the set-reasoning guided patient-trial matching.",
        "qtype": "Experimental_Results",
        "response": "Here is a step‐by‐step reasoning trace:\n\n1.  After extracting the raw diagnosis from the patient note, we first “normalize” it by finding its closest match in SNOMED CT (e.g. mapping “sleep problems” → c₁ = Sleep disorder).\n\n2.  The red‐boxed “1 Level Retrieval” then pulls in all SNOMED concepts one IS-A hop away from c₁ (e.g. Parasomnia, Sleep Paralysis, Primary Hypersomnia, Narcolepsy, etc.). Call that set D₁.\n\n3.  In the blue‐boxed “Retrieval & Filtering” stage, we now only retrieve those CTRs whose CTR.Condition exactly lies in D₁.\n\n4.  By expanding the patient’s diagnosis into its immediate ontological neighborhood, we turn brittle string‐matching into a semantically grounded lookup. This both widens recall (we pick up trials targeting any closely related concept) and sharpens precision (we ignore trials whose condition is truly unrelated), thus optimizing the pool of CTR candidates before any further age/gender filtering."
    },
    {
        "question": "How does fine-grained labeling inform the scoring function in re-ranking CTR candidates?",
        "relevant_section_ids": [
            "4.4",
            "5"
        ],
        "relevant_context": [
            "Next, we perform further eligibility (as opposed to just relevance) analysis by means of labeling. The eligibility of some CTR $r$ with respect to some patient note $p$, denoted by $\\eta$, is an attribute of the CTR indicating that its inclusion/exclusion criteria allow the patient’s participation in the study. We formally define eligibility with respect to inclusion/exclusion treatment ($\\eta_{tt}$), inclusion/exclusion demographics ($\\eta_{dd}$), and inclusion/exclusion disease ($\\eta_{\\delta\\delta}$). In each case, we employ a labeling function $\\mathcal{L}_{label}$, implemented via a LLM-based instruction, that takes as input a natural language description of some attribute value $e_i$ of $r$ and additional instructive information $I$ to label each $e_i$ with one or more labels based on its $l$ attributes, as described next.",
            "Every scoring function defined below adheres to principle 1: any CTR that is not condition-, age-, and gender-relevant is discarded. The remaining two principles are captured in the scoring functions at various degrees, with the aid of a count function $c(\\cdot)$, defined over $E^{\\text{fine}}$ or $E^{\\text{coarse}}$, that counts the occurrences of some eligibility label $l$."
        ],
        "final_answer": "Fine-grained labeling uses an LLM-based labeling function to assign per-criterion eligibility labels (e.g., “Eligible,” “Excluded,” “No information”) to each inclusion and exclusion predicate in a candidate trial. The re-ranking scoring functions then rely on simple count functions over these fine-grained labels – tallying how many criteria are labeled eligible or excluded – to compute each trial’s final score and order the candidates.",
        "relevant_elements": [
            "Fine-Grained Labeling",
            "Re-Ranking"
        ],
        "id": 232,
        "masked_question": "How does [mask1] inform the scoring function in re-ranking CTR candidates?",
        "masked_number": 1,
        "masked_elements": [
            "Fine-Grained Labeling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.18998v1_figure_1.png",
        "paperid": "2409.18998v1",
        "paper_path": "./papers/2409.18998v1.json",
        "figure_id": "2409.18998v1_figure_1.png",
        "caption": "Figure 1: End-to-end depiction of the set-reasoning guided patient-trial matching.",
        "qtype": "Experimental_Results",
        "response": "The red box in the figure is nothing mystical – it is simply the “Fine‐Grained Labeling” module (step 5) that goes through every single inclusion and exclusion clause in a candidate trial and, via an LLM, tags it as one of:\n\n  • Eligible  \n  • Excluded  \n  • Not Enough Information  \n\nOnce you have those three label‐sets for each trial (one for Disease criteria, one for Treatment, one for Demographics), your re‐ranking score is nothing more than a small arithmetic formula over the sizes of those sets.  Concretely you pick:\n\n  – how many “Eligible” labels did this trial accrue?  \n  – how many “Excluded” labels did it incur?  \n  – (and you can also count the “Not Enough Information” as a denominator or simply ignore it)  \n\nEach of the deontic scoring functions described in § 4.4.3 is just a different way of combining those counts – e.g. reward the # of Eligible matches, subtract or heavily penalize the # of Excluded matches, and either ignore or lightly penalize “unknowns.”  In other words, step 5 produces the atomic counts that feed directly into your s( patient, trial ) re‐ranking formula."
    },
    {
        "question": "How does Sparse Upcycling support modular integration of pretrained weights in the MoE Module?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In this work, we propose to incorporate MoE training into any existing dense LLM checkpoints via the Sparse Upcycling technique (Komatsuzaki et al., 2022), which duplicates the original model to create experts and continue training them on a downstream dataset as a normal MoE.",
            "Importantly, we follow Li et al. (2024b) to only upcycle the MLP connector and the visual encoder since upcycling a dense LLM is found to be worse than just using an MoE LLM. Moreover, we highlight that the dense training stage is unrelated to MoE, and thus the checkpoints can be re-used to train different MoE algorithms."
        ],
        "final_answer": "Sparse Upcycling makes the MoE module fully modular by taking any pretrained dense checkpoint (e.g., the MLP connector and visual encoder), cloning it to form multiple experts, and then plugging these pretrained experts directly into the MoE layer (with its router, balancing losses, etc.). This lets the MoE Module reuse off-the-shelf pretrained weights without costly pretraining or architecture changes, simply extending a dense model into a sparse Mixture-of-Experts.",
        "relevant_elements": [
            "Sparse Upcycling",
            "MoE Module"
        ],
        "id": 233,
        "masked_question": "How does [mask1] support modular integration of pretrained weights in the MoE Module?",
        "masked_number": 1,
        "masked_elements": [
            "Sparse Upcycling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00918v1_figure_1.png",
        "paperid": "2411.00918v1",
        "paper_path": "./papers/2411.00918v1.json",
        "figure_id": "2411.00918v1_figure_1.png",
        "caption": "Figure 1: The detailed design of LiBMoE, which comprises three major modules. First, the MoE module implements various MoE algorithms. Second, the training modules handles the training process and supports various configurations. Lastly, the evaluation module supports almost 100 zero-shot benchmarks and a wide-range of metrics.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what the red box is pointing at.  If you look at the bottom‐left of the MoE Module in Figure 1, you see a blue-dashed arrow labeled “Sparse Upcycling” feeding from the “Pre‐trained Vision-Language Model” block into one of the experts (E1).  That blue arrow and label are exactly what the red box is enclosing, so [mask1] = “Sparse Upcycling.”\n\nChain of thought:\n\n1.  The text tells us that LibMoE can “incorporate MoE training into any existing dense LLM checkpoints via the Sparse Upcycling technique.”  \n2.  In the diagram, Sparse Upcycling is drawn as a blue-dashed line from the pre‐trained model into the expert modules of the MoE block.  \n3.  In practice, Sparse Upcycling simply takes the weights of your off-the-shelf dense vision-language (or language) model, clones them into N experts, and then continues training only through the MoE machinery.  \n4.  By doing so, you never have to rewrite the pre-training step: you drop your checkpoint in, Sparse Upcycling copies its parameters into each expert, and the router learns to pick among them during downstream training.  \n5.  This is exactly how LibMoE achieves a truly plug-and-play, modular integration of any pretrained model into its MoE Module.\n\nAnswer\n\n“Sparse Upcycling” lets you take a fully-trained dense model, duplicate its parameters into each expert, and slot those experts straight into the MoE layer.  In this way you re-use pretrained weights as the expert bank without retraining from scratch, giving you a clean, modular path to integrate any checkpoint into the MoE Module."
    },
    {
        "question": "How does the Dense Training Module support upcycling into the MoE Training Module for efficient MoE training?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "In this work, we propose to incorporate MoE training into any existing dense LLM checkpoints via the Sparse Upcycling technique (Komatsuzaki et al., 2022), which duplicates the original model to create experts and continue training them on a downstream dataset as a normal MoE. Consequently, we can bypass the expensive pre-training step and evaluate MoE algorithms with the most advanced public LLMs.",
            "Importantly, we follow Li et al. (2024b) to only upcycle the MLP connector and the visual encoder since upcycling a dense LLM is found to be worse than just using an MoE LLM. Moreover, we highlight that the dense training stage is unrelated to MoE, and thus the checkpoints can be re-used to train different MoE algorithms.",
            "Second, the training module optimization processes for all three training stages, which supports loading state-of-the-art LLMs, handling custom datasets, hyper-parameter configurations, the sparse upcycling algorithms, and the main training loop."
        ],
        "final_answer": "The Dense Training Module first produces a fully trained ‘dense’ checkpoint (by pre-training the model and then fine-tuning the MLP connector to link the visual encoder and LLM). This dense checkpoint is then passed to the MoE Training Module via LibMoE’s built-in sparse upcycling pipeline, which duplicates the checkpoint across multiple experts and continues training them as a sparse MoE. By reusing the dense checkpoint and only upcycling the connector and encoder, the framework avoids costly full pre-training runs and enables efficient MoE training on downstream data.",
        "relevant_elements": [
            "Dense Training Module",
            "MoE Training Module"
        ],
        "id": 234,
        "masked_question": "How does the [mask1] support upcycling into the [mask2] for efficient MoE training?",
        "masked_number": 2,
        "masked_elements": [
            "Dense Training Module",
            "MoE Training Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.00918v1_figure_1.png",
        "paperid": "2411.00918v1",
        "paper_path": "./papers/2411.00918v1.json",
        "figure_id": "2411.00918v1_figure_1.png",
        "caption": "Figure 1: The detailed design of LiBMoE, which comprises three major modules. First, the MoE module implements various MoE algorithms. Second, the training modules handles the training process and supports various configurations. Lastly, the evaluation module supports almost 100 zero-shot benchmarks and a wide-range of metrics.",
        "qtype": "Experimental_Results",
        "response": "Let’s start by mapping the two masked regions to the diagram:\n\n  1.  [mask1] is the red‐boxed “Dense Training Module” in the lower-left, which is responsible for initializing and training the small MLP connector (and, to a limited extent, the visual encoder) against a vision-language objective.  \n  2.  [mask2] is the blue-boxed “MoE Training” stage (and its accompanying MoE Training Module) in the bottom-center/right, where that dense-trained checkpoint is lifted into a sparse Mixture-of-Experts and further trained with routers, balancing losses, etc.\n\nHow the dense stage supports upcycling into the MoE stage, step by step:\n\n  •  Dense initialization only needs a relatively modest vision-language corpus (∼10^9 tokens) and a handful of GPUs.  You train the small MLP connector to route visual features into the LLM, yielding a solid, functioning vision-language checkpoint.  \n  •  “Sparse upcycling” then takes that dense checkpoint, clones the connector (and optionally parts of the encoder) into N parallel “experts,” and plugs them into a Mixture-of-Experts wrapper (the MoE Module).  \n  •  Because the experts all start from the same well‐tuned dense weights, you can immediately begin sparse training—learning router parameters, applying the noisy‐top-K gate, and optimizing the balancing loss—without re-pretraining the entire backbone.  \n  •  In other words, the Dense Training Module provides a lightweight, already-frozen core that can be duplicated into experts.  The MoE Training Module then efficiently adapts that core into a full sparse-expert model, avoiding the huge cost of training from scratch.\n\nBy splitting the work into a cheap dense warm-up and then a quick “upcycling” into sparse experts, LibMoE achieves efficient MoE training on existing LLM checkpoints."
    },
    {
        "question": "What ethical considerations emerge from Sparse Upcycling of a Pre-trained Vision-Language Model?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Sparse Upcycling",
            "Pre-trained Vision-Language Model"
        ],
        "id": 235,
        "masked_question": "What ethical considerations emerge from [mask1] of a Pre-trained Vision-Language Model?",
        "masked_number": 1,
        "masked_elements": [
            "Sparse Upcycling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.00918v1_figure_1.png",
        "paperid": "2411.00918v1",
        "paper_path": "./papers/2411.00918v1.json",
        "figure_id": "2411.00918v1_figure_1.png",
        "caption": "Figure 1: The detailed design of LiBMoE, which comprises three major modules. First, the MoE module implements various MoE algorithms. Second, the training modules handles the training process and supports various configurations. Lastly, the evaluation module supports almost 100 zero-shot benchmarks and a wide-range of metrics.",
        "qtype": "Others",
        "response": "Here’s a step-by-step unpacking of the question and the salient ethical issues:\n\n1.  What is “[mask1]”?  \n    – In Figure 1 (and § 3.3 of the text) the red box encloses the *sparse upcycling* step—the blue dashed arrow that injects a handful of new sparse “expert” sub-networks into an existing, frozen vision–language backbone rather than training everything from scratch.\n\n2.  Why does *sparse upcycling* matter ethically?  \n    – Because it reuses a large, pre-trained model (along with all of its hidden assumptions, biases, and licensing restrictions) and makes it trivial for smaller labs to spin up a powerful multimodal system with only modest compute.\n\n3.  Main ethical considerations:  \n    •  **Bias propagation & amplification**  \n       – All of the original V–L model’s cultural, demographic or gender biases remain intact.  The upcycled experts may even learn to exploit and amplify those biases if left unchecked.  \n    •  **Transparency & accountability**  \n       – Sparse upcycling obscures which parts of the network were changed and which were held fixed.  That can make it much harder to audit or explain why the system behaves the way it does.  \n    •  **Licensing & provenance**  \n       – Many large pre-trained V–L models are subject to non-commercial or restricted-use licenses.  Upcycling without clear provenance tracking could violate those terms or muddy the chain-of-custody.  \n    •  **Dual-use & misuse**  \n       – By dramatically lowering the cost of building a high-quality V–L system, sparse upcycling also lowers the barrier for malicious actors to deploy it for disinformation, surveillance, or privacy violations.  \n    •  **Environmental & resource justice**  \n       – Although upcycling is more compute-efficient than full retraining, it still locks groups into expensive GPU-based workflows.  This can exacerbate global inequities in who can afford to build and audit these systems.\n\nIn short, *sparse upcycling* inherits all of the original model’s social and legal baggage—and by making these giant V–L backbones cheaper to retrofit, it raises fresh concerns around bias, transparency, licensing, dual use, and fair access."
    },
    {
        "question": "What limitations might Hybrid Loss face balancing linguistic, classification, and robustness objectives in aligned LLMs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Hybrid Loss",
            "Aligned LLMs"
        ],
        "id": 237,
        "masked_question": "What limitations might [mask1] face balancing linguistic, classification, and robustness objectives in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Hybrid Loss",
            "Aligned LLMs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.08661v1_figure_1.png",
        "paperid": "2408.08661v1",
        "paper_path": "./papers/2408.08661v1.json",
        "figure_id": "2408.08661v1_figure_1.png",
        "caption": "Figure 1: The overall framework of MIA-Tuner and the two pipelines designed for aligned and unaligned LLMs, resprectively.",
        "qtype": "Others",
        "response": "In our case, [mask1] is the three‐part “hybrid” loss (linguistics + classification + robustness) and [mask2] is the aligned‐LLM pipeline in Figure 1A.  The very thing that makes the hybrid loss attractive—its simultaneous emphasis on fluency, detection accuracy, and “yes/no” safety—also makes it brittle in practice:\n\n  • Conflicting gradients.  \n    – The NLL term wants the model to remain as close as possible to its original, open-ended language behavior.  \n    – The classification term wants to push the model to a rigid yes/no decision boundary.  \n    – The robustness term aggressively suppresses any token except “Yes” or “No.”  \n    In many steps of fine-tuning these forces pull in different directions, which can stall or destabilize training.\n\n  • Hyperparameter sensitivity.  \n    You must pick three weights (α, β, γ) to trade off fluency vs. detection accuracy vs. answer validity.  If you bias too heavily toward classification or robustness, you rapidly destroy the model’s ability to produce coherent language; if you lean too hard on the linguistics term, you end up with a model that refuses to make a confident membership prediction.  Tuning those weights often requires per-model or per-task grid-searches.\n\n  • Degraded generality.  \n    Because the robustness term harshly penalizes everything but “Yes”/“No,” the model can learn pathological shortcuts—always defaulting to a single answer regardless of content—which looks robust on your test set but fails miserably on slightly out-of-distribution examples.\n\n  • Slower convergence and higher compute.  \n    Balancing three objectives typically means more training iterations, more careful learning-rate schedules, and more intermediate checkpoints—driving up the cost of deploying an aligned‐LLM detector.\n\nIn short, while the hybrid loss can produce very sharp “Yes/No” detectors, it comes at the expense of extra hyperparameter tuning, possible conflicts between fluency and classification, and a fragile training process."
    },
    {
        "question": "What motivates distinct pipeline designs for aligned and unaligned LLMs?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Subsequently, considering the different intentions of aligned and unaligned LLMs, we design two distinct pipelines for fine-tuning aligned and unaligned LLMs, respectively.",
            "For aligned LLMs, which are already aligned with human feedback, we fully explore this characteristic to fine-tune LLMs to become pre-training text detection assistants. We use instruction fine-tuning to align LLMs with our intention of directly answering “Yes” or “No” [to whether a pending text belongs to their pre-training set].",
            "Unlike aligned LLM, unaligned LLM cannot directly answer the pre-training text detection question. Therefore, following existing research, we use the loss as a metric to discriminate member texts and fine-tune LLM to amplify the obscured differences in this distribution."
        ],
        "final_answer": "The pipelines differ because aligned LLMs—being instruction-tuned—can be prompted to directly answer yes/no membership queries, whereas unaligned LLMs lack that capability and must rely on loss-based detection metrics, motivating two separate fine-tuning strategies.",
        "relevant_elements": [
            "Aligned LLMs",
            "Unaligned LLMs"
        ],
        "id": 239,
        "masked_question": "What motivates distinct pipeline designs for [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Aligned LLMs",
            "Unaligned LLMs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.08661v1_figure_1.png",
        "paperid": "2408.08661v1",
        "paper_path": "./papers/2408.08661v1.json",
        "figure_id": "2408.08661v1_figure_1.png",
        "caption": "Figure 1: The overall framework of MIA-Tuner and the two pipelines designed for aligned and unaligned LLMs, resprectively.",
        "qtype": "Design_Rationale",
        "response": "The reason we actually end up with two totally different “MIA-Tuner” recipes is simply that aligned and unaligned LLMs behave very differently out‐of‐the‐box:\n\n1. Aligned models (the red box / [mask1]) have already been instruction-tuned to follow human prompts and give you a crisp “Yes” or “No.”  So here we can literally teach the model to play a membership-detecting assistant by prepending a soft prompt, adding a short system/user instruction, and then training with a hybrid loss that (a) keeps the model fluent, (b) enforces correct yes/no classification, and (c) penalizes anything else.\n\n2. Unaligned models (the blue box / [mask2]) have no built-in “give me a yes/no answer” head—they just spit out probabilities over the next token.  You can’t force them to say “Yes” or “No” without completely re-architecting them, so instead we treat their own token-prediction loss as the signal.  We learn a soft prompt that widens the gap between the loss distributions for members vs. non-members, using a contrastive-style loss so that member texts get unusually low loss and non-members unusually high loss.\n\nIn short: because one family of models is already an instruction-following assistant (so you train it as a classifier) and the other is just a bare next‐token predictor (so you train it to exaggerate its loss differences), you need two different pipelines."
    },
    {
        "question": "What motivates including classification loss and robustness loss components in the hybrid loss?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Specifically, we designed a new hybrid loss from three dimensions to ensure that aligned large language models can assist users in identifying pre-training set texts through dialogue: 1) Linguistics: LLM should resist basic linguistic capability to answer user questions. 2) Classification: LLM should be proficient in distinguishing between member and non-member texts. 3) Robustness: LLM should ensure the validity of output answers.",
            "We further adopt the cross-entropy loss as the classification part of the hybrid loss. Particularly, we first renormalize the probability that the victim aligned LLM answers “Yes” or “No”, then measure the negative log-likelihood of the victim LLM performs a correct answer.",
            "Furthermore, we assign a penalty value to illegal tokens other than “Yes” or “No” as part of the robustness of the hybrid loss: where  refers to all illegal answer tokens."
        ],
        "final_answer": "The classification loss is included so that the LLM becomes proficient at distinguishing member texts (those seen during pre-training) from non-member texts. The robustness loss is included to ensure that the model’s outputs remain valid—penalizing any illegal tokens other than the permitted answers “Yes” or “No.”",
        "relevant_elements": [
            "Hybrid Loss",
            "Classification Loss",
            "Robustness Loss"
        ],
        "id": 240,
        "masked_question": "What motivates including [mask1] and [mask2] components in the hybrid loss?",
        "masked_number": 2,
        "masked_elements": [
            "Classification Loss",
            "Robustness Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.08661v1_figure_1.png",
        "paperid": "2408.08661v1",
        "paper_path": "./papers/2408.08661v1.json",
        "figure_id": "2408.08661v1_figure_1.png",
        "caption": "Figure 1: The overall framework of MIA-Tuner and the two pipelines designed for aligned and unaligned LLMs, resprectively.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought reasoning  \n1. In Figure 1A (Aligned LLMs) we see three loss terms in the so-called “hybrid loss”:  \n   – the red-boxed term, marked “[mask1]”, is the classification loss (cross-entropy on the model’s “Yes/No” renormalized probabilities).  \n   – the blue-boxed term, marked “[mask2]”, is the robustness loss (a penalty on any token other than the two legal answers).  \n2. From the text (§“Hybrid Loss for Aligned LLMs”):  \n   – The authors want the tuned LLM not only to preserve its basic language ability (the linguistics loss) but also to become good at telling apart member versus non-member text. That goal is captured by the classification loss—i.e. forcing the model to push up the probability of the correct “Yes”/“No” answer on each example.  \n   – They further observe that, without any additional constraint, the model might “escape” by spitting out spurious or malformed tokens instead of a clean “Yes” or “No.” To prevent that, they add a robustness term that explicitly penalizes any token outside of the two legal answers.  \n3. Therefore, the inclusion of [mask1] and [mask2] is motivated by the need to (a) sharpen the model’s discrimination between member and non-member texts, and (b) guarantee it produces only valid “Yes”/“No” replies rather than arbitrary tokens.  \n\nAnswer  \nThey add the classification term ([mask1]) to force the model to learn a strong, high-confidence separation between member and non-member examples, and the robustness term ([mask2]) to ensure it never “cheats” by outputting illegal tokens—i.e. to guarantee that its answer is always a well-formed “Yes” or “No.”"
    },
    {
        "question": "What motivates placing Deduplication before Transformation in the raw code pipeline?",
        "relevant_section_ids": [
            "2.1.1"
        ],
        "relevant_context": [
            "The purpose of deduplication is to construct an unbiased and diverse training set while significantly reducing the data volume.",
            "Owing to the extremely high repetition of the source code in Github, we prioritize the deduplication process early in the pipeline and adopt an aggressive file-level deduplication strategy."
        ],
        "final_answer": "Because GitHub hosts an extremely high volume of duplicate code, deduplication is performed early to remove redundant files, reduce dataset size, and ensure an unbiased, diverse training set before applying transformations.",
        "relevant_elements": [
            "Deduplication",
            "Transformation"
        ],
        "id": 241,
        "masked_question": "What motivates placing [mask1] before Transformation in the raw code pipeline?",
        "masked_number": 1,
        "masked_elements": [
            "Deduplication"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04905v2_figure_2.png",
        "paperid": "2411.04905v2",
        "paper_path": "./papers/2411.04905v2.json",
        "figure_id": "2411.04905v2_figure_2.png",
        "caption": "Figure 2: The illustration of our pretraining data processing workflow.",
        "qtype": "Design_Rationale",
        "response": "Here “[mask1]” is the Deduplication step.  We choose to dedupe before Transformation for two tightly linked reasons:\n\n1. Code on GitHub is wildly redundant (roughly 75% of files are bit-for-bit copies because of forks and paste-overs).  If we don’t dedupe up front, we end up running expensive transformation and filtering passes over the same content hundreds of times.  \n\n2. Many transformations (e.g. PII masking, copyright‐notice stripping) would alter the raw text and could even break downstream exact‐ or fuzzy‐hashing schemes.  By deduplicating on the pristine source, we both maximize removal of true duplicates and avoid “polluting” the dedupe signal with transformation artifacts.  \n\nTaken together, an early, aggressive file‐level deduplication yields a smaller, more diverse, and unbiased code corpus—saving compute and improving overall data quality."
    },
    {
        "question": "What drives using FastText Model Training prior to Recall From Common Crawl?",
        "relevant_section_ids": [
            "2.1.2"
        ],
        "relevant_context": [
            "As shown in Figure 2, the processing pipeline of code-related web data comprises four main components: 1) FastText Model Training: To maintain a controllable vocabulary size in fastText and enable tokenization of Chinese texts using spaces, we first apply the BPE tokenizer to segment the corpus. Subsequently, the open-source FastText framework is utilized for model training.",
            "2) Recall From Common Crawl: We perform recall on Common Crawl to generate the code-related web corpus."
        ],
        "final_answer": "FastText Model Training is used first to build a classifier with a manageable (controllable) vocabulary size and to support tokenization of Chinese text via space‐separated BPE tokens. Once trained, this classifier is then applied to recall relevant code‐related content from the Common Crawl.",
        "relevant_elements": [
            "FastText Model Training",
            "Recall From Common Crawl"
        ],
        "id": 242,
        "masked_question": "What drives using [mask1] prior to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "FastText Model Training",
            "Recall From Common Crawl"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04905v2_figure_2.png",
        "paperid": "2411.04905v2",
        "paper_path": "./papers/2411.04905v2.json",
        "figure_id": "2411.04905v2_figure_2.png",
        "caption": "Figure 2: The illustration of our pretraining data processing workflow.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1. In the “code‐related web data” pipeline (Figure 2b), the red‐boxed module is “FastText Model Training” and the blue‐boxed module is “Recall From Common Crawl.”  \n2. You cannot pull out code pages from the raw Common Crawl dump without first having something that tells you “this is code‐related” vs. “this is not.”  \n3. To get that, the authors manually label a small seed set of positive (code) and negative (non-code) pages and then train a lightweight text classifier using FastText (hence the red box).  \n4. Only once that classifier exists can you apply it to the billions of pages in Common Crawl to “recall” the ones that look code‐related (the blue box).  \n\nAnswer  \nThey train the FastText classifier first so that they have a model capable of distinguishing code-related from non-code content; only then can they efficiently recall (i.e. retrieve) the code-related pages from the Common Crawl data."
    },
    {
        "question": "How does Deduplication determine file retention to balance volume reduction and data diversity?",
        "relevant_section_ids": [
            "2.1",
            "2.1.1"
        ],
        "relevant_context": [
            "The purpose of deduplication is to construct an unbiased and diverse training set while significantly reducing the data volume.",
            "Exact Deduplication: … We compute the SHA256 hash value for each document, where files with identical hash values are compared, and only the code files with the highest star count as well as the latest commit time are retained.",
            "Fuzzy Deduplication: … we split the raw text into 5-gram pieces, and then calculate the 2048 MinHash functions. Additionally, we utilize LSH … to retain only those distinct files with the highest stars and latest commit time."
        ],
        "final_answer": "Deduplication first groups identical files by computing SHA256 hashes (exact deduplication) and then groups near-identical files using 5-gram MinHash with LSH (fuzzy deduplication). Within each identical or near-identical group, it retains only the file with the highest GitHub star count and most recent commit time, thereby cutting duplicate volume while preserving diverse, high-quality code examples.",
        "relevant_elements": [
            "Deduplication"
        ],
        "id": 243,
        "masked_question": "How does [mask1] determine file retention to balance volume reduction and data diversity?",
        "masked_number": 1,
        "masked_elements": [
            "Deduplication"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04905v2_figure_2.png",
        "paperid": "2411.04905v2",
        "paper_path": "./papers/2411.04905v2.json",
        "figure_id": "2411.04905v2_figure_2.png",
        "caption": "Figure 2: The illustration of our pretraining data processing workflow.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module in Figure 2(a) is the “Deduplication” step.  Its goal is to collapse highly redundant files (GitHub is ∼75 % exact copies plus a further ∼6 % near-duplicates) while still preserving the breadth of the code corpus.  To do that it works in two stages:\n\n1.  Exact deduplication  \n    •  Compute a SHA-256 hash of each file’s text  \n    •  Group files with identical hashes  \n    •  Within each group keep exactly one file—the one with the highest GitHub star count (as a proxy for importance/popularity) and, in case of a tie, the most recent commit time  \n    •  Discard all the other bit-for-bit duplicates\n\n2.  Fuzzy deduplication  \n    •  Break each file into overlapping 5-grams and compute a 2 048-dimensional MinHash sketch (Broder, 1997)  \n    •  Use locality-sensitive hashing (LSH) with 16 bands of 128 rows to identify clusters of near-identical files  \n    •  Again, in each cluster retain only the file with highest star count and newest commit, dropping the rest (this prunes another ∼6 % of files)\n\nBy always keeping the “best” representative (highest stars, latest commit) from each exact or fuzzy duplicate cluster, it aggressively cuts down on sheer volume yet maintains as much code‐style and topical diversity as possible."
    },
    {
        "question": "How does FastText Model Training calibrate classifier thresholds to optimize recall from Common Crawl?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "FastText Model Training",
            "Recall From Common Crawl"
        ],
        "id": 244,
        "masked_question": "How does [mask1] calibrate classifier thresholds to optimize recall from [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "FastText Model Training",
            "Recall From Common Crawl"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.04905v2_figure_2.png",
        "paperid": "2411.04905v2",
        "paper_path": "./papers/2411.04905v2.json",
        "figure_id": "2411.04905v2_figure_2.png",
        "caption": "Figure 2: The illustration of our pretraining data processing workflow.",
        "qtype": "Implementation_Details",
        "response": "In our pipeline the little red‐boxed module (“FastText Model Training”) is nothing more exotic than a binary text‐classifier that we train on our hand‐annotated seed of code vs. non‐code pages, and then use that very same classifier to pull (“recall”) code‐related pages out of the huge CommonCrawl dump (the blue box).  The only “magic” step is how we choose where on the classifier’s score axis to cut—i.e. how we set the decision threshold so that we pick up as many real code‐pages as possible (high recall) while dumping most of the pure noise.\n\nHere’s the nutshell recipe:\n\n1.  Split your 500 K seed pages into train/validation (roughly 80/20).  \n2.  Train a FastText binary model to distinguish code‐pages (StackOverflow, GitHub, etc.) from non‐code pages (news sites, blogs, etc.).  \n3.  On the held‐out validation slice, sweep the FastText “positive” score threshold from 0.0→1.0 and record the recall of true code pages at each point.  \n4.  Pick the lowest score threshold that achieves your target recall (e.g. 95 – 99 %), accepting that you’ll let a bit more non‐code slip in.  \n5.  Apply that calibrated cutoff to the full CommonCrawl index (that is “Recall From Common Crawl”) and retrieve everything scoring above the threshold.  \n\nBecause our goal is to maximize coverage of code‐related web‐pages, we sacrifice a bit of precision in favor of recall—hence the phrase “calibrate classifier thresholds to optimize recall.”"
    },
    {
        "question": "How does CKA process Z_A^fθ and Z_A^fϕ to yield each entry in s(A,Θ,Φ)?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "We compute the similarity between two models fθ and fφ for a dataset by applying CKA [14, 42] to their vector representations Z_A^fθ and Z_A^fφ.",
            "CKA computes the similarity based on the normalized Hilbert–Schmidt Independence Criterion [28], applied to the kernel matrices of both representations.",
            "Using CKA with a linear kernel focuses on global similarity structure, while an RBF kernel with small σ measures local similarity structure [42, 2]."
        ],
        "final_answer": "Each entry in s(A, Θ, Φ) is obtained by taking the two representation matrices Z_A^fθ and Z_A^fφ, forming their kernel matrices (here with a linear kernel), computing the Hilbert–Schmidt Independence Criterion between these kernels, and then normalizing by the product of their norms. This normalized HSIC value is the CKA similarity score that populates s(A, Θ, Φ) for the model pair (fθ, fφ).",
        "relevant_elements": [
            "CKA",
            "Z_A^fθ",
            "Z_A^fϕ"
        ],
        "id": 245,
        "masked_question": "How does [mask1] process Z_A^fθ and Z_A^fϕ to yield each entry in s(A,Θ,Φ)?",
        "masked_number": 1,
        "masked_elements": [
            "CKA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05561v1_figure_1.png",
        "paperid": "2411.05561v1",
        "paper_path": "./papers/2411.05561v1.json",
        "figure_id": "2411.05561v1_figure_1.png",
        "caption": "Figure 1: Pairwise similarity analysis framework. Let 𝑨𝑨{\\bm{A}}bold_italic_A and 𝑩𝑩{\\bm{B}}bold_italic_B be two sets of stimuli and ΘΘ\\Thetaroman_Θ and ΦΦ\\Phiroman_Φ be two sets of models. For each dataset, we extract representations from all functions within the two model sets. Subsequently, for each pair of models (fθ,fϕ)subscript𝑓𝜃subscript𝑓italic-ϕ(f_{\\theta},f_{\\phi})( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ), where fθ∈Θsubscript𝑓𝜃Θf_{\\theta}\\in\\Thetaitalic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∈ roman_Θ and fϕ∈Φsubscript𝑓italic-ϕΦf_{\\phi}\\in\\Phiitalic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ∈ roman_Φ, we compute the CKA similarities between their representations, yielding a similarity vector 𝒔(𝑨,Θ,Φ)subscript𝒔𝑨ΘΦ{\\bm{s}}_{\\left({\\bm{A}},\\Theta,\\Phi\\right)}bold_italic_s start_POSTSUBSCRIPT ( bold_italic_A , roman_Θ , roman_Φ ) end_POSTSUBSCRIPT. This vector can be displayed as a matrix, where each entry represents the similarity between two models for a single dataset. In the scatter plot, we contrast two such vectors computed on the same models but evaluated on different datasets. The Pearson correlation coefficient ρ𝜌\\rhoitalic_ρ between the similarities quantifies the consistency of similarities across the two datasets. The distribution of ρ𝜌\\rhoitalic_ρ across all dataset pairs indicates the stability of (relative) representational similarities across stimuli.",
        "qtype": "Implementation_Details",
        "response": "First, note that the little red‐boxed label in the schematic is “CKA” (Centered Kernel Alignment).  What that box is doing is taking the two N×D feature‐matrices  \n  Z_A^{f_θ}  and  Z_A^{f_φ}  \nand turning them into a single scalar similarity.  Concretely:\n\n1. Form (for a linear kernel) the two N×N Gram (kernel) matrices  \n   K_θ = Z_A^{f_θ} (Z_A^{f_θ})^T  \n   K_φ = Z_A^{f_φ} (Z_A^{f_φ})^T  \n\n2. Center both kernels (subtracting row‐ and column‐means, adding back the grand mean).\n\n3. Compute the Hilbert–Schmidt Independence Criterion  \n   HSIC(K_θ, K_φ) = trace( K_θ_centered ⋅ K_φ_centered )  \n\n4. Normalize by the self‐HSIC of each kernel to get  \n   CKA(K_θ, K_φ) =  \n     HSIC(K_θ, K_φ)  \n     ⁄ √[ HSIC(K_θ, K_θ) · HSIC(K_φ, K_φ) ]  \n\nThat final normalized value is exactly the single entry in the vector s(A,Θ,Φ) corresponding to the model‐pair (f_θ, f_φ)."
    },
    {
        "question": "How is Pearson correlation ρ computed across s(A,Θ,Φ) and s(B,Θ,Φ)?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "To quantify the consistency of similarities between two datasets A and B, we use the Pearson correlation coefficient between the similarity vectors s(A,Θ,Φ) and s(B,Θ,Φ), i.e., ρ(s(A,Θ,Φ), s(B,Θ,Φ)).",
            "The Pearson correlation measures the degree to which the similarity trends between models are preserved across datasets, focusing on the relative positioning of model pairs rather than the absolute similarity values."
        ],
        "final_answer": "ρ is computed as the Pearson correlation coefficient between the two similarity vectors s(A,Θ,Φ) and s(B,Θ,Φ).",
        "relevant_elements": [
            "ρ",
            "s(A,Θ,Φ)",
            "s(B,Θ,Φ)"
        ],
        "id": 246,
        "masked_question": "How is Pearson correlation [mask1] computed across [mask2] and s(B,Θ,Φ)?",
        "masked_number": 2,
        "masked_elements": [
            "ρ",
            "s(A,Θ,Φ)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05561v1_figure_1.png",
        "paperid": "2411.05561v1",
        "paper_path": "./papers/2411.05561v1.json",
        "figure_id": "2411.05561v1_figure_1.png",
        "caption": "Figure 1: Pairwise similarity analysis framework. Let 𝑨𝑨{\\bm{A}}bold_italic_A and 𝑩𝑩{\\bm{B}}bold_italic_B be two sets of stimuli and ΘΘ\\Thetaroman_Θ and ΦΦ\\Phiroman_Φ be two sets of models. For each dataset, we extract representations from all functions within the two model sets. Subsequently, for each pair of models (fθ,fϕ)subscript𝑓𝜃subscript𝑓italic-ϕ(f_{\\theta},f_{\\phi})( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ), where fθ∈Θsubscript𝑓𝜃Θf_{\\theta}\\in\\Thetaitalic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∈ roman_Θ and fϕ∈Φsubscript𝑓italic-ϕΦf_{\\phi}\\in\\Phiitalic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ∈ roman_Φ, we compute the CKA similarities between their representations, yielding a similarity vector 𝒔(𝑨,Θ,Φ)subscript𝒔𝑨ΘΦ{\\bm{s}}_{\\left({\\bm{A}},\\Theta,\\Phi\\right)}bold_italic_s start_POSTSUBSCRIPT ( bold_italic_A , roman_Θ , roman_Φ ) end_POSTSUBSCRIPT. This vector can be displayed as a matrix, where each entry represents the similarity between two models for a single dataset. In the scatter plot, we contrast two such vectors computed on the same models but evaluated on different datasets. The Pearson correlation coefficient ρ𝜌\\rhoitalic_ρ between the similarities quantifies the consistency of similarities across the two datasets. The distribution of ρ𝜌\\rhoitalic_ρ across all dataset pairs indicates the stability of (relative) representational similarities across stimuli.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed term is simply the Pearson correlation coefficient ρ, and the blue‐boxed term is the similarity vector s(A,Θ,Φ).  In other words, the Pearson correlation is computed as\n\n ρ ( s(A,Θ,Φ) , s(B,Θ,Φ) )\n\ni.e. the Pearson correlation between the two similarity vectors s(A,Θ,Φ) and s(B,Θ,Φ)."
    },
    {
        "question": "How does CKA kernel choice influence Pearson correlation of s(A,Θ,Φ) across dataset pairs?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Using CKA with a linear kernel focuses on global similarity structure, while an RBF kernel with small σ measures local similarity structure [42, 2].",
            "We compared their behavior (shown in Fig. 3 and in Appx. E) and observed similar trends for both kernels. Therefore, we use CKA with a linear kernel for the remainder of this paper if not mentioned otherwise."
        ],
        "final_answer": "The choice of CKA kernel (linear versus RBF) does not materially affect the Pearson correlation of s(A,Θ,Φ) across dataset pairs: both kernels yield very similar correlation trends.",
        "relevant_elements": [
            "CKA",
            "Pearson correlation",
            "s(A,Θ,Φ)"
        ],
        "id": 247,
        "masked_question": "How does [mask1] kernel choice influence Pearson correlation of s(A,Θ,Φ) across dataset pairs?",
        "masked_number": 1,
        "masked_elements": [
            "CKA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05561v1_figure_1.png",
        "paperid": "2411.05561v1",
        "paper_path": "./papers/2411.05561v1.json",
        "figure_id": "2411.05561v1_figure_1.png",
        "caption": "Figure 1: Pairwise similarity analysis framework. Let 𝑨𝑨{\\bm{A}}bold_italic_A and 𝑩𝑩{\\bm{B}}bold_italic_B be two sets of stimuli and ΘΘ\\Thetaroman_Θ and ΦΦ\\Phiroman_Φ be two sets of models. For each dataset, we extract representations from all functions within the two model sets. Subsequently, for each pair of models (fθ,fϕ)subscript𝑓𝜃subscript𝑓italic-ϕ(f_{\\theta},f_{\\phi})( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ), where fθ∈Θsubscript𝑓𝜃Θf_{\\theta}\\in\\Thetaitalic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∈ roman_Θ and fϕ∈Φsubscript𝑓italic-ϕΦf_{\\phi}\\in\\Phiitalic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ∈ roman_Φ, we compute the CKA similarities between their representations, yielding a similarity vector 𝒔(𝑨,Θ,Φ)subscript𝒔𝑨ΘΦ{\\bm{s}}_{\\left({\\bm{A}},\\Theta,\\Phi\\right)}bold_italic_s start_POSTSUBSCRIPT ( bold_italic_A , roman_Θ , roman_Φ ) end_POSTSUBSCRIPT. This vector can be displayed as a matrix, where each entry represents the similarity between two models for a single dataset. In the scatter plot, we contrast two such vectors computed on the same models but evaluated on different datasets. The Pearson correlation coefficient ρ𝜌\\rhoitalic_ρ between the similarities quantifies the consistency of similarities across the two datasets. The distribution of ρ𝜌\\rhoitalic_ρ across all dataset pairs indicates the stability of (relative) representational similarities across stimuli.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. The red‐boxed term in the diagram is the “CKA” step, i.e. the choice of kernel used in the CKA similarity (linear versus RBF).  \n2. In the Methods text they say: “Using CKA with a linear kernel focuses on global similarity structure, while an RBF kernel with small σ measures local similarity structure … We compared their behavior (shown in Fig. 3 and in Appx E) and observed similar trends for both kernels. Therefore, we use CKA with a linear kernel for the remainder of this paper.”  \n3. Hence the “CKA kernel choice” hardly changes the final Pearson ρ between s(A,Θ,Φ) vectors across dataset pairs – both linear and RBF give near‐identical correlation distributions.  \n\nAnswer  \nThe choice of CKA kernel (linear vs. RBF) has virtually no effect on the Pearson correlation of s(A,Θ,Φ) across dataset pairs – both kernels yield the same trends in similarity consistency, so the correlations remain essentially unchanged."
    },
    {
        "question": "How does partitioning models into Θ and Φ affect Pearson correlation-based similarity consistency measurement?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Specifically, we scrutinize how the pairwise similarities of models from two model sets, Θ and Φ, correspond between two datasets, D_A and D_B. By computing the pairwise representational similarities between all model pairs fθ, fφ, separately for each dataset, where fθ ∈ Θ and fφ ∈ Φ, we obtain a similarity vector s_(A,Θ,Φ) ∈ R^{|Θ|⋅|Φ|}.",
            "To quantify the consistency of similarities between two datasets D_A and D_B, we use the Pearson correlation coefficient between the similarity vectors s_(A,Θ,Φ) and s_(B,Θ,Φ), i.e., ρ(s_(A,Θ,Φ), s_(B,Θ,Φ))."
        ],
        "final_answer": "Partitioning the models into two sets Θ and Φ means that only the pairwise similarities between models in Θ and models in Φ are collected into a single similarity vector for each dataset. The Pearson correlation is then computed between these two vectors—one for each dataset—so the consistency measurement specifically reflects how well the ordering of cross-set (Θ vs. Φ) similarities is preserved across datasets.",
        "relevant_elements": [
            "Θ",
            "Φ",
            "Pearson correlation"
        ],
        "id": 248,
        "masked_question": "How does partitioning models into [mask1] and [mask2] affect Pearson correlation-based similarity consistency measurement?",
        "masked_number": 2,
        "masked_elements": [
            "Θ",
            "Φ"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.05561v1_figure_1.png",
        "paperid": "2411.05561v1",
        "paper_path": "./papers/2411.05561v1.json",
        "figure_id": "2411.05561v1_figure_1.png",
        "caption": "Figure 1: Pairwise similarity analysis framework. Let 𝑨𝑨{\\bm{A}}bold_italic_A and 𝑩𝑩{\\bm{B}}bold_italic_B be two sets of stimuli and ΘΘ\\Thetaroman_Θ and ΦΦ\\Phiroman_Φ be two sets of models. For each dataset, we extract representations from all functions within the two model sets. Subsequently, for each pair of models (fθ,fϕ)subscript𝑓𝜃subscript𝑓italic-ϕ(f_{\\theta},f_{\\phi})( italic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ), where fθ∈Θsubscript𝑓𝜃Θf_{\\theta}\\in\\Thetaitalic_f start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ∈ roman_Θ and fϕ∈Φsubscript𝑓italic-ϕΦf_{\\phi}\\in\\Phiitalic_f start_POSTSUBSCRIPT italic_ϕ end_POSTSUBSCRIPT ∈ roman_Φ, we compute the CKA similarities between their representations, yielding a similarity vector 𝒔(𝑨,Θ,Φ)subscript𝒔𝑨ΘΦ{\\bm{s}}_{\\left({\\bm{A}},\\Theta,\\Phi\\right)}bold_italic_s start_POSTSUBSCRIPT ( bold_italic_A , roman_Θ , roman_Φ ) end_POSTSUBSCRIPT. This vector can be displayed as a matrix, where each entry represents the similarity between two models for a single dataset. In the scatter plot, we contrast two such vectors computed on the same models but evaluated on different datasets. The Pearson correlation coefficient ρ𝜌\\rhoitalic_ρ between the similarities quantifies the consistency of similarities across the two datasets. The distribution of ρ𝜌\\rhoitalic_ρ across all dataset pairs indicates the stability of (relative) representational similarities across stimuli.",
        "qtype": "Literature_Background",
        "response": "The key thing to realize is that nothing mystical is happening to Pearson‐r itself – what changes is *which* model–model similarities you feed into it.  By “cutting” your full zoo of vision models into two groups (the red‐boxed group, call it Θ, and the blue‐boxed group, call it Φ) you simply change the entries of your similarity‐vector  \n   \n   s(A,Θ,Φ)   \n   \nthat go into  \n   \n   ρ = corr(s(A,Θ,Φ), s(B,Θ,Φ)).   \n   \nIn other words:\n\n  1.  If you choose Θ–Θ pairs only (i.e. you put all your models into the red box and only look at red‐to‐red similarities), then Pearson‐r is measuring how *within‐Θ* similarity *rankings* agree between dataset A and dataset B.  \n  2.  If you choose Φ–Φ pairs only (blue‐to‐blue), you get a different vector and thus possibly a different ρ, now telling you how stable the *within‐Φ* rankings are.  \n  3.  If you choose cross–group pairs (Θ–Φ), you’ll again get a new ρ that quantifies how consistent *between*‐group relationships are across datasets.\n\nWhat one typically finds is that some partitions (for instance, groups of models trained with the same objective or on the same data) give very high ρ—i.e. their relative similarities hardly change from one dataset to another—whereas other partitions (mixing very different training recipes) yield lower ρ, revealing that those cross‐group relationships are much more dataset‐dependent."
    },
    {
        "question": "How does fine-tuning the control encoder while freezing text and image encoders reflect ControlNet’s zero convolution technique?",
        "relevant_section_ids": [
            "2",
            "3.1"
        ],
        "relevant_context": [
            "ControlNet (Zhang et al., 2023a) integrates image-based conditions by incorporating an additional encoder copy into frozen T2I diffusion models via zero convolutions.",
            "We fix the weight of the off-the-shelf image encoder and the text encoder, while finetuning the control encoder."
        ],
        "final_answer": "By freezing the original text and image encoders and only fine-tuning the control encoder, the method mirrors ControlNet’s zero convolution approach: the pre-trained backbone remains untouched while a new, zero-initialized conditional branch (here, the control encoder) is added and trained to inject control signals.",
        "relevant_elements": [
            "Control Encoder",
            "Text Encoder",
            "Image Encoder"
        ],
        "id": 249,
        "masked_question": "How does fine-tuning the [mask1] while freezing [mask2] and image encoders reflect ControlNet’s zero convolution technique?",
        "masked_number": 2,
        "masked_elements": [
            "Control Encoder",
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.11236v1_figure_2.png",
        "paperid": "2410.11236v1",
        "paper_path": "./papers/2410.11236v1.json",
        "figure_id": "2410.11236v1_figure_2.png",
        "caption": "Figure 2: \nA brief overview of our pipeline. Here, we take the segmentation mask as a conditional generation example. (a) Conditional Generation. Given text, source image x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and the conditional control c𝑐citalic_c, we extract feature z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, fcsubscript𝑓𝑐f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, respectively. Then, we fine-tune the Diffusion model to generate two intermediate features for the image decoder.\n(b) Uncertainty Learning. Given the two features, we decode the two images, i.e., x^01superscriptsubscript^𝑥01\\hat{x}_{0}^{1}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and x^02superscriptsubscript^𝑥02\\hat{x}_{0}^{2}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then we apply the reward model to obtain the two layout predictions c^1subscript^𝑐1\\hat{c}_{1}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c^2subscript^𝑐2\\hat{c}_{2}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.\nWe leverage the prediction discrepancy as the uncertainty indicator to rectify the original reward loss.",
        "qtype": "Literature_Background",
        "response": "First, let’s pin down what the two masked regions are: in the figure the red‐lined box (​[mask1]​) is the extra “Control Encoder” module, and the blue‐lined box (​[mask2]​) encloses the pre‐trained text‐ and image‐encoders whose weights we keep frozen.\n\nNow recall how ControlNet’s zero-conv trick works: you take a frozen diffusion U-Net, you “copy” each convolutional layer, you zero‐initialize the copy and you hook your condition pathway into those zeroed convolutions.  Because they start at zero, they initially do absolutely nothing, which means the behavior of the pretrained model is untouched.  Then you train only those side‐branch convolutions so that the control signal is gradually learned, again without ever changing the backbone.\n\nIn exactly the same spirit, our pipeline\n\n  1. freezes the original text- and image-encoders (​[mask2]​) entirely,  \n  2. adds a brand-new Control Encoder (​[mask1]​),  \n  3. zero‐initializes its coupling into the diffusion model,  \n  4. and fine-tunes only that control branch during training.  \n\nThus we preserve the intact, pretrained text/image features while the “zeroed” control pathway alone grows to absorb the new conditional information.  That is a direct analogue of ControlNet’s zero‐conv methodology."
    },
    {
        "question": "How does two-stage reward model evaluation on decoder outputs parallel ensemble uncertainty estimation methods?",
        "relevant_section_ids": [
            "2",
            "3.1"
        ],
        "relevant_context": [
            "ensemble methods (Lakshminarayanan et al., 2017; Malinin et al., 2019; Wenzel et al., 2020) combine various deterministic models in the prediction process to improve prediction accuracy, but is constrained by the computational burden associated with operating multiple independent networks and the requisite diversity across ensemble models.",
            "In particular, we conduct two generation forward with identical condition c but different t1 and t2, and resampled Gaussian noise ε1 and ε2.",
            "We explicitly leverage the reward variance between these two generations as an uncertainty indicator.",
            "Therefore, instead of directly regressing uncertainty, we perform predictions twice to estimate the prediction variance, which serves as a cognitive uncertainty indicator for the reward model. Additionally, this method has the side benefit of not introducing the extra training parameters."
        ],
        "final_answer": "By running two independent decoder passes with the same conditioning (but different noise levels), extracting rewards from each, and measuring the variance between them, Ctrl-U mimics an ensemble: multiple ‘models’ (here, two stochastic decodes) produce reward predictions and their spread serves as an uncertainty estimate. This parallels classical ensemble uncertainty methods, which use the variance across multiple deterministic models to quantify predictive uncertainty, but without needing extra networks or parameters.",
        "relevant_elements": [
            "Image Decoder",
            "Reward Model"
        ],
        "id": 250,
        "masked_question": "How does two-stage [mask1] evaluation on [mask2] outputs parallel ensemble uncertainty estimation methods?",
        "masked_number": 2,
        "masked_elements": [
            "Reward Model",
            "Image Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.11236v1_figure_2.png",
        "paperid": "2410.11236v1",
        "paper_path": "./papers/2410.11236v1.json",
        "figure_id": "2410.11236v1_figure_2.png",
        "caption": "Figure 2: \nA brief overview of our pipeline. Here, we take the segmentation mask as a conditional generation example. (a) Conditional Generation. Given text, source image x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and the conditional control c𝑐citalic_c, we extract feature z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, fcsubscript𝑓𝑐f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, respectively. Then, we fine-tune the Diffusion model to generate two intermediate features for the image decoder.\n(b) Uncertainty Learning. Given the two features, we decode the two images, i.e., x^01superscriptsubscript^𝑥01\\hat{x}_{0}^{1}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and x^02superscriptsubscript^𝑥02\\hat{x}_{0}^{2}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then we apply the reward model to obtain the two layout predictions c^1subscript^𝑐1\\hat{c}_{1}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c^2subscript^𝑐2\\hat{c}_{2}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.\nWe leverage the prediction discrepancy as the uncertainty indicator to rectify the original reward loss.",
        "qtype": "Literature_Background",
        "response": "Here’s the completed sentence, with a brief sketch of why it holds:\n\n“How does two‐stage reward‐model evaluation on decoded image outputs parallel ensemble uncertainty estimation methods?”\n\n– mask₁ = “reward‐model”  \n– mask₂ = “decoded image outputs”\n\nChain of thought (very compact):\n\n1. In our pipeline we do two stochastic forward passes through the diffusion model (lines (a)→(b)), decode each latent into an image, and then apply the frozen reward model D to each image (the blue‐boxed “reward‐model evaluation on decoded images”).  \n2. We then measure the discrepancy between the two reward‐model outputs (the red‐boxed uncertainty module) — e.g. KL‐divergence between two segmentation predictions.  \n3. This construction is essentially a 2-member “ensemble” (two draws under different noise/timesteps), and the variance of their reward outputs plays the same role as the output variance across an ensemble of independently trained models in classical epistemic‐uncertainty methods."
    },
    {
        "question": "How does using two Add Noise injections affect Uncertainty Learning precision in reward adjustment?",
        "relevant_section_ids": [
            "3.1",
            "4.3"
        ],
        "relevant_context": [
            "In particular, we conduct two generation forwards with identical condition c but different t₁ and t₂, and resampled Gaussian noise ε₁ and ε₂.",
            "To estimate inaccurate rewards, we explicitly leverage two diffusion forwards for the identical input conditions. We compare the reward discrepancy between extracted conditions ĉ₁ and ĉ₂ from generated images, which can be considered as a reward indicator at the current timestep.",
            "Discussion. 1). Why not use an auxiliary network to directly regress uncertainty? ... Instead of directly regressing uncertainty, we perform predictions twice to estimate the prediction variance, which serves as a cognitive uncertainty indicator for the reward model.",
            "Design of Uncertainty Estimation. We present an ablation study on the design of the two-time generation in Table 5(b). To mitigate the adverse effects of inaccurate rewards, we forward the identical input condition twice with different noise timesteps to estimate uncertainty.",
            "A short interval, such as Δt=1, where the only randomness stems from resampled noise ε, limits the diversity of the generated images. ... the reward discrepancy is small, and could not serve as an uncertainty indicator. Conversely, a long interval indicates a significant gap between the two noisy latents, which, in turn, increase the generation discrepancy. Too large discrepancy also impacts the accurate uncertainty estimation, and thus compromises reward modeling. When Δt=10, the model achieves the optimal FID and relatively strong mIoU and CLIP‐score."
        ],
        "final_answer": "Injecting noise twice with different timesteps and resampled Gaussian signals enables the model to directly estimate its own prediction variance—i.e. its uncertainty—by measuring how much the two generated outputs disagree.  That variance is then used to modulate the consistency loss: large disagreement (high uncertainty) down‐weights the reward term, while small disagreement leaves it unaffected.  Ablations (§4.3) further show that choosing an appropriate timestep gap is critical: too small a gap yields images too similar to detect uncertainty, too large a gap yields excessive variance that harms estimation, with an intermediate Δt (e.g. 10) giving the best trade‐off of controllability and image quality.",
        "relevant_elements": [
            "Add Noise",
            "Uncertainty Learning"
        ],
        "id": 251,
        "masked_question": "How does using two [mask1] injections affect [mask2] precision in reward adjustment?",
        "masked_number": 2,
        "masked_elements": [
            "Add Noise",
            "Uncertainty Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.11236v1_figure_2.png",
        "paperid": "2410.11236v1",
        "paper_path": "./papers/2410.11236v1.json",
        "figure_id": "2410.11236v1_figure_2.png",
        "caption": "Figure 2: \nA brief overview of our pipeline. Here, we take the segmentation mask as a conditional generation example. (a) Conditional Generation. Given text, source image x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and the conditional control c𝑐citalic_c, we extract feature z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, fcsubscript𝑓𝑐f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, respectively. Then, we fine-tune the Diffusion model to generate two intermediate features for the image decoder.\n(b) Uncertainty Learning. Given the two features, we decode the two images, i.e., x^01superscriptsubscript^𝑥01\\hat{x}_{0}^{1}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and x^02superscriptsubscript^𝑥02\\hat{x}_{0}^{2}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then we apply the reward model to obtain the two layout predictions c^1subscript^𝑐1\\hat{c}_{1}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c^2subscript^𝑐2\\hat{c}_{2}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.\nWe leverage the prediction discrepancy as the uncertainty indicator to rectify the original reward loss.",
        "qtype": "Experimental_Results",
        "response": "In our setup the little red‐box is simply two separate “Add Gaussian noise” steps (at timesteps t₁ and t₂) running the exact same conditional inputs twice, and the big blue‐box is our “uncertainty” or “reward rectification” machinery that uses the difference between those two forwards to re‐weight the reward loss.  Concretely:\n\n 1. By injecting noise twice at two different timesteps we get two slightly different reconstructions of the same condition.  \n 2. We decode both, run our reward model on each, and measure the pixel‐wise discrepancy (KL for probabilistic masks, L₂ for depth/edges).  \n 3. That discrepancy is our uncertainty estimate, which we plug back into the reward loss as an adaptive weight (high discrepancy → low weight, low discrepancy → high weight).  \n\nBecause we are sampling two distinct noisy latents instead of just one, the divergence between them is a much sharper signal of when the reward model is reliable versus when it is not.  In short, the two noise injections give us a more accurate uncertainty (blue‐box) estimate, which in turn makes the reward‐adjustment far more precise."
    },
    {
        "question": "How does control encoder fine-tuning impact feature fusion within the diffusion module for conditional generation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We fix the weight of the off-the-shelf image encoder and the text encoder, while finetuning the control encoder.",
            "Following the ControlNet pipeline (Zhang et al., 2023b), we further fuse text condition f_t and image condition f_c to predict the added noise."
        ],
        "final_answer": "By fine-tuning only the control encoder (while keeping the image and text encoders frozen), the model learns control-specific feature representations (f_c) that are better aligned for fusion with the text features (f_t). This adapted control feature is then merged with the text feature within the diffusion module to more accurately predict and remove noise, thereby strengthening conditional guidance during image generation.",
        "relevant_elements": [
            "Control Encoder",
            "Diffusion"
        ],
        "id": 252,
        "masked_question": "How does [mask1] fine-tuning impact feature fusion within the diffusion module for conditional generation?",
        "masked_number": 1,
        "masked_elements": [
            "Control Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.11236v1_figure_2.png",
        "paperid": "2410.11236v1",
        "paper_path": "./papers/2410.11236v1.json",
        "figure_id": "2410.11236v1_figure_2.png",
        "caption": "Figure 2: \nA brief overview of our pipeline. Here, we take the segmentation mask as a conditional generation example. (a) Conditional Generation. Given text, source image x0subscript𝑥0x_{0}italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, and the conditional control c𝑐citalic_c, we extract feature z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT, ftsubscript𝑓𝑡f_{t}italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT, fcsubscript𝑓𝑐f_{c}italic_f start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT, respectively. Then, we fine-tune the Diffusion model to generate two intermediate features for the image decoder.\n(b) Uncertainty Learning. Given the two features, we decode the two images, i.e., x^01superscriptsubscript^𝑥01\\hat{x}_{0}^{1}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT and x^02superscriptsubscript^𝑥02\\hat{x}_{0}^{2}over^ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT. Then we apply the reward model to obtain the two layout predictions c^1subscript^𝑐1\\hat{c}_{1}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT and c^2subscript^𝑐2\\hat{c}_{2}over^ start_ARG italic_c end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT.\nWe leverage the prediction discrepancy as the uncertainty indicator to rectify the original reward loss.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module in (a) is the control encoder.  In our setup we freeze both the image encoder and the text encoder, but we do allow the control encoder to be fine-tuned.  Concretely, that means that during diffusion training the network learns to adjust the weights of the control encoder so that its output feature fc is maximally compatible with the frozen UNet backbone and the frozen text embeddings ft.  \n\nWhen we actually run the diffusion UNet, we inject both the text feature ft and this fine-tuned control feature fc (alongside the noisy latent zt) into the UNet’s conditioning layers (via concatenation or cross‐attention in the ControlNet style).  Because the control encoder has been fine-tuned, its features now carry exactly the right scale, semantics and spatial structure that the UNet expects to see.  In practice this makes the fusion of “what the user wants” (text) and “how they want it laid out” (control mask) much tighter, so the noise-prediction head in the diffusion model can honour both signals more faithfully."
    },
    {
        "question": "How does sharing visual encoding via MoV unify multi-resolution image features with prompt representations?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In particular, the images and corresponding visual prompts share a visual encoding mechanism for feature sharing, enabling the visual encoders to better understand and associate the relationship between images and visual prompts.",
            "To refine the robust multi-scale visual features, the input images are downsampled to different resolutions denoted as R_i and then respectively fed into the MoV module to encode. Leveraging the strengths of various visual backbones, visual perception is enhanced and key details in images are refined. Subsequently, the encoded visual features are transformed to the same dimension and concatenated channel-wisely to obtain the integrated multi-scale feature maps represented as F.",
            "Notably, a key step to the encoder-sharing mechanism is the “Visual Prompt as Images”. Especially, the dimension (H×W×1) of the visual prompts is processed to the same dimension (H×W×3) with the images. Then, the transformed visual prompts P also can be fed into MoV together with the images, the encoded visual prompts expressed as P′."
        ],
        "final_answer": "EarthMarker treats both the downsampled multi-resolution images and the visual prompts as image inputs to the same Mixture of Visual Experts (MoV) encoder. By resizing all inputs to a common dimension, MoV encodes the various resolutions of the image and the prompt markings jointly. Their feature maps are then projected to the same channel size and concatenated channel-wise, producing an integrated representation that unifies multi-scale image details with prompt embeddings.",
        "relevant_elements": [
            "Sharing Visual Encoding",
            "Mixture of Visual Experts (MoV)"
        ],
        "id": 253,
        "masked_question": "How does [mask1] via MoV unify multi-resolution image features with prompt representations?",
        "masked_number": 1,
        "masked_elements": [
            "Sharing Visual Encoding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.13596v2_figure_2.png",
        "paperid": "2407.13596v2",
        "paper_path": "./papers/2407.13596v2.json",
        "figure_id": "2407.13596v2_figure_2.png",
        "caption": "Figure 2: Left: Overall model architecture of the proposed EarthMarker. Right: Cross-domain Phased Training method.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. The red‐boxed module in Fig. 2 is the “Mixture of Visual Experts (MoV)” with two parallel streams: one for multi‐resolution images and one for visual prompts.  \n2. In the multi‐resolution stream the input image is downsampled to several scales (e.g. full‐, medium‐, and low‐resolution), and each scale is fed into the MoV.  \n3. In the prompt stream the 1‐channel prompt mask (bounding box or point) is simply “promoted” to a 3‐channel image by copying it three times, so it has the same H×W×3 shape as the RGB image.  \n4. Both the downsampled images and the 3-channel prompt images are passed through the same pair of expert backbones (DINOv2-ViT and CLIP-ConvNeXt).  \n5. Their output feature maps are then linearly projected to a common channel dimension and concatenated channel‐wise, yielding one fused multi‐scale feature tensor that carries both the image content at multiple resolutions and the prompt cues.  \n\nAnswer  \nBy turning the prompt mask into a 3-channel “image” and feeding it alongside each downsampled version of the input into the same pair of visual encoders, MoV produces feature maps for both the multi-resolution image and the prompt. Those feature maps are then projected to the same dimensionality and concatenated across channels, thereby unifying the image’s multi-scale details with the prompt representation in a single shared feature space."
    },
    {
        "question": "How does disjoint parameter LoRA tuning preserve cross-domain knowledge during RS visual prompting phase?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Furthermore, the disjoint parameters strategy is proposed, namely, the updated parameters of each stage are different. This strategy is conducive to the step-by-step solid understanding of images, and naturally solves the interference between image-text understanding, visual prompting comprehension, and fine-grained instruction-following.",
            "RS Visual Prompting Tuning. The last stage focuses on accurately following user instructions and achieving complex region-level and point-level visual reasoning tasks. The MoV, alignment projection, and LLM are fixed. The LoRA method is adopted for tuning.",
            "It should be emphasized that during the whole training, our updatable parameters are disjoint, preventing interference between understanding images at different granularity and the capability to follow visual prompts."
        ],
        "final_answer": "By using a disjoint‐parameter strategy in the RS visual prompting stage, EarthMarker freezes its previously trained modules (MoV encoder, alignment projection layer, and the core LLM weights) and only updates separate low‐rank LoRA adapter matrices. Since LoRA tuning modifies only these adapter parameters—and leaves all earlier weights untouched—it avoids overwriting the cross‐domain image‐text alignment and spatial perception capabilities learned in the prior training phases.",
        "relevant_elements": [
            "LoRA",
            "Disjoint Parameters"
        ],
        "id": 254,
        "masked_question": "How does disjoint parameter [mask1] tuning preserve cross-domain knowledge during RS visual prompting phase?",
        "masked_number": 1,
        "masked_elements": [
            "LoRA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.13596v2_figure_2.png",
        "paperid": "2407.13596v2",
        "paper_path": "./papers/2407.13596v2.json",
        "figure_id": "2407.13596v2_figure_2.png",
        "caption": "Figure 2: Left: Overall model architecture of the proposed EarthMarker. Right: Cross-domain Phased Training method.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what the red-boxed module in Phase 3 is:\n\n1. In the right-hand schematic under “Phase 3: RS Visual Prompting Tuning,” you see the LLM icon followed by a small red box labeled “LoRA.”  \n2. The text of III-B (RS Visual Prompting Tuning) then explains that “the MoV, alignment projection, and LLM are fixed. The LoRA method is adopted for tuning. … several learnable low-rank adapter matrices … are inserted into Transformer layers of LLM.”\n\nThus [mask1] = “LoRA” (i.e. the low-rank adapter module).\n\nNow, how does using disjoint-parameter LoRA tuning preserve the cross-domain knowledge learned in Phases 1 and 2?\n\nChain-of-thought:\n\n• In Phases 1 & 2 we align natural+RS image–text and then tune spatial perception. Those phases update only  \n  – Phase 1: the projection layer (MoV frozen, LLM frozen)  \n  – Phase 2: LLM attention layers (projection + MoV frozen)  \n\n• All along, different parts of the network are being “activated” for update in each phase.\n\n• In Phase 3 we now want fine-grained region/point prompting in RS imagery, but we do **not** want to overwrite the broad image–text and spatial knowledge already learned.  \n\n• Hence we employ **disjoint parameter tuning** via **LoRA adapters**:  \n  – We freeze the entire LLM backbone, the MoV visual encoders, and the projection head.  \n  – We introduce a small set of **low-rank adapter matrices** (LoRA) inside each Transformer block of the LLM.  \n  – **Only** those LoRA weights are updated on the RSVP-3M region/text data.  \n\n• Because every other parameter remains unchanged, the previously acquired cross-domain and spatial-perception parameters cannot drift or be “forgotten.” The LoRA modules simply “sit on top” of the frozen model and absorb all the RS-specific adaptation.  \n\nIn short, by isolating all parameter updates in Phase 3 to a disjoint, lightweight adapter module (LoRA) and keeping the rest of the network frozen, EarthMarker preserves the broad, cross-domain image–text alignment and spatial reasoning capabilities learned in earlier phases."
    },
    {
        "question": "What limitations might arise from sharing visual encoding when merging multi-resolution images and visual prompts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "sharing visual encoding",
            "multi-resolution"
        ],
        "id": 255,
        "masked_question": "What limitations might arise from [mask1] when merging multi-resolution images and visual prompts?",
        "masked_number": 1,
        "masked_elements": [
            "sharing visual encoding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.13596v2_figure_2.png",
        "paperid": "2407.13596v2",
        "paper_path": "./papers/2407.13596v2.json",
        "figure_id": "2407.13596v2_figure_2.png",
        "caption": "Figure 2: Left: Overall model architecture of the proposed EarthMarker. Right: Cross-domain Phased Training method.",
        "qtype": "Others",
        "response": "Chain-of-thought:  \n1. The red box ([mask1]) encloses the “Mixture of Visual Experts (MoV)” block, which (a) takes several down-sampled versions of the input RS image (multi-resolution) and (b) treats each visual prompt (boxes/points) as a pseudo-RGB image by replicating its single channel three times so it can share the same encoder.  \n2. In effect, MoV must simultaneously encode true image content at multiple scales and encode artificially manufactured “prompt images,” then stitch all of those features together.  \n3. That fusion requires repeated resizing (up- and down-sampling), channel replication, and channel-wise concatenation across very different feature maps.  \n4. Such operations can introduce several issues:  \n   • Computational and memory bloat from processing multiple resolutions of the same image plus extra prompt “images.”  \n   • Spatial misalignment between scales (e.g. small objects in the high-res branch don’t line up cleanly with features in the low-res branch).  \n   • Semantic dilution or confusion, since the model sees replicated prompt channels just like any other image patches and may not robustly distinguish them from actual content.  \n   • Potential loss of fine details (through repeated resizing) or interference between prompt features and genuine image features.  \n\nAnswer:  \nWhen you merge multi-resolution image streams and visual prompts inside a single Mixture-of-Visual-Experts module, you pay a high price in compute and memory, risk misaligning spatial features across scales, and can confuse or dilute the prompt signals (since they’ve been shoe-horned into the same RGB encoder), all of which can degrade fine-grained reasoning."
    },
    {
        "question": "What are the trade-offs of replacing the Feature extractor with a transformer-based alternative in BEV projection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Feature extractor",
            "BEV Projection"
        ],
        "id": 257,
        "masked_question": "What are the trade-offs of replacing the [mask1] with a transformer-based alternative in BEV projection?",
        "masked_number": 1,
        "masked_elements": [
            "Feature extractor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06851v1_figure_2.png",
        "paperid": "2411.06851v1",
        "paper_path": "./papers/2411.06851v1.json",
        "figure_id": "2411.06851v1_figure_2.png",
        "caption": "Figure 2: Diagram of the proposed architecture. First, the features of all the images are extracted for the whole input sequence. Each set of features of each instant is projected to the BEV using the information generated in the depth channels. For the past frames, it is necessary to apply a transformation that translates them to a unified system in the present frame. The generated BEV feature map is applied to two branches that generate the backward flow and segmentation values.",
        "qtype": "Others",
        "response": "The little red‐boxed module in Fig. 2 is nothing mystical – it is simply the EfficientNet–B4 backbone (shared across all cameras and time steps) that simultaneously emits a                                                  \nA)  set of “depth” logits (D=48 channels)  \nB)  a “feature” tensor (C=256 or 128, depending on full/tiny)  \nand then uses the L×S outer–product to do the classic Lift–Splat–Shoot projection into BEV.  \n\nIf you were to rip that out and swap in, say, a Vision-Transformer (ViT/Swin/PVT/SegFormer-style) backbone or even a full “transformer-based” BEV encoder (as in TBP-Former) the payoffs and penalties pan out roughly as follows:\n\n1.  Better global context, fewer “blind spots.”  \n    •  Transformers excel at very long-range dependencies, so you may see sharper instance boundaries, especially where two camera fringes overlap or when context far outside a single CNN receptive field helps resolve occlusions.  \n    •  Potentially smoother, more coherent depth embeddings (if you let the transformer learn its own “lift” rather than fixing a hand-designed depth‐bin outer product).\n\n2.  Heavier compute & memory footprint.  \n    •  Even the tiniest ViT or PVT runs 2–3× more FLOPs than an EfficientNet–B4 for the same H×W input (and usually needs substantial GPURAM for all those {Q,K,V} mat–muls).  \n    •  In practice you’d see both “parameter count” and “latency” creep up – often by tens of milliseconds – which in an on-vehicle, 2 Hz pipeline can be a hard sell.\n\n3.  Longer, more finicky training.  \n    •  Transformers typically require more epochs or very careful LR schedules and warm-ups.  \n    •  You often need pre-training on massive image collections (e.g. ImageNet-22k or specialized synthetic BEV data) before fine-tuning, whereas CNC backbones like EfficientNet happily train end-to-end from scratch or from standard ImageNet‐1k.\n\n4.  Loss of geometric inductive bias.  \n    •  The built-in “lift via depth-outer-product + known camera intrinsics/extrinsics” is mathematically exact, very fast, and gives you literal projective geometry.  \n    •  A pure attention-based BEV encoder throws away a lot of that bias; yes, it can learn to compensate, but you lose interpretability and you may need extra cues (positional embeddings, explicit extrinsic-aware tokens, etc.) to get back that level of spatial precision.\n\nIn short, a transformer-in-place-of-EfficientNet will almost certainly drive up accuracy in very complex scenes (occlusions, multi-camera fusion) but at a non-trivial cost in parameters, flops, memory, inference latency and training complexity. For an edge-constrained automotive application the CNN + depth-outer-product hack often remains the “sweet spot” between speed, simplicity and accuracy."
    },
    {
        "question": "What limitations arise from repeatedly applying ego warping and flow warp for long input sequences?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Ego warping",
            "Flow warp"
        ],
        "id": 258,
        "masked_question": "What limitations arise from repeatedly applying [mask1] and [mask2] for long input sequences?",
        "masked_number": 2,
        "masked_elements": [
            "Ego warping",
            "Flow warp"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06851v1_figure_2.png",
        "paperid": "2411.06851v1",
        "paper_path": "./papers/2411.06851v1.json",
        "figure_id": "2411.06851v1_figure_2.png",
        "caption": "Figure 2: Diagram of the proposed architecture. First, the features of all the images are extracted for the whole input sequence. Each set of features of each instant is projected to the BEV using the information generated in the depth channels. For the past frames, it is necessary to apply a transformation that translates them to a unified system in the present frame. The generated BEV feature map is applied to two branches that generate the backward flow and segmentation values.",
        "qtype": "Others",
        "response": "Here’s the reasoning step by step:\n\n1.  Identifying the masks  \n    - [mask1] is the “Ego-warping” or “present ego transformation” block (red box), which spatially reprojects past BEV feature maps into the current ego‐vehicle frame.  \n    - [mask2] is the “Flow-warp” step (blue box), which uses the predicted backward flow to propagate instance IDs from one BEV frame to the next.\n\n2.  What each does  \n    - Ego-warping takes each past BEV feature map and resamples it via an estimated rigid‐body transform (the change in ego pose) so that all features line up in the current frame.  \n    - Flow-warping takes the newly predicted per-pixel flow field and uses it to “pull” or “push” instance labels from the previous BEV into the current BEV, thereby carrying ID assignments forward (or backward).\n\n3.  Repeated application over many frames  \n    - For an input sequence of T frames you must do (T–1) ego-warps and (T–1) flow-warps in order to align every frame back to the reference (current) frame and propagate instance IDs through time.  \n\n4.  The limitations that arise  \n    a.  **Accumulation of geometric/interpolation error**  \n        • Each warp step involves discrete resampling (bilinear or nearest‐neighbor interpolation). Even tiny misalignments in ego pose or flow lead to small pixel offsets post-warp. Over many warps those offsets compound, drifting features/instances away from their true positions.  \n        • Repeated resampling also blurs high-frequency detail—edges get softer, small objects can “wash out.”  \n    b.  **Drift in instance association**  \n        • Small flow errors get carried forward and magnified, so an object’s ID may gradually slip or jump, leading to tracking failures over long horizons.  \n    c.  **Computational and memory overhead**  \n        • Each additional frame adds another warp operation (two per frame—one ego warp, one flow warp). As sequence length grows, so does the latency and GPU memory needed to store intermediate BEV maps.  \n\n5.  Summary answer  \n    “If you keep re-warping past BEV features (ego-warping) and re-applying the backward‐flow warping for every frame in a long input sequence, you begin to suffer two key issues: (1) interpolation-and-transform errors accumulate, causing feature misalignment, blurring, and ID–drift over time; and (2) the cost in compute and memory grows linearly with sequence length, which makes very long sequences prohibitively expensive to process.”"
    },
    {
        "question": "What motivates using shared-weight Feature extractor and BEV Projection for unified multi-camera BEV representation?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Usually, these end-to-end systems have relatively high processing times and number of parameters, which can be a challenge if they have to be deployed in a real vehicle.",
            "With this problem as the main focus, we propose a multi-camera BEV instance prediction architecture that uses the simplified paradigm presented in [6] and efficient attention modules specialized in dense tasks. The proposed architecture aims for fewer parameters and inference time than other SOTA architectures.",
            "All of the images are processed by a single EfficientNet-B4 [25] simultaneously, obtaining a feature map for each of them, in which there are C_d channels dedicated to depth information and C_f to the features of the environment itself.",
            "Each of the extracted features with C_f channels is transformed into a BEV representation using an outer product with the information of the depth channels D generated by EfficientNet, obtaining a unified map in BEV M with the features of all cameras for each instant."
        ],
        "final_answer": "By sharing the weights of a single EfficientNet-B4 across all cameras, the model greatly reduces its total parameter count and inference time—critical for on-vehicle deployment—while the BEV projection step (“Lift, Splat, Shoot”) fuses each camera’s features into one consistent bird’s-eye view. This unified BEV map preserves depth and spatial information from every camera stream, simplifying downstream instance segmentation and flow prediction.",
        "relevant_elements": [
            "Feature extractor",
            "BEV Projection"
        ],
        "id": 259,
        "masked_question": "What motivates using shared-weight [mask1] and [mask2] for unified multi-camera BEV representation?",
        "masked_number": 2,
        "masked_elements": [
            "Feature extractor",
            "BEV Projection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06851v1_figure_2.png",
        "paperid": "2411.06851v1",
        "paper_path": "./papers/2411.06851v1.json",
        "figure_id": "2411.06851v1_figure_2.png",
        "caption": "Figure 2: Diagram of the proposed architecture. First, the features of all the images are extracted for the whole input sequence. Each set of features of each instant is projected to the BEV using the information generated in the depth channels. For the past frames, it is necessary to apply a transformation that translates them to a unified system in the present frame. The generated BEV feature map is applied to two branches that generate the backward flow and segmentation values.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through the diagram and text step by step:\n\n1.  The red‐boxed module is the “Feature extractor” – in fact an EfficientNet-B4 – which is run on every camera image (and every past frame) but with one single shared set of weights.  \n2.  The blue‐boxed module is the “BEV projection” (the Lift–Splat step) that turns each feature map + its learned depth distribution into a little bird’s-eye‐view tile – again using one shared projection network for all cameras and time-steps.  \n\nWhy share both?  \n\n-  Consistency Across Views and Time: by forcing every camera (and past frame) through the exact same backbone and the exact same BEV-projection head, you guarantee that the network learns a truly view-invariant embedding.  \n-  Parameter and Compute Efficiency: you only pay once for an EfficientNet-B4 and one projection network, rather than instantiating separate ones per camera. That slashes both memory footprint and runtime.  \n-  Simplified Fusion: since all views produce feature maps of the same “format,” you can simply sum (or concatenate) them in BEV space without having to learn camera-specific adapters.  \n\nIn short, shared weights on the feature‐extractor (mask1) and on the BEV‐projection head (mask2) enforce a unified, compact, and consistent multi-camera BEV representation."
    },
    {
        "question": "Why adopt parallel SegFormer segmentation and flow branches instead of sequential decoders?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Once we have managed to unify all the information from the cameras in a feature map, two parallel branches are incorporated to process all the spatio-temporal information, generating the segmentation and flow values.",
            "Our proposed model seeks to alleviate the computational impact introduced by the two branches, therefore we decide to implement an architecture based on SegFormer [24] that efficiently uses attention to process the multi-scale features."
        ],
        "final_answer": "They use two parallel SegFormer‐based branches—one for segmentation and one for flow—so that both tasks can be computed simultaneously rather than in sequence. By replacing two costly U-Net decoders with efficient attention modules from SegFormer, the model reduces its computational burden and number of parameters while still capturing rich multi-scale features for both segmentation and flow.",
        "relevant_elements": [
            "SegFormer",
            "segmentation branch",
            "flow branch"
        ],
        "id": 260,
        "masked_question": "Why adopt parallel [mask1] segmentation and flow branches instead of sequential decoders?",
        "masked_number": 1,
        "masked_elements": [
            "SegFormer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.06851v1_figure_2.png",
        "paperid": "2411.06851v1",
        "paper_path": "./papers/2411.06851v1.json",
        "figure_id": "2411.06851v1_figure_2.png",
        "caption": "Figure 2: Diagram of the proposed architecture. First, the features of all the images are extracted for the whole input sequence. Each set of features of each instant is projected to the BEV using the information generated in the depth channels. For the past frames, it is necessary to apply a transformation that translates them to a unified system in the present frame. The generated BEV feature map is applied to two branches that generate the backward flow and segmentation values.",
        "qtype": "Design_Rationale",
        "response": "The big red‐boxed modules are simply two SegFormer‐style decoders sitting side-by-side, one predicting mask (instance segmentation) maps and the other predicting backward flow maps.  In other words, rather than “stacking” one decoder on top of the other (i.e. run a segmentation decoder, take its output, then feed that into a flow decoder), they fork the shared BEV feature tensor into two *parallel* SegFormer heads.  \n\nWhy?  \n\n1.  Task decoupling and specialization  \n    – Segmentation and flow are related but distinct tasks: segmentation wants per‐pixel class/instance labels, flow wants per‐pixel displacement vectors.  By giving each task its own SegFormer decoder, each head can learn its own feature priorities (e.g. sharp boundaries vs. smooth motion fields) without having to untangle them in a single, monolithic decoder.  \n\n2.  Avoid error propagation  \n    – In a sequential setup, any mistakes in the segmentation stage would feed directly into the flow stage, compounding errors.  Parallel heads each get the *same* clean BEV feature map and produce their outputs independently, so one head’s hiccups don’t derail the other.  \n\n3.  Computational and latency savings  \n    – Stacking two decoders end-to-end would effectively double the decoding workload and increase inference time.  Running two lightweight SegFormer decoders in parallel lets you share heavy compute in the encoder and only pay once for the multi-scale feature extraction.  \n\n4.  Cleaner multi-task gradients  \n    – Parallel heads let each task back-propagate its own loss directly into the shared encoder, rather than having to “wait” for the upstream decoder to finish.  This tends to stabilize training and helps each task steer the encoder features in the right directions.  \n\nIn short: parallel SegFormer branches give you two task-specialized decoders that both tap into the same rich, multi‐scale BEV features, avoid compounding errors, and run more efficiently than a long sequential decoder chain."
    },
    {
        "question": "What benefits justify using Omni-Lens-Field to represent PSF Library for Controllable DoF Imaging across multiple lenses?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "To address the huge storage space consumption of the original 4D PSF Library (PSFLib), we introduce the storage-efficient Omni-Lens-Field model to accurately characterize the 4D PSFLib for various known lens parameters while occupying low storage consumption.",
            "However, ray tracing is computationally expensive, especially when frequently adjusting the object distance.",
            "Furthermore, the calculated PSFLib requires large storage space.",
            "To address these challenges, we train the Omni-Lens-Field model to represent the 4D PSFLib for a variety of lenses."
        ],
        "final_answer": "Using Omni-Lens-Field avoids the repeated, expensive ray tracing required for each lens and dramatically reduces the storage footprint of the 4D PSF library by learning a single, compact model that can generate PSFs for multiple lenses.",
        "relevant_elements": [
            "Omni-Lens-Field",
            "Controllable DoF Imaging"
        ],
        "id": 262,
        "masked_question": "What benefits justify using [mask1] to represent PSF Library for Controllable DoF Imaging across multiple lenses?",
        "masked_number": 1,
        "masked_elements": [
            "Omni-Lens-Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.09754v1_figure_2.png",
        "paperid": "2409.09754v1",
        "paper_path": "./papers/2409.09754v1.json",
        "figure_id": "2409.09754v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed Depth-aware Controllable DoF Imaging (DCDI) framework. (a) We simulate the Depth-aware Aberration MOS (DAMOS) dataset. The Depth-aware 4D PSFLib is constructed by performing multiple ray tracing simulations, varying the position of the object plane from near to far. (b) The MDE model predicts scene depth map. (c) The latent AiF image is recovered by jointly combining synthetic data, predicted depth map, and depth-aware network architecture. (d) Controllable DoF imaging of multiple lenses is achieved through predicted depth maps, restored AiF images, and depth-aware PSF maps predicted by Omni-Lens-Field.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is the “Omni-Lens-Field” module that replaces a brute-force 4D PSF look-up table with a small neural network.  There are three things this buys us when we want to do controllable DoF with many different lenses:\n\n 1. Huge storage savings.  A naïve PSF library would have to store an RGB PSF for every spatial patch × every depth plane × every lens – easily tens or hundreds of gigabytes of data.  The Omni-Lens-Field encodes all of that in a few megabytes of network weights.  \n 2. Instant, on-the-fly PSF queries.  Rather than re–ray‐tracing or loading huge PSF cubes for each new lens or scene depth, we just feed in (lens ID, x–y patch coords, depth) to the network and get back the correct PSF immediately.  \n 3. One model for all lenses.  Because it learns the mapping from lens parameters to PSF, we don’t have to rebuild or retrain a separate PSF database each time we swap in a different optics design.  \n\nTogether these advantages make it practical to generate accurate, depth‐aware PSF maps in real time for arbitrary lenses, which is exactly what controllable DoF imaging needs."
    },
    {
        "question": "How does Depth-aware Image Simulation select PSFs from the Depth-aware 4D PSF Library for each image patch?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Different from [38, 59], which neglect the imaging scene depth in the simulation process, we take the scene depth into account in our pipeline.",
            "The depth-aware aberration image is approximately generated by\n$where \\ast$ denotes the convolution operation, $I$ and $I^*$ denote ground truth and aberration image respectively, $\\mathrm{search}$ denotes the searching function that queries the PSFLib and outputs the corresponding PSF of every patch with different scene depths. The depth value of each patch is the average depth value for all pixels within that patch."
        ],
        "final_answer": "For each patch, we first compute its average depth from the ground-truth depth map, then use a ‘search’ function to look up and retrieve the PSF in the Depth-aware 4D PSF Library corresponding to that patch’s average depth, and finally convolve the patch with this depth-matched PSF.",
        "relevant_elements": [
            "Depth-aware Image Simulation",
            "Depth-aware 4D PSF Library"
        ],
        "id": 263,
        "masked_question": "How does [mask1] select PSFs from the [mask2] for each image patch?",
        "masked_number": 2,
        "masked_elements": [
            "Depth-aware Image Simulation",
            "Depth-aware 4D PSF Library"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.09754v1_figure_2.png",
        "paperid": "2409.09754v1",
        "paper_path": "./papers/2409.09754v1.json",
        "figure_id": "2409.09754v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed Depth-aware Controllable DoF Imaging (DCDI) framework. (a) We simulate the Depth-aware Aberration MOS (DAMOS) dataset. The Depth-aware 4D PSFLib is constructed by performing multiple ray tracing simulations, varying the position of the object plane from near to far. (b) The MDE model predicts scene depth map. (c) The latent AiF image is recovered by jointly combining synthetic data, predicted depth map, and depth-aware network architecture. (d) Controllable DoF imaging of multiple lenses is achieved through predicted depth maps, restored AiF images, and depth-aware PSF maps predicted by Omni-Lens-Field.",
        "qtype": "Implementation_Details",
        "response": "The “red‐boxed” depth‐aware image simulator simply looks up, for each small patch, the one PSF in the 4D library whose depth (and spatial location) matches that patch’s mean depth.  In practice we\n\n  1. tile the ground‐truth image into, say, 64×64 pixel patches,  \n  2. compute the average scene‐depth over each patch,  \n  3. feed that average depth (and the patch’s coordinates) into a little “search” function 𝑞,  \n  4. have 𝑞 interpolate/rotate/resize the pre‐computed 4D PSFLib (the blue box) at exactly that depth, and  \n  5. convolve the patch with the resulting RGB PSF.  \n\nIn this way the red‐boxed module automatically selects the correct depth‐varying PSF from the blue‐boxed PSFLib for every patch in the image."
    },
    {
        "question": "How does Omni-Lens-Field map lens parameters and depth map to generate the depth-aware PSF map?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Different from [65, 66], where a network is used to characterize a specific lens, we employ the Omni-Lens-Field model to represent the 4D PSFLib of multiple lenses. As illustrated in Fig. 4, we train the network T to fit the PSFs of multiple imaging lenses, by minimizing the discrepancy between the estimated PSF and the ray-traced PSF. Following [66], L2 Loss is applied during the fitting process, formulated as:\n\n    L_{PSF} = ||T(lens\\_id, x, y, z) - PSF_{raytraced}||_2^2\n\n    where lens_id denotes the lens ID, and x, y, z are the normalized patch coordinates and scene depth.",
            "To fit the mapping of the four-parameter input into an RGB PSF, the Omni-Lens-Field adopts an MLP network. It consists of one input layer with 4 channels, n hidden layers, and three independent output layers, each predicting one of the R, G, B PSF channels.",
            "The depth-aware PSF map of any lens can be predicted by Omni-Lens-Field according to the corresponding depth map and can be expressed as follows:\n\n    PSF_map = T(lens\\_id, x, y, depth_map)"
        ],
        "final_answer": "Omni-Lens-Field is a small MLP that takes as input four values—lens identifier, normalized patch position (x, y), and scene depth—and outputs the corresponding RGB point-spread function. At inference time, for each patch in the image it looks up its (x,y) location and depth from the estimated depth map, feeds these plus the chosen lens ID into the network, and collects the predicted PSFs into a full depth-aware PSF map.",
        "relevant_elements": [
            "Omni-Lens-Field",
            "PSF map"
        ],
        "id": 264,
        "masked_question": "How does [mask1] map lens parameters and depth map to generate the depth-aware PSF map?",
        "masked_number": 1,
        "masked_elements": [
            "Omni-Lens-Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.09754v1_figure_2.png",
        "paperid": "2409.09754v1",
        "paper_path": "./papers/2409.09754v1.json",
        "figure_id": "2409.09754v1_figure_2.png",
        "caption": "Figure 2: Overview of the proposed Depth-aware Controllable DoF Imaging (DCDI) framework. (a) We simulate the Depth-aware Aberration MOS (DAMOS) dataset. The Depth-aware 4D PSFLib is constructed by performing multiple ray tracing simulations, varying the position of the object plane from near to far. (b) The MDE model predicts scene depth map. (c) The latent AiF image is recovered by jointly combining synthetic data, predicted depth map, and depth-aware network architecture. (d) Controllable DoF imaging of multiple lenses is achieved through predicted depth maps, restored AiF images, and depth-aware PSF maps predicted by Omni-Lens-Field.",
        "qtype": "Implementation_Details",
        "response": "First, recall that the red‐boxed module in (d) is the “Omni‐Lens‐Field” (OLF), our learned 4D PSF emulator.  In Sec. III-C we explain that instead of expensive ray-tracing each time, we train a small MLP  \nfϕ(l, u, v, d) → PSF₍R,G,B₎  \nwhere  \n • l is the lens ID (i.e. which optical design you want),  \n • (u,v) are the normalized patch coordinates on the image plane,  \n • d is the (normalized) scene‐depth at that patch.  \n\nAt training time we supervise fϕ to match ray-traced PSFs via an ℓ₂ loss.  At inference, we take the recovered depth map, break it into patches, average the depth within each patch, and then call  \nPSF_map[u,v] = fϕ(lensID, u, v, D̄(u,v))  \nto fill in the RGB PSF for every patch.  The result is a full, depth‐aware PSF map for that lens, ready to be convolved with the all-in-focus image to yield controllable DoF."
    },
    {
        "question": "How does the acquisition function utilize the multi-task Gaussian process covariance to select hyperparameter queries?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Multi-task Bayesian optimization leverages the commonality between tasks to facilitate time-saving learning [7, 8]. The expensive-to-evaluate function is represented by a cheap multi-task Gaussian surrogate model updated by feeding new observations. To lighten the burden of acquiring new observations, the inter-task dependence guides selection of promising points. This dependence is learned by a positive semi-definite covariance matrix over tasks. It sped up the search for hyperparameters.",
            "Then the predictive mean and covariance of the new point x on the t task can be inferred as …",
            "To find the minimum value of f, it is usually to maximize an acquisition function α to determine the next query point x. We use the expected improvement (EI) criterion, let ŷ be the currently observed minimum, I(x,t)=max(ŷ–f(x,t),0). Since f(x,t) follows Gaussian distribution, the expectation can be computed analytically as follows … We can choose the next query point by optimizing the acquisition function corresponding to all tasks {x*,t*}=argmax EI(x,t)."
        ],
        "final_answer": "The acquisition function (expected improvement) uses the posterior predictive mean and variance provided by the multi-task Gaussian process—which encodes inter-task similarity via a positive semi-definite task covariance matrix—to compute, for each candidate hyperparameter and task, the expected reduction below the current best loss. It then selects the next hyperparameter (and task) to evaluate by maximizing this expected improvement across all tasks.",
        "relevant_elements": [
            "acquisition function",
            "Multi-task Gaussian process"
        ],
        "id": 265,
        "masked_question": "How does the [mask1] utilize the multi-task Gaussian process covariance to select hyperparameter queries?",
        "masked_number": 1,
        "masked_elements": [
            "acquisition function"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06184v1_figure_1.png",
        "paperid": "2411.06184v1",
        "paper_path": "./papers/2411.06184v1.json",
        "figure_id": "2411.06184v1_figure_1.png",
        "caption": "Figure 1: Workflow of the proposed MTBO based SVMs hyperparameter-tuning method on pulmonary nodule classification. The collected whole body CT scans are first preprocessed to delineate the pulmonary nodules. Then, the segmented nodules are discretized via multiple strategies. After that, for the result of each discretization strategy, various types of features are extracted as the input to a specific SVM. Finally, a multi-task Gaussian process is established through the correlation between tasks, the query point is selected by maximizing the acquisition function, and the optimal hyperparameters are determined by iterative query.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed block in the figure is exactly the step in which the method “selects the next query (hyperparameters of the selected SVM) by maximizing the acquisition function.”  In our case that acquisition is the Expected Improvement (EI) computed under a multi-task Gaussian process whose kernel is the Kronecker product of:\n\n  • A Matérn 5/2 spatial kernel k(x,x′) over hyperparameter inputs  \n  • A positive semidefinite task–covariance matrix Ω that encodes how strongly each SVM’s performance is correlated with the others.  \n\nHere is how the Ω matrix comes into play when choosing the next hyperparameter jump:  \n\n1.  After t rounds we have observations of cross-validation loss for each of the M tasks (SVMs) at the points Xt={x1, …, xt}.  \n2.  We fit a multi-task GP, which has the joint prior  \n       f∼GP(0, Ω⊗k(·,·))  \n   so that when we condition on the data we get a joint posterior mean μ_*(x) and covariance Σ_*(x) for all M tasks at any candidate x.  The off-diagonal blocks in Σ_*(x) come from Ω, so information from one task shrinks uncertainty in its neighbors.  \n3.  To drive our next evaluation we compute, for the currently “best” loss seen so far y^+, the expected improvement  \n       EI_m(x)=E[max(y^+−f_m(x),0)]  \n   for each task m, where the expectation and variance in that formula are read off μ_*(x) and the mth diagonal entry of Σ_*(x).  \n4.  Finally, the red box’s “select next query” is simply  \n       x_{t+1}=arg max_x EI_m(x)  \n   (either optimizing across all M tasks jointly or focusing on one at a time).  Because Σ_*(x) carries the inter-task covariances from Ω, choosing x_{t+1} automatically trades off exploration and exploitation across the entire set of correlated SVM tasks rather than one in isolation."
    },
    {
        "question": "How does the radiomic features extraction module integrate GLCM and GLRLM metrics into unified input vectors?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Radiomic Features Extraction",
            "GLCM",
            "GLRLM"
        ],
        "id": 266,
        "masked_question": "How does the [mask1] module integrate GLCM and GLRLM metrics into unified input vectors?",
        "masked_number": 1,
        "masked_elements": [
            "Radiomic Features Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06184v1_figure_1.png",
        "paperid": "2411.06184v1",
        "paper_path": "./papers/2411.06184v1.json",
        "figure_id": "2411.06184v1_figure_1.png",
        "caption": "Figure 1: Workflow of the proposed MTBO based SVMs hyperparameter-tuning method on pulmonary nodule classification. The collected whole body CT scans are first preprocessed to delineate the pulmonary nodules. Then, the segmented nodules are discretized via multiple strategies. After that, for the result of each discretization strategy, various types of features are extracted as the input to a specific SVM. Finally, a multi-task Gaussian process is established through the correlation between tasks, the query point is selected by maximizing the acquisition function, and the optimal hyperparameters are determined by iterative query.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is simply the “Radiomic Feature Extraction” block (step 3 in Fig. 1).  What it does is:\n\n1.  From the discretized nodule volume it builds  \n    – a gray‐level co‐occurrence matrix (GLCM) and  \n    – a gray‐level run‐length matrix (GLRLM).\n\n2.  It then computes 23 summary statistics from the GLCM and 12 from the GLRLM (e.g. contrast, entropy, run‐length non‐uniformity, etc.).\n\n3.  Finally, those 35 second‐order features are concatenated with the 8 shape descriptors and the 13 first‐order (intensity) statistics to form a single, fixed‐length feature vector (8 + 13 + 23 + 12 = 56 dimensions) for each discretization strategy.  \n\nThat 56-dimensional vector is the unified input to the corresponding SVM."
    },
    {
        "question": "How does U-net segmentation integrate with active contour refinement compared to previous segmentation pipelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Trained U-net",
            "active contour refinement"
        ],
        "id": 267,
        "masked_question": "How does [mask1] segmentation integrate with active contour refinement compared to previous segmentation pipelines?",
        "masked_number": 1,
        "masked_elements": [
            "Trained U-net"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06184v1_figure_1.png",
        "paperid": "2411.06184v1",
        "paper_path": "./papers/2411.06184v1.json",
        "figure_id": "2411.06184v1_figure_1.png",
        "caption": "Figure 1: Workflow of the proposed MTBO based SVMs hyperparameter-tuning method on pulmonary nodule classification. The collected whole body CT scans are first preprocessed to delineate the pulmonary nodules. Then, the segmented nodules are discretized via multiple strategies. After that, for the result of each discretization strategy, various types of features are extracted as the input to a specific SVM. Finally, a multi-task Gaussian process is established through the correlation between tasks, the query point is selected by maximizing the acquisition function, and the optimal hyperparameters are determined by iterative query.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the pre‐trained U-net block at the very top of the preprocessing pipeline.  In our scheme we first run a lung‐parenchyma U-net to get a coarse mask, then resample and hand that mask to a classical active-contour (plus morphology) refinement step for the nodule.  In older pipelines all of the nodule work – even the rough outline – came from thresholding, region growing or pure active-contour runs (often with manual initialization) and then a radiologist would tinker with it by hand.  By contrast, we now use the U-net to do the heavy lifting of separating lung from non‐lung tissue (and to free us from having to choose seeds for region growing), and only the finer nodule boundary comes from active contours and morphological cleanup.  That hybrid (U-net → active contour → radiologist check) both speeds up the workflow and makes the initial contours far more stable than the fully classical approaches."
    },
    {
        "question": "How do multiple discretization strategies affect surrogate modeling in multi-task Bayesian optimization versus single-task?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "In this study, we designed multiple discretization strategies, that is, different combinations on θ = (nBins, qRange, qRef). We chose three bin numbers (16, 32, and 64) and three quantization ranges (min-max, mean ± 2SD, and mean ± 3SD). Nine discretization strategies were generated in total. Instead of evaluating one by one, we adopted the multi-task Bayesian optimization to evaluate them simultaneously to save computational costs.",
            "Multi-task Bayesian optimization leverages the commonality between tasks to facilitate time-saving learning. The expensive-to-evaluate function is represented by a cheap multi-task Gaussian surrogate model updated by feeding new observations. To lighten the burden of acquiring new observations, the inter-task dependence guides selection of promising points. This dependence is learned by a positive semi-definite covariance matrix over tasks."
        ],
        "final_answer": "When one generates multiple discretization strategies, each strategy creates a related but distinct evaluation task. Under single-task Bayesian optimization, each strategy (task) would require its own Gaussian-process surrogate and separate optimization. In contrast, multi-task Bayesian optimization builds a single joint (multi-task) Gaussian-process surrogate that includes a covariance matrix capturing inter-task similarities. By sharing information across the nine discretization-strategy tasks via that covariance, the multi-task surrogate converges faster and avoids redundant searches compared to running nine independent (single-task) surrogates.",
        "relevant_elements": [
            "Multiple Image Discretization Strategies",
            "Multi-task Bayesian Optimization Based Hyperparameter Search for SVMs"
        ],
        "id": 268,
        "masked_question": "How do [mask1] affect surrogate modeling in [mask2] versus single-task?",
        "masked_number": 2,
        "masked_elements": [
            "Multiple Image Discretization Strategies",
            "Multi-task Bayesian Optimization Based Hyperparameter Search for SVMs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.06184v1_figure_1.png",
        "paperid": "2411.06184v1",
        "paper_path": "./papers/2411.06184v1.json",
        "figure_id": "2411.06184v1_figure_1.png",
        "caption": "Figure 1: Workflow of the proposed MTBO based SVMs hyperparameter-tuning method on pulmonary nodule classification. The collected whole body CT scans are first preprocessed to delineate the pulmonary nodules. Then, the segmented nodules are discretized via multiple strategies. After that, for the result of each discretization strategy, various types of features are extracted as the input to a specific SVM. Finally, a multi-task Gaussian process is established through the correlation between tasks, the query point is selected by maximizing the acquisition function, and the optimal hyperparameters are determined by iterative query.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify what the two masks refer to:\n\n1. [mask1] (red box) is “the multiple image discretization strategies” – i.e. the nine different ways we bin voxel intensities.  \n2. [mask2] (blue box) is “the multi-task Bayesian optimization” phase – where we use a multi-task Gaussian process as our surrogate model to tune the SVM hyperparameters jointly across those nine discretizations.\n\nNow:  \n“How do the multiple image discretization strategies affect surrogate modeling in multi-task Bayesian optimization versus single-task?”\n\nChain-of-thought:\n\n1. In our pipeline each discretization strategy produces its own dataset and hence its own SVM-tuning task.  \n2. In a **single-task** BO setting you would build one Gaussian-process surrogate per discretization (per task), optimize each in isolation, and never share information between them.  \n3. In **multi-task** BO you instead stack all nine tasks into one joint Gaussian process with a covariance of the form  \n   K((x, i),(x′, j)) = K_task(i,j) · K_point(x,x′).  \n   Here K_task encodes how “close” two discretizations are.  \n4. Because different discretizations are correlated (they all discretize the same nodules, just with different bin counts/ranges), the multi-task surrogate borrows strength: a good evaluation on one discretization informs the others via the off-diagonal entries of K_task.  \n5. The result is that the **multi-task surrogate converges faster** and needs fewer expensive SVM evaluations, whereas in single-task you pay the full cost per discretization with no cross-task transfer.\n\nIn short, the nine discretization strategies become nine related tasks.  In a multi-task GP surrogate those tasks talk to one another through a learned task–covariance matrix, dramatically improving sample efficiency over treating each discretization in isolation with its own single-task GP."
    },
    {
        "question": "How does Pooling complement Concept Heatmap extraction compared to standard activation mapping?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "As average pooling performs better in downstream medical classification tasks yan2023robust, we apply average pooling to the heatmaps to deduce the connection between the image and concepts: s = avgpool(h) ∈ R. Intuitively, s is the refined similarity score between the image and concept c. Thus, a concept vector c can be obtained, representing the similarity between an image input x and a set of concepts."
        ],
        "final_answer": "Instead of leaving the concept heatmap as a purely spatial activation map, Med-MICN applies average pooling over each heatmap to collapse the pixel-wise similarity scores into a single “concept score.” This pooling step produces a compact, refined similarity measure for each concept—directly comparable across images and amenable to thresholding for pseudo-labeling—whereas a standard activation map on its own only highlights regions of interest without yielding a concise, globally comparable concept presence score.",
        "relevant_elements": [
            "Pooling",
            "Concept Heatmap"
        ],
        "id": 269,
        "masked_question": "How does [mask1] complement [mask2] extraction compared to standard activation mapping?",
        "masked_number": 2,
        "masked_elements": [
            "Pooling",
            "Concept Heatmap"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.21494v1_figure_1.png",
        "paperid": "2410.21494v1",
        "paper_path": "./papers/2410.21494v1.json",
        "figure_id": "2410.21494v1_figure_1.png",
        "caption": "Figure 1: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.",
        "qtype": "Literature_Background",
        "response": "First, let’s restate what the two boxed regions are doing:\n\n  • [mask2] (the blue‐boxed “Concept Heatmap” block) is where, for each concept in our GPT–4V–generated dictionary, we compute a spatial map over the H×W grid by taking the cosine similarity between the D–dim feature at each location and the D–dim text embedding of that concept.  This gives us a 2D “heatmap” of where in the image that particular concept may live.  \n\n  • [mask1] (the red‐boxed “Concept Score Vector” / “Pooling” step) then takes each of those H×W heatmaps and simply average-pools them down to a single scalar.  That scalar is our global “similarity score” for each concept in the image, which we can threshold to decide whether that concept is truly present.\n\nChain-of-thought on how this differs from “standard activation mapping” (e.g. Grad‐CAM):\n\n1. Standard activation maps:  \n   – You pick one class (say “COVID”), backprop its gradient through the CNN, weight the intermediate feature–maps by those gradients, sum them up, maybe ReLU, and you get a single saliency map that tells you “where in the image the model is looking to decide COVID vs. non–COVID.”  \n   – You do not get any explicit per-concept score or a disentangled map for “ground‐glass opacities,” “bilateral involvement,” etc.  \n\n2. Concept Heatmaps (mask2):  \n   – Because we have a bank of text embeddings—one per medically meaningful concept—we get one heatmap per concept, directly by comparing local image features to that concept’s text vector.  \n   – We therefore have fine-grained, multi-heatmap localization of *all* the concepts at once, without any gradient tricks.  \n\n3. Pooling → Concept Score Vector (mask1):  \n   – We then collapse each H×W map to a single number (by average‐pooling).  Each of those scalars is interpretable as “how strongly that concept appears anywhere in the image.”  \n   – That gives us a ready-made, global presence score for each concept, which we can threshold into a pseudo-label.  \n\nWhy is that complementary and (in practice) preferable?\n  – You get explicit, per-concept quantitative scores, not just a class-level saliency.  \n  – The spatial maps (mask2) tell you *where* to look; the pooled scores (mask1) tell you *how much* of each concept there is.  \n  – No reliance on backprop gradients or heuristics for patching together a saliency map—you simply use the same VLM embedding space twice (once for localization, once for scoring).  \n\nIn short, the red‐boxed pooling step turns the multi–concept spatial evidence from the blue‐boxed heatmaps into crisp, ordered scores, giving you both “where” and “how much” for each concept, something that a vanilla Grad-CAM style map does not provide."
    },
    {
        "question": "How does Text Encoder refine GPT-4V outputs compared to prior concept embedding pipelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GPT-4V",
            "Text Encoder"
        ],
        "id": 270,
        "masked_question": "How does [mask1] refine [mask2] outputs compared to prior concept embedding pipelines?",
        "masked_number": 2,
        "masked_elements": [
            "Text Encoder",
            "GPT-4V"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.21494v1_figure_1.png",
        "paperid": "2410.21494v1",
        "paper_path": "./papers/2410.21494v1.json",
        "figure_id": "2410.21494v1_figure_1.png",
        "caption": "Figure 1: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.",
        "qtype": "Literature_Background",
        "response": "Here is the key intuition.  In our figure the blue‐boxed module is GPT-4V, which spits out a *set* of putative medical concepts (e.g. “bilateral involvement,” “crazy-paving pattern,” etc.).  The red-boxed module is a **pretrained text encoder** (in our case essentially the CLIP‐style or language‐model text encoder) that takes each of those raw concept names or phrases and turns them into a fixed, dense embedding vector.  \n\nWhy is that *refinement* step important, and what exactly does it buy you over the “old” concept-embedding recipes?  \n\n  1.  **Semantic grounding.**  Prior works often either learned concept embeddings entirely from scratch on the image features (or even randomly initialized them), which meant the learned vectors could drift away from any human‐readable meaning.  By contrast, by running all of your concept labels through a *frozen* text encoder you guarantee that each concept vector sits in a space where distances actually reflect language‐model semantics.  That immediately cuts down on noisy, overlapping concept detectors.  \n  2.  **Sharper, more discriminative detectors.**  Once you have CLIP/LM-backed vectors for each concept, you can compute cosine similarities against image‐feature patches and get crisp heatmaps.  In earlier pipelines without this text step, those heatmaps were much blurrier – the network had to invent its own “meaning” for each concept purely from vision data.  \n  3.  **Easier filtering and thresholding.**  When your concepts live in a well-structured embedding space, it becomes trivial to discard any concept whose average similarity across your training set never rises above a sanity threshold (in our case 0.45).  Previous pipelines had no principled way to prune out bad or spurious concepts.  \n  4.  **Plug-and-play interpretability.**  Because the text encoder is off-the-shelf and frozen, any doctor can look up the *exact* phrase we fed it (“alveolar opacity,” “crazy-paving”) and know precisely which point in embedding space we’re talking about.  In earlier end-to-end systems the concept vectors themselves had no direct linguistic label attached.  \n\nIn short, the red-boxed text encoder *refines* the raw, noisy GPT-4V concept proposals by mapping them into a semantically rich, pretrained embedding space.  That yields far cleaner concept heatmaps, more robust pseudo-labels, easier concept pruning, and ultimately much more faithful and accurate downstream concept embeddings than pipelines that learn everything from scratch in the vision model alone."
    },
    {
        "question": "How does average pooling influence the concept score vector’s capacity to distinguish relevant disease concepts?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "As average pooling performs better in downstream medical classification tasks yan2023robust, we apply average pooling to the heatmaps to deduce the connection between the image and concepts: s_i = pool(M_i).",
            "Intuitively, s_i is the refined similarity score between the image and concept C_i. Thus, a concept vector s can be obtained, representing the similarity between an image input x and a set of concepts C: s = {s_i}_{i=1}^k."
        ],
        "final_answer": "By applying average pooling over each concept’s similarity heatmap, Med-MICN collapses spatially distributed evidence into a single, refined similarity score per concept. This pooling operation smooths out local noise and captures the overall strength of association between the image and each disease concept—thereby yielding a concept score vector whose entries more cleanly and distinctly reflect which concepts are genuinely relevant.",
        "relevant_elements": [
            "Pooling",
            "Concept Score Vector"
        ],
        "id": 271,
        "masked_question": "How does average [mask1] influence the concept score vector’s capacity to distinguish relevant disease concepts?",
        "masked_number": 1,
        "masked_elements": [
            "Pooling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.21494v1_figure_1.png",
        "paperid": "2410.21494v1",
        "paper_path": "./papers/2410.21494v1.json",
        "figure_id": "2410.21494v1_figure_1.png",
        "caption": "Figure 1: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.",
        "qtype": "Experimental_Results",
        "response": "Here is a step‐by‐step reasoning:\n\n1. In the VLM-based alignment module (Figure 1(b)), for each candidate concept we first compute an H×W “heatmap” of local cosine‐similarities between that concept’s text embedding and each spatial location in the image feature map.\n\n2. That heatmap H_i contains both true positives (areas where the concept genuinely appears) and spurious high‐similarity noise (artifacts, isolated pixels, etc.).\n\n3. To turn this 2D map into a single scalar score per concept, the network applies average pooling (the operation in the red box).  Concretely:\n   s_i  =  (1/(H·W)) · Σ_{x,y} H_i(x,y)\n\n4. By averaging over all H×W locations, this “average pooling” smooths out random spikes and emphasizes concepts that have consistent, widespread activation in the image.\n\n5. The resulting vector s = [s_1, s_2, …, s_k] is the concept score vector that goes on to thresholding.  Because average pooling suppresses isolated noise and highlights true concept presence, it makes the scores far more separable—i.e. concepts that are really present will have higher s_i, and absent concepts will stay low.\n\n6. Empirically, prior work in medical tasks (yan2023robust) also shows that this global summarization step gives better downstream classification performance than, say, max‐pooling or no pooling.\n\nAnswer  \nThe average pooling of each concept’s heatmap turns a noisy H×W similarity map into a single “refined” similarity score.  By smoothing out isolated activations and capturing the overall strength of a concept across the image, it makes the concept score vector much more discriminative—relevant disease concepts end up with high average scores, irrelevant ones stay low—thus easing the separation at the subsequent thresholding stage."
    },
    {
        "question": "How does threshold filtering of the concept score vector refine concept label accuracy?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "To align images with concept labels, we determine the presence of a concept attribute in an image based on a threshold value derived from an experiment. If the value s_j exceeds this threshold, we consider the image to possess that specific concept attribute and set the concept label to be True.",
            "Finally, to ensure the truthfulness of concepts, we discard all concepts for which the similarity across all images is below 0.45."
        ],
        "final_answer": "By applying a threshold to the pooled concept‐image similarity scores, only concepts whose score exceeds the threshold are marked as present for a given image, eliminating spurious low‐score detections. Additionally, any concept whose average similarity across all images falls below 0.45 is removed entirely. This two‐stage filtering—per‐image thresholding and global concept pruning—reduces noise and ensures that only high‐confidence concept labels are retained, thereby refining overall label accuracy.",
        "relevant_elements": [
            "Threshold",
            "Concept Label"
        ],
        "id": 272,
        "masked_question": "How does [mask1] filtering of the concept score vector refine [mask2] accuracy?",
        "masked_number": 2,
        "masked_elements": [
            "Threshold",
            "Concept Label"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.21494v1_figure_1.png",
        "paperid": "2410.21494v1",
        "paper_path": "./papers/2410.21494v1.json",
        "figure_id": "2410.21494v1_figure_1.png",
        "caption": "Figure 1: (a) module, output rich dimensional interpretable conceptual information for the specified disease through the multimodal model and convert the conceptual information into text vectors through the text embedding module; (b) module, access the image to the image embedder to get the image features, and then match with the conceptual textual information to get the relevant attention region. Then, we get the influence score of the relevant region information through pooling, and finally send it to the filter to sieve out the concept information with weak relevance to get the disease concept of image information.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is simply the threshold operation shown in the red box: it takes the raw concept‐similarity scores (the “concept score vector”) and zeroes out—or altogether discards—any entry below a predefined cutoff. Mask2 is the final binary “concept label” vector in the blue box. By only promoting those similarity scores that exceed the threshold into a “1” and forcing everything else to “0,” we automatically drop low‐confidence (and often spurious) concept activations. That pruning step cuts down false positives in the label vector and so yields much more accurate per‐concept labels for each image."
    },
    {
        "question": "How does MLM task refine structure-sequence embeddings within pre-trained SSLM?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Next, using the 3Di alphabet as the vocabulary of structural elements and based on the Transformer architecture, we pre-train a protein structural language model, SSLM, from scratch. This aims to effectively model “structure-sequences” of proteins.",
            "The pre-training process employs the classic masked language modeling (MLM) objective, predicting masked elements based on the context of the “structure-sequence”. The probability distribution for predicting a masked element yi is used, where yi is the masked structural element and … are its contexts. The loss function is defined as follows: … The loss is computed only on elements that are masked."
        ],
        "final_answer": "Within SSLM, the MLM task randomly masks out tokens in the discretized structure-sequence (the 3Di alphabet) and trains the Transformer model to predict these masked tokens from their surrounding context. By minimizing the cross-entropy loss on only the masked positions, the self-attention layers learn to capture the sequential and spatial dependencies among structural elements, thereby refining the embeddings of the structure-sequence tokens.",
        "relevant_elements": [
            "MLM task",
            "structure-sequence",
            "pre-trained SSLM"
        ],
        "id": 273,
        "masked_question": "How does [mask1] refine [mask2] embeddings within pre-trained SSLM?",
        "masked_number": 2,
        "masked_elements": [
            "MLM task",
            "structure-sequence"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.15592v2_figure_1.png",
        "paperid": "2410.15592v2",
        "paper_path": "./papers/2410.15592v2.json",
        "figure_id": "2410.15592v2_figure_1.png",
        "caption": "Figure 1: a. Protein representation methods. Proteins can be input into the model in various forms, including amino acid sequences, feature maps, three-dimensional coordinates, functional descriptions, and sequences composed of structural tokens, capturing the multi-level features of proteins. b. Pre-training of SSLM. SSLM is pre-trained on over 100,000 protein structures from the Swiss-Prot database and trained on various masked language modeling tasks, learning the relationships between “structure-sequences” and their corresponding three-dimensional structural features, thereby effectively representing protein structural information. c. CPE-Pro model architecture. The CPE-Pro model integrates a pre-trained protein structure language model with a graph embedding module, inputting the combined representation into the GVP-GNN module for computation. The pooling module aggregates structural information using attention masking, enhancing the quality of the representation. Ultimately, a multilayer perceptron serves as the source discriminator, outputting predicted probabilities.",
        "qtype": "Experimental_Results",
        "response": "Step 1 – Identify mask1 and mask2  \n• mask1 (red-boxed item) in panel b is the “MLM task” (i.e. Masked Language Modeling objective).  \n• mask2 (blue-boxed item) in panel c is the “Struct-seq” (the 3Di-alphabet discretized structural sequence).  \n\nStep 2 – What does the MLM task do?  \nIn pre-training the SSLM, we randomly “mask out” a subset of the structural tokens in the Struct-seq and ask the Transformer to predict those missing tokens from the remaining context.  \n\nStep 3 – How does this refine the Struct-seq embeddings?  \n1. By masking tokens, the model cannot simply memorize each 3Di symbol – it must attend to the neighboring structural elements.  \n2. The Transformer’s self‐attention layers learn which parts of the sequence are most informative for filling in the gap.  \n3. Back-propagating the MLM loss updates both the token embedding matrix and the attention-weight parameters.  \n4. Over many such masking/prediction cycles, the embeddings of each 3Di symbol become enriched with information about both its local tertiary contacts and longer‐range structural context.  \n\nAnswer  \nThe MLM task refines the Struct-seq embeddings by continually replacing random 3Di tokens with a special mask symbol during pre-training and training the SSLM to reconstruct those missing tokens from their structural context.  This forces the Transformer to learn rich, context‐aware embeddings for each element of the discretized structure sequence."
    },
    {
        "question": "How does pooling module aggregate fused embeddings to optimize classification head inputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "pooling",
            "classification head"
        ],
        "id": 274,
        "masked_question": "How does [mask1] module aggregate fused embeddings to optimize classification head inputs?",
        "masked_number": 1,
        "masked_elements": [
            "pooling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.15592v2_figure_1.png",
        "paperid": "2410.15592v2",
        "paper_path": "./papers/2410.15592v2.json",
        "figure_id": "2410.15592v2_figure_1.png",
        "caption": "Figure 1: a. Protein representation methods. Proteins can be input into the model in various forms, including amino acid sequences, feature maps, three-dimensional coordinates, functional descriptions, and sequences composed of structural tokens, capturing the multi-level features of proteins. b. Pre-training of SSLM. SSLM is pre-trained on over 100,000 protein structures from the Swiss-Prot database and trained on various masked language modeling tasks, learning the relationships between “structure-sequences” and their corresponding three-dimensional structural features, thereby effectively representing protein structural information. c. CPE-Pro model architecture. The CPE-Pro model integrates a pre-trained protein structure language model with a graph embedding module, inputting the combined representation into the GVP-GNN module for computation. The pooling module aggregates structural information using attention masking, enhancing the quality of the representation. Ultimately, a multilayer perceptron serves as the source discriminator, outputting predicted probabilities.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed component in Figure 1c is nothing mystical, it is simply the “attention-masked pooling” layer that sits between the GVP-GNN stack and the final MLP.  During the forward pass it takes all of the per-node (i.e. per-residue) embeddings that have already been fused (graph + structure-sequence), applies an attention mask to zero out any padding or masked positions, and then uses a learned attention vector to score each remaining node.  Those scores become weights in a weighted sum (or weighted average) of the node embeddings, yielding one fixed-size global feature vector.  That single vector is then handed off to the three-layer perceptron head for the crystal-vs-predicted decision.  By learning to pay attention only to the most discriminative residues, this pooling step greatly sharpens the signal going into the classifier."
    },
    {
        "question": "What limitations might arise from pooling outputs of GVP-GNN blocks using the attention mask?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GVP-GNN blocks",
            "Pooling",
            "Attention mask"
        ],
        "id": 275,
        "masked_question": "What limitations might arise from [mask1] outputs of [mask2] using the attention mask?",
        "masked_number": 2,
        "masked_elements": [
            "Pooling",
            "GVP-GNN blocks"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.15592v2_figure_1.png",
        "paperid": "2410.15592v2",
        "paper_path": "./papers/2410.15592v2.json",
        "figure_id": "2410.15592v2_figure_1.png",
        "caption": "Figure 1: a. Protein representation methods. Proteins can be input into the model in various forms, including amino acid sequences, feature maps, three-dimensional coordinates, functional descriptions, and sequences composed of structural tokens, capturing the multi-level features of proteins. b. Pre-training of SSLM. SSLM is pre-trained on over 100,000 protein structures from the Swiss-Prot database and trained on various masked language modeling tasks, learning the relationships between “structure-sequences” and their corresponding three-dimensional structural features, thereby effectively representing protein structural information. c. CPE-Pro model architecture. The CPE-Pro model integrates a pre-trained protein structure language model with a graph embedding module, inputting the combined representation into the GVP-GNN module for computation. The pooling module aggregates structural information using attention masking, enhancing the quality of the representation. Ultimately, a multilayer perceptron serves as the source discriminator, outputting predicted probabilities.",
        "qtype": "Others",
        "response": "First, let’s disambiguate the two masked regions in Fig. 1c:\n\n  • [mask2] is the stack of GVP‐GNN blocks (highlighted in blue).  \n  • [mask1] is the pooling module (highlighted in red) that takes the per‐node embeddings out of the GVP‐GNN and, via an attention mask, collapses them into a single graph‐level vector.\n\nNow: “What limitations might arise from pooling outputs of GVP‐GNN blocks using the attention mask?”\n\nChain of thought:\n\n1.  What the pooling is doing  \n    – After the GVP‐GNN layers have produced a learned embedding for each residue (node), the pooling layer (with its attention mask) computes a weighted sum (or similar) to produce one fixed‐size summary vector.  \n\n2.  The role of the attention mask  \n    – The attention mask steers the pooling to focus on a subset of nodes (for example those with high predicted importance). Nodes that receive very low attention weights are effectively “ignored.”\n\n3.  Where information can get lost  \n    – Any residue whose embedding is given a near‐zero attention score never contributes to the final graph vector. Subtle but globally important structural motifs can therefore be dropped.  \n    – Pooling collapses all the spatially distinct node embeddings into one vector; relative positions and fine‐grained topology are no longer explicitly represented.  \n\n4.  Consequences for downstream discrimination  \n    – If only a handful of nodes dominate the attention, the model may become blind to smaller local differences elsewhere in the fold.  \n    – Over‐smoothing can occur: different proteins whose key differences lie in low‐attention regions will look “the same” after pooling, making it hard to separate, say, two predicted structures that only differ by a subtle loop shift.  \n    – The fixed‐size vector is bottlenecked: the richer the GVP‐GNN output (many node features), the greater the risk that pooling will wash out minority signals.  \n\n5.  Summary of limitations  \n    – Loss of fine‐grained, node‐specific structural information (especially if masked out).  \n    – Potential bias toward only the highest‐attention nodes or substructures.  \n    – Reduced sensitivity to subtle, distributed differences in overall graph topology.  \n    – A single global vector may not carry enough capacity to distinguish closely related structures once attention has pruned away so much detail.  \n\nAnswer:\n\nBecause the red‐boxed pooling layer turns all of the node embeddings from the blue‐boxed GVP‐GNN into one fixed‐size summary via an attention mask, any residues or regions that receive low attention weight are essentially discarded.  That means subtle but globally important structural features can be lost, the full 3D topology is flattened away, and distinctions between two very similar folds (especially those located in masked-out nodes) can no longer be teased apart by the final MLP.  In short, pooling + attention risks drowning out minority signals and over‐smoothing the graph representation."
    },
    {
        "question": "What limitations emerge when using YOLO-detected bounding boxes for scene-aware CBF in dynamic environments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "YOLO",
            "scene-aware CBF"
        ],
        "id": 277,
        "masked_question": "What limitations emerge when using [mask1]-detected bounding boxes for [mask2] in dynamic environments?",
        "masked_number": 2,
        "masked_elements": [
            "YOLO",
            "scene-aware CBF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.10283v1_figure_1.png",
        "paperid": "2409.10283v1",
        "paper_path": "./papers/2409.10283v1.json",
        "figure_id": "2409.10283v1_figure_1.png",
        "caption": "Figure 1: Overview of the ASMA framework, integrating CLIP for vision-language understanding, YOLO for object detection, and scene-aware CBFs via cropped depth-map for safe vision-language drone navigation.",
        "qtype": "Others",
        "response": "Mask1 turns out to be the YOLO‐generated bounding boxes, and Mask2 is the Scene-Aware CBF.  If you rely on YOLO’s 2D boxes to carve out the regions on which your scene-aware control‐barrier function operates, you inherit all of YOLO’s shortcomings as safety‐critical inputs.  In a nutshell:\n\n 1. Bounding-box localization errors  \n    • YOLO often “jiggles” around a moving object or under partial occlusion, so the cropped depth region can slide off the true obstacle boundary.  \n    • Any misalignment directly perturbs the CBF’s h(x) and its lie‐derivatives, which govern your safety constraint.  \n\n 2. Coarse 2D approximation of 3D shape  \n    • A rectangular box will include background clutter or miss thin protrusions (wires, posts, tree branches).  \n    • The resulting Point-to-obstacle distances from the depth map are biased, so the CBF underestimates proximity in some directions.  \n\n 3. Detection latency & false negatives/positives  \n    • In fast-changing scenes (e.g. walking people, moving vehicles), YOLO may drop detections or hallucinate objects.  \n    • A late or spurious box translates into a stale or phantom “unsafe set” for the barrier, causing either unnecessary evasive maneuvers or unguarded collisions.  \n\n 4. Domain-shift sensitivity  \n    • Pretrained YOLO models often mis‐detect novel or poorly‐lit objects (e.g. in outdoor, low-light, or highly cluttered environments).  \n    • Any missed landmark or obstacle yields blind spots in the CBF formulation, compromising forward invariance of the safe set.  \n\nTogether, these limitations can make the “scene-aware” CBF behave unpredictably—either overconservative (detouring too far) or overly risky (failing to register a real obstacle)—whenever objects move rapidly, occlude one another, or deviate from YOLO’s training distribution."
    },
    {
        "question": "What potential drawbacks arise from cropping the depth map for scene-aware CBF evaluations instead of using full depth maps?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "depth map",
            "scene-aware CBF"
        ],
        "id": 278,
        "masked_question": "What potential drawbacks arise from cropping the [mask1] for [mask2] evaluations instead of using full [mask1]s?",
        "masked_number": 2,
        "masked_elements": [
            "depth map",
            "scene-aware CBF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.10283v1_figure_1.png",
        "paperid": "2409.10283v1",
        "paper_path": "./papers/2409.10283v1.json",
        "figure_id": "2409.10283v1_figure_1.png",
        "caption": "Figure 1: Overview of the ASMA framework, integrating CLIP for vision-language understanding, YOLO for object detection, and scene-aware CBFs via cropped depth-map for safe vision-language drone navigation.",
        "qtype": "Others",
        "response": "Let’s unpack what happens if you only feed a small, cropped‐out portion of the depth image (the red‐boxed “Depth Map” in Fig. 1) into your scene‐aware CBF block (the blue‐boxed “Scene‐Aware CBF”) instead of the full depth frame:\n\n 1. Loss of global context  \n    • Any obstacles or free‐space cues that lie outside the tight crop simply vanish. Your CBF now “thinks” the world beyond that little window is obstacle-free, which can easily violate safety if something lurks just outside your ROI.  \n 2. Blind spots and missed hazards  \n    • Since the crop is usually centered on a detected landmark, you get virtually zero information about obstacles to the left, right, above or below that landmark. The controller can’t react to anything it never sees.  \n 3. Degraded gradient estimates  \n    • The CBF relies on computing ∇h(x) across a range of depths. A tiny crop means far fewer valid pixels to estimate that gradient, leading to noisy or biased barrier‐function derivatives—and in turn, less reliable safety guarantees.  \n 4. Narrow field-of-view limitations  \n    • Your barrier function is supposed to carve out a forward-invariant safe set in the full 3D scene. If you only look at a sliver of the depth buffer you effectively shrink the “horizon” of your safety check, negating the very point of a scene-aware CBF.  \n 5. Potential control chatter or oscillations  \n    • As you drift in and out of a small crop, points will enter and leave your ROI rapidly. This can make h(x) jump around, causing the CBF optimizer to switch aggressively between “safe” and “unsafe” modes, leading to jerky, unpredictable drone motions.  \n\nIn short, cropping the depth map for your CBF evaluation trades off safety for locality. You gain computational thrift at the expense of blind spots, noisy gradients and ultimately the formal safety guarantees that a full‐FOV depth-map would provide."
    },
    {
        "question": "What motivates extracting keypoints via GPT-2 before applying YOLO detection?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The operator views images from a ROS topic and issues a command like \"Go to the tree on the right.\" We use the GPT-2 LLM to parse this command into key components: the action (\"go to\"), the landmark (\"tree\"), and the attribute (\"on the right\").",
            "Given the input image X, the YOLO object detection function f_YOLO outputs a bounding box B for the detected landmark y, using which we extract the cropped image X_crop from the original image X."
        ],
        "final_answer": "Extracting keypoints via GPT-2 is motivated by the need to parse the operator’s natural‐language instruction into its constituent action, landmark, and attribute. This ensures YOLO is then applied specifically to the identified landmark (e.g., “tree”), enabling focused and accurate object detection.",
        "relevant_elements": [
            "LLM (GPT-2)",
            "Object Detection (YOLO)"
        ],
        "id": 279,
        "masked_question": "What motivates extracting keypoints via [mask1] before applying YOLO detection?",
        "masked_number": 1,
        "masked_elements": [
            "LLM (GPT-2)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.10283v1_figure_1.png",
        "paperid": "2409.10283v1",
        "paper_path": "./papers/2409.10283v1.json",
        "figure_id": "2409.10283v1_figure_1.png",
        "caption": "Figure 1: Overview of the ASMA framework, integrating CLIP for vision-language understanding, YOLO for object detection, and scene-aware CBFs via cropped depth-map for safe vision-language drone navigation.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1.  The drone only ever knows how to run YOLO on specific object classes (e.g. “tree,” “bench,” “car”), but the operator’s command is free-form (“Go to the tree on the right”).  \n2.  In order to tell YOLO which class to look for, we first need to pull out from that sentence the “landmark” (tree), any spatial qualifier (“on the right”), and the intended action (“go to”).  \n3.  That is exactly what the GPT-2 LLM block (the red-boxed [mask1]) does—it parses the raw instruction into a small set of keypoints:  \n     •  action = “go”  \n     •  landmark = “tree”  \n     •  attribute = “right”  \n4.  Once we have “tree” as the landmark, we can feed that label into the YOLO detector, which then only searches for trees rather than attempting to detect every possible object.  \n5.  This targeted detection drastically cuts down on false positives, speeds up inference, and ensures that the downstream CLIP-based verification (which checks if the cropped patch really is the right tree) is only ever run on the candidate that the instruction actually referred to.  \n\nAnswer:\n\nWe extract keypoints (action, landmark, attribute) via the GPT-2 LLM so that YOLO knows exactly which object class (“tree” in “on the right”) to detect.  This semantic parsing narrows the detection scope, reduces false positives, and binds the human instruction directly to the vision module before any bounding-box proposals are generated."
    },
    {
        "question": "Why integrate cosine similarity verification with scene-aware CBF safety enforcement?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "Subsequently, ASMA identifies risky observations along the baseline drone VLN’s path by processing ego-centric depth maps to dynamically evaluate scene-aware CBFs. This results in a hybrid approach utilizing AI as well as symbolic rules which adjusts the control commands of a VLN-powered drone using formal safety methods emerging from control theory.",
            "The similarity score S is calculated by measuring the cosine similarity between the embeddings of the original and cropped images with respect to the text prompt. ... These dynamics ensure that S triggers appropriate downstream navigational tasks when verified."
        ],
        "final_answer": "By combining cosine‐similarity‐based landmark verification with scene‐aware control‐barrier‐function enforcement, the system ensures that (1) it is steering toward the correct, operator‐specified landmark (cosine similarity check) and (2) it dynamically enforces safety constraints around that landmark and any obstacles (scene‐aware CBF). This fusion of high‐level vision‐language grounding with low‐level formal safety guarantees produces more robust and reliable drone navigation.",
        "relevant_elements": [
            "Cosine Similarity",
            "Scene-Aware CBF"
        ],
        "id": 280,
        "masked_question": "Why integrate [mask1] verification with [mask2] safety enforcement?",
        "masked_number": 2,
        "masked_elements": [
            "Cosine Similarity",
            "Scene-Aware CBF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.10283v1_figure_1.png",
        "paperid": "2409.10283v1",
        "paper_path": "./papers/2409.10283v1.json",
        "figure_id": "2409.10283v1_figure_1.png",
        "caption": "Figure 1: Overview of the ASMA framework, integrating CLIP for vision-language understanding, YOLO for object detection, and scene-aware CBFs via cropped depth-map for safe vision-language drone navigation.",
        "qtype": "Design_Rationale",
        "response": "Mask₁ is the cosine-similarity based “verification” step (the CLIP check that the detected object really matches the text prompt), and Mask₂ is the scene-aware CBF “safety enforcement” module that constraints the drone’s low-level controls to avoid collisions.  We integrate them because:\n\n 1. Without verifying via cosine similarity, the system can’t be sure it’s actually looking at the right landmark—so it might steer toward the wrong object or direction.  \n 2. Without the CBF, even a correctly identified landmark could lead the drone into obstacles or unsafe regions.  \n\nBy chaining them, we get semantic correctness and formal safety guarantees.  First the VLN module only issues waypoints when CLIP’s similarity score confirms the landmark; then the CBF adjusts those waypoints in real time to enforce provable obstacle-avoidance constraints.  This fusion of high-level perception checks with low-level control barriers yields robust, safe vision-language navigation."
    },
    {
        "question": "What motivates balancing reducing S_C and increasing S_C~ in the adversarial loss formulation?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "While a traditional adversarial attack’s goal is to make the perturbation imperceptible in the input space, in the HiPS attack, we want the difference in the output space to also be minimal – the only difference should be the omission of the target class.",
            "The corresponding adversarial loss can be computed as L(δ)=−λ₁S_C+λ₂S_{C~}. L aims to reduce the score for the original caption S_C while increase the score for the target caption S_{C~}, where the target object is missing."
        ],
        "final_answer": "The HiPS-cap loss balances reducing S_C and increasing S_{C~} in order to achieve only a minimal change in the model’s output—namely, to remove the target object from the caption while leaving the rest of the description intact. By pushing the adversarial image embedding away from the original caption and towards the target caption (which omits the object), the attack ensures that the only difference between the original and adversarial outputs is the omission of the target class.",
        "relevant_elements": [
            "S_C",
            "S_C~"
        ],
        "id": 281,
        "masked_question": "What motivates balancing reducing [mask1] and increasing [mask2] in the adversarial loss formulation?",
        "masked_number": 2,
        "masked_elements": [
            "S_C",
            "S_C~"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.13010v1_figure_1.png",
        "paperid": "2410.13010v1",
        "paper_path": "./papers/2410.13010v1.json",
        "figure_id": "2410.13010v1_figure_1.png",
        "caption": "Figure 1: A schematic illustration of the Hiding-in-Plain-Sight (HiPS-cap) Attack.",
        "qtype": "Design_Rationale",
        "response": "Here, the two terms in the loss  \nL(δ) = −λ₁ S_c  + λ₂ S_{ĉ}  \ncorrespond exactly to the two boxes in your figure:\n\n• [mask1] is S_c, the CLIP cosine‐similarity between the perturbed image and the *original* caption.  \n• [mask2] is S_{ĉ}, the cosine‐similarity between the perturbed image and the *target* (counterfactual) caption that omits the object.\n\nWhy do we need to balance *both* of these?  Because our goal in a HiPS attack is twofold:\n\n1.  **Suppress the model’s “belief” in the original description** (so it no longer mentions the target object) ⇒ drive S_c *down*.  \n2.  **Pull the model towards the new, object‐free description** (so it still describes everything else correctly) ⇒ drive S_{ĉ} *up*.  \n\nIf you only minimized one of these terms, you’d either\n\n- just break the caption completely (by only minimizing S_c) or  \n- force a different—but potentially unrelated—caption (by only maximizing S_{ĉ}).  \n\nBy *simultaneously* lowering the score on the true caption and raising the score on the counterfactual caption, you ensure the perturbation is both subtle (hiding *only* the chosen object) and semantically precise (keeping all other details intact)."
    },
    {
        "question": "What motivates iterating perturbation updates N times in the HiPS-cap attack?",
        "relevant_section_ids": [
            "2",
            "3.3"
        ],
        "relevant_context": [
            "Section 2: \"Another widely used technique is the Projected Gradient Descent (PGD) attack [2], which is the strongest first-order attack. PGD is an iterative, first-order optimization-based attack, defined as: x_{i+1} = Π_{x+S}(x_i + α sign(∇_x L(x_i))), where i denotes the iteration number ...\"",
            "Section 3.3: \"The adversarial loss L₁ and L₂ can be optimized using existing adversarial attacks such as FGSM and PGD attacks (See Section 2).\""
        ],
        "final_answer": "Because HiPS-cap adopts a PGD-style adversarial optimization, it performs multiple small updates (N iterations) to gradually refine the perturbation and better optimize the adversarial loss (reduce the original caption score and increase the target caption score) within the allowed perturbation budget.",
        "relevant_elements": [
            "perturbations",
            "Repeat N times"
        ],
        "id": 282,
        "masked_question": "What motivates iterating perturbation updates [mask1] in the HiPS-cap attack?",
        "masked_number": 1,
        "masked_elements": [
            "Repeat N times"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.13010v1_figure_1.png",
        "paperid": "2410.13010v1",
        "paper_path": "./papers/2410.13010v1.json",
        "figure_id": "2410.13010v1_figure_1.png",
        "caption": "Figure 1: A schematic illustration of the Hiding-in-Plain-Sight (HiPS-cap) Attack.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1.  In HiPS-cap we define a loss  \n     L(δ) = –λ₁·S_c + λ₂·S_{ĉ}  \n   where S_c is the cosine similarity between the perturbed image and the original caption, and S_{ĉ} is that with the “target” (object-omitted) caption.  \n2.  We want to find a tiny perturbation δ (‖δ‖∞ ≤ ε) that simultaneously drives S_c down and S_{ĉ} up.  \n3.  A single FGSM step (one‐shot) often cannot satisfy both objectives under a tight ℓ∞ budget— the loss landscape is non-convex and multi-modal.  \n4.  Projected Gradient Descent (PGD), i.e. taking many small steps of the form  \n     δ_{i+1} = Π_{‖δ‖∞≤ε} [δ_i + α·sign(∇_δ L(δ_i))]  \n   is known to be the strongest first-order adversary and more reliably maximizes complex losses.  \n5.  Hence we “Repeat N times” (red box) those tiny gradient‐sign updates—each step refines δ closer to the optimum, yielding a subtler but more effective HiPS perturbation.  \n\nAnswer  \nWe iterate (“Repeat N times”) because HiPS-cap uses a PGD‐style optimization to maximize its two-term cosine‐similarity loss. Multiple small gradient steps (rather than one big FGSM step) are needed to reliably drive down the original‐caption score while boosting the target‐caption score, all under a strict ℓ∞ perturbation budget."
    },
    {
        "question": "How does adversarial loss use cosine similarity scores to update perturbations?",
        "relevant_section_ids": [
            "2",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "One of the seminal methods for generating adversarial attacks is the Fast Gradient Sign Method (FGSM) [1], a simple, single-step ε-bounded attack, defined as: x_adv = x + ε · sign(∇_x L(x, y)).",
            "Another widely used technique is the Projected Gradient Descent (PGD) attack [2], which is an iterative, first-order optimization-based attack, defined as: x_{t+1} = P_{x+S}(x_t + α · sign(∇_x L(x_t, y))), where P is a projection back into the ε-ball.",
            "Since the objective of the HiPS attack is to remove the target object t, our goal is to perturb the image x in such a way that the cosine similarity score for the target object, S_t, is reduced (as if it is absent), while the scores for all other objects remain unchanged.",
            "The corresponding adversarial loss can be computed as L = –S_C + S_{Ĉ}, which aims to reduce the score for the original caption C while increasing the score for the target caption Ĉ."
        ],
        "final_answer": "The attack first computes cosine similarities between the image embedding and either (a) each class label (HiPS-cls) or (b) the original versus target captions (HiPS-cap). It then defines an adversarial loss that penalizes similarity to the target (class or original caption) and rewards similarity to the non-target (other classes or the adversarial caption). Finally, standard gradient-based methods such as FGSM or PGD are used to update the image perturbation δ by taking steps in the direction of the sign of the loss gradient, e.g. δ_{i+1} = δ_i + α · sign(∇_δ L(δ)).",
        "relevant_elements": [
            "adversarial loss",
            "cosine similarity scores",
            "perturbations"
        ],
        "id": 283,
        "masked_question": "How does [mask1] use cosine similarity scores to update perturbations?",
        "masked_number": 1,
        "masked_elements": [
            "adversarial loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.13010v1_figure_1.png",
        "paperid": "2410.13010v1",
        "paper_path": "./papers/2410.13010v1.json",
        "figure_id": "2410.13010v1_figure_1.png",
        "caption": "Figure 1: A schematic illustration of the Hiding-in-Plain-Sight (HiPS-cap) Attack.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is nothing more exotic than the CLIP‐based loss that drives a PGD/FGSM‐style update of the pixel perturbation.  Concretely:\n\n 1. We pass the adversarial image through CLIP’s image encoder and two captions (the original caption and the “masked” target caption) through CLIP’s text encoder to get embeddings I, T₁ and T₂.  \n 2. We compute cosine similarities  \n    • S_c = cosine(I, T₁) (how well the image still matches the original caption)  \n    • S_ĉ = cosine(I, T₂) (how well it matches the “object‐removed” caption)  \n 3. We form the HiPS‐cap loss:  \n        L(δ) = –λ₁·S_c + λ₂·S_ĉ  \n    – Driving S_c down makes the model “forget” the original object.  \n    – Driving S_ĉ up makes it “believe” the object has been removed.  \n 4. Finally we update the perturbation via a single‐step or multi‐step sign‐gradient update (FGSM/PGD):  \n        δ_{i+1} = δ_i + α·sign(∇_δ L(δ))  \n\nIn this way the cosine similarity scores directly provide the gradient signal that sculpts the adversarial noise."
    },
    {
        "question": "How does Text Encoder distinguish original and adversarial caption embeddings for cosine similarity computation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In this variant of the HiPS attack, termed HiPS-cap, rather than using class labels, we generate the attack on CLIP by utilizing the original caption c and a target caption c̃.",
            "Similar to the HiPS-cls approach, we calculate the cosine similarities between the image I and both the original caption c and the target caption c̃ as follows: … The corresponding adversarial loss can be computed as L(δ) = -λ₁ S_c + λ₂ S_c̃. L aims to reduce the score for the original caption S_c while increase the score for the target caption S_c̃, where the target object is missing."
        ],
        "final_answer": "The CLIP text encoder treats the original caption and the adversarial (target) caption as two separate text inputs. It encodes each one independently into its own embedding vector (T for the original caption and T̃ for the target caption) and then computes two separate cosine similarities with the image embedding, yielding S_c and S_c̃ respectively.",
        "relevant_elements": [
            "Text Encoder",
            "original caption",
            "adversarial caption",
            "cosine similarity scores"
        ],
        "id": 284,
        "masked_question": "How does [mask1] distinguish original and adversarial caption embeddings for cosine similarity computation?",
        "masked_number": 1,
        "masked_elements": [
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.13010v1_figure_1.png",
        "paperid": "2410.13010v1",
        "paper_path": "./papers/2410.13010v1.json",
        "figure_id": "2410.13010v1_figure_1.png",
        "caption": "Figure 1: A schematic illustration of the Hiding-in-Plain-Sight (HiPS-cap) Attack.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in Figure 1 is nothing more exotic than the CLIP “Text Encoder.”  At each attack step you simply feed both the original caption and the adversarial caption through that same encoder and collect two different output vectors (labelled T₁ and T₂ in the figure).  Those two embeddings are then each paired with the image embedding I₁ and their cosine similarities S_C and Ŝ_C are computed.  In other words, the red box “distinguishes” the originals from the fakes purely by encoding the two different text strings into two separate vectors."
    },
    {
        "question": "How does similarity search handle high-dimensional embeddings while optimizing retrieval latency and accuracy?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Embeddings are indexed into the server database, whose indexes are constructed by the hierarchical navigable small world graphs (HNSWG).",
            "At searching phase, a query embedding   is submitted to the vector database. The database returns the correlated embeddings that is closest to the query based on similarity metrics like cosine similarity  and Euclidean distance ."
        ],
        "final_answer": "Similarity search on high-dimensional embeddings is made efficient and accurate by building an HNSWG index over the embedding vectors. At query time, this graph‐based structure enables sublinear‐time approximate nearest‐neighbor lookups, returning the top matches by cosine similarity or Euclidean distance and thus balancing retrieval latency with accuracy.",
        "relevant_elements": [
            "similarity search",
            "vector database"
        ],
        "id": 285,
        "masked_question": "How does [mask1] handle high-dimensional embeddings while optimizing retrieval latency and accuracy?",
        "masked_number": 1,
        "masked_elements": [
            "similarity search"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05034v1_figure_1.png",
        "paperid": "2411.05034v1",
        "paper_path": "./papers/2411.05034v1.json",
        "figure_id": "2411.05034v1_figure_1.png",
        "caption": "Figure 1: Overview of the embedding vector database: Clients upload queries or documents to embedding models, such as Azure OpenAI, to generate and store embeddings. By searching for the most relevant embeddings and using them to prompt the LLM, clients can obtain satisfying results and fine-tune customized private applications.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In the diagram, the red box sits immediately downstream of the “Vector Database” and is labelled “Results” – it’s clearly the component that returns the top-k embeddings (e.g. code snippets or text passages) once a query embedding has been looked up.  \n2. In the accompanying text, “Embedding Vector Database” is defined as a system that (a) takes in high-dimensional dense vectors, (b) builds an index using Hierarchical Navigable Small World graphs (HNSW), and (c) at query-time runs an approximate nearest-neighbor search over that graph using cosine or Euclidean distance.  \n3. HNSW delivers sublinear search complexity by organizing points into a multi-layer small-world graph. Tunable parameters—such as the maximum number of links per node (M) and the size of the candidate queue at query time (ef)—let you dial in a sweet-spot between retrieval latency and recall/accuracy.  \n4. Many systems also apply vector quantization (e.g. product quantization) or graph pruning to further shrink memory footprints and speed up the inner-loop distance computations—all without a large hit to accuracy.  \n\nAnswer  \nThe “Results” box is produced by an embedding-vector database that handles very high-dimensional embeddings by building an HNSW (Hierarchical Navigable Small World) index over them. At query time it performs an approximate nearest-neighbor graph‐traversal (using cosine or Euclidean metrics) whose cost grows sublinearly in the number of indexed vectors. By tuning key HNSW parameters (e.g. the graph’s connectivity M and the search width ef) – and optionally layering in vector-quantization for compression – it simultaneously achieves very low latency lookups and high accuracy (recall) in returning the top-k nearest embeddings."
    },
    {
        "question": "How can LLM-generated embeddings be optimized for vector database similarity search efficiency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM",
            "Vector Database"
        ],
        "id": 287,
        "masked_question": "How can [mask1]-generated embeddings be optimized for [mask2] similarity search efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "LLM",
            "Vector Database"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05034v1_figure_1.png",
        "paperid": "2411.05034v1",
        "paper_path": "./papers/2411.05034v1.json",
        "figure_id": "2411.05034v1_figure_1.png",
        "caption": "Figure 1: Overview of the embedding vector database: Clients upload queries or documents to embedding models, such as Azure OpenAI, to generate and store embeddings. By searching for the most relevant embeddings and using them to prompt the LLM, clients can obtain satisfying results and fine-tune customized private applications.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the LLM itself (the box in red), and Mask2 is the user’s search‐query (“How should I improve my healthy lifestyle?”) highlighted in blue.  In other words the question becomes:\n\n  “How can LLM-generated embeddings be optimized for user-query similarity search efficiency?”\n\nHere’s a step-by-step reasoning:\n\n1. Alignment  \n   - The red box encloses the LLM block, which is where all embeddings are born.  \n   - The blue box is the user’s query “How should I improve my healthy lifestyle?”, whose embedding must be looked up in the vector store.\n\n2. Bottleneck  \n   - Embeddings from the LLM are high-dimensional dense vectors.  \n   - Brute-force nearest-neighbor on millions of these vectors is too slow for real-time query serving.\n\n3. Two axes of optimization  \n   a. Better embeddings for retrieval  \n     • Contrastive fine-tuning (dual-encoder training) on query–passage pairs so that “nearby” items are really close in cosine space and “irrelevant” ones are pushed far away.  \n     • Dimensionality reduction or distillation (e.g. project 1,536→256 dims) to reduce the cost per distance computation.  \n   b. Faster nearest-neighbor search  \n     • Use an approximate nearest-neighbor index (HNSW, IVF, or Annoy) instead of brute force.  \n     • Apply product quantization (PQ) or optimized PQ (OPQ) to compress vectors into a few bytes each.  \n     • Optionally binarize (e.g. locality-sensitive hashing) for ultra-fast Hamming‐distance lookups.\n\n4. Combined recipe  \n   1) Fine-tune the LLM (or a lightweight projection head on its embeddings) with a retrieval loss (e.g. InfoNCE) so genuine “How should I improve my healthy lifestyle?”‐type queries pull the right documents.  \n   2) Immediately apply dimensionality reduction + PQ to all stored and query vectors.  \n   3) Build an HNSW (or IVF-PQ) index over those compressed vectors.  \n\n5. Result  \n   • Embeddings remain “searchable” (high recall/F1 on real queries) yet occupy far less memory.  \n   • Cosine (or Euclidean) lookups become orders of magnitude faster, making real-time similarity search practical even at web-scale."
    },
    {
        "question": "How does vector database retrieval guide LLM prompt formulation in retrieval-augmented generation?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "By integrating embedding databases with LLMs through a retrieval mechanism, retrieval-augmented generation (RAG) has emerged as a powerful tool for developers in AI assistant APIs.",
            "Upon receiving a query (e.g., a partial sentence), the system first retrieves the top-k most relevant passages from the vector database to serve as prompts, thereby enhancing the quality and factual accuracy of the generated text."
        ],
        "final_answer": "In retrieval-augmented generation, when the LLM receives a query it issues that query to the vector database, retrieves the top-k most similar stored passages, and then uses those retrieved passages as part of its prompt. This incorporation of relevant context guides the LLM’s output, improving both its factual accuracy and overall response quality.",
        "relevant_elements": [
            "Vector Database",
            "LLM"
        ],
        "id": 288,
        "masked_question": "How does [mask1] retrieval guide LLM prompt formulation in retrieval-augmented generation?",
        "masked_number": 1,
        "masked_elements": [
            "Vector Database"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05034v1_figure_1.png",
        "paperid": "2411.05034v1",
        "paper_path": "./papers/2411.05034v1.json",
        "figure_id": "2411.05034v1_figure_1.png",
        "caption": "Figure 1: Overview of the embedding vector database: Clients upload queries or documents to embedding models, such as Azure OpenAI, to generate and store embeddings. By searching for the most relevant embeddings and using them to prompt the LLM, clients can obtain satisfying results and fine-tune customized private applications.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought:\n\n1. In the figure, the red box highlights the link between the vector database’s similarity-search output and the LLM’s prompt.  \n2. When a user issues a query (“How should I improve my healthy lifestyle?”), the system first embeds that query and runs a nearest-neighbor (top-k) search over the stored embeddings.  \n3. The most semantically similar passages (those top-k documents) are pulled out of the database.  \n4. These retrieved passages are then concatenated with or prepended to the user’s original query, forming an augmented prompt.  \n5. The LLM uses this enriched prompt—containing both the user’s question and the grounding snippets—to generate a more accurate, contextually relevant answer.\n\nAnswer:\n\nThe red-boxed retrieval is simply a vector-similarity (nearest-neighbor) search over the embedding database. By fetching the top-k semantically related passages and injecting them directly into the LLM’s prompt, it provides the model with concrete, grounded context, steering its generation toward factual, on-topic responses."
    },
    {
        "question": "How does alternating graph-based pure geometric optimization enhance pose convergence in bundle-adjusting neural LiDAR fields?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "In contrast to prior pose-free NeRF methods, our pipeline employs a hybrid approach to optimize poses. As shown in Fig. 2, the framework can be divided into two alternately executed parts: global optimization of bundle-adjusting neural LiDAR fields (Sec. 3.2) and graph-based pure geometric optimization (Sec. 3.3) with the proposed Geo-optimizer.",
            "As illustrated in Fig. 3(b), insufficient geometric guidance leads to certain frame poses being optimized in the wrong direction. Geometric optimizer can address this issue by preventing pose updates strictly following NeRF and correcting wrong optimization directions that do not conform to global geometric consistency. This method involves externally modifying pose parameters and providing effective geometric guidance early in the ill-conditioned optimization process. Consequently, few iterations of graph-based RCD computation suffice to offer ample guidance for NeRF."
        ],
        "final_answer": "By interleaving standard bundle-adjustment (NeRF-based) updates with a few iterations of pure geometric pose refinement over a graph of frames, the Geo-optimizer injects explicit geometric constraints (via a robust, overlapping-aware Chamfer distance) into the otherwise photometric-driven training. This alternating scheme corrects wrong update directions, enforces global consistency across multiple LiDAR scans, and steers the network away from local optima—leading to faster, more accurate convergence of the estimated poses.",
        "relevant_elements": [
            "Bundle-Adjusting Neural LiDAR Fields",
            "Geometry Optimizer"
        ],
        "id": 291,
        "masked_question": "How does alternating graph-based pure geometric optimization enhance pose convergence in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Bundle-Adjusting Neural LiDAR Fields"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.05597v1_figure_2.png",
        "paperid": "2407.05597v1",
        "paper_path": "./papers/2407.05597v1.json",
        "figure_id": "2407.05597v1_figure_2.png",
        "caption": "Figure 2: Overview of our proposed GeoNLF. We alternatively execute global optimization of bundle-adjusting neural LiDAR fields and graph-based pure geometric optimization. By integrating selective-reweighting strategy and explicit geometric constraints derived from point clouds, GeoNLF implements outlier-aware and geometry-aware mechanisms.",
        "qtype": "Experimental_Results",
        "response": "In our figure the red‐boxed module is the “BA-Neural LiDAR Fields” – the NeRF‐style, bundle‐adjusting optimization of both the implicit LiDAR field and all sensor poses.  By itself, this stage often falls into the same kinds of local minima that pure pose-free NeRFs do: the rendered range images and intensities can be matched quite well even with slightly wrong poses, and without any explicit 3D cue the solver can wander off in an incorrect direction.  \n\nWhat the paper does (and what the question is asking about) is to interleave that bundle-adjustment (BA) step with a light, “pure geometric” optimization over a simple graph of overlapping point clouds.  In practice, every few gradient steps on the BA-Neural LiDAR Fields they pause, build a small graph whose vertices are frames and whose edges carry a differentiable, robust Chamfer‐distance loss between overlapping point sets, and they backpropagate *through* that Chamfer loss into the very same global transforms (the poses) that the BA step is working on.  \n\nBecause the Chamfer loss is directly measuring *3D* shape alignment, it will in most cases point back toward the one true global alignment—whereas the NeRF‐only bundle‐adjustment step can be satisfied by a family of locally plausible but globally inconsistent pose configurations.  By alternating:  \n\n 1.  you let the NeRF step fit the dense “radiance + density + intensity + ray-drop” field and tweak the poses so as to match the rendered 2D range/intensity images,  \n 2.  then you let the graph-based Chamfer step yank the poses back into a configuration that actually lines up the 3D point clouds,  \n\nyou fuse the best of both worlds.  The NeRF step brings global notice of all frames simultaneously, and the Chamfer step provides rock-solid local geometric grounding.  Empirically this alternation breaks the usual NeRF-style “pose holes” and needle-in-a-haystack local minima and gives much faster, more reliable convergence of the red-boxed BA-Neural LiDAR Fields."
    },
    {
        "question": "How does selective-reweighting adjust gradient flow between pose updates and neural LiDAR field optimization?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Inspired by the capabilities of NeRF in pose inference [64], we decrease the learning rate (η) of neural fields for the top k frames with the highest rendering losses as Eq. 14, while keeping η of poses unchanged.",
            "The strategy facilitates gradient propagation towards outlier poses, while the gradient flow to the radiance fields is concurrently diminished."
        ],
        "final_answer": "Selective-reweighting lowers the learning rate of the neural LiDAR fields for the worst-performing (highest-loss) frames, but leaves the pose learning rate unchanged. This causes more of the overall gradient to flow into correcting the outlier poses while reducing gradient updates to the neural fields.",
        "relevant_elements": [
            "Selective-Reweighting",
            "Bundle-Adjusting Neural LiDAR Fields"
        ],
        "id": 292,
        "masked_question": "How does [mask1] adjust gradient flow between pose updates and neural LiDAR field optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Selective-Reweighting"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.05597v1_figure_2.png",
        "paperid": "2407.05597v1",
        "paper_path": "./papers/2407.05597v1.json",
        "figure_id": "2407.05597v1_figure_2.png",
        "caption": "Figure 2: Overview of our proposed GeoNLF. We alternatively execute global optimization of bundle-adjusting neural LiDAR fields and graph-based pure geometric optimization. By integrating selective-reweighting strategy and explicit geometric constraints derived from point clouds, GeoNLF implements outlier-aware and geometry-aware mechanisms.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the “Selective-Reweighting” module.  In Sec. 3.4 the authors explain that, at each training step, they pick the k frames with the highest rendering losses (presumed outliers), and multiply the learning rate on the field parameters η_field by a small reweighting factor α ≪ 1, while keeping the pose learning rate η_pose unchanged.  Because η_field is shrunk for those bad frames, most of the back-propagated gradient from them flows into the pose variables instead of the network weights.  In this way, outlier frames drive pose correction, and the neural LiDAR field only sees a diminished gradient from them, preventing the network from overfitting to bad poses."
    },
    {
        "question": "How does Speech Audio Enhancer output enhance Prosody Detection Tools reliability?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Speech Audio Enhancer",
            "Prosody Detection Tools"
        ],
        "id": 293,
        "masked_question": "How does [mask1] output enhance Prosody Detection Tools reliability?",
        "masked_number": 1,
        "masked_elements": [
            "Speech Audio Enhancer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.13608v1_figure_1.png",
        "paperid": "2408.13608v1",
        "paper_path": "./papers/2408.13608v1.json",
        "figure_id": "2408.13608v1_figure_1.png",
        "caption": "Figure 1. System framework of the automatic speech annotation system.",
        "qtype": "Experimental_Results",
        "response": "Chain of thought  \n1. In the diagram the red‐boxed module is the “Speech Audio Enhancer.”  \n2. Its sole output is denoised, high-signal-to-noise speech (i.e. “Denoised Audio”).  \n3. All of our downstream prosody tools – pitch/energy/speed analyzers and emphasis detectors – rely on clean spectral and temporal cues to make accurate judgments.  \n4. By stripping away background noise, reverberation and other artifacts, the enhancer delivers a much cleaner waveform.  \n5. That higher SNR means the pitch tracker can lock onto fundamental frequency more stably, the energy extractor isn’t fooled by noise bursts, and the timing/speed estimator isn’t skewed by spurious transients.  \n6. In practice this translates directly into fewer mis-detections of emphasis, more consistent categorization of “low/normal/high pitch,” and overall better reliability of all the prosody‐detection tools.  \n\nAnswer  \nBecause the speech audio enhancer spits out a noise-reduced, high-SNR version of the input, the subsequent pitch, energy, speed and emphasis detectors receive much cleaner signals. That cleaner input yields far more stable feature extraction (fewer spurious peaks, more accurate fundamental‐frequency tracking, etc.), and thus substantially boosts the reliability of all the prosody‐detection modules."
    },
    {
        "question": "How does merging ASR transcripts and Meta Data through LLM influence description richness?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "Other sundry items available in the original audio information such as title, raw descriptions from the data uploader, and video category tags from the website, are mutually transferred by a language model to summarize the topic of the speech utterance.",
            "It is worth noting that we do not provide any structured formats for the description in advance to fill in the blanks as PromptTTS 2 does, but put emphasis on the richness of vocabulary and the accuracy in conveying the meaning of labels.",
            "The speech description (denoted as the Description version) contains all available attributes regardless of the transcript. Besides, we involved speech transcript as an extra attribute to form a so-called speech instruction (denoted as the Instruction version)."
        ],
        "final_answer": "By feeding both ASR transcripts and original metadata (titles, uploader notes, category tags) into a fine-tuned LLM, the system not only summarizes topic information but also weaves the verbatim content into its outputs.  This end-to-end rewriting produces natural‐language descriptions with richer vocabulary, greater detail, and higher fidelity to both what was said (transcript) and the broader context (metadata).",
        "relevant_elements": [
            "ASR",
            "LLM",
            "Meta Data"
        ],
        "id": 294,
        "masked_question": "How does merging [mask1] transcripts and Meta Data through LLM influence description richness?",
        "masked_number": 1,
        "masked_elements": [
            "ASR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.13608v1_figure_1.png",
        "paperid": "2408.13608v1",
        "paper_path": "./papers/2408.13608v1.json",
        "figure_id": "2408.13608v1_figure_1.png",
        "caption": "Figure 1. System framework of the automatic speech annotation system.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in the figure is simply the ASR front‐end – i.e. it produces the transcript.  By taking those ASR‐generated words and feeding them, together with the topic/title/category metadata, into the fine-tuned LLM we get two big wins in the “Instruction”–style outputs:\n\n 1.  **Richer vocabulary and context**  \n    –  The raw transcript gives the LLM actual phrases and terms the speaker used, while the metadata tells it *why* they’re speaking (politics, news, etc.).  Stitching them together lets the model pepper its description with both the speaker’s exact wording *and* the larger situational frame.  \n\n 2.  **Finer‐grained, unified control**  \n    –  Instead of describing style in isolation (pitch, speed, emotion) and content in isolation (topic, transcript) you now have a single prompt that interleaves them.  That lets the LLM reference specific words (“he stresses ‘people’”) in the proper topical context, producing much more detailed, coherent, and varied renderings of “how it was said” than either source alone could yield."
    },
    {
        "question": "What ethical risks arise from employing the Gender Predictor in the Speaker Info Identify module?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Gender Predictor",
            "Speaker Info Identify"
        ],
        "id": 295,
        "masked_question": "What ethical risks arise from employing the [mask1] in the [mask2] module?",
        "masked_number": 2,
        "masked_elements": [
            "Gender Predictor",
            "Speaker Info Identify"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.13608v1_figure_1.png",
        "paperid": "2408.13608v1",
        "paper_path": "./papers/2408.13608v1.json",
        "figure_id": "2408.13608v1_figure_1.png",
        "caption": "Figure 1. System framework of the automatic speech annotation system.",
        "qtype": "Others",
        "response": "The little red‐boxed “Gender Predictor” lives inside the blue‐boxed “Speaker Info Identify” sub-module of the style‐recognition pipeline.  In practical terms that means every time we run a clip through the system it will guess “male” or “female” (and often only those two) from voice alone.  That simple inference carries a surprising number of ethical landmines:\n\n 1.  Misgendering and Exclusion.  \n    –  By forcing speakers into a binary “male/female” decision you automatically erase non-binary, trans, gender-fluid or intersex people.  \n    –  The acoustic cues you’ve trained on are almost always drawn from cis-gendered, stereotypical voices—anyone who falls outside those narrow norms will be routinely misclassified and mis-described.  \n\n 2.  Bias Reinforcement.  \n    –  If your downstream tasks (captioning, synthesis, personalization) have separate behavior or styling rules for “men” vs. “women,” you cement and amplify cultural stereotypes.  \n    –  In the worst case you bake in unequal treatment—e.g. speaking faster or louder for one “gender” group than another—simply because the model has learned spurious correlations.  \n\n 3.  Privacy and Surveillance Risks.  \n    –  Inferring immutable demographic traits from speech crosses a privacy boundary: people may not have consented to be profiled by gender at all.  \n    –  Such technology can be repurposed for intrusive monitoring, targeted advertising, or even coercive state surveillance.  \n\n 4.  Regulatory and Consent Issues.  \n    –  In many jurisdictions “gender” is considered a sensitive attribute requiring explicit opt-in consent; automatically predicting it may violate data-protection laws (e.g. GDPR).  \n    –  If you later share or publish those gender labels, you risk exposing private personal data without clear user agreement.  \n\nIn short, embedding a hard‐line Gender Predictor into your Speaker‐Info module risks misgendering and erasing non-binary identities, baked-in stereotyping, undesired profiling or surveillance, and outright legal problems around consent and data privacy."
    },
    {
        "question": "What alternative approaches could replace Expertised LLaMA 2 for rewriting speech expressiveness descriptions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Expertised LLaMA 2"
        ],
        "id": 296,
        "masked_question": "What alternative approaches could replace [mask1] for rewriting speech expressiveness descriptions?",
        "masked_number": 1,
        "masked_elements": [
            "Expertised LLaMA 2"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.13608v1_figure_1.png",
        "paperid": "2408.13608v1",
        "paper_path": "./papers/2408.13608v1.json",
        "figure_id": "2408.13608v1_figure_1.png",
        "caption": "Figure 1. System framework of the automatic speech annotation system.",
        "qtype": "Others",
        "response": "First, let’s be clear what “\\[mask1\\]” is in the figure.  In the red‐boxed rewrite block the paper actually says “Expertised LLaMA 2” (i.e. a fine-tuned LLaMA 2 model).  So the question is: if you didn’t want to use a fine-tuned LLaMA 2 to turn your low-level style tags and raw transcript into a fluid English/Chinese description of the speaking style, what else might you plug in there instead?\n\nA few obvious alternatives spring to mind:\n\n  1. Off-the-shelf (or lightly tuned) commercial LLMs  \n     • GPT-4 or GPT-4 Turbo via API (with a few-shot or zero-shot prompt)  \n     • GPT-3.5 Turbo / ChatGPT  \n     • Claude 2 / Claude Instant  \n\n  2. Other open-source chat / instruction models  \n     • Baichuan2, Mistral, Mixtral, Vicuna, Alpaca, etc.  \n       – either zero-shot/in-context or fine-tuned on your annotation data  \n     • Flan-T5 or T0 family models (e.g. Flan-T5 XXL)  \n\n  3. Retrieval-augmented generation  \n     • Instead of a single monolithic LLM, embed your style-tag + transcript, retrieve the top-k most similar human-written style descriptions, and then use an LLM (any of the above) to “blend” them into a new sentence.  \n\n  4. Template-based NLG or hybrid rule+LLM systems  \n     • Revert to the PromptTTS 2 style of hand-crafted templates with slot filling—but augment the slot expansion with synonyms drawn from a small language model or a lexical resource, to recoup some diversity.  \n     • Or build a small finite-state NLG engine that knows how to talk about pitch, speed, emphasis, etc., then invoke that rather than a full LLM.  \n\n  5. Sequence-to-sequence text models  \n     • A fine-tuned BART / PEGASUS / T5 that takes as input a linearized list of tags and the transcript and outputs a single description.  \n\nAll of these would slot into exactly the same place in the pipeline where the authors currently call “Expertised LLaMA 2.”"
    },
    {
        "question": "What limitations arise when using perturbation score to diagnose overconfidence-inducing discontinuity?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "perturbation score",
            "overconfidence-inducing discontinuity"
        ],
        "id": 297,
        "masked_question": "What limitations arise when using [mask1] to diagnose [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "perturbation score",
            "overconfidence-inducing discontinuity"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.16608v1_figure_1.png",
        "paperid": "2410.16608v1",
        "paper_path": "./papers/2410.16608v1.json",
        "figure_id": "2410.16608v1_figure_1.png",
        "caption": "Figure 1: Overview: assessment of embeddings by neighbor embedding methods with an example of image data. a We use a standard pre-trained convolutional neural network (CNN) to obtain features of image samples from the CIFAR10 dataset, and then\nvisualize the features using a neighbor embedding method, specifically t-SNE. b Basic ideas of singularity scores and perturbation scores.\nc t-SNE tends to embed image features into separated clusters even for images with ambiguous semantic meanings (as quantified by higher entropy of predicted class probabilities by the CNN). Perturbation scores identify the embedding points which have ambiguous class membership but less visual uncertainty.\nd Incorrect choice of the hyperparameter leads to visual fractures (FI discontinuity), which is more severe with a smaller perplexity.\nWe recommend choosing the perplexity no smaller than the elbow point.",
        "qtype": "Others",
        "response": "Here’s the key point.  The perturbation‐score (the quantity boxed in red) was designed to pick up the “over-confidence” or global jumps (the OI discontinuity) – it measures how far a point’s embedding moves under a moderate change in its input coordinates.  It is not sensitive to the tiny, highly local kinks in the mapping that give rise to fracture-inducing (FI) discontinuities (the structure in the blue box).  In other words, if you try to use the perturbation score to locate FI discontinuities you will mostly miss them – it has too coarse a view of the loss landscape to resolve the small spurious sub-clusters and local minima that characterize FI artifacts."
    },
    {
        "question": "What alternative methods could complement singularity score for detecting fracture-inducing discontinuity?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "We propose two point-wise diagnostic scores, namely perturbation scores and singularity scores, to quantify the severity of the two types of map discontinuity at each embedding point."
        ],
        "final_answer": "The perturbation score could be used alongside the singularity score as a complementary diagnostic.",
        "relevant_elements": [
            "singularity score",
            "fracture-inducing discontinuity"
        ],
        "id": 298,
        "masked_question": "What alternative methods could complement [mask1] for detecting fracture-inducing discontinuity?",
        "masked_number": 1,
        "masked_elements": [
            "singularity score"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.16608v1_figure_1.png",
        "paperid": "2410.16608v1",
        "paper_path": "./papers/2410.16608v1.json",
        "figure_id": "2410.16608v1_figure_1.png",
        "caption": "Figure 1: Overview: assessment of embeddings by neighbor embedding methods with an example of image data. a We use a standard pre-trained convolutional neural network (CNN) to obtain features of image samples from the CIFAR10 dataset, and then\nvisualize the features using a neighbor embedding method, specifically t-SNE. b Basic ideas of singularity scores and perturbation scores.\nc t-SNE tends to embed image features into separated clusters even for images with ambiguous semantic meanings (as quantified by higher entropy of predicted class probabilities by the CNN). Perturbation scores identify the embedding points which have ambiguous class membership but less visual uncertainty.\nd Incorrect choice of the hyperparameter leads to visual fractures (FI discontinuity), which is more severe with a smaller perplexity.\nWe recommend choosing the perplexity no smaller than the elbow point.",
        "qtype": "Others",
        "response": "Here, “[mask1]” is the Singularity Score that the authors introduce to flag tiny, spuriously “fractured” islands in a t-SNE/UMAP projection.  In principle you could—and we would recommend you do—layer on additional, complementary diagnostics such as:\n\n  •  A Perturbation‐based Stability Score (the “Perturbation Score” from the same paper), which measures how much a point’s embedding jumps if you re‐optimize after a tiny change in the input.  \n  •  Standard neighborhood‐distortion metrics from the manifold‐learning literature, for example  \n     –  Trustworthiness and Continuity  \n     –  The Local Continuity Meta‐Criterion (LCMC)  \n     –  k-Nearest‐Neighbor recall/precision or Jaccard‐index distortion  \n  •  Leave-one-out or influence‐function–based diagnostics (e.g. the Johnson et al. or Xia et al. scores for neighborhood distortion)  \n  •  Sweeps over hyperparameters (e.g. t-SNE perplexity or UMAP’s n_neighbors) and looking for “elbows” in global distortion or stress plots  \n\nAll of these will pick up different facets of where a neighbor‐embedding map is tearing or over‐fragmenting, and together with the Singularity Score give you a much fuller picture of potential fracture‐inducing discontinuities."
    },
    {
        "question": "What is the reasoning behind freezing existing embeddings when computing the LOO-map for new input extension?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "It is based on a classical strategy from statistics known as the leave-one-out (LOO) method, which posits that adding, deleting, or changing a single input point has negligible effects on the overall inferential results.",
            "Using LOO, we can decouple the pairwise interaction in the algorithm : we add a new input point  to  and freeze  in the optimization problem, allowing only one free variable ."
        ],
        "final_answer": "The existing embeddings are frozen to embody the leave-one-out assumption that inserting one more point hardly alters the original embedding. Freezing decouples the pairwise interactions so that only the new point’s embedding needs to be optimized, yielding a well-defined extension map.",
        "relevant_elements": [
            "LOO-map",
            "embeddings"
        ],
        "id": 299,
        "masked_question": "What is the reasoning behind freezing existing [mask1] when computing the LOO-map for new input extension?",
        "masked_number": 1,
        "masked_elements": [
            "embeddings"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.16608v1_figure_1.png",
        "paperid": "2410.16608v1",
        "paper_path": "./papers/2410.16608v1.json",
        "figure_id": "2410.16608v1_figure_1.png",
        "caption": "Figure 1: Overview: assessment of embeddings by neighbor embedding methods with an example of image data. a We use a standard pre-trained convolutional neural network (CNN) to obtain features of image samples from the CIFAR10 dataset, and then\nvisualize the features using a neighbor embedding method, specifically t-SNE. b Basic ideas of singularity scores and perturbation scores.\nc t-SNE tends to embed image features into separated clusters even for images with ambiguous semantic meanings (as quantified by higher entropy of predicted class probabilities by the CNN). Perturbation scores identify the embedding points which have ambiguous class membership but less visual uncertainty.\nd Incorrect choice of the hyperparameter leads to visual fractures (FI discontinuity), which is more severe with a smaller perplexity.\nWe recommend choosing the perplexity no smaller than the elbow point.",
        "qtype": "Design_Rationale",
        "response": "In the paper’s own words, when we “add a new input point x* to X and freeze the existing embeddings in the optimization problem, allowing only one free variable y*,” what we are doing is exactly implementing a leave-one-out (LOO) style approximation of the full neighbor-embedding black box.  The reason we hold the previously computed embedding points fixed (“freeze” them) is two-fold:\n\n 1.  Decouple the pairwise interactions.  In a typical t-SNE or UMAP run, every point is tugging on every other point.  By freezing the n existing embeddings, we collapse all of that complexity into one single optimization: only y* moves.  \n 2.  Enshrine the LOO assumption that “adding a single extra datum should barely perturb the rest of the embedding.”  That lets us turn a non-parametric, global procedure into a local, almost‐parametric map f(x) ≈ y* without re-solving the full n+1-point problem.  \n\nIn short, freezing the old embeddings both makes the computation of y* vastly cheaper and gives us a bona fide “embedding map” f that we can evaluate point‐by‐point."
    },
    {
        "question": "What motivates computing perturbation score as L2 distance between f(x_i+η) and y_i?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Perturbation Score",
            "f(x_i+η)"
        ],
        "id": 300,
        "masked_question": "What motivates computing [mask1] as L2 distance between [mask2] and y_i?",
        "masked_number": 2,
        "masked_elements": [
            "Perturbation Score",
            "f(x_i+η)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.16608v1_figure_1.png",
        "paperid": "2410.16608v1",
        "paper_path": "./papers/2410.16608v1.json",
        "figure_id": "2410.16608v1_figure_1.png",
        "caption": "Figure 1: Overview: assessment of embeddings by neighbor embedding methods with an example of image data. a We use a standard pre-trained convolutional neural network (CNN) to obtain features of image samples from the CIFAR10 dataset, and then\nvisualize the features using a neighbor embedding method, specifically t-SNE. b Basic ideas of singularity scores and perturbation scores.\nc t-SNE tends to embed image features into separated clusters even for images with ambiguous semantic meanings (as quantified by higher entropy of predicted class probabilities by the CNN). Perturbation scores identify the embedding points which have ambiguous class membership but less visual uncertainty.\nd Incorrect choice of the hyperparameter leads to visual fractures (FI discontinuity), which is more severe with a smaller perplexity.\nWe recommend choosing the perplexity no smaller than the elbow point.",
        "qtype": "Design_Rationale",
        "response": "Here, [mask1] is the “perturbation score” and [mask2] is the LOO‐map evaluation f(xᵢ + η).  The very purpose of the perturbation score is to measure how “jumpy” the embedding is at point i under a small but finite change in the input.  Concretely, by computing yᵢ′ = f(xᵢ + η) (the new embedding of the slightly perturbed input) and taking the Euclidean (L₂) distance ‖yᵢ′ − yᵢ‖₂, we get a direct, interpretable scalar that tells us how far the embedding map “jumps.”  Points where this distance is large are precisely those locations where the embedding map is discontinuous or over‐confidently fractured, so L₂ displacement is the natural choice to quantify that effect."
    },
    {
        "question": "What advantage does globally sharing Transformer Block weights bring to interleaving DiM blocks?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Motivated by Zamba [1], we introduce the globally-shared transformer (attention) block. This shared attention block is added after each of four DiM blocks as shown in Fig. 1 since we want to preserve the continuity of the 4-sweep alternative scanning order.",
            "By using shared weights, we significantly reduce the number of parameters introduced by different attention blocks.",
            "This layer complements the flow of Mamba since transformers excel at extracting global relations without relying on manually defined orders of input sequences, as in Mamba."
        ],
        "final_answer": "By sharing the transformer block’s weights globally across the interleaved DiM blocks, the model drastically cuts down on additional parameters while still providing a global, order-invariant mixing layer that complements Mamba’s sequence-based processing.",
        "relevant_elements": [
            "Transformer Block",
            "DiM Block",
            "Globally shared weights"
        ],
        "id": 301,
        "masked_question": "What advantage does [mask1] bring to interleaving [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Globally shared weights",
            "Transformer Block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04168v1_figure_1.png",
        "paperid": "2411.04168v1",
        "paper_path": "./papers/2411.04168v1.json",
        "figure_id": "2411.04168v1_figure_1.png",
        "caption": "Figure 1: Overview of DiMSUM architecture.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning in steps:\n\n1.  From the picture, the red‐boxed region (mask1) is the “DiMSUM Block,” which is exactly the little pair of blocks in each stage—a Transformer (attention) block followed by a DiM (Mamba) block, all of whose Transformer weights are shared globally.  \n2.  The blue‐boxed region (mask2) is the internal structure of the DiM (Mamba) block itself—the stack of Layer-Norms, MLPs, Spatial‐Frequency Mamba modules, gated MLPs, etc.  \n3.  In pure Mamba (DiM) you get very strong long-range dependency modeling via its carefully ordered recurrence, but it has no explicit mechanism for “in‐context” or global information, and it is sensitive to the scan order you choose.  \n4.  By interleaving each DiM block with a **single Transformer block whose weights are shared across every stage**, DiMSUM:  \n   •  Injects true self-attention into the flow, giving the network the ability to “look anywhere” in the sequence and learn global correlations that the strictly ordered Mamba would miss,  \n   •  Neutralizes the scan-order bias of pure Mamba,  \n   •  Retains Mamba’s long-range strengths,  \n   •  And—because the Transformer is *weight-shared*—adds almost no extra parameters.  \n5.  Empirically, this hybrid (DiM ↔ shared-Transformer) interleaving drives down FID and boosts image quality far beyond using stacks of either module alone.  \n\nAnswer  \nWhat the DiMSUM Block (mask1) brings to interleaving the DiM (Mamba) block (mask2) is exactly this: the shared Transformer injects global, in-context self-attention into every Mamba sweep, curing Mamba’s order-dependence and lack of global view, while adding negligible parameter cost thanks to weight sharing."
    },
    {
        "question": "What rationale drives swapping Sweep scan Mamba and Window scan Mamba queries in Cross-Attention fusion layer?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Cross-Attention fusion layer. Given f_spatial and f_freq are spatial and wavelet features obtained by sweep and window scan. We combine these features using a cross-attention fusion layer as follows:",
            "More specifically, we first compute each feature’s query (q), key (k), and value (v) using linear layers. To fuse the information between spatial and wavelet features, we do cross-attention by swapping the queries (q_spatial <-> q_wavelet) of spatial and wavelet before applying a self-attention module onto each key, query, and value triplet."
        ],
        "final_answer": "They swap the queries of the two modalities so that each modality (spatial vs. frequency) attends to the other’s keys and values, thereby fusing spatial and wavelet information via cross-attention.",
        "relevant_elements": [
            "Sweep scan Mamba",
            "Window scan Mamba",
            "Cross-Attention fusion layer"
        ],
        "id": 302,
        "masked_question": "What rationale drives swapping [mask1] and [mask2] queries in Cross-Attention fusion layer?",
        "masked_number": 2,
        "masked_elements": [
            "Sweep scan Mamba",
            "Window scan Mamba"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04168v1_figure_1.png",
        "paperid": "2411.04168v1",
        "paper_path": "./papers/2411.04168v1.json",
        "figure_id": "2411.04168v1_figure_1.png",
        "caption": "Figure 1: Overview of DiMSUM architecture.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In the DiMSUM “cross‐attention fusion” box (far right of Fig 1) we have two streams of features:\n    –  f_spatial (produced by the sweep-scan Mamba, red box in the overall diagram)  \n    –  f_freq    (produced by the window-scan / wavelet Mamba, blue box in the overall diagram)\n\n2.  For each stream we build the usual attention triplets:\n    –  q₁, k₁, v₁ ← linear projections of f_spatial  \n    –  q₂, k₂, v₂ ← linear projections of f_freq  \n\n3.  If we simply did self-attention (q₁ attends to k₁/v₁, q₂ to k₂/v₂) we would end up fusing spatial with itself and frequency with itself.  That would never mix the two modalities.\n\n4.  To force **cross-modal** attention, the paper’s trick is to **swap the queries** before computing the two attentions:\n    –  Use q₁′=q₂ together with k₁,v₁  →  “spatial keys/values” get queried by the **frequency** query  \n    –  Use q₂′=q₁ together with k₂,v₂  →  “frequency keys/values” get queried by the **spatial** query  \n\n5.  Each branch now attends **across** modalities rather than just to itself, so spatial features absorb information from the frequency branch and vice versa.\n\n6.  Finally the two cross-attention outputs are concatenated and linearly projected to produce the fused feature.\n\n**In short**: swapping the red-box (sweep/spatial) query with the blue-box (window/frequency) query turns two self-attentions into two cross-attentions, guaranteeing bidirectional information flow between spatial and frequency streams."
    },
    {
        "question": "How does the cross-attention fusion layer fuse outputs of Sweep scan Mamba and Window scan Mamba?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Cross-Attention fusion layer. Given f_spatial and f_wavelet are spatial and wavelet features obtained by sweep and window scan. We combine these features using a cross-attention fusion layer as follows:",
            "More specifically, we first compute each feature’s query (Q), key (K), and value (V) using linear layers. To fuse the information between spatial and wavelet features, we do cross-attention by swapping the queries of spatial and wavelet before applying a self-attention module onto each key, query, and value triplet. Finally, we concat the outputs of two cross attentions by channel followed by a linear projection to obtain the output feature."
        ],
        "final_answer": "The fusion layer takes the two token sequences (the Sweep‐scan Mamba output as “spatial” and the Window‐scan Mamba output as “wavelet”), projects each into queries, keys, and values, then swaps the two query sets and runs self‐attention on each (so spatial keys/values attend to wavelet queries and vice versa). The two resulting attention outputs are concatenated along the channel dimension and passed through a linear projection to yield the fused feature.",
        "relevant_elements": [
            "Sweep scan Mamba",
            "Window scan Mamba",
            "Cross-Attention fusion layer"
        ],
        "id": 303,
        "masked_question": "How does the [mask1] fuse outputs of Sweep scan Mamba and Window scan Mamba?",
        "masked_number": 1,
        "masked_elements": [
            "Cross-Attention fusion layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04168v1_figure_1.png",
        "paperid": "2411.04168v1",
        "paper_path": "./papers/2411.04168v1.json",
        "figure_id": "2411.04168v1_figure_1.png",
        "caption": "Figure 1: Overview of DiMSUM architecture.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:\n\n1. The red-boxed module sits immediately after the IDWT that brings back the Sweep (spatial) and Window (frequency) Mamba outputs into the same feature space.  \n2. The text calls this the “Cross-Attention fusion layer.”  \n3. That layer takes the two streams—call them f_spatial (from Sweep Mamba) and f_freq (from Window Mamba)—and projects each one through three small linear heads to get queries Q, keys K, and values V.  \n4. To let each stream attend to the other, it *swaps* the queries: the spatial K/V are queried by the frequency Q, and the frequency K/V are queried by the spatial Q.  \n5. Two separate attention outputs result; these are then concatenated along the channel dimension and passed through a final linear projection.  \n\nAnswer:\n\nThe red-boxed block fuses the Sweep and Window Mamba outputs by computing Q/K/V for each feature stream, swapping their queries to perform cross-attention (so each stream attends to the other), then concatenating the two attended outputs and projecting them with a linear layer."
    },
    {
        "question": "How are weights shared among Transformer Blocks inserted between DiM Blocks?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "we introduce the globally-shared transformer (attention) block. This shared attention block is added after each of four DiM blocks as shown in Fig. 1 since we want to preserve the continuity of the 4-sweep alternative scanning order.",
            "By using shared weights, we significantly reduce the number of parameters introduced by different attention blocks."
        ],
        "final_answer": "All of the Transformer Blocks inserted between DiM Blocks use the exact same parameters: a single Transformer Block is inserted after each group of four DiM Blocks and its weights are shared (re-used) globally across all those insertion points.",
        "relevant_elements": [
            "Transformer Block",
            "DiM Block"
        ],
        "id": 304,
        "masked_question": "How are weights shared among [mask1] inserted between DiM Blocks?",
        "masked_number": 1,
        "masked_elements": [
            "Transformer Block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.04168v1_figure_1.png",
        "paperid": "2411.04168v1",
        "paper_path": "./papers/2411.04168v1.json",
        "figure_id": "2411.04168v1_figure_1.png",
        "caption": "Figure 1: Overview of DiMSUM architecture.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in Fig. 1 is not a fresh transformer every time, but a *single* transformer (attention) block that is repeatedly “slotted in” between the DiM blocks.  In the text (§ 3.4 “Globally‐shared attention block”) they explain that rather than instantiating an independent transformer at each insertion point, they *tie* the weights—i.e. they use one set of parameters for that attention block and reuse it after each DiM block.  This global weight‐sharing is what dramatically cuts down on parameter count."
    },
    {
        "question": "How does the self-attention module compute its Q, K, V operations over VGG16-extracted features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Self-Attention",
            "Pretrained VGG16"
        ],
        "id": 305,
        "masked_question": "How does the self-attention module compute its Q, K, V operations over [mask1]-extracted features?",
        "masked_number": 1,
        "masked_elements": [
            "Pretrained VGG16"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.00718v1_figure_2.png",
        "paperid": "2409.00718v1",
        "paper_path": "./papers/2409.00718v1.json",
        "figure_id": "2409.00718v1_figure_2.png",
        "caption": "Figure 2: The proposed framework for the combination of the Fundus and OCT images for the classification of AMD",
        "qtype": "Implementation_Details",
        "response": "The key thing to notice is that nothing magic is hiding behind that red box – it is simply the stack of feature‐maps that come out of each of the three VGG16 “encoder” paths.  Those feature‐maps are then handed to the self–attention block exactly as you would in any Transformer‐style attention:\n\n 1.  Take the VGG16‐extracted features (call them F; in the figure they are the outputs of the red cubes).  \n 2.  Run three parallel linear (or 1×1–conv) layers over F to produce  \n    •  Q = F ⋅ W_Q  \n    •  K = F ⋅ W_K  \n    •  V = F ⋅ W_V  \n    where W_Q, W_K and W_V are learned projection matrices.  \n 3.  Compute the usual scaled‐dot‐product attention:  \n       Attention(Q,K,V) = softmax( Q Kᵀ / √d_k ) V  \n\nSo in one sentence: the self–attention block simply takes the feature‐maps coming out of the red‐boxed VGG16 encoders, projects them via learned Q, K and V matrices, and then does scaled‐dot‐product attention on those projections."
    },
    {
        "question": "In the fusion module, how are concatenated encoder and attention features integrated before RFC classification?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "Then, features extracted from the pre-trained VGG16 and self-attention module are concatenated. Finally, we fused all the extracted features from each path and fed them to the RFC for the classification of AMD.",
            "The attention features and extracted prior features from the pre-trained model from each path are fused together by concatenation for a better representation of each feature acquired through the Fundus and OCT images, which are used as input for the classification model."
        ],
        "final_answer": "In the fusion module, the features produced by each VGG16 encoder and their corresponding self-attention outputs are first concatenated together, and then this single fused feature vector is passed directly into the Random Forest Classifier for the final AMD classification.",
        "relevant_elements": [
            "Fusion Module",
            "RFC"
        ],
        "id": 306,
        "masked_question": "In the [mask1], how are concatenated encoder and attention features integrated before [mask2] classification?",
        "masked_number": 2,
        "masked_elements": [
            "Fusion Module",
            "RFC"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.00718v1_figure_2.png",
        "paperid": "2409.00718v1",
        "paper_path": "./papers/2409.00718v1.json",
        "figure_id": "2409.00718v1_figure_2.png",
        "caption": "Figure 2: The proposed framework for the combination of the Fundus and OCT images for the classification of AMD",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step fill‐in:\n\n1. The red‐boxed block ([mask1]) is the “Fusion Module.”  \n2. The blue‐boxed block ([mask2]) is the “RFC” (Random Forest Classifier).  \n\nSo the question becomes:\n\n  “In the Fusion Module, how are concatenated encoder and attention features integrated before RFC classification?”\n\nAnswer:\n\n  Within the Fusion Module, the VGG16‐extracted features and their corresponding self‐attention outputs from all three encoder paths are simply concatenated (along the channel dimension) into one large fused feature tensor. That fused tensor is then flattened into a single feature vector, which is handed off to the RFC for the final three‐way AMD classification (Normal, Neovascular, Non‐Neovascular)."
    },
    {
        "question": "How does Self-Attention complement pretrained VGG16 to enhance global feature modeling beyond local multiscale embeddings?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "The extracted features through the pre-trained VGG16 model are confined by structure to concentrate primarily on local features of fundus and OCT images while incompetent to acquire the global feature information. Self-attention focuses on specific global features of the images.",
            "In the proposed framework, the extracted features from each path through the pre-trained VGG16 model are used as input for the self-attention module.",
            "The attention features and extracted prior features from the pre-trained model from each path are fused together by concatenation for a better representation of each feature acquired through the Fundus and OCT images, which are used as input for the classification model."
        ],
        "final_answer": "In the MCGAEc framework, pretrained VGG16 at each color-space and scale path yields rich local and multiscale feature embeddings but remains limited in modeling long-range dependencies across the image. By feeding these VGG16-extracted features into a self-attention module, the network computes weighted associations (queries, keys, values) that highlight salient, global feature interactions and inter-variability between paths. Finally, concatenating the self-attention outputs with the original VGG16 features produces a fused representation that combines both fine-grained local details and global contextual information, thereby enhancing the model’s overall feature expressiveness beyond what multiscale VGG16 alone can capture.",
        "relevant_elements": [
            "Self-Attention",
            "pretrained VGG16",
            "Multiscale"
        ],
        "id": 307,
        "masked_question": "How does [mask1] complement [mask2] to enhance global feature modeling beyond local multiscale embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "Self-Attention",
            "pretrained VGG16"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.00718v1_figure_2.png",
        "paperid": "2409.00718v1",
        "paper_path": "./papers/2409.00718v1.json",
        "figure_id": "2409.00718v1_figure_2.png",
        "caption": "Figure 2: The proposed framework for the combination of the Fundus and OCT images for the classification of AMD",
        "qtype": "Literature_Background",
        "response": "First, let’s be crystal‐clear what the two blocks are:\n\n  • “[mask2]” (the blue‐outlined box) is simply the stack of three VGG-16 encoders—one per color‐space (YCbCr, HSV) or modality (grayscale OCT)—each of which spits out “local” multiscale feature maps.  \n  • “[mask1]” (the red box) is the self–attention sub‐network (and the little concatenation blocks immediately downstream of it) that sits on top of each VGG path.\n\nNow, why do we need both?\n\n1.  Locality of CNNs (the VGG-16 paths)  \n    – Each VGG-16 encoder, by its very architecture, is a succession of small (3×3) convolutions and local pooling.  Even though we feed it multiple scales, its receptive‐field at any layer is still fundamentally “local.”  It is excellent at picking out edges, textures, vessel‐like structures, etc., at different resolutions, but it does not know “how those bits fit together globally.”\n\n2.  Global context from self–attention  \n    – The self–attention block computes a set of queries Q, keys K and values V over the entire feature map.  In practice this means every position (every pixel‐feature) can look at every other position, learn which locations are most correlated, and re‐weight them accordingly.  \n    – In other words, self–attention explicitly models long-range dependencies (for example, the fact that a faint drusen in the periphery and a blob of subretinal fluid in the macula may jointly signal neovascular AMD) whereas a purely convolutional path would have to rely on stacking dozens of layers (and very large receptive fields) to see that relationship.\n\n3.  Fusion of local + global  \n    – In our figure the output of the self–attention block is concatenated with the original VGG-16 feature map for that path (that’s the little two-cube icon inside the red box).  This “skip‐concat” ensures we do not throw away any of the fine‐grained, multiscale local cues, but we augment them with a globally aware weighting.\n\n4.  Net effect on classification  \n    – The Random Forest classifier at the very end now sits on features that are both \n      a) richly detailed at multiple scales and in multiple color‐spaces, and  \n      b) have been globally re‐weighted so that the most diagnostically salient structures anywhere in the image are brought to the fore.\n\nIn short, the self–attention block ([mask1]) takes the purely local, multiscale embeddings from the three VGG-16 encoders ([mask2]), computes long–range correlations across the entire image, and then fuses those global‐context features back in.  Together they give you a representation that knows both “where the little bits are” and “how they relate to one another at a distance,” which a vanilla multiscale CNN by itself cannot do."
    },
    {
        "question": "How does RFC build on decision tree ensembles to handle concatenated Self-Attention and pretrained VGG16 features?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "The computed features through the modality-specific multiscale color space encoder model with self-attention are transferred into the supervised machine learning RFC [19].",
            "The RFC comprises an amalgamation of tree classifiers where a particular classifier is acquired through a random vector, which is sampled individually from the input data, and the respective tree provides a unit vote for the most prevalent class to categorize input data.",
            "The RFC is utilized for the classification of the input data, which relies on the utilization of randomly chosen features at each node to expand a tree.",
            "In RFC, features are illustrated on the inner nodes, which are called decision nodes, and enable to generate the predictions from a sequence of feature-based fragmentation.",
            "RFC employs a collection of random decision trees and integrates them concurrently to construct a robust model that is less acute to the training data.",
            "The RFC algorithm is capable of handling higher dimensional data and utilizing an enormous number of trees in the combination.",
            "The output of the RFC is computed by a majority counting of votes obtained through trees.",
            "Here, RFC is used, which includes considerably lower computational complexity as each particular tree exclusively utilizes a part of the input vector in a Random Forest."
        ],
        "final_answer": "RFC takes the concatenated feature vector coming from the pretrained VGG16 encoders and the self-attention module and feeds it into a Random Forest.  This Random Forest builds an ensemble of decision trees, each trained on a bootstrap sample of the data and a random subset of the concatenated features at each split.  Each tree votes for a class, and the forest aggregates these votes to give the final prediction.  By selecting only a subset of features per tree, the method efficiently handles the high-dimensional concatenated features with low computational cost and high robustness.",
        "relevant_elements": [
            "RFC",
            "Self-Attention",
            "pretrained VGG16"
        ],
        "id": 308,
        "masked_question": "How does [mask1] build on decision tree ensembles to handle concatenated Self-Attention and pretrained VGG16 features?",
        "masked_number": 1,
        "masked_elements": [
            "RFC"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.00718v1_figure_2.png",
        "paperid": "2409.00718v1",
        "paper_path": "./papers/2409.00718v1.json",
        "figure_id": "2409.00718v1_figure_2.png",
        "caption": "Figure 2: The proposed framework for the combination of the Fundus and OCT images for the classification of AMD",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is the Random Forest Classifier (RFC).  In our pipeline we first concatenate the VGG-16–extracted and self-attention–refined feature vectors into one high-dimensional representation.  Instead of feeding that into a single decision tree, the RFC builds on the usual decision‐tree ensemble idea by:\n\n1. Drawing a (bootstrapped) random subset of the training samples for each tree  \n2. At each split in each tree, considering only a random subset of the total feature dimensions (i.e. a random subspace of the concatenated vector)  \n3. Growing many such random trees in parallel  \n4. Letting every tree “vote” on the class label and taking the majority vote as the final prediction  \n\nBecause of the randomization both in sampling cases and in selecting features at each node, the forest is far less prone to overfitting than a single tree—even when the input is a very large, concatenated feature set of self‐attention and VGG-16 embeddings."
    },
    {
        "question": "How does Student leverage Teacher: Fine-tuned Model through distillation to minimize private computation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Teacher: Fine-tuned Model",
            "Student"
        ],
        "id": 310,
        "masked_question": "How does [mask1] leverage Teacher: Fine-tuned Model through distillation to minimize private computation?",
        "masked_number": 1,
        "masked_elements": [
            "Student"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.03561v1_figure_1.png",
        "paperid": "2408.03561v1",
        "paper_path": "./papers/2408.03561v1.json",
        "figure_id": "2408.03561v1_figure_1.png",
        "caption": "Figure 1: End-to-end workflow of our system. The private and public components are highlighted in red and blue, respectively. The gray region represents our fine-tuning framework, Marill, that outputs an MPC-minimized inference model. Note that Marill differs from prior works such as MPCFormer mpcformer  since they output a (fully) fine-tuned model after fine-tuning. Consequently, the inference phase (steps 3-5) in our system also differs from the prior works in two ways: (i) only a part of the inference model is private, and thus, only that part is fed to the MPC engine, and (ii) instead of directly feeding its private input, the client inputs the partial inference result of the model s public component on its private input.\nIn the figure, we only show single token generation; subsequent tokens can be generated similarly since the client has access to all the tokens generated so far.\nAdditionally, we only show two parties each running an MPC engine instance. Orthogonally, there is also an additional helper party in some protocols that helps speed up secure inference (Appendix A).",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the “student” model in the red box.  Here’s what happens, step by step:\n\n1.  We start from a large, pre‐trained public model and the private, fine‐tuned teacher (the red model on the right of the “+” in the gray box).\n\n2.  Before doing any secure inference, we apply an MPC‐minimizing transformation to the public model to carve out a lightweight student architecture.  That student is architecturally designed so that most of its layers can remain public (and run in the clear), leaving only a small private subnetwork that must run inside MPC.\n\n3.  To teach that small private subnetwork to do the same job as the full teacher, we run a standard knowledge‐distillation step (step ② in the figure): we feed inputs through the private teacher, collect its soft targets or logits, and train the student to match those outputs.\n\n4.  Because the student has been distilled to reproduce the teacher’s behavior, it needs only its tiny private component (plus the public backbone) to get essentially the same accuracy.  At inference time the client runs the public backbone on its own, and only the small distilled private piece is evaluated under MPC—dramatically cutting down the amount of private computation."
    },
    {
        "question": "How does MPC-minimizing Transformation reshape the foundational model to enable partitioned public and private inference?",
        "relevant_section_ids": [
            "5.1"
        ],
        "relevant_context": [
            "Our solution (Fig. 2(a)) effectively leverages public weights by deferring fine-tuning to only the final layers of the transformer, thereby also deferring MPC to these final layers.",
            "During inference, the client receives the weights for the bottom layers (identical to the open-source pre-trained model) from the server, computes the output of these layers locally, and then engages in MPC with the server for the top layers."
        ],
        "final_answer": "By applying an MPC-minimizing transformation (layer-freezing), the foundational model is restructured so that all early layers remain the public pre-trained weights and only the final layers are fine-tuned (kept private). At inference time, the public bottom layers are executed locally by the client, and only the private top layers are jointly evaluated under MPC with the server, thereby cleanly partitioning public and private computation.",
        "relevant_elements": [
            "MPC-minimizing Transformation",
            "Foundational Model"
        ],
        "id": 311,
        "masked_question": "How does [mask1] reshape the [mask2] to enable partitioned public and private inference?",
        "masked_number": 2,
        "masked_elements": [
            "MPC-minimizing Transformation",
            "Foundational Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.03561v1_figure_1.png",
        "paperid": "2408.03561v1",
        "paper_path": "./papers/2408.03561v1.json",
        "figure_id": "2408.03561v1_figure_1.png",
        "caption": "Figure 1: End-to-end workflow of our system. The private and public components are highlighted in red and blue, respectively. The gray region represents our fine-tuning framework, Marill, that outputs an MPC-minimized inference model. Note that Marill differs from prior works such as MPCFormer mpcformer  since they output a (fully) fine-tuned model after fine-tuning. Consequently, the inference phase (steps 3-5) in our system also differs from the prior works in two ways: (i) only a part of the inference model is private, and thus, only that part is fed to the MPC engine, and (ii) instead of directly feeding its private input, the client inputs the partial inference result of the model s public component on its private input.\nIn the figure, we only show single token generation; subsequent tokens can be generated similarly since the client has access to all the tokens generated so far.\nAdditionally, we only show two parties each running an MPC engine instance. Orthogonally, there is also an additional helper party in some protocols that helps speed up secure inference (Appendix A).",
        "qtype": "Experimental_Results",
        "response": "Here’s what’s going on, step by step:\n\n1.  In the original (“foundational,” blue‐boxed) model you have an entire stack of transformer layers, plus all the standard attention and feed-forward weights.  \n2.  During our MPC-minimizing transform (the step that produces the red-boxed “MPC-minimized model”) we do three things:  \n    •  **Layer freezing**.  We freeze (and therefore keep public) the bottom \\(L\\!-\\!k\\) layers of the pre-trained transformer and only fine-tune the top \\(k\\) layers on the private data.  That means the bulk of the network can be run in plaintext by the client.  \n    •  **LoRA partitioning**.  Instead of merging the low-rank LoRA adapters back into the public weights (which would force all of those multiplications into MPC), we keep the LoRA updates as a separate, small private subnetwork.  \n    •  **Head-merging**.  We reorganize (merge) attention heads so that we reduce the *number* of heads (thus reducing expensive non-arithmetic operations under MPC) while preserving the *total* parameter count.  \n\n3.  The net effect is that the red-boxed model is exactly the same function as the blue-boxed foundational model, but **factored** into  \n    – A big, public sub-model (the frozen base layers + public projection matrices), and  \n    – A small, private tail (the fine-tuned top layers, the separate LoRA matrices, and the merged-head attention).  \n\n4.  At inference time the client can run the public sub-model on its private input entirely in the clear, producing an intermediate activation.  Only that small private tail goes into the MPC engine with the server.  \n\nBy “reshaping” the foundational model in this way—freezing early layers, peeling off LoRA adapters, and merging heads—you end up with a two-piece network that cleanly partitions into (i) a large public portion you run locally and (ii) a minimal private portion you run under MPC."
    },
    {
        "question": "How does merging Text Encoder with 3D U-Net features streamline Mask Predictor training under binary supervision?",
        "relevant_section_ids": [
            "3.1",
            "3.2",
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Section 3.1: “we design Point-Word Cross-Modal Alignment (PWCA) module … Cross-attention module aligns point-wise and word-wise feature to get the language-aware visual feature.”",
            "Section 3.2: “Query Mask Predictor (QMP) module … takes fused feature Z and learnable queries Q as input and progressively distinguishes the referring target by multi-layer cross-modal transformers. Finally, we extract the proposal masks M based on query embeddings and fused feature Z.”",
            "Section 3.3: “we adopt Query-Sentence Alignment (QSA) to better align the query feature with sentence-level feature … The final mask prediction M is produced by weighted sum of similarity score and proposed mask prediction M′.”",
            "Section 3.4: “we take the Referring 3D Segmentation task as segmentation task with only binary mask Y. Here we utilize the Binary Cross-Entropy (BCE) loss function to compute the segmentation loss.”"
        ],
        "final_answer": "By fusing the text encoder’s word-level and sentence-level features directly into the multi-scale 3D U-Net features via the PWCA cross-attention and QSA alignment modules, the network produces language-aware visual representations that feed straight into a single-stage Query Mask Predictor. This design allows training under only binary mask supervision (using BCE loss), eliminating the need for separate instance or semantic labels and simplifying the entire mask prediction pipeline.",
        "relevant_elements": [
            "Text Encoder",
            "3D U-Net",
            "Mask Predictor"
        ],
        "id": 313,
        "masked_question": "How does merging [mask1] with 3D U-Net features streamline Mask Predictor training under binary supervision?",
        "masked_number": 1,
        "masked_elements": [
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.13294v2_figure_1.png",
        "paperid": "2410.13294v2",
        "paper_path": "./papers/2410.13294v2.json",
        "figure_id": "2410.13294v2_figure_1.png",
        "caption": "Figure 1: Comparison between the two-stage method and our single-stage method. (a) The two-stage method initially performs instance segmentation with instance labels then semantic labels to get the instance proposals and bases on the provided query to match the most relevant instance proposal. (b) Our single-stage method only utilizes the binary mask of the described object for training and integrates language and vision features during feature extraction.",
        "qtype": "Experimental_Results",
        "response": "Here is the step-by-step reasoning:\n\n1. In the single-stage diagram, the red‐boxed module labeled “[mask1]” is the Text Encoder’s output (a sentence‐level embedding of the query).  \n2. Instead of running a separate two-stage pipeline (first doing instance segmentation, then matching one of those instances to the language), we inject that sentence embedding straight into the 3D U-Net feature extractor. Concretely, at one or more encoder stages the network concatenates or cross-attends the U-Net’s multi-scale point features with the Text Encoder’s sentence feature.  \n3. This early fusion produces a “language-aware” 3D feature map: every spatial location in the U-Net now carries both geometric/color context and the semantics of “what to look for.”  \n4. The Mask Predictor then simply decodes these fused features into a single binary mask, trained with a standard binary cross‐entropy loss (plus the area‐regularization and contrastive losses).  \n5. Because the query semantics are already baked into the 3D features, the Mask Predictor never has to learn to (a) propose object instances, (b) classify them into object categories, or (c) match them to the sentence. It only needs to learn “given these language-aware features, label each point foreground vs. background.”  \n6. In short, merging the Text Encoder output with U-Net features lets you go straight to a binary-mask prediction and train end-to-end under only binary supervision, collapsing what used to be a multi-stage, heavily-annotated process into a single, BCE-trained network."
    },
    {
        "question": "What limitations arise from Mask Predictor using only Binary Label supervision in complex scenes?",
        "relevant_section_ids": [
            "1",
            "3.4"
        ],
        "relevant_context": [
            "However, 3D point cloud inherently provide a higher level of complexity and a large scale. There exists numerous different objects in a single 3D scene compared to the referring image segmentation task. Besides, binary mask has less semantic meanings compared to instance labels and semantic labels. These challenges make it difficult to supervise our model to localize and segment target objects with only binary mask.",
            "For Referring 3D Segmentation task, each query always corresponds to one target object in the point cloud scene. The target objects occupy a smaller area in the large scale of 3D point cloud. As a result, the predicted mask often includes backgrounds or other objects.",
            "Area regularization loss uniformly penalizes the predicted probabilities of all points, which can reduce the majority of the background points. However, the network struggles to differentiate between objects that possess characteristics similar to those described target object in the latent space."
        ],
        "final_answer": "Using only binary‐mask supervision in complex 3D scenes makes it hard for the Mask Predictor to accurately localize and segment the correct object. Because point clouds contain many objects and large background regions, the model tends to include background or unrelated objects in its mask, and it cannot reliably distinguish between objects with similar appearance or geometry when trained with only a coarse binary label.",
        "relevant_elements": [
            "Mask Predictor",
            "Binary Label"
        ],
        "id": 314,
        "masked_question": "What limitations arise from [mask1] using only Binary Label supervision in complex scenes?",
        "masked_number": 1,
        "masked_elements": [
            "Mask Predictor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.13294v2_figure_1.png",
        "paperid": "2410.13294v2",
        "paper_path": "./papers/2410.13294v2.json",
        "figure_id": "2410.13294v2_figure_1.png",
        "caption": "Figure 1: Comparison between the two-stage method and our single-stage method. (a) The two-stage method initially performs instance segmentation with instance labels then semantic labels to get the instance proposals and bases on the provided query to match the most relevant instance proposal. (b) Our single-stage method only utilizes the binary mask of the described object for training and integrates language and vision features during feature extraction.",
        "qtype": "Others",
        "response": "The red‐boxed “Mask Predictor” in our single-stage pipeline is trained with only a per-point foreground/background (binary) mask as supervision.  In isolation, that binary mask carries almost no information about *which* object it is, only that it belongs to the “thing I care about” versus “everything else.”  In a real 3D scene, where you often have dozens of objects packed closely together, two things happen:\n\n 1.  **Poor semantic discrimination.**  Because the network never sees per‐instance or per‐class labels, it never learns explicit object-level categories or boundaries.  The only signal it has is “foreground vs. background,” so it cannot reliably tell two adjacent chairs apart, or tell a chair from a similarly shaped stool, purely from the mask loss.\n\n 2.  **Background and multi-object confusion.**  With just a binary mask loss, the model is free to cover all the “likely” points it thinks belong to *something*—which often means it over-segments into nearby objects or spills into the background to boost its mask accuracy.  There is no penalty for grouping different objects together, or for picking up unrelated clutter, so the predictions tend to be noisy, imprecise, and liable to miss the precise extent of the query object.\n\nIn short, training the Mask Predictor with only a flat foreground/background target is *too weak* a supervision signal to teach the network the semantic, instance-level distinctions you need to reliably pull out a single, correctly bounded object in a complex 3D scene."
    },
    {
        "question": "How might 3D U-Net and Text Encoder integration limit scalability to larger point clouds?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "3D U-Net",
            "Text Encoder"
        ],
        "id": 315,
        "masked_question": "How might [mask1] and Text Encoder integration limit scalability to larger point clouds?",
        "masked_number": 1,
        "masked_elements": [
            "3D U-Net"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.13294v2_figure_1.png",
        "paperid": "2410.13294v2",
        "paper_path": "./papers/2410.13294v2.json",
        "figure_id": "2410.13294v2_figure_1.png",
        "caption": "Figure 1: Comparison between the two-stage method and our single-stage method. (a) The two-stage method initially performs instance segmentation with instance labels then semantic labels to get the instance proposals and bases on the provided query to match the most relevant instance proposal. (b) Our single-stage method only utilizes the binary mask of the described object for training and integrates language and vision features during feature extraction.",
        "qtype": "Others",
        "response": "First, let’s be precise about what “[mask1]” is in that red box.  If you look at the right‐hand side of Fig. 1(b), the red outline is simply the 3D sparse U-Net backbone into which we inject word-level (and later sentence-level) features from the text encoder.  In other words, instead of a pure visual U-Net that you could run once and cache, we are fusing language into every level of that U-Net via cross-attention (the PWCA module).  \n\nNow, why does that make it hard to scale to very large point clouds?\n\n1.  Heavy per-point cross-attention.  \n    •  At each stage of the U-Net you’re performing a full cross-attention between every voxel (or point) and every textual token.  As the number of voxels grows, the cost of those attention maps grows roughly quadratically in the worst case (or at least linearly times the query length).  \n    •  Even if you use sparse convolutions, you still must attend from each non-empty voxel to every word, so memory and FLOPs balloon as scenes get more detailed.\n\n2.  No precomputation of purely geometric features.  \n    •  Because language is fused early and deeply, you can’t run the U-Net once and cache a “visual feature map” for all future queries.  Every new sentence forces a fresh pass through the entire 3D U-Net, multiplying the cost by the number of queries.\n\n3.  Multi-scale fusion overhead.  \n    •  The design injects text at multiple scales—low resolution for global context, high resolution for fine localization—so you pay the cross-modal price repeatedly.  For a very large scene split into many voxels at multiple scales, those repeated fusions quickly overwhelm GPU memory and runtime budgets.\n\nIn sum, by tightly coupling the text encoder’s outputs into every layer of the 3D U-Net (rather than separating a heavy offline visual backbone from a lightweight language fusion head), the method incurs per-query, per-voxel attention costs that grow rapidly as point clouds become larger, thereby limiting real-time operation or handling of massive scenes."
    },
    {
        "question": "What vulnerabilities emerge when encoder training ignores generative model parameter updates?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Encoder",
            "Generative Model"
        ],
        "id": 316,
        "masked_question": "What vulnerabilities emerge when [mask1] training ignores [mask2] parameter updates?",
        "masked_number": 2,
        "masked_elements": [
            "Encoder",
            "Generative Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.10471v2_figure_1.png",
        "paperid": "2407.10471v2",
        "paper_path": "./papers/2407.10471v2.json",
        "figure_id": "2407.10471v2_figure_1.png",
        "caption": "Figure 1. The diagram illustrates the process of supervising generated content through generative watermarking. The synthetic content generated via GMs by Alice will be subject to regulation, while Bob s may pose a high risk to society.",
        "qtype": "Others",
        "response": "First, let us pin down what the two masks refer to in the figure:\n\n  •  [mask1] is the red‐boxed “Encoder” in the generative–watermarking path, i.e. the network that converts the bit–stream into a latent perturbation.  \n  •  [mask2] is the blue‐boxed “Generative Model” (GM), the pre‐trained diffusion network whose parameters are held fixed during watermark–encoder/decoder training.\n\n---\n\nChain of thought:\n\n1.  In Groot, we deliberately freeze all the GM’s weights when we train only the encoder & decoder.  That way we never have to re‐train or fine‐tune the expensive diffusion model.  \n2.  But by never updating the GM during encoder training, the encoder only ever “sees” one static model.  It learns to hide its watermark in that exact mapping from latents to audio.  \n3.  As soon as an attacker tweaks the GM – even slightly – the encoder’s learned watermark pattern no longer aligns with the new model’s behavior.  Two things immediately follow:  \n    –  The watermark becomes much easier to erase or garble with minor post-processing or fine-tuning of the GM.  \n    –  An adversary can outright substitute in a different diffusion network (or steal and continue training the original one) and generate “clean,” non-watermarked audio.  \n4.  In short, by ignoring GM parameter updates in the encoder’s training loop, we create a brittle watermark that (a) cannot adapt to model drift and (b) is trivial for an attacker to defeat by changing the GM’s weights or swapping in a surrogate model.\n\nAnswer:\n\nWhen the watermark encoder is trained while freezing the generative model’s parameters, the watermark embedding becomes narrowly tied to that one fixed GM.  Any slight fine-tuning, replacement, or post-processing of the GM will destroy or entirely bypass the watermark.  In other words, the tag is fragile and can be removed or evaded by an adversary who updates or swaps out the diffusion model."
    },
    {
        "question": "What ethical risks accompany extracting watermarks for supervision and tracing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Extractor",
            "Watermark"
        ],
        "id": 317,
        "masked_question": "What ethical risks accompany extracting [mask1] for supervision and tracing using [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Watermark",
            "Extractor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.10471v2_figure_1.png",
        "paperid": "2407.10471v2",
        "paper_path": "./papers/2407.10471v2.json",
        "figure_id": "2407.10471v2_figure_1.png",
        "caption": "Figure 1. The diagram illustrates the process of supervising generated content through generative watermarking. The synthetic content generated via GMs by Alice will be subject to regulation, while Bob s may pose a high risk to society.",
        "qtype": "Others",
        "response": "Step 1 – Image–text alignment  \n•  In the bottom part of the figure the red‐boxed element is simply the “Watermark” that has been hidden inside the generated audio.  \n•  The blue‐boxed element is labeled “Supervision & Tracing GMs,” i.e. the special extractor and monitoring machinery that looks for those embedded watermarks so that one can identify which model (and ultimately which user) produced a given clip.  \n\nStep 2 – Chain of thought on ethical risks  \n1.  Loss of anonymity and privacy  \n    –  By routinely extracting the hidden watermark from every piece of audio, anyone operating the “supervision & tracing” system can tie a clip back to its source model or user, even if that user believed their generation to be pseudonymous.  \n    –  In the worst case, this could expose innocent users’ identities or private habits, turning a benign creative tool into a mass‐surveillance device.  \n\n2.  Chilling‐effect on free expression  \n    –  If every utterance can be traced back to its producer, speakers may self‐censor for fear of being monitored or punished for content that is merely unpopular or politically sensitive.  \n    –  This “chilling effect” runs counter to open, uninhibited discourse and can stifle dissent or experimentation.  \n\n3.  Misuse by authoritarian or corporate powers  \n    –  Governments or large platforms might weaponize watermark extraction to track down dissidents or critics who use generative tools.  \n    –  Similarly, commercial interests could trace and penalize whistle­blowers or pranksters, crushing innovation and whistle­blower protections.  \n\n4.  Scope creep and function creep  \n    –  A system introduced to catch illegal deepfakes might expand into broad surveillance of all user content (e.g. tracking political opinions, financial talks, even private speech).  \n    –  Once the infrastructure is in place, it is often repurposed for uses far beyond its original mandate.  \n\n5.  Discrimination and profiling  \n    –  Tracing watermarks could be combined with demographic profiling to single out particular groups (e.g. by region, accent, or topic).  \n    –  This opens the door to biased enforcement, algorithmic discrimination, and unequal threat of punishment.  \n\nAnswer  \nExtracting the embedded watermark at scale in order to “supervise and trace” every piece of generated audio carries serious ethical hazards.  In effect it allows wholesale loss of user anonymity and privacy, induces a chilling effect on free speech, and creates infrastructure that can be readily abused by authoritarian regimes or dominant platforms to surveil, profile or silence dissenting voices.  Moreover, once built, such tracing systems tend to undergo scope-creep—expanding from catching illicit deepfakes to sweeping up all manner of user-generated content—which in turn exacerbates discrimination, censorship and social control."
    },
    {
        "question": "What motivates integrating Encoder outputs directly into the Generative Model for watermarked audio synthesis?",
        "relevant_section_ids": [
            "1",
            "4"
        ],
        "relevant_context": [
            "In contrast, generative watermarking integrates the watermarking process with content synthesis, utilizing the same GM for both tasks.",
            "To tackle these challenges, we proposed a generative robust audio watermarking (Groot) method tailored for diffusion-model-based audio synthesis. By directly generating watermarked audio through DMs, we can regulate and trace the use conditions of the generated audio and its originating DMs.",
            "Our approach marries generative watermarking with proactive supervision, with the training overhead being exclusive to the encoder and decoder. This eliminates the necessity for complex retraining of DMs. Such a feature makes our method versatile and readily implementable as a plug-and-play solution for any diffusion model."
        ],
        "final_answer": "By feeding the encoder’s output directly into the diffusion model, Groot embeds the watermark as an integral part of the synthesis process. This generative approach enables proactive supervision and reliable tracing of every generated audio sample back to its source model, while avoiding the need to retrain or modify the diffusion model itself—thus delivering a plug-and-play, robust watermarking solution.",
        "relevant_elements": [
            "Encoder",
            "Generative Model"
        ],
        "id": 318,
        "masked_question": "What motivates integrating [mask1] outputs directly into the [mask2] for watermarked audio synthesis?",
        "masked_number": 2,
        "masked_elements": [
            "Encoder",
            "Generative Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.10471v2_figure_1.png",
        "paperid": "2407.10471v2",
        "paper_path": "./papers/2407.10471v2.json",
        "figure_id": "2407.10471v2_figure_1.png",
        "caption": "Figure 1. The diagram illustrates the process of supervising generated content through generative watermarking. The synthetic content generated via GMs by Alice will be subject to regulation, while Bob s may pose a high risk to society.",
        "qtype": "Design_Rationale",
        "response": "Chain of thought:\n\n1.  In our diagram the red‐boxed module is the watermark encoder, which turns the binary watermark into a latent vector, and the blue‐boxed module is the pretrained generative (diffusion) model that actually synthesizes audio.  \n2.  If we simply generated audio and then “stamped” a watermark on top, the signal would be easily distorted or removed by normal audio processing.  Instead, by feeding the encoder’s output directly into the generator’s input latent, the watermark becomes an integral part of the synthesis process.  \n3.  This has three key benefits:  \n    •  Fidelity: because the watermark is built into the denoising chain, the perceptual quality of the audio is preserved.  \n    •  Robustness: the watermark is embedded throughout the generation trajectory, making it far more resistant to post‐processing and compound attacks.  \n    •  Plug-and-play traceability: we never touch the generator’s weights—only the encoder and decoder—so the method can be added to any off-the-shelf diffusion model to proactively supervise and trace its outputs.  \n\nAnswer:\n\nThey inject the watermark encoder’s latent outputs directly into the diffusion model’s input so that watermarking happens inside the synthesis itself.  By conditioning the generator on the encoded watermark rather than applying it after the fact, the watermark is distributed throughout the denoising process, yielding higher fidelity, far better robustness to attacks, and a plug-and-play way to trace any audio the model produces."
    },
    {
        "question": "Why integrate Extractor for Edited content instead of relying solely on watermark embedding during generation?",
        "relevant_section_ids": [
            "4.2",
            "4.4"
        ],
        "relevant_context": [
            "The watermark extraction process is an independent process that does not necessitate the use of diffusion and denoising process. The watermark decoder is designed to disentangle features between audios and watermarks for recovering the watermark. Our approach enables precise supervision of generated content and corresponding DMs by ensuring that the extracted watermark aligns with the embedded watermark.",
            "Regulating generated contents and tracing associated DMs are achieved by verifying the existence of the watermark within the generated audio through test hypothesis. ... In simpler terms, when the watermark is 100 bits long, the extraction accuracy of 99.63% can be utilized to confirm the existence of the watermark."
        ],
        "final_answer": "Embedding a watermark at generation time alone does not guarantee that the mark can be reliably detected or verified once the audio has been edited or subjected to post-processing. By integrating a dedicated extractor (decoder), the system can recover and check the embedded watermark from edited content, ensuring robust supervision, traceability, and the ability to confirm the watermark’s presence even after arbitrary edits or attacks.",
        "relevant_elements": [
            "Extractor",
            "Edited",
            "Watermark"
        ],
        "id": 319,
        "masked_question": "Why integrate [mask1] for [mask2] content instead of relying solely on watermark embedding during generation?",
        "masked_number": 2,
        "masked_elements": [
            "Extractor",
            "Edited"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.10471v2_figure_1.png",
        "paperid": "2407.10471v2",
        "paper_path": "./papers/2407.10471v2.json",
        "figure_id": "2407.10471v2_figure_1.png",
        "caption": "Figure 1. The diagram illustrates the process of supervising generated content through generative watermarking. The synthetic content generated via GMs by Alice will be subject to regulation, while Bob s may pose a high risk to society.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key point in one sentence: the red-boxed “Extractor” is not just a luxury add-on, it’s absolutely necessary to recover your watermark from the blue-boxed “Edited” audio once that audio has been modified—plain generation-time embedding by itself would simply be too fragile once someone re-mixes, re-samples or otherwise tampers with the clip.\n\nChain of thought:\n\n1.   In the figure, the red-box highlights the **Extractor** network and the blue-box highlights the **Edited** audio (i.e. generated audio that’s been post-processed or tampered with).  \n2.   If you only rely on embedding the watermark during diffusion-based generation, any subsequent edit (filtering, compression, re-sampling, overlay, etc.) can easily destroy or obscure the tiny signal patterns you planted.  \n3.   The Extractor is trained to **disentangle** the watermark from whatever distortions the audio has suffered. It doesn’t need to re-run the entire diffusion pipeline—only this lightweight network—so you can still pull out the watermark bits even after heavy post-processing.  \n4.   Without that dedicated extractor you would have no robust way to test for ownership or trace which model produced a given clip as soon as someone “edits” it.  \n\nIn short, embedding alone plants the secret code, but the extractor is what actually reads it back once the audio is edited."
    },
    {
        "question": "Why inject identity embedding from Identity Encoder into Rig Param Decoder to support joint multi-avatar training?",
        "relevant_section_ids": [
            "3.2.3"
        ],
        "relevant_context": [
            "Since different avatars possess varying rigs and unique physiognomy, the generated rig parameters of different avatars not only need to contain consistent expression information but also possess unique facial attributes. Hence, in the decoding process, we also incorporate the identity embedding $z^\\mathrm{id}$ and concatenate it to the expression representation."
        ],
        "final_answer": "Injecting the identity embedding into the Rig Param Decoder allows the network to condition its output on which avatar it is driving. Because each avatar has its own rig structure and facial physiognomy, supplying an identity embedding ensures the decoder can produce avatar-specific rig parameters (capturing unique facial attributes) while still using the same shared network. This enables joint training of multiple avatars in a single model.",
        "relevant_elements": [
            "Identity Encoder",
            "Rig Param Decoder"
        ],
        "id": 321,
        "masked_question": "Why inject identity embedding from [mask1] into [mask2] to support joint multi-avatar training?",
        "masked_number": 2,
        "masked_elements": [
            "Identity Encoder",
            "Rig Param Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.13180v2_figure_2.png",
        "paperid": "2409.13180v2",
        "paper_path": "./papers/2409.13180v2.json",
        "figure_id": "2409.13180v2_figure_2.png",
        "caption": "Figure 2. Pipeline overview. FreeAvatar first constructs an expression foundation model in two steps: facial feature space construction with Masked Autoencoder (MAE) and expression feature space optimization via contrastive learning. After that, an Expression-driven Multi-avatar Animator is constructed\nto encode the expression representations into rig parameters. Then, perceptual constraints are employed in a differentiable manner to ensure that the expressions between the input and the avatars remain consistent.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition, broken down step by step:\n\n1.  Goal of “multi-avatar” training  \n    – We want a *single* decoder network that, given an expression code, can drive *any* one of N different avatars rather than training N separate models.  \n\n2.  The role of the rig-parameter decoder  \n    – The decoder (the blue-boxed block) ingests the expression embedding and spits out the 3D-rig controls for the *target* character.  \n    – But each avatar has its own rig topology, controller ranges, default poses, facial proportions, etc.  \n\n3.  Why pure expression alone is not enough  \n    – If the decoder only saw the expression vector, it would have no way of knowing “I’m supposed to produce rig values for Avatar #3, not Avatar #17.”  \n    – In other words the mapping  \n         expression → rig_params  \n      must be *conditioned* on which avatar we’re animating.\n\n4.  Injecting identity as a conditioning signal  \n    – The red-boxed identity encoder converts a one-hot (or index) of the chosen avatar into a low-dimensional embedding.  \n    – By concatenating (or otherwise fusing) that embedding into the decoder’s input, the network learns a conditional function  \n         (expression, identity) → rig_params  \n      rather than a one-size-fits-all expression→rig mapping.\n\n5.  How this supports joint training  \n    – During training you randomly pick an avatar ID, compute its identity embedding, and feed it into the decoder alongside the expression code.  \n    – The same set of decoder weights now learns to produce *avatar-specific* rig outputs for every identity it sees, sharing all of the bulk of the computation across avatars but still adapting to each one’s unique rig.  \n\nBottom line:  \nInjecting the identity embedding into the rig decoder tells the network “which character am I driving right now?” so that a single shared model can learn how to map expressions into the *correct* set of rig parameters for each different avatar."
    },
    {
        "question": "How is the confidence-weighted triplet loss margin calibrated during expression feature space optimization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Expression feature space optimization",
            "Triplet Loss"
        ],
        "id": 322,
        "masked_question": "How is the confidence-weighted [mask1] margin calibrated during expression feature space optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Triplet Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.13180v2_figure_2.png",
        "paperid": "2409.13180v2",
        "paper_path": "./papers/2409.13180v2.json",
        "figure_id": "2409.13180v2_figure_2.png",
        "caption": "Figure 2. Pipeline overview. FreeAvatar first constructs an expression foundation model in two steps: facial feature space construction with Masked Autoencoder (MAE) and expression feature space optimization via contrastive learning. After that, an Expression-driven Multi-avatar Animator is constructed\nto encode the expression representations into rig parameters. Then, perceptual constraints are employed in a differentiable manner to ensure that the expressions between the input and the avatars remain consistent.",
        "qtype": "Implementation_Details",
        "response": "During the fine‐tuning of the ViT on triplets, we do not treat every “hard” example equally – instead we scale the separation margin by how confident we are in the human labels for that triplet.  Concretely:\n\n 1.  For each anchor–positive–negative triplet (a,p,n), we compute a confidence score s∈[0,1], namely the fraction of annotators who agreed on that ordering.  \n 2.  We start from a fixed base margin m.  \n 3.  We then either  \n    • multiply the entire triplet‐loss term by s:  L = s·[d(a,p)–d(a,n)+m]₊   or  \n    • equivalently, we can think of it as using a per‐sample margin mᵢ = s·m in the hinge:  L = [d(a,p)–d(a,n)+mᵢ]₊.  \n 4.  Thus high‐confidence triplets (s close to 1) enforce nearly the full margin m, while low‐confidence ones (s≪1) pull/push their embeddings only weakly."
    },
    {
        "question": "How does the identity encoder integrate its embedding into the rig parameter decoder to preserve avatar-specific attributes?",
        "relevant_section_ids": [
            "3.2.2",
            "3.2.3"
        ],
        "relevant_context": [
            "3.2.2. Dynamic identity injection. … Specifically, for each iteration, we randomly choose a, which indicates the identity number of the target avatar. Then we employ an Embedding Layer as the identity encoder E_id to extract identity embedding z_id.",
            "3.2.3. Rig parameter decoder. … Since different avatars possess varying rigs and unique physiognomy, the generated rig parameters of different avatars not only need to contain consistent expression information but also possess unique facial attributes. Hence, in the decoding process, we also incorporate the identity embedding z_id and concatenate it to the expression representation."
        ],
        "final_answer": "The identity encoder produces an embedding z_id which is then concatenated with the expression representation before being fed into the rig parameter decoder, thereby conditioning the decoder on avatar-specific attributes.",
        "relevant_elements": [
            "Identity Encoder",
            "Rig Param. Decoder"
        ],
        "id": 323,
        "masked_question": "How does the [mask1] integrate its embedding into the [mask2] to preserve avatar-specific attributes?",
        "masked_number": 2,
        "masked_elements": [
            "Identity Encoder",
            "Rig Param. Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.13180v2_figure_2.png",
        "paperid": "2409.13180v2",
        "paper_path": "./papers/2409.13180v2.json",
        "figure_id": "2409.13180v2_figure_2.png",
        "caption": "Figure 2. Pipeline overview. FreeAvatar first constructs an expression foundation model in two steps: facial feature space construction with Masked Autoencoder (MAE) and expression feature space optimization via contrastive learning. After that, an Expression-driven Multi-avatar Animator is constructed\nto encode the expression representations into rig parameters. Then, perceptual constraints are employed in a differentiable manner to ensure that the expressions between the input and the avatars remain consistent.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module is the identity encoder (also called the “Embedding Layer”) and the blue‐boxed module is the rig‐parameter decoder.  During each training step we pick a target avatar ID, run it through the identity encoder to get a low‐dimensional identity embedding, and then simply append (concatenate) that ID embedding onto the expression embedding before feeding both together into the rig‐parameter decoder’s MLPs.  In this way the decoder always sees “who” it’s drawing for (via the ID embedding) alongside “what face shape to make” (via the expression embedding), so the final rig parameters carry both the correct expression and the avatar’s unique physiognomy."
    },
    {
        "question": "How does GS-Adapter decoupled cross-attention integrate joint and image embeddings for higher-order semantics fusion?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The GS-Adapter employs a decoupled cross-attention mechanism to independently process joint and image embeddings. Let Q represent the query matrix, and K and V denote key-value pairs for joint and image embeddings, respectively.",
            "The combined output is:  A = softmax( Q K_{joint}^T / √d ) V_{joint}  +  λ · softmax( Q K_{image}^T / √d ) V_{image} ,  where λ balances image and joint feature contributions."
        ],
        "final_answer": "The GS-Adapter uses two separate cross-attention operations—one over joint embeddings and one over image embeddings—both sharing the same query Q. It computes attention outputs with their respective key–value pairs, then fuses them by summing the two results, weighted by a balancing coefficient λ, to produce higher-order semantic features.",
        "relevant_elements": [
            "GS-Adapter",
            "cross-attention"
        ],
        "id": 324,
        "masked_question": "How does [mask1] decoupled cross-attention integrate joint and image embeddings for higher-order semantics fusion?",
        "masked_number": 1,
        "masked_elements": [
            "GS-Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17017v1_figure_2.png",
        "paperid": "2411.17017v1",
        "paper_path": "./papers/2411.17017v1.json",
        "figure_id": "2411.17017v1_figure_2.png",
        "caption": "Figure 2: Overview of TED-VITON: We present the architecture of the proposed model along with details of its block modules. (a) Our model consists of 1) DiT-GarmentNet that encodes fine-grained features of Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, 2) GS-Adapter [45] that captures higher-order semantics of garment image Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, and 3) DiT-TryOnNet, the main Transformer for processing person images. The Transformer input is formed by concatenating the noised latents Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with the segmentation mask m𝑚mitalic_m, masked image ℰ⁢(Xmodel)ℰsubscript𝑋model\\mathcal{E}(X_{\\text{model}})caligraphic_E ( italic_X start_POSTSUBSCRIPT model end_POSTSUBSCRIPT ), and Densepose [12] ℰ⁢(xpose)ℰsubscript𝑥pose\\mathcal{E}(x_{\\text{pose}})caligraphic_E ( italic_x start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT ). Additionally, a detailed description of the garment (e.g., “[D]: The clothing item is a black T-shirt…”) is generated through an LLM and fed as input to both the DiT-GarmentNet and DiT-TryOnNet. The model aims to preserve garment-specific details through a text preservation loss, which ensures that key textual features are retained. (b) Intermediate features from DiT-TryOnNet and DiT-GarmentNet are concatenated. These are then refined through joint-attention and cross-attention layers, with the GS-Adapter further contributing to the refinement process. In this architecture, the DiT-TryOnNet and GS-Adapter modules are fine-tuned, while other components remain frozen.",
        "qtype": "Implementation_Details",
        "response": "Step 1: Identify [mask1]  \nBy inspecting the red‐boxed module in Fig. 2 (b), we see that [mask1] is the GS-Adapter’s “decoupled cross-attention” block.  \n\nStep 2: Locate its inputs  \n– Query Q comes from the joint‐attention hidden state (which already fuses the person’s structure, pose, mask and fine-grained garment details).  \n– We have two separate key–value streams:  \n  1. K₁, V₁ ← joint embeddings (the “joint‐attended” features)  \n  2. K₂, V₂ ← image embeddings (the high-order semantic features produced by the DINOv2 encoder in the GS-Adapter)  \n\nStep 3: Perform two parallel cross-attentions  \nThe module computes:  \n  A₁ = softmax(Q K₁ᵀ / √d) V₁   (joint cross-attention)  \n  A₂ = softmax(Q K₂ᵀ / √d) V₂   (image cross-attention)  \n\nStep 4: Fuse with a learnable weight  \nThese two attention outputs are then merged via a learned gating scalar λ:  \n  H_semantic = A₁  +  λ · A₂  \n\nResult: H_semantic is the GS-Adapter’s higher-order semantic embedding. By decoupling the two attentions and then recombining them, the model can flexibly balance structural/joint cues against global garment semantics, improving generalization to new poses, deformations, and lighting conditions."
    },
    {
        "question": "What mechanism merges fine-grained DiT-GarmentNet features with DiT-TryOnNet representations within MM-DiT-Block?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Within the MM-DiT-Block (Fig. 2 (b)), fine-grained garment details fᵍₗ extracted from the l-th transformer layer of DiT-GarmentNet merge with the feature representation fᵗₗ from the corresponding l-th layer of DiT-TryOnNet to form fᵐₗ, which serves as the primary input for attention processing.",
            "Descriptive text embeddings eᵈ, generated by multimodal text encoders, are concatenated with fᵐₗ within the query, key, and value components of the joint attention mechanism (i.e., Q, K, V)."
        ],
        "final_answer": "They are merged via the joint attention mechanism: the fine-grained DiT-GarmentNet features and DiT-TryOnNet representations are combined into a single tensor fᵐₗ, which is then fed into the joint Q/K/V cross-attention block in the MM-DiT-Block.",
        "relevant_elements": [
            "DiT-GarmentNet",
            "DiT-TryOnNet",
            "MM-DiT-Block"
        ],
        "id": 325,
        "masked_question": "What mechanism merges fine-grained [mask1] features with DiT-TryOnNet representations within MM-DiT-Block?",
        "masked_number": 1,
        "masked_elements": [
            "DiT-GarmentNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17017v1_figure_2.png",
        "paperid": "2411.17017v1",
        "paper_path": "./papers/2411.17017v1.json",
        "figure_id": "2411.17017v1_figure_2.png",
        "caption": "Figure 2: Overview of TED-VITON: We present the architecture of the proposed model along with details of its block modules. (a) Our model consists of 1) DiT-GarmentNet that encodes fine-grained features of Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, 2) GS-Adapter [45] that captures higher-order semantics of garment image Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, and 3) DiT-TryOnNet, the main Transformer for processing person images. The Transformer input is formed by concatenating the noised latents Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with the segmentation mask m𝑚mitalic_m, masked image ℰ⁢(Xmodel)ℰsubscript𝑋model\\mathcal{E}(X_{\\text{model}})caligraphic_E ( italic_X start_POSTSUBSCRIPT model end_POSTSUBSCRIPT ), and Densepose [12] ℰ⁢(xpose)ℰsubscript𝑥pose\\mathcal{E}(x_{\\text{pose}})caligraphic_E ( italic_x start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT ). Additionally, a detailed description of the garment (e.g., “[D]: The clothing item is a black T-shirt…”) is generated through an LLM and fed as input to both the DiT-GarmentNet and DiT-TryOnNet. The model aims to preserve garment-specific details through a text preservation loss, which ensures that key textual features are retained. (b) Intermediate features from DiT-TryOnNet and DiT-GarmentNet are concatenated. These are then refined through joint-attention and cross-attention layers, with the GS-Adapter further contributing to the refinement process. In this architecture, the DiT-TryOnNet and GS-Adapter modules are fine-tuned, while other components remain frozen.",
        "qtype": "Implementation_Details",
        "response": "Within each MM-DiT-Block the two streams are simply concatenated and then run through a single multi-head attention layer – i.e. a “joint-attention” module.  The fine-grained GarmentNet features and the corresponding TryOnNet features are stacked together into the Q/K/V of that attention block, so that a single self-attention operation fuses them."
    },
    {
        "question": "How does DiT-GarmentNet utilize CFM loss differently than DiT-TryOnNet to retain garment details?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "SD3 leverages the Conditional Flow Matching (CFM) loss to guide rectified flow during training.",
            "The CFM loss guides the model in generating the VTO result \\(\\hat{X}\\) leveraging DiT-GarmentNet for detail retention and DiT-TryOnNet for fit adjustments based on pose and body type."
        ],
        "final_answer": "DiT-GarmentNet uses the CFM loss specifically to preserve and reconstruct fine-grained garment features—such as textures, patterns, and logos—whereas DiT-TryOnNet applies the same CFM loss primarily to ensure the garment is correctly aligned and fitted onto the person’s body and pose.",
        "relevant_elements": [
            "DiT-GarmentNet",
            "DiT-TryOnNet",
            "CFM loss"
        ],
        "id": 326,
        "masked_question": "How does [mask1] utilize CFM loss differently than DiT-TryOnNet to retain garment details?",
        "masked_number": 1,
        "masked_elements": [
            "DiT-GarmentNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17017v1_figure_2.png",
        "paperid": "2411.17017v1",
        "paper_path": "./papers/2411.17017v1.json",
        "figure_id": "2411.17017v1_figure_2.png",
        "caption": "Figure 2: Overview of TED-VITON: We present the architecture of the proposed model along with details of its block modules. (a) Our model consists of 1) DiT-GarmentNet that encodes fine-grained features of Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, 2) GS-Adapter [45] that captures higher-order semantics of garment image Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, and 3) DiT-TryOnNet, the main Transformer for processing person images. The Transformer input is formed by concatenating the noised latents Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with the segmentation mask m𝑚mitalic_m, masked image ℰ⁢(Xmodel)ℰsubscript𝑋model\\mathcal{E}(X_{\\text{model}})caligraphic_E ( italic_X start_POSTSUBSCRIPT model end_POSTSUBSCRIPT ), and Densepose [12] ℰ⁢(xpose)ℰsubscript𝑥pose\\mathcal{E}(x_{\\text{pose}})caligraphic_E ( italic_x start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT ). Additionally, a detailed description of the garment (e.g., “[D]: The clothing item is a black T-shirt…”) is generated through an LLM and fed as input to both the DiT-GarmentNet and DiT-TryOnNet. The model aims to preserve garment-specific details through a text preservation loss, which ensures that key textual features are retained. (b) Intermediate features from DiT-TryOnNet and DiT-GarmentNet are concatenated. These are then refined through joint-attention and cross-attention layers, with the GS-Adapter further contributing to the refinement process. In this architecture, the DiT-TryOnNet and GS-Adapter modules are fine-tuned, while other components remain frozen.",
        "qtype": "Literature_Background",
        "response": "First, it helps to see that the red‐boxed module in your figure is exactly the DiT‐GarmentNet “detail extractor,” whereas the pink module is DiT‐TryOnNet “body‐fusion.”  Although both branches ultimately contribute to a CFM loss, they use it in two quite different ways:\n\n1. DiT-GarmentNet (the red box) is frozen and only ever “rebuilds” the garment by itself.  At train time it takes the garment latent Xg and, through its frozen transformer blocks, produces a reconstructed garment latent \\hat X′.  We then apply the CFM loss directly between E(Xg) and \\hat X′, which forces the network to linearly align (match) every fine-grained pattern, texture and logo in the garment.  Because those blocks never see pose or body features, the CFM loss here is purely a garment-fidelity constraint.\n\n2. DiT-TryOnNet (the pink box) is the only branch that actually “pastes” that garment info onto a human.  It concatenates the person latent, mask, pose embedding and the fine-grained features from DiT-GarmentNet, and then applies the CFM loss on the final try-on output \\hat X against E(X_model).  In other words, its CFM loss supervises the end‐to‐end alignment of garment + body in one shot, making sure the garment sits correctly on the pose and the overall look remains realistic.\n\nBy splitting the CFM loss in this way—one copy up front just for pure garment reconstruction, and one copy at the very end for the combined try-on image—they guarantee that DiT-GarmentNet locks in all the little details, and DiT-TryOnNet takes care of fitting those details onto the person."
    },
    {
        "question": "How do GS-Adapter and DiT-TryOnNet collaboratively leverage text preservation loss for accurate text rendering?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Prior Preservation for Text Generation. To retain the model’s ability to generate accurate and clear text, such as logos and labels, we introduce a prior preservation mechanism inspired by DreamBooth [37]. This mechanism incorporates a text preservation loss to ensure text clarity and fidelity, preventing the model from losing this capability while fine-tuning for VTO tasks. As the final component of our framework, prior preservation complements the GS-Adapter and DiT-TryOnNet.",
            "As shown in Fig. 2 (a), the total loss function combines two main components: (1) the CFM loss ℒ_CFM defined in Eq. 2, which ensures high-quality VTO outputs by aligning generated images with the desired garment and pose, and (2) the text preservation loss ℒ_TP, which maintains clarity in text details. ... The text preservation loss ℒ_TP is computed as ℒ_TP = ||z – z_b||, where z is the baseline latent representation from the original model, helping to retain text fidelity in the fine-tuned output. The final loss function is given by:\n\nℒ = ℒ_CFM + λ_TP ℒ_TP\n\nwhere λ_TP controls the balance between VTO adaptation and text retention. This approach enables high-quality garment realism while preserving essential text rendering for realistic try-on images."
        ],
        "final_answer": "GS-Adapter and DiT-TryOnNet are trained together with a text preservation loss that penalizes deviation from the original diffusion model’s latent text representation. The GS-Adapter provides high-order semantic garment cues (including logos and text structure) and DiT-TryOnNet renders them onto the person. During fine-tuning, the text preservation loss (‖z – z_b‖) is added to the CFM loss, forcing the network to keep text and logo embeddings close to their pre-trained baselines. In this way, GS-Adapter’s semantics and DiT-TryOnNet’s rendering are jointly optimized to maintain accurate, clear text in the final try-on images.",
        "relevant_elements": [
            "GS-Adapter",
            "DiT-TryOnNet"
        ],
        "id": 327,
        "masked_question": "How do [mask1] and [mask2] collaboratively leverage text preservation loss for accurate text rendering?",
        "masked_number": 2,
        "masked_elements": [
            "GS-Adapter",
            "DiT-TryOnNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17017v1_figure_2.png",
        "paperid": "2411.17017v1",
        "paper_path": "./papers/2411.17017v1.json",
        "figure_id": "2411.17017v1_figure_2.png",
        "caption": "Figure 2: Overview of TED-VITON: We present the architecture of the proposed model along with details of its block modules. (a) Our model consists of 1) DiT-GarmentNet that encodes fine-grained features of Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, 2) GS-Adapter [45] that captures higher-order semantics of garment image Xgsubscript𝑋𝑔X_{g}italic_X start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT, and 3) DiT-TryOnNet, the main Transformer for processing person images. The Transformer input is formed by concatenating the noised latents Xtsubscript𝑋𝑡X_{t}italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT with the segmentation mask m𝑚mitalic_m, masked image ℰ⁢(Xmodel)ℰsubscript𝑋model\\mathcal{E}(X_{\\text{model}})caligraphic_E ( italic_X start_POSTSUBSCRIPT model end_POSTSUBSCRIPT ), and Densepose [12] ℰ⁢(xpose)ℰsubscript𝑥pose\\mathcal{E}(x_{\\text{pose}})caligraphic_E ( italic_x start_POSTSUBSCRIPT pose end_POSTSUBSCRIPT ). Additionally, a detailed description of the garment (e.g., “[D]: The clothing item is a black T-shirt…”) is generated through an LLM and fed as input to both the DiT-GarmentNet and DiT-TryOnNet. The model aims to preserve garment-specific details through a text preservation loss, which ensures that key textual features are retained. (b) Intermediate features from DiT-TryOnNet and DiT-GarmentNet are concatenated. These are then refined through joint-attention and cross-attention layers, with the GS-Adapter further contributing to the refinement process. In this architecture, the DiT-TryOnNet and GS-Adapter modules are fine-tuned, while other components remain frozen.",
        "qtype": "Literature_Background",
        "response": "Here is a step-by-step reasoning showing how the GS-Adapter (red box) and DiT-TryOnNet (blue box) work together under the text-preservation loss to keep logos and other written details sharp and correct:\n\n1.  \n   • DiT-TryOnNet’s role (blue box)  \n     – Takes in the person’s noised latent image, garment mask, DensePose map and, crucially, the garment description embedding (“The shirt says GALAXY in big letters…”).  \n     – At each MM-DiT block it uses a joint‐attention module to fuse those text tokens with the visual features coming from its own layers and from DiT-GarmentNet.  \n     – This sets up a strong text–image alignment signal inside the diffusion network.  \n\n2.  \n   • GS-Adapter’s role (red box)  \n     – Independently encodes high-level garment semantics (via a frozen DINOv2 encoder) – structure, style, logo placement, word geometry, etc.  \n     – Uses a decoupled cross-attention to inject those “where” and “how big” cues for the words into DiT-TryOnNet’s key/value streams.  \n     – In effect it tells TryOnNet not only “what the text says” but “how it sits on the cloth.”  \n\n3.  \n   • Text-preservation loss  \n     – After fine-tuning, we compare the new latent code of the output image to the original latent code on regions containing text (e.g. the shirt logo).  \n     – Any drift is penalized, so the network is explicitly rewarded for producing the exact same characters, shapes, spacing and sharpness it saw before fine-tuning.  \n\n4.  \n   • Collaboration under the loss  \n     – DiT-TryOnNet ensures text tokens are part of its generative attention, so it “knows” there’s words to draw.  \n     – GS-Adapter feeds in the semantic priors about text location and style, so the model “knows” how to draw them.  \n     – The text-preservation loss then locks that knowledge in place: if TryOnNet+GS-Adapter ever misplace or blur the letters, the loss goes up.  \n\nTogether, this loop of (a) text tokens and semantics → (b) fused attentions in TryOnNet → (c) tight “keep-the-same” text preservation loss forces the final VTO output to render logos and labels both accurately and crisply."
    },
    {
        "question": "How does prompt guidance from Text Encoder enhance Frame Encoder feature extraction relative to vanilla frame methods?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "While the CLIP Text Encoder transforms the point- and frame-related prompts of the event label into the frame-prompt feature F_txt^f and the point-prompt feature F_txt^p, which guide the semantic consistency of F^f and F^p via contrastive loss L^f and L^p, respectively.",
            "Text-Prompt Embedding. The CLIP Text Encoder extracts text features from the frame- and point-related prompts, converting them into corresponding text features, e.g., F_txt^f and F_txt^p, where C indicates the number of action class.",
            "Training Process. We constrain the event-text consistency via the contrastive loss L^f between the event-frame feature and the frame-prompt feature."
        ],
        "final_answer": "By converting manually designed, class-specific prompts into text embeddings via the CLIP Text Encoder and then enforcing a contrastive loss between those prompt embeddings and the Frame Encoder’s output, the model aligns visual frame features with language semantics. This ‘prompt guidance’ encourages the Frame Encoder to learn more discriminative, semantically meaningful features than a vanilla frame-only encoder that lacks any language-based supervision.",
        "relevant_elements": [
            "Text Encoder",
            "Frame Encoder"
        ],
        "id": 328,
        "masked_question": "How does prompt guidance from [mask1] enhance [mask2] feature extraction relative to vanilla frame methods?",
        "masked_number": 2,
        "masked_elements": [
            "Text Encoder",
            "Frame Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18328v1_figure_1.png",
        "paperid": "2411.18328v1",
        "paper_path": "./papers/2411.18328v1.json",
        "figure_id": "2411.18328v1_figure_1.png",
        "caption": "Figure 1: Insight of our work. Previous methods are limited to using “heavy” frame-specific encoders to extract features from densely stacked event frames or employing “light” point encoders to process sparse raw event points. Our Frame&Point-based approach integrates prompt-aware language semantic information to achieve a synergistic enhancement among the distinct event representations, balancing efficiency and accuracy.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the picture with the text:\n\n– In the “vanilla” frame‐based pipeline (Fig. 1 (a)), you simply stack a few event frames and send them through a Frame Encoder.  There is no external semantic guidance, so the encoder must learn purely from the raw pixel‐change patterns and the final class label.\n\n– In our joint frame & point model (Fig. 1 (d)), we have added a CLIP‐style Text Encoder ( red box ) that turns automatically generated “prompts” (e.g.  \n  – “A series of photos recording human action for [cls]”  \n  – “A sequence of points capturing human motion for [cls]”  \n) into two text embeddings, one matched to the frame branch and one to the point branch.\n\n– The blue box in that same panel is exactly the old Frame Encoder, now operating under the influence of a contrastive loss that ties its output to the “frame prompt” embedding.\n\nChain of thought to answer the question:\n\n1.  Vanilla frame methods only see the stacked event‐frames; they learn to classify by trying to separate classes in feature space, but they have no notion of *what* the action *means* beyond a one‐hot label.\n\n2.  By running a natural‐language prompt (e.g. “A series of photos recording human action for [cls]”) through a pretrained Text Encoder, we get a dense *semantic* vector for each action class—call it the **frame‐prompt embedding**.  \n\n3.  We then force the Frame Encoder’s video‐feature output to be *close* to that prompt‐embedding (and *far* from the other class prompts) via a simple contrastive loss.  \n\n4.  This extra supervision injects into the Frame Encoder a strong, class‐specific *prior* about what “kick‐forward” or “take‐off” *means* in natural language.  The Frame Encoder learns to pay attention to those aspects of the event images that correlate with the linguistic description, rather than simply fitting the training labels.\n\n5.  In practice this yields much more discriminative, semantically‐grounded frame features than a vanilla frame‐only encoder can produce.\n\nAnswer (fill‐in):\n\n“Prompt guidance from the CLIP-style **Text Encoder** enhances the **Frame Encoder** by adding a contrastive alignment to the language‐derived prompt embeddings.  In effect, the Frame Encoder no longer learns just from raw pixel changes and a one‐hot label—it is also forced to match its output to a rich, natural‐language description of the action.  That extra semantic signal sharpens and regularizes the frame‐feature extractor, yielding more accurate, action‐aware representations than vanilla frame‐only methods.”"
    },
    {
        "question": "How does contextualizing event points compare with sliding window sampling for Point Encoder input?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "For these methods, the sparse and asynchronous event stream is sampled and aggregated through a size-fixed sliding window, as shown in Fig. 1 (b). Thus, such a sampling strategy not only disrupts the temporal correlations between events but also operates independently of the subsequent feature extraction.",
            "We address the challenges mentioned above by introducing a Spiking-like Context Learner (SCL) that can extract contextual event points from redundant raw events using Spiking Residual Recurrent Neural Network (SRRNN) and effectively integrate them with subsequent feature exploration."
        ],
        "final_answer": "Unlike fixed-size sliding windows that break event-to-event temporal correlations and sample independently of feature extraction, contextualizing event points via the Spiking-like Context Learner dynamically selects and aggregates points based on learned spiking thresholds, preserving temporal dependencies and tightly coupling sampling with subsequent point‐feature encoding.",
        "relevant_elements": [
            "Contextualizing",
            "Point Encoder"
        ],
        "id": 329,
        "masked_question": "How does [mask1] event points compare with sliding window sampling for Point Encoder input?",
        "masked_number": 1,
        "masked_elements": [
            "Contextualizing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18328v1_figure_1.png",
        "paperid": "2411.18328v1",
        "paper_path": "./papers/2411.18328v1.json",
        "figure_id": "2411.18328v1_figure_1.png",
        "caption": "Figure 1: Insight of our work. Previous methods are limited to using “heavy” frame-specific encoders to extract features from densely stacked event frames or employing “light” point encoders to process sparse raw event points. Our Frame&Point-based approach integrates prompt-aware language semantic information to achieve a synergistic enhancement among the distinct event representations, balancing efficiency and accuracy.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the red‐boxed module in the diagram with the text.  In Fig. 1(d) the red box is labeled “Contextualizing” and in Sec. 3.2 the authors introduce the Spiking-like Context Learner (SCL) to produce “contextual event points,” denoted \\(P_{\\mathrm{ctx}}\\), from the raw stream \\(P\\).  In contrast, the vanilla point-based methods of Fig. 1(b) simply chop the incoming events into fixed‐size, non‐overlapping time windows (“sliding window sampling”) and feed those raw windows directly to the Point Encoder.\n\nSo the question is asking:\n\n  “How does [mask1] event points compare with sliding window sampling for Point Encoder input?”\n\nThe masked term is the “contextualized” event points produced by the SCL.  Concretely:\n\n  • Sliding windows:  \n    – Pick a constant time‐span or number of events per window.  \n    – Break the stream into disjoint slices, independent of downstream processing.  \n    – Tends to disrupt fine‐grained temporal correlations and wastes capacity on uninformative regions.  \n\n  • Contextualized event points (our red-boxed module):  \n    – Use spiking neuron dynamics (LIF + recurrent convolutions) to accumulate incoming events until a firing threshold is hit.  \n    – Each spike defines an adaptive interval \\([t_{i-1},t_i]\\), yielding \\(P_{\\mathrm{ctx}}^i\\), a dense, information-rich slice.  \n    – These slices preserve the true temporal structure and can be directly consumed by the Event-Point Encoder (EPE).  \n\nIn short, the contextualizing step replaces the crude, fixed sliding window with an adaptive, threshold-driven sampling that better preserves temporal correlations and hands the Point Encoder a much richer, more coherent sequence of event points."
    },
    {
        "question": "How does Text Encoder integration facilitate alignment between Frame Encoder and Point Encoder outputs?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "While the CLIP Text Encoder transforms the point- and frame-related prompts of the event label into the frame-prompt feature T_f and the point-prompt feature T_p, which guide the semantic consistency of F_f and F_p via contrastive loss L_f and L_p, respectively. Here, P indicates the number of action classes.",
            "Text-Prompt Embedding. The CLIP Text Encoder extracts text features from the frame- and point-related prompts, converting them into corresponding text features, e.g., T_f and T_p, where P indicates the number of action class.",
            "Training Process. We constrain the event-text consistency via the contrastive loss L between the event-frame/point feature and the frame/point-prompt feature as follows: … Based on Eq. (10), we obtain specific contrastive losses L_f and L_p for the event-frame embedding and event-point embedding branches, respectively. The final overall recognition loss is composed of L_f and L_p, as follows:"
        ],
        "final_answer": "By feeding class‐specific language prompts for both event frames and event points into a shared CLIP Text Encoder, the model obtains two sets of text features (T_f for frames and T_p for points). These text features serve as anchors in two parallel contrastive losses (L_f and L_p) that pull the Frame Encoder’s outputs (F_f) toward T_f and the Point Encoder’s outputs (F_p) toward T_p. In this way, text‐guided contrastive learning aligns both visual modalities in a common semantic space.",
        "relevant_elements": [
            "Text Encoder",
            "Frame Encoder",
            "Point Encoder"
        ],
        "id": 330,
        "masked_question": "How does [mask1] integration facilitate alignment between [mask2] and Point Encoder outputs?",
        "masked_number": 2,
        "masked_elements": [
            "Text Encoder",
            "Frame Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18328v1_figure_1.png",
        "paperid": "2411.18328v1",
        "paper_path": "./papers/2411.18328v1.json",
        "figure_id": "2411.18328v1_figure_1.png",
        "caption": "Figure 1: Insight of our work. Previous methods are limited to using “heavy” frame-specific encoders to extract features from densely stacked event frames or employing “light” point encoders to process sparse raw event points. Our Frame&Point-based approach integrates prompt-aware language semantic information to achieve a synergistic enhancement among the distinct event representations, balancing efficiency and accuracy.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify the two masked modules in the lower‐right panel (d) of the figure:  \n  – [mask1] (red box) is the CLIP Text Encoder.  \n  – [mask2] (blue box) is the Event‐Frame Encoder.  \n\nChain of thought:  \n1. In our architecture we have two visual encoders – the Event‐Frame Encoder (blue) and the Event‐Point Encoder (orange) – and one shared Text Encoder (red).  \n2. The Text Encoder takes as input two carefully‐designed prompts: one “frame‐prompt” (e.g. “A series of photos recoding human action for [CLS]”) and one “point‐prompt” (e.g. “A sequence of points capturing human motion for [CLS]”).  \n3. These two prompts are run through the same Text Encoder to produce a frame‐prompt feature f_text_frame and a point‐prompt feature f_text_point.  \n4. We then form two contrastive losses:  \n   • L_frame = −⟨f_frame, f_text_frame⟩/τ  \n   • L_point = −⟨f_point, f_text_point⟩/τ  \n   where f_frame comes from the blue‐boxed frame‐encoder and f_point from the orange‐boxed point‐encoder.  \n5. By forcing both f_frame and f_point to align to their respective text embeddings, we in effect project both the frame and point representations into the same CLIP language feature space.  \n6. Because they share the same encoding backbone (the Text Encoder) and are both being “pulled” toward text anchors that describe the *same* underlying class, the frame features and point features become indirectly aligned with each other.  \n\nAnswer to the question, then:  \nBy plugging in the CLIP Text Encoder (red box) and using its two prompt embeddings as *common* anchors, we impose separate contrastive losses on both the blue‐boxed frame‐encoder outputs and the point‐encoder outputs.  Since both branches are being driven to match the *same* language‐space representations of the action class, their feature distributions collapse into a shared semantic space, thus aligning the frame and point modalities."
    },
    {
        "question": "How does contextualizing enhance Point Encoder’s representation of asynchronous event points?",
        "relevant_section_ids": [
            "3.2",
            "4.5"
        ],
        "relevant_context": [
            "Inspired by [51], we find that spiking firing in Spiking Neural Networks (SNN) aligns well with event-based sampling. Therefore, we address the challenges mentioned above by introducing a Spiking-like Context Learner (SCL) that can extract contextual event points from redundant raw events using Spiking Residual Recurrent Neural Network (SRRNN) and effectively integrate them with subsequent feature exploration.",
            "Taking into account the sparsity in spatial dimensions and the density in temporal dimensions of event data, we employ recurrent synaptic connectivity to extract a contiguous and information-dense subset of event points embedded with contextual information, as follows: ... Finally, we aggregate contextual event points Pc from E, where H and W denote the spatial size, T denotes the sampled timestep and C is the channel.",
            "To explore the superiority of the designed SCL, we visualize the events before/after processed by SCL on the SeAct dataset. Notably, we employ the intuitive stacked frames to represent the event points, with red indicating the event before SCL processing and blue indicating the event after SCL processing. As shown in Fig. 4, redundant event points are markedly diminished while critical event points are retained by the SCL. The results demonstrate that our proposed SCL effectively extracts raw event points by leveraging spatiotemporal contextual information, thereby alleviating the burden of feature exploration from event points."
        ],
        "final_answer": "Contextualizing—via the Spiking-like Context Learner—selects and aggregates raw event points into contiguous, information-dense slices governed by spiking thresholds. By pruning redundant points and emphasizing those with rich spatiotemporal context, it delivers a compact, context-aware sequence of event points to the Point Encoder, greatly enhancing its representation of asynchronous streams.",
        "relevant_elements": [
            "Contextualizing",
            "Point Encoder"
        ],
        "id": 331,
        "masked_question": "How does [mask1] enhance [mask2]’s representation of asynchronous event points?",
        "masked_number": 2,
        "masked_elements": [
            "Contextualizing",
            "Point Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.18328v1_figure_1.png",
        "paperid": "2411.18328v1",
        "paper_path": "./papers/2411.18328v1.json",
        "figure_id": "2411.18328v1_figure_1.png",
        "caption": "Figure 1: Insight of our work. Previous methods are limited to using “heavy” frame-specific encoders to extract features from densely stacked event frames or employing “light” point encoders to process sparse raw event points. Our Frame&Point-based approach integrates prompt-aware language semantic information to achieve a synergistic enhancement among the distinct event representations, balancing efficiency and accuracy.",
        "qtype": "Experimental_Results",
        "response": "The module in the red box is nothing more exotic than our Spiking-like Context Learner (SCL), and the blue box is the Event-Point Encoder (EPE).  What SCL does is sit in front of the EPE and turn a wildly sparse, purely “point‐by‐point” stream of events into a much more informative, context–aware subsequence before you ever touch the point encoder.  \n\nConcretely:  \n1.  SCL treats each incoming event as a spike in a leaky integrate-and-fire (LIF) neuron.  As events arrive, their “charge” accumulates in the membrane potential.  Once that potential crosses a threshold, SCL “fires,” records the run of events that led to it, resets, and carries on.  \n2.  Because it is a small recurrent convolutional network around that LIF core, SCL not only enforces the correct microsecond-level timing (no more arbitrary sliding windows) but also carries forward spatial features from one spike to the next.  In practice it means it throws away a huge amount of redundant noise and hands the downstream encoder a compact, information-dense block of events that are guaranteed to be temporally coherent.  \n3.  These “contextualized” event slices are exactly what the blue-boxed EPE (our Hilbert-scan Spiking Mamba) needs to do its job.  Instead of wrestling with millions of isolated, asynchronized points, it sees a small sequence of spikes that already carry local spatial topology and short-term temporal correlations.  That lets the EPE focus its SSMs and spiking feed-forward layers on true motion patterns, boosting the quality of the final point‐based feature representation.  \n\nIn short, the red SCL front-end “contextualizes” and prunes the raw event stream so that the blue point encoder never has to learn its own sampling or waste effort on redundant events.  The result is a much richer, much more temporally faithful embedding of asynchronous event points."
    },
    {
        "question": "How does HS-Adapter's integration of heterogeneous spaces refine attention maps for diverse nuclei domains?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "However, such homogeneous space mapping methods are difficult to learn heterogeneous relationships between different nuclei domains. To tackle the issue, we propose the HS-Adapter that leverages heterogeneous space integration to enhance the domain-specific feature representation of nuclei images.",
            "To improve the information interaction within Multi-Head Attention (MHA) layers, the HS-Adapter respectively concatenates learnable parameters A and B with the query Q and value V branches of SAM, where A and B are projection layers that map embeddings Z into feature spaces with i-th target mapping channel, U_i and V_i are up-projections. Additionally, we place the softmax operation s on W to calculate the weight of each feature space. Finally, W-weighted different feature spaces are merged into a heterogeneous space that is used to update the original query and value projection layers of SAM, guiding the computation of attention maps as: Attention(Q′,K,V′)=Softmax( Q′K^T/√d )V′."
        ],
        "final_answer": "The HS-Adapter projects the image embeddings into multiple learnable feature spaces (heterogeneous spaces) via parallel projection and up-projection layers. It then uses a softmax-weighted combination of these spaces to form a fused heterogeneous embedding, which replaces the standard query and value inputs in the Multi-Head Attention. By doing so, the attention maps are computed over a richer, domain-specific mixture of representations, allowing the model to better capture and distinguish diverse nuclei appearances across different domains.",
        "relevant_elements": [
            "HS-Adapter"
        ],
        "id": 332,
        "masked_question": "How does [mask1]'s integration of heterogeneous spaces refine attention maps for diverse nuclei domains?",
        "masked_number": 1,
        "masked_elements": [
            "HS-Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.11787v2_figure_1.png",
        "paperid": "2408.11787v2",
        "paper_path": "./papers/2408.11787v2.json",
        "figure_id": "2408.11787v2_figure_1.png",
        "caption": "Figure 1: The overview of our NuSegDG for domain-generalized nuclei image segmentation. (a) Heterogeneous Space Adapter. (b) Gaussian-Kernel Prompt Encoder. (c) Two-Stage Mask Decoder.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the red‐boxed module – the Heterogeneous-Space Adapter (HS-Adapter) – sharpens the self‐attention maps so that they work well across wildly different nuclei appearances:\n\n1.  **What lives in the red box?**  \n    That box is exactly the “(a) HS-Adapter” in Figure 1, inserted into each Multi-Head Attention (MHA) block of the frozen SAM encoder.\n\n2.  **Why “heterogeneous spaces”?**  \n    Simply using one shared Q–K–V projection for every domain tends to wash out subtle, domain-specific cues (shape, staining, blur, etc.). Instead, HS-Adapter carves the embedding space into several smaller “sub-spaces,” each of which can specialize in a different domain’s idiosyncrasies.\n\n3.  **How are they built?**  \n    – We take the incoming patch embeddings X and push them through a bank of learnable linear layers \\{A₁…Aₙ\\} (for Q) and \\{H₁…Hₙ\\} (for V), each producing its own “view” of X.  \n    – A tiny softmax gate across those n views produces weights α₁…αₙ that tell us which sub-spaces are most relevant on a per-patch basis.\n\n4.  **How do they refine attention?**  \n    – Instead of a single Q=WX and V=UX, we form  \n         Q′ = Q + ∑ₖ αₖ·(Aₖ(X))  \n         V′ = V + ∑ₖ αₖ·(Hₖ(X))  \n      so the original SAM’s query/value matrices are “nudged” by a learned mixture of domain-specialized projections.  \n    – When you then compute softmax(Q′Kᵀ)·V′, your attention weights and attended features now carry a fingerprint of those heterogeneous sub-spaces.  \n\n5.  **What’s the payoff?**  \n    By fusing multiple low-dimensional projections, each tuned to a different nuclei domain, HS-Adapter lets the self-attention heads dynamically re-weight and combine features in a way that is **both** globally consistent (via the frozen SAM core) **and** locally adaptive (via the heterogeneous sub-spaces). The result is attention maps that can, for example, pay more heed to texture in one stain and to shape in another, seamlessly handling diverse nuclear appearances without over-fitting to any single source domain."
    },
    {
        "question": "How do semantic prompts from GKP-Encoder guide cross-attention in TSM-Decoder to improve instance mask decoding?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "In this way, the proposed GKP-Encoder, driven by the single-point annotation, not only is labor-saving compared to the box annotation but also provides efficient semantic prompts P_sem and position prompts P_pos for guiding segmentation decoding.",
            "Following this, we combine the image embedding E with semantic information prompts P_sem: E' = E ⊕ P_sem, where ⊕ stands for the element-wise addition operation. Further, we conduct cross-attention with updated query Q' to generate decoding embeddings D."
        ],
        "final_answer": "The GKP-Encoder produces a set of semantic prompt embeddings (P_sem) from the point-based density map. In the TSM-Decoder, these semantic prompts are added element-wise to the frozen image embeddings (E) to form enriched embeddings (E′=E⊕P_sem). During cross-attention, the decoder’s query tokens attend over these enriched keys and values, which injects nucleus-specific semantic cues directly into the attention mechanism and leads to more accurate instance mask decoding.",
        "relevant_elements": [
            "GKP-Encoder",
            "TSM-Decoder"
        ],
        "id": 333,
        "masked_question": "How do semantic prompts from [mask1] guide cross-attention in [mask2] to improve instance mask decoding?",
        "masked_number": 2,
        "masked_elements": [
            "GKP-Encoder",
            "TSM-Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.11787v2_figure_1.png",
        "paperid": "2408.11787v2",
        "paper_path": "./papers/2408.11787v2.json",
        "figure_id": "2408.11787v2_figure_1.png",
        "caption": "Figure 1: The overview of our NuSegDG for domain-generalized nuclei image segmentation. (a) Heterogeneous Space Adapter. (b) Gaussian-Kernel Prompt Encoder. (c) Two-Stage Mask Decoder.",
        "qtype": "Experimental_Results",
        "response": "In NuSegDG, the “semantic prompts” (red‐boxed GKP‐Encoder output) are nothing more than a learned embedding of your single‐point density map.  During decoding (blue‐boxed TSM‐Decoder) we literally inject those embeddings into the cross‐attention as extra key/value information.  Concretely:\n\n 1. GKP‐Encoder (mask1) takes your user clicks → builds a Gaussian density map → runs a tiny CNN → spits out a 256‐channel semantic prompt tensor, P_sem.  \n 2. In the TSM‐Decoder (mask2), before we do any cross‐attention, we add P_sem, element‐wise, to the image feature map.  This augmented feature map now “remembers” where nuclei were hinted at by the point prompts.  \n 3. At cross‐attention time, our trainable query embeddings q′ (already fused with positional prompts) attend over this P_sem-augmented feature map as keys/values.  Because those keys/values carry strong, location‐specific nuclei cues, the attention heads learn to focus on the right spots and produce much crisper semantic masks, which then collapse naturally into an accurate instance map."
    },
    {
        "question": "What are possible limitations of HS-Adapter when handling highly imbalanced nuclei domain distributions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "HS-Adapter"
        ],
        "id": 334,
        "masked_question": "What are possible limitations of [mask1] when handling highly imbalanced nuclei domain distributions?",
        "masked_number": 1,
        "masked_elements": [
            "HS-Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.11787v2_figure_1.png",
        "paperid": "2408.11787v2",
        "paper_path": "./papers/2408.11787v2.json",
        "figure_id": "2408.11787v2_figure_1.png",
        "caption": "Figure 1: The overview of our NuSegDG for domain-generalized nuclei image segmentation. (a) Heterogeneous Space Adapter. (b) Gaussian-Kernel Prompt Encoder. (c) Two-Stage Mask Decoder.",
        "qtype": "Others",
        "response": "Here is a step‐by‐step breakdown of why the HS-Adapter (the red-boxed module in Fig. 1a) can run into trouble when the training domains are very imbalanced:\n\n1. What HS-Adapter does, at a glance  \n   – It takes the ViT’s queries and values and projects them into a small number (K) of “heterogeneous” sub-spaces, then re–weights and recombines them via a learned softmax.  \n   – The goal is to let the model carve out multiple parallel “views” of the data (e.g. different staining styles, cell shapes, textures) without changing the bulk of the ViT.\n\n2. How imbalance hurts that mechanism  \n   – Softmax Bias toward Majority  Because the weighting of each sub-space is learned across all patches and all domains, if one domain dominates the training set it will tend to drive up the softmax weights for the sub-spaces best tuned to that domain.  Rare‐domain sub-spaces get down‐weighted and learn very little.  \n   – Gradient Starvation on Minority Domains  When you have 90 % of your data from Domain A and only 10 % from Domain B, the adapter’s projection matrices mostly see A examples.  The small “islands” of B simply don’t contribute enough to shift the adapter parameters, so the model underfits B.  \n   – Fixed Number of Sub-spaces K  In the paper they used K = 2.  If you have more than two very different domains—some tiny, some huge—you may not have enough capacity or flexibility.  Cranking K up helps represent more domains but also blows up parameter and compute cost.  \n   – Overfitting to the “big” domains  With limited capacity the adapter will shape its projections around the data it sees most often, which not only starves the small domains, it can actually over‐specialize on the big ones and lose the ability to generalize.  \n   – Need for Careful Hyper-tuning  The softmax temperature, choice of K, and initialization all become much more sensitive when one or two domains dominate, so in practice you may find yourself constantly tweaking to rescue the minority cases.\n\nIn short, the HS-Adapter’s reliance on a small set of shared projections plus a global softmax reweighting makes it intrinsically vulnerable to training‐set imbalances: majority domains hog the projection “slots,” minority domains are under-represented or ignored, and the whole adapter can overfit the large classes while under-fitting the rare ones."
    },
    {
        "question": "What alternative approaches could replace GKP-Encoder’s Gaussian prompts to reduce sensitivity to varied nuclei sizes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GKP-Encoder"
        ],
        "id": 335,
        "masked_question": "What alternative approaches could replace [mask1]’s Gaussian prompts to reduce sensitivity to varied nuclei sizes?",
        "masked_number": 1,
        "masked_elements": [
            "GKP-Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2408.11787v2_figure_1.png",
        "paperid": "2408.11787v2",
        "paper_path": "./papers/2408.11787v2.json",
        "figure_id": "2408.11787v2_figure_1.png",
        "caption": "Figure 1: The overview of our NuSegDG for domain-generalized nuclei image segmentation. (a) Heterogeneous Space Adapter. (b) Gaussian-Kernel Prompt Encoder. (c) Two-Stage Mask Decoder.",
        "qtype": "Others",
        "response": "In our paper, the thing we’ve boxed in red is the “GKP-Encoder,” i.e. the Gaussian-Kernel Prompt Encoder.  Its only job is to turn each single‐point click into a little 2D Gaussian spot (of fixed σ) so that the segmentation decoder knows roughly where each nucleus sits.  The obvious weakness of that strategy is that if your σ is too small you miss large nuclei, if it’s too large you swamp small ones.\n\nIf you wanted to break that size-sensitivity altogether you could simply swap out the fixed-σ Gaussians for any one of a number of size‐adaptive center-prompts that have appeared in the literature:\n\n  1.  Multi-scale or learnable Gaussian banks  \n     •  Instead of one σ you build a small set of Gaussians at different σ’s (say 3, 7, 11 pixels) and let the network learn to weight them per point.  \n     •  Or even more flexibly, have a tiny “σ‐prediction” head that looks at the local image patch and spits out the best σ for that particular nucleus before you form your heatmap.  \n\n  2.  Distance functions / level sets  \n     •  Compute a signed (or unsigned) distance-to-center map (i.e. the result of a morphological distance transform from the clicked point).  Large nuclei get gently sloping hills, small ones steep hills, but everyone gets a unique gradient shape.  \n     •  Alternatively, use a full level-set (i.e. binary interior/exterior plus distance) as the prompt.  \n\n  3.  Vector or flow-field prompts (à la Cellpose)  \n     •  Instead of a scalar heatmap you learn a 2-D vector field that points every pixel back to its cell center.  Because the magnitude of the vector is proportional to the distance to the center, it automatically encodes size.  \n\n  4.  Star-convex polygon encodings (à la StarDist)  \n     •  From each center point you predict radii along, say, 32 rays.  You can then render a prompt that is literally the union of all those line segments — large cells have long rays, small cells have short ones.  \n\nAll of these strategies remove the need to hand-tune a single σ for every possible nucleus size in your dataset.  Instead the network either picks the right scale itself (multi-σ or a σ-predictor), or uses a fundamentally size-adaptive shape (distance map, flow field or star-convex polygon)."
    },
    {
        "question": "What are limitations of binary node pair classifier in capturing community structures under extreme class imbalance?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "binary node pair classifier"
        ],
        "id": 336,
        "masked_question": "What are limitations of [mask1] in capturing community structures under extreme class imbalance?",
        "masked_number": 1,
        "masked_elements": [
            "binary node pair classifier"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.00670v1_figure_1.png",
        "paperid": "2409.00670v1",
        "paper_path": "./papers/2409.00670v1.json",
        "figure_id": "2409.00670v1_figure_1.png",
        "caption": "Figure 1: An overview about the applications of (i) foundation models (e.g., LLMs) and (ii) our PR-GPT method, including the (a) offline pre-training and (b) online inference. The inference of PR-GPT includes the (b.1) online generalization and (b.2) online refinement.",
        "qtype": "Others",
        "response": "Image–text alignment  \n– In Fig. 1 (b.1) the red box highlights the “Binary Node Pair Classifier,” i.e. the module in PR-GPT that, given two node embeddings, predicts 1 if they belong to the same block and 0 otherwise.  \n\nChain-of-thought  \n1.  PR-GPT reformulates graph partitioning as a binary classification over node pairs: positive examples are edges within the same community, negatives are edges across communities.  \n2.  In many real graphs the number of “across-community” pairs far exceeds “within-community” pairs, so training/testing sees extreme class imbalance.  \n3.  A simple MLP on distance/inner-product features will tend to be biased toward the majority (negative) class: it will overpredict “different block,” missing true intra-community links.  \n4.  Because it treats each pair in isolation, it cannot exploit higher-order or global connectivity cues (e.g. triangles, motifs) that help distinguish tight communities.  \n5.  Under severe imbalance this leads to poor recall on the minority (within‐block) class, threshold‐sensitivity, and a higher false‐negative rate—splitting real communities.  \n\nAnswer  \nUnder extreme class imbalance between “same‐block” and “different‐block” pairs, the Binary Node Pair Classifier (the red‐boxed module) tends to be overwhelmed by the abundant negative examples, biasing it toward predicting “different block.”  It suffers from low recall on true intra‐community links, poor calibration of its decision threshold, and—in relying solely on local pairwise features—cannot recover long‐range or higher‐order connectivity patterns that are often crucial to delineate tight-knit communities."
    },
    {
        "question": "What scalability bottlenecks arise in online refinement using an efficient GP method on massive graphs?",
        "relevant_section_ids": [
            "3.3.2",
            "3.4"
        ],
        "relevant_context": [
            "“Compared with running a refinement method on G from scratch, online refinement may be much more efficient, since it reduces the number of nodes to be processed (e.g., reducing N nodes to N′ super-nodes in Fig. 2). Therefore, PR-GPT has the potential to achieve faster GP w.r.t. the refinement method.” (Section III-C2)",
            "“Our analysis about inference time (see Table VII) shows that online refinement is the major bottleneck of PR-GPT.” (Section III-D)"
        ],
        "final_answer": "Even though PR-GPT’s online refinement works on a much smaller super-graph (by merging blocks into super-nodes), running the chosen GP refinement algorithm (e.g., InfoMap) on that weighted super-graph still dominates the total inference time. In practice, this refinement step becomes the major scalability bottleneck when applying PR-GPT to very large graphs.",
        "relevant_elements": [
            "online refinement",
            "efficient GP method"
        ],
        "id": 337,
        "masked_question": "What scalability bottlenecks arise in [mask1] using an [mask2] on massive graphs?",
        "masked_number": 2,
        "masked_elements": [
            "online refinement",
            "efficient GP method"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.00670v1_figure_1.png",
        "paperid": "2409.00670v1",
        "paper_path": "./papers/2409.00670v1.json",
        "figure_id": "2409.00670v1_figure_1.png",
        "caption": "Figure 1: An overview about the applications of (i) foundation models (e.g., LLMs) and (ii) our PR-GPT method, including the (a) offline pre-training and (b) online inference. The inference of PR-GPT includes the (b.1) online generalization and (b.2) online refinement.",
        "qtype": "Others",
        "response": "Here is the step‐by‐step reasoning:\n\n1.  Image–text alignment  \n    - In the figure, the **red box** surrounds the final graph sketch under the label “Refined GP Result” and the subheading “(b.2) Online Refinement.”  \n    - The **blue box** inside that red region calls out “Efficient GP Method (e.g., InfoMap).”  \n\n2.  Mapping masks to terms  \n    - [mask1] → the **online refinement** phase of PR-GPT (i.e. step b.2 in the diagram).  \n    - [mask2] → the **efficient GP method** (e.g. InfoMap) used during that refinement.  \n\n3.  Locating the bottleneck in the text  \n    - In Section III-D the authors explicitly state that **“online refinement is the major bottleneck of PR-GPT,”** and that it stems from invoking an off-the-shelf GP routine (e.g. InfoMap) on the (super-)graph.  \n\n4.  Why it doesn’t scale  \n    - Although the graph is coarsened into super-nodes, running a full GP algorithm on that weighted super-graph still requires traversing all its edges, optimizing code lengths (in the case of InfoMap), and holding large adjacency structures in memory.  \n    - As the original graph grows into the millions or beyond, even the coarsened graph can be too large, so the refinement step again dominates both time and space.  \n\nAnswer:\n\nWhat scalability bottlenecks arise in **online refinement** using an **efficient GP method** on massive graphs?\n\n→ Even though PR-GPT first merges nodes into a smaller super-graph, running a full GP algorithm (e.g. InfoMap) on that super-graph still incurs high computational cost (edge traversals, global optimization passes) and large memory overhead. This makes the online refinement stage the dominant runtime and memory bottleneck when scaling to very large graphs."
    },
    {
        "question": "What motivates freezing parameters during online generalization rather than fine-tuning on new large graphs?",
        "relevant_section_ids": [
            "1",
            "3.3.1"
        ],
        "relevant_context": [
            "In this study, we explore the potential of deep graph learning (DGL) to obtain a better trade-off between the quality and efficiency of GP. … After that, we directly generalize the pre-trained model (with frozen model parameters) to large graphs  (e.g., more than M nodes) via inductive inference [16 ###reference_b16###] and derive feasible GP results  without re-training (i.e., online generalization).",
            "III-C1 Online Generalization: After the offline pre-training, we can generalize PR-GPT to a large graph  with frozen parameters  and derive a feasible partition  via Algorithm 1 ###reference_###."
        ],
        "final_answer": "Freezing the parameters enables purely inductive inference on new, large graphs without any additional training, which dramatically reduces online compute cost and latency. By not fine-tuning, PR-GPT can immediately produce high-quality partitions at inference time—akin to how an LLM like ChatGPT serves responses without retraining—thus ensuring high efficiency in the online generalization phase.",
        "relevant_elements": [
            "online generalization",
            "frozen parameters"
        ],
        "id": 338,
        "masked_question": "What motivates freezing parameters during [mask1] rather than fine-tuning on new large graphs?",
        "masked_number": 1,
        "masked_elements": [
            "online generalization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.00670v1_figure_1.png",
        "paperid": "2409.00670v1",
        "paper_path": "./papers/2409.00670v1.json",
        "figure_id": "2409.00670v1_figure_1.png",
        "caption": "Figure 1: An overview about the applications of (i) foundation models (e.g., LLMs) and (ii) our PR-GPT method, including the (a) offline pre-training and (b) online inference. The inference of PR-GPT includes the (b.1) online generalization and (b.2) online refinement.",
        "qtype": "Design_Rationale",
        "response": "First, note that the red‐boxed region in the figure is exactly the “online generalization” phase (b.1), where PR-GPT applies the frozen, pre-trained GNN and classifier to a brand-new, large graph without any further weight updates.  Now, why do we freeze instead of fine-tuning?\n\nChain of thought:\n\n1.  **Scale and cost of large graphs.**  Fine-tuning a GNN on a huge graph is extremely expensive in both time and memory (it can even blow out GPU RAM on the largest Graph Challenge instances).\n\n2.  **Inductive transfer.**  By pre-training on a diverse corpus of small graphs, the model has already seen a wide variety of topologies.  Those learned weights act as a universal “community-detector” that generalizes out of the box.\n\n3.  **Analogy to LLMs.**  Just as you don’t re-train ChatGPT every time you ask it a question, you can “freeze” PR-GPT and get high-quality partitions instantly, without any extra gradient steps.\n\n4.  **Downstream refinement.**  The frozen-parameter pass gives a good initial partition which is then cheaply refined by a much lighter-weight graph-partitioning heuristic (e.g. InfoMap) on a compressed super‐graph.  That two-stage approach is far faster than re-training the entire GNN on the full graph.\n\nPutting it all together, PR-GPT freezes its parameters during the online generalization stage because:\n\n‐ It avoids the prohibitive compute and memory cost of fine-tuning on very large graphs  \n‐ The pre-trained weights already capture general community structure via inductive learning  \n‐ It mirrors the “inference-only” usage of modern foundation models to get rapid, high-quality results  \n‐ It yields a strong initialization which downstream refinement can polish far more cheaply than full re-training"
    },
    {
        "question": "What rationale supports binary node pair classification combined with GNN embeddings for graph partitioning?",
        "relevant_section_ids": [
            "3.1",
            "3.1.2",
            "3.1.3"
        ],
        "relevant_context": [
            "PR-GPT reformulates GP as the binary node pair classification and follows a GNN-based end-to-end architecture. An auxiliary variable yᵢⱼ is introduced to represent the binary classification result, where yᵢⱼ=1 if nodes i,j are in the same block and 0 otherwise.",
            "The extracted features H are fed into a multi-layer GNN, which further derives community-preserving embeddings. ... This mechanism further enhances the ability of H to capture community structures, since it forces nodes i with similar neighbors (i.e., dense local linkage) to have similar representations.",
            "Given a node pair (i,j), PR-GPT adopts the following binary classifier ... Namely, ŷᵢⱼ is estimated via a combination of the (i) Euclidean distance and (ii) inner product w.r.t. corresponding embeddings."
        ],
        "final_answer": "By treating graph partitioning as a collection of binary decisions on whether two nodes belong to the same block, PR-GPT can leverage a standard GNN to produce embeddings that naturally cluster nodes with dense mutual connectivity. The GNN’s neighbor-aggregation ensures that nodes in the same community have similar representations, and a lightweight pairwise classifier (using distances and inner products over these embeddings) can then accurately predict pairwise block membership. Finally, positive predictions induce connected components that form the final partition.",
        "relevant_elements": [
            "binary node pair classifier",
            "GNN"
        ],
        "id": 339,
        "masked_question": "What rationale supports [mask1] combined with [mask2] embeddings for graph partitioning?",
        "masked_number": 2,
        "masked_elements": [
            "binary node pair classifier",
            "GNN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.00670v1_figure_1.png",
        "paperid": "2409.00670v1",
        "paper_path": "./papers/2409.00670v1.json",
        "figure_id": "2409.00670v1_figure_1.png",
        "caption": "Figure 1: An overview about the applications of (i) foundation models (e.g., LLMs) and (ii) our PR-GPT method, including the (a) offline pre-training and (b) online inference. The inference of PR-GPT includes the (b.1) online generalization and (b.2) online refinement.",
        "qtype": "Design_Rationale",
        "response": "Here is a step‐by‐step on why we take our frozen GNN embeddings (blue box) and feed them into a simple binary node–pair classifier (red box) to do graph partitioning:\n\n1.  Community-preserving embeddings  \n    – We first train a multi-layer (but linearized) GNN that, by repeated averaging over neighbors, pushes nodes in the same dense “community” to nearby points in a low-dimensional space.  After ℓ₂–normalizing each row, nodes in the same block lie close together on the unit sphere.  \n\n2.  Binary pair classification  \n    – Rather than try to directly cut the graph or maximize modularity at inference time, we freeze those GNN weights and learn one tiny MLP that, given any two frozen embeddings xᵢ and xⱼ, predicts “same block?” as a 0/1 label.  Concretely it looks at the Euclidean distance ‖xᵢ–xⱼ‖ and the inner product xᵢᵀ xⱼ, which are both meaningful measures of similarity on ℓ₂-normalized vectors.\n\n3.  Why this combo works  \n    – The GNN embeddings already encode the high-order connectivity structure (so that intra-community nodes cluster).  \n    – A shallow binary classifier on top is all you need to tease apart “same vs. different” communities by learning a simple boundary in that embedding space.  \n    – This turns the global partitioning problem into many independent pairwise decisions—very fast at inference, easily frozen for transfer to new graphs, and still strong enough to recover high‐quality blocks.\n\nIn short, frozen GNN embeddings give you a community‐aware geometry, and the tiny binary node‐pair MLP exploits that geometry via distance and dot‐product features to recover each graph partition efficiently."
    },
    {
        "question": "What guides the design of an embedding-based anomaly detector preceding slow chain-of-thought reasoning?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "As shown in Fig. 1, AESOP splits the monitoring task into two separate stages: The first is rapid, real-time detection of anomalies—conditions that deviate from the nominal conditions where the robot performs reliably—by querying similarity with previously recorded observations within the contextual embedding space of an LLM.",
            "Fast Anomaly Detection: To detect anomalies, we need to inform a FM of the context within which the autonomous system is known to be trustworthy. The prior, nominal experiences of the robot serve as such grounding. We construct an anomaly score function s to query whether a current observation oₜ differs from the previous experiences in Dₙ. We do not require any particular methodology to generate the score, we just require that scoring an observation is computationally feasible in real-time; that is, within a single time step. This work emphasizes the value of computing anomaly scores using language-based representations, which we show capture the semantics of the observation within the context of the robot’s task."
        ],
        "final_answer": "The embedding-based anomaly detector is guided by grounding the current observation in the robot’s prior nominal experiences and the need for a lightweight, real-time score: it embeds the observation with a small FM, compares it (e.g., via cosine similarity) against a cache of embeddings from safe, previously seen data, and flags an anomaly if the score crosses a threshold calibrated on those nominal examples.",
        "relevant_elements": [
            "Embedding-based Anomaly Detector",
            "Autoregressive Generation"
        ],
        "id": 340,
        "masked_question": "What guides the design of a [mask1] preceding slow [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Embedding-based Anomaly Detector",
            "Autoregressive Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.08735v1_figure_1.png",
        "paperid": "2407.08735v1",
        "paper_path": "./papers/2407.08735v1.json",
        "figure_id": "2407.08735v1_figure_1.png",
        "caption": "Figure 1: We present an embedding-based runtime monitoring scheme using fast and slow language model reasoners in concert. During nominal operation, the fast reasoner differentiates between nominal and anomalous robot observations. If an anomaly is flagged, the system enters a fallback-safe state while the slow reasoner determines the anomaly s hazard. In this fallback-safe state, we guarantee access to a set of safe recovery plans (if the anomaly is consequential) and access to continued nominal operation (if the anomaly is inconsequential).",
        "qtype": "Design_Rationale",
        "response": "The key driver is simply real‐time safety.  In order to close the loop on an agile robot you cannot afford to run a full, chain-of-thought LLM every time you get a new observation – it’s too slow and expensive.  So you front-end the slow, autoregressive (chain-of-thought) generator with a lightweight, embedding-based anomaly detector whose job is:\n\n  1.  To live in the LLM’s frozen embedding space and compare the current observation to a cache of prior (“nominal”) embeddings.  \n  2.  To compute a simple similarity or distance score (e.g. max‐cosine, kNN) in a single control time step.  \n  3.  To gate the expensive, slow generator so that you only invoke full chain-of-thought reasoning when something truly out‐of‐distribution has been flagged.  \n\nIn short, the requirement to detect out-of-distribution inputs within one control cycle (bounded latency) guides the design of the embedding-based anomaly detector, and it sits in front of the slow, autoregressive chain-of-thought LLM so that you only pay that computational cost when—and only when—you really need it."
    },
    {
        "question": "How does leveraging MPC-maintained Tree of Recovery Trajectories mitigate autoregressive generation latency?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "Whether inference is run onboard or the model is queried remotely over unreliable networks in the cloud, we must account for the latency that autoregressive reasoning introduces. For example, a fast moving vehicle may collide with an anomalous obstacle if its reaction time is too slow. Therefore, we account for the LLM’s compute latency by assuming that it takes at most τ timesteps to receive the output string from the slow reasoner.",
            "The second fixes consensus for τ timesteps along the set of recovery trajectories, in effect generating a branching tree of recovery trajectories. If we then use the fast anomaly detector to both trigger execution of the first τ actions of the recovery trajectories and the slower reasoning, we ensure that the options we provide to the slow reasoner are still available when it returns its output."
        ],
        "final_answer": "By maintaining a branching tree of recovery trajectories that fixes the first τ actions of each candidate intervention, the MPC controller can immediately begin executing those pre-planned fallback steps as soon as an anomaly is detected. This effectively buys the τ timesteps needed for the slow, autoregressive LLM reasoning to complete, ensuring that all safe intervention options remain dynamically feasible despite the LLM’s inference latency.",
        "relevant_elements": [
            "MPC-maintained Tree of Recovery Trajectories",
            "Autoregressive Generation"
        ],
        "id": 341,
        "masked_question": "How does leveraging [mask1] mitigate [mask2] latency?",
        "masked_number": 2,
        "masked_elements": [
            "MPC-maintained Tree of Recovery Trajectories",
            "Autoregressive Generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.08735v1_figure_1.png",
        "paperid": "2407.08735v1",
        "paper_path": "./papers/2407.08735v1.json",
        "figure_id": "2407.08735v1_figure_1.png",
        "caption": "Figure 1: We present an embedding-based runtime monitoring scheme using fast and slow language model reasoners in concert. During nominal operation, the fast reasoner differentiates between nominal and anomalous robot observations. If an anomaly is flagged, the system enters a fallback-safe state while the slow reasoner determines the anomaly s hazard. In this fallback-safe state, we guarantee access to a set of safe recovery plans (if the anomaly is consequential) and access to continued nominal operation (if the anomaly is inconsequential).",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:  \n1. The blue box in the figure is the “Slow” leg of the monitor (the autoregressive LLM-based reasoner), which can take up to T timesteps to return its judgment.  \n2. If we simply stopped or waited for the LLM to finish, the robot might violate safety constraints or run out of room to maneuver.  \n3. The red box is the MPC routine that, at every step, optimizes not only the nominal plan but also a small tree of “recovery” branches—one branch for each possible intervention the LLM might eventually choose.  \n4. By enforcing consensus constraints on the first action of every branch, the robot can begin executing a safe, slowdown maneuver that keeps *all* those branches (i.e. all possible recovery sets) dynamically feasible for the next T timesteps.  \n5. In effect, the robot “buys time”: it doesn’t have to make a hard choice until the LLM finishes, because it has already committed to a buffer of viable fallback trajectories.  \n6. Thus, when the slow reasoner finally outputs “go nominal” or “take recovery #k,” the robot can immediately snap to that branch without ever violating safety or having to replan at the last second.  \n\nAnswer:  \nLeveraging the MPC-maintained tree of recovery trajectories ensures that, as soon as an anomaly is flagged, the robot begins a controlled slow-down along a family of pre-computed fallback branches. Because each branch is guaranteed to reach its designated recovery set within the LLM’s worst-case latency window, the system can safely “buy” the T timesteps needed for the autoregressive reasoner to finish, then instantly commit to whichever branch the LLM selects—thereby masking the slow-reasoner latency and preserving safety."
    },
    {
        "question": "How does the embedding-based anomaly detector calibrate its anomaly threshold using nominal experience embeddings online?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Finally, to classify whether an observation should be treated as nominal or anomalous, we can calibrate a threshold τ as the p quantile of the nominal prior experiences, i.e., the smallest value of A(x) that upper bounds at least p nominal samples.",
            "Note that for nominal embeddings, we must compute the anomaly score A(x_i) in a leave-one-out fashion, since A(x_i) for i."
        ],
        "final_answer": "The detector computes anomaly scores for all prior (nominal) embeddings—scoring each nominal point in a leave-one-out manner—and then sets its threshold τ to the empirical p-th quantile of those scores (i.e. the smallest score exceeding at least p of the nominal samples).",
        "relevant_elements": [
            "Embedding-based Anomaly Detector"
        ],
        "id": 342,
        "masked_question": "How does the [mask1] calibrate its anomaly threshold using nominal experience embeddings online?",
        "masked_number": 1,
        "masked_elements": [
            "Embedding-based Anomaly Detector"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.08735v1_figure_1.png",
        "paperid": "2407.08735v1",
        "paper_path": "./papers/2407.08735v1.json",
        "figure_id": "2407.08735v1_figure_1.png",
        "caption": "Figure 1: We present an embedding-based runtime monitoring scheme using fast and slow language model reasoners in concert. During nominal operation, the fast reasoner differentiates between nominal and anomalous robot observations. If an anomaly is flagged, the system enters a fallback-safe state while the slow reasoner determines the anomaly s hazard. In this fallback-safe state, we guarantee access to a set of safe recovery plans (if the anomaly is consequential) and access to continued nominal operation (if the anomaly is inconsequential).",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed “Embedding-based Anomaly Detector” settles on its threshold at run‐time:\n\n1. Offline, collect a cache E = {e₁, e₂, …, eₙ} of embedding vectors for known good (“nominal”) observations, all produced by your chosen embedding FM.\n\n2. At each new time step:\n   a. Embed the incoming observation x into eₓ.  \n   b. Compute an anomaly score s(eₓ) by comparing eₓ to the cache E (for instance, 1 – max cosine‐similarity against E, or any other fast distance‐based heuristic).\n\n3. To decide “how big is big enough to call an anomaly?”, you take all the nominal embeddings eᵢ and (in leave-one-out fashion) compute their own scores s(eᵢ) against the remainder of the cache.  \n   - This gives you a distribution of scores you know are *not* anomalous.\n\n4. You pick a quantile φ (say 95 %) and set your threshold τ to be the φ‐quantile of that nominal score distribution.  \n   - Concretely, τ is the smallest value such that at least φ · n of the nominal s(eᵢ) lie below τ.\n\n5. Online, any new score s(eₓ) > τ is flagged “anomalous,” otherwise it’s “nominal.”\n\nBy continually comparing each new embedding against the cache and keeping τ anchored to the empirical φ‐quantile of past nominal scores (computed in a leave-one-out way), the system self-calibrates its anomaly detector in real time."
    },
    {
        "question": "How does the MPC-maintained tree of recovery trajectories coordinate consensus across embedding-based anomaly detector and autoregressive generation latencies?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "4.1: Whether inference is run onboard or the model is queried remotely over unreliable networks in the cloud, we must account for the latency that autoregressive reasoning introduces. For example, a fast moving vehicle may collide with an anomalous obstacle if its reaction time is too slow. Therefore, we account for the LLM’s compute latency by assuming that it takes at most k timesteps to receive the output string from the slow reasoner.",
            "4.2: In addition, the MPC problem includes two consensus constraints, one associated with the fast anomaly detector and the other with the slow reasoner. First, by fixing consensus along the first input of the nominal trajectory and all the recovery trajectories, we ensure that the set of feasible interventions is non-empty during nominal operation. The second fixes consensus for k timesteps along the set of recovery trajectories, in effect generating a branching tree of recovery trajectories. If we then use the fast anomaly detector to both trigger execution of the first k actions of the recovery trajectories and the slower reasoning, we ensure that the options we provide to the slow reasoner are still available when it returns its output."
        ],
        "final_answer": "The MPC enforces two consensus constraints: (1) it locks the very first control input across the nominal plan and every recovery trajectory so that, as soon as the embedding‐based anomaly detector fires, the robot can immediately begin following one of the recovery branches; and (2) it keeps all recovery trajectories identical for k timesteps (the worst‐case LLM response time). This constructs a branching tree of fallback plans that remain dynamically feasible while the slow, autoregressive LLM is still reasoning, guaranteeing that whichever intervention the LLM eventually selects will still be available.",
        "relevant_elements": [
            "MPC-maintained Tree of Recovery Trajectories",
            "Embedding-based Anomaly Detector",
            "Autoregressive Generation"
        ],
        "id": 343,
        "masked_question": "How does the [mask1] coordinate consensus across embedding-based anomaly detector and autoregressive generation latencies?",
        "masked_number": 1,
        "masked_elements": [
            "MPC-maintained Tree of Recovery Trajectories"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.08735v1_figure_1.png",
        "paperid": "2407.08735v1",
        "paper_path": "./papers/2407.08735v1.json",
        "figure_id": "2407.08735v1_figure_1.png",
        "caption": "Figure 1: We present an embedding-based runtime monitoring scheme using fast and slow language model reasoners in concert. During nominal operation, the fast reasoner differentiates between nominal and anomalous robot observations. If an anomaly is flagged, the system enters a fallback-safe state while the slow reasoner determines the anomaly s hazard. In this fallback-safe state, we guarantee access to a set of safe recovery plans (if the anomaly is consequential) and access to continued nominal operation (if the anomaly is inconsequential).",
        "qtype": "Implementation_Details",
        "response": "First, notice that the red‐boxed label in the figure reads “MPC-maintained Tree of Recovery Trajectories.”  In the text this corresponds to the special finite-horizon MPC they introduce in §IV-B.  The way that MPC tree “bridges” the fast detector and the slow LLM is by enforcing two sets of consensus constraints on all of its planned trajectories:\n\n  1.  It forces the very first control input of every trajectory—both the nominal (no‐anomaly) plan and all of the candidate recovery plans—to be identical.  As soon as the embedding‐based detector flags an anomaly, the robot can immediately execute that single, shared action without yet knowing which recovery branch it will ultimately pick.\n\n  2.  It further “locks in” a common prefix of length T̄ (the worst‐case LLM latency) across all of the recovery trajectories.  In other words, for T̄ steps after an anomaly is detected, every recovery option still shares the same state–input path.  That guarantees that whatever intervention the slow LLM finally outputs (after at most T̄ steps), the robot can still follow that branch without having violated any constraints or lost feasibility.\n\nIn this way the MPC tree hands off instantly to the fast detector (by executing the shared first action) and then “holds open” all the candidate fallback options for exactly as long as the slow LLM needs to answer.  When the LLM finishes reasoning, its chosen branch in the recovery tree is still viable, so the robot can safely commit to it."
    },
    {
        "question": "How does dynamic camera pose synthesis apply perturbation κ to OEM parameters while sampling φ, θ, and α?",
        "relevant_section_ids": [
            "4.1.1",
            "4.1.2"
        ],
        "relevant_context": [
            "To create a robust model capable of handling changes in calibration, we introduce perturbations to the OEM camera parameters, generating perturbed parameters P′, where δ represents the perturbation defined as P′ = P + δ, with κ controlling the maximum desired perturbation.",
            "Specifically, the centroid’s position P on the hemisphere is determined by: P = [r sin φ cos θ, r sin φ sin θ, r cos φ], where r denotes the hemisphere radius, and φ, θ are angles sampled from a uniform distribution, ensuring the centroid is randomly positioned over the hemisphere.",
            "Next, a rotation R is applied to align the camera’s viewing direction towards the fiducials’ centroid, and then a secondary random rotation R′ is applied by an intrinsic rotation angle α around the centroid point."
        ],
        "final_answer": "Dynamic camera pose synthesis first perturbs the OEM intrinsic and extrinsic parameters by adding a random offset δ to each parameter (P′ = P + δ) with δ bounded by κ. Then it samples φ and θ uniformly to place the camera’s centroid on a hemisphere of radius r, applies a rotation R to point the camera towards the calibration object, and finally applies an additional random rotation of angle α about the viewing axis to introduce further variability.",
        "relevant_elements": [
            "dynamic camera pose synthesis",
            "perturbation κ",
            "φ, θ, and α"
        ],
        "id": 344,
        "masked_question": "How does [mask1] apply perturbation κ to OEM parameters while sampling φ, θ, and α?",
        "masked_number": 1,
        "masked_elements": [
            "dynamic camera pose synthesis"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.14505v1_figure_2.png",
        "paperid": "2410.14505v1",
        "paper_path": "./papers/2410.14505v1.json",
        "figure_id": "2410.14505v1_figure_2.png",
        "caption": "Figure 2: Technical overview. Our methodology begins with the synthesis of dynamic camera poses (see top fig.). Given spherical angles ϕitalic-ϕ\\phiitalic_ϕ (azimuth), θ𝜃\\thetaitalic_θ (elevation), along with the intrinsic rotation angle α𝛼\\alphaitalic_α, the OEM calibration parameters, the maximum perturbation limit κ𝜅\\kappaitalic_κ, and known 3D fiducials (e.g. a cube calibration object), this module performs two primary functions: (i) it synthesizes poses for the multi-camera system, and (ii) it computes the projected 2D points. Subsequently, it employs point splatting to render images of these points. During training (see bottom fig.), the synthesizes poses and projected points (alternatively rendered images) are used to train the neural network. A differentiable projection ensures the propagation of gradients from the loss ℒℒ\\mathcal{L}caligraphic_L back to the predicted camera parameters.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module on the top row of Fig. 2 is nothing more exotic than the “perturb‐and‐sample” block described in §4.1.1.  In practice what it does is:\n\n  1. Take the OEM parameter vector P₀ = [r, t, fₓ, f_y, cₓ, c_y, k₁,₂,₃, p₁,₂]  \n  2. Draw a random offset ΔP of the same dimensionality, with each component Δpᵢ ∼ Uniform(−κ, +κ)  \n  3. Form the perturbed parameters P′ = P₀ + ΔP  \n  4. At the same time, draw the three viewing angles for the hemisphere placement:  \n     • φ ∼ Uniform(0, 2π) (azimuth)  \n     • θ ∼ Uniform(0, π/2) (elevation over the upper hemisphere)  \n     • α ∼ Uniform(0, 2π) (intrinsic roll about the viewing axis)  \n\nIn short, the module simply adds uniformly bounded noise (±κ) to every camera parameter and, in parallel, samples φ, θ, α uniformly to generate a new, perturbed camera pose for that training iteration."
    },
    {
        "question": "How does differentiable projection enable gradient flow from 2D projected points to camera parameters?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "First, we introduce a real-time neural calibration method for multi-camera systems, marking a departure from traditional offline calibration methods. Our method employs a differentiable projection model to flow gradients between 3D geometries and their 2D projections, allowing for direct optimization of camera parameters.",
            "Differentiable Projection. The image formation process with the pinhole camera model is designed to be differentiable, facilitating the backpropagation of gradients from the loss - a function of the difference between the observed and projected points - to the camera parameters."
        ],
        "final_answer": "By formulating the pinhole camera projection (including lens distortion) as a differentiable function, the network can compute how small changes in the camera parameters affect the 2D projected points. When a reprojection loss is computed between observed and predicted 2D points, gradients can be back-propagated through this differentiable projection step directly to the intrinsic and extrinsic camera parameters, enabling their end-to-end optimization.",
        "relevant_elements": [
            "differentiable projection",
            "2D projected points",
            "camera parameters"
        ],
        "id": 345,
        "masked_question": "How does [mask1] enable gradient flow from 2D projected points to camera parameters?",
        "masked_number": 1,
        "masked_elements": [
            "differentiable projection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.14505v1_figure_2.png",
        "paperid": "2410.14505v1",
        "paper_path": "./papers/2410.14505v1.json",
        "figure_id": "2410.14505v1_figure_2.png",
        "caption": "Figure 2: Technical overview. Our methodology begins with the synthesis of dynamic camera poses (see top fig.). Given spherical angles ϕitalic-ϕ\\phiitalic_ϕ (azimuth), θ𝜃\\thetaitalic_θ (elevation), along with the intrinsic rotation angle α𝛼\\alphaitalic_α, the OEM calibration parameters, the maximum perturbation limit κ𝜅\\kappaitalic_κ, and known 3D fiducials (e.g. a cube calibration object), this module performs two primary functions: (i) it synthesizes poses for the multi-camera system, and (ii) it computes the projected 2D points. Subsequently, it employs point splatting to render images of these points. During training (see bottom fig.), the synthesizes poses and projected points (alternatively rendered images) are used to train the neural network. A differentiable projection ensures the propagation of gradients from the loss ℒℒ\\mathcal{L}caligraphic_L back to the predicted camera parameters.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be clear what the red‐boxed module is doing.  If you look at the diagram and read the caption (“Differentiable Projection”), that box implements the entire pinhole + distortion camera model—i.e.  \n  1)  Rotates and translates the 3D fiducial points by the predicted 6D‐rotation/vector and translation,  \n  2)  Applies the focal‐length / principal‐point projection (X/Z, Y/Z → u, v)  \n  3)  Warps (u, v) with the radial‐ and tangential‐distortion polynomials  \n\nBecause every single one of those steps is written in terms of elementary, smooth operations (matrix multiplies, divides by Z, polynomial evaluations, additions), modern deep‐learning toolkits can automatically compute exact partial derivatives through them.  Concretely, when you form the reprojection‐error loss L = ‖(u, v)pred – (u, v)gt‖², the autograd engine applies the chain rule through step 3 back into step 2 and all the way into step 1.  In this way the network “knows” how a tiny tweak to fₓ or to one of the rotation parameters would nudge the final (u, v) and thus reduce the loss.  That is exactly how the Differentiable Projection module enables gradient flow from the 2D projections back to every camera parameter."
    },
    {
        "question": "How does dynamic camera pose synthesis leverage OEM calibration parameters compared to offline multi-view calibration methods?",
        "relevant_section_ids": [
            "1",
            "4.1",
            "4.1.1"
        ],
        "relevant_context": [
            "Traditional calibration methods provide analytical frameworks for addressing camera calibration. However, they require capturing an object of known geometry from multiple viewpoints, then extracting points and establishing correspondences.",
            "Traditional methods for detecting calibration errors, such as those based on epipolar geometry, face significant computational challenges in multi-camera setups and do not support on-the-fly recalibration, making them ineffective in dynamic environments.",
            "Dynamic camera pose synthesis begins with the OEM calibration parameters of a multi-camera setup, typically determined by the manufacturing process.",
            "To create a robust model capable of handling changes in calibration, we introduce perturbations to the OEM camera parameters, generating perturbed parameters , where  represents the perturbation defined as , with  controlling the maximum desired perturbation."
        ],
        "final_answer": "Unlike offline multi-view calibration—which starts from scratch by capturing a known object from many viewpoints, extracting correspondences, and solving an analytical calibration problem—dynamic camera pose synthesis begins with the manufacturer’s (OEM) calibration parameters as a baseline and then applies controlled perturbations to those parameters at each training epoch. This on-the-fly synthesis of diverse, perturbed camera poses both leverages the OEM calibration and enables real-time recalibration without the need for new multi-view captures.",
        "relevant_elements": [
            "dynamic camera pose synthesis",
            "OEM calibration parameters"
        ],
        "id": 346,
        "masked_question": "How does dynamic camera pose synthesis leverage [mask1] compared to offline multi-view calibration methods?",
        "masked_number": 1,
        "masked_elements": [
            "OEM calibration parameters"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.14505v1_figure_2.png",
        "paperid": "2410.14505v1",
        "paper_path": "./papers/2410.14505v1.json",
        "figure_id": "2410.14505v1_figure_2.png",
        "caption": "Figure 2: Technical overview. Our methodology begins with the synthesis of dynamic camera poses (see top fig.). Given spherical angles ϕitalic-ϕ\\phiitalic_ϕ (azimuth), θ𝜃\\thetaitalic_θ (elevation), along with the intrinsic rotation angle α𝛼\\alphaitalic_α, the OEM calibration parameters, the maximum perturbation limit κ𝜅\\kappaitalic_κ, and known 3D fiducials (e.g. a cube calibration object), this module performs two primary functions: (i) it synthesizes poses for the multi-camera system, and (ii) it computes the projected 2D points. Subsequently, it employs point splatting to render images of these points. During training (see bottom fig.), the synthesizes poses and projected points (alternatively rendered images) are used to train the neural network. A differentiable projection ensures the propagation of gradients from the loss ℒℒ\\mathcal{L}caligraphic_L back to the predicted camera parameters.",
        "qtype": "Literature_Background",
        "response": "Dynamic camera‐pose synthesis uses the OEM calibration as a live “anchor’’ rather than as a one‐time gold standard.  At each training epoch the network starts from the OEM intrinsics and extrinsics, injects small, controlled perturbations (up to κ) into them, and then samples new viewpoints on the hemisphere around the fiducials.  Those perturbed poses become the training examples for the differentiable projection and loss.  In contrast, offline multi-view calibration treats the OEM values as fixed (or discards them entirely), collects a fixed set of images and point correspondences once, and then solves in batch for the camera parameters—without ever systematically perturbing an OEM prior or regenerating viewpoints on the fly."
    },
    {
        "question": "How does Extraction Decoder complement Large Multimodal Model multistep thinking compared to Feature-level TSR?",
        "relevant_section_ids": [
            "1",
            "3.1.2",
            "3.2.1"
        ],
        "relevant_context": [
            "Some methods through unsupervised learning or feature matching have been proposed to solve the problems of this cross-country TSR problem [20–24]. These methods utilize strategies such as zero-shot learning or few-shot learning for TSR, thus reducing the dependence on training data and alleviating the applicability problem of cross-country traffic signs. However, cross-domain biases exist between the target and template traffic signs as shown in Fig. 1 (b), and performing pairwise matching at the feature level increases this important difference. Therefore, the recognition accuracy of these methods remains to be further improved.",
            "The extraction detector finally retrieves the traffic sign image S from M using the corresponding coordinates of the traffic signs. S represents the final extracted traffic sign image. Note that while S can also be obtained directly from the original road image R via the coordinates, the extracted traffic sign image contains unnecessary backgrounds. In contrast, the designed extraction detector can remove the backgrounds and avoid potential interference for subsequent recognition.",
            "In addition, when multiple traffic signs exist in the original road image, it is difficult for the LMM to perform context description and the prior traffic sign hypotheses generation. Therefore, we simplify the complex and propose a prompt optimization method based on center coordinates. The prompt optimization method provides the center coordinates of traffic signs to inspire the LMM to locate the target traffic sign from the original road image. ... The center coordinates help the LMM to locate the target traffic sign and generate corresponding background descriptions and prior traffic sign hypotheses."
        ],
        "final_answer": "Feature-level TSR methods match visual features of target and template signs directly, which exacerbates cross-domain biases and limits accuracy. By contrast, the Extraction Decoder first segments and crops out each sign—removing distracting background—and supplies precise center-coordinate cues to the Large Multimodal Model. This clean, localized input lets the LMM generate accurate context descriptions and hypothesis prompts and then carry out its multistep reasoning (context → characteristic → differential descriptions) without suffering from feature-level domain gaps.",
        "relevant_elements": [
            "Extraction Decoder",
            "Large Multimodal Model",
            "Feature-level TSR"
        ],
        "id": 348,
        "masked_question": "How does [mask1] complement [mask2] multistep thinking compared to Feature-level TSR?",
        "masked_number": 2,
        "masked_elements": [
            "Extraction Decoder",
            "Large Multimodal Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.01534v1_figure_1.png",
        "paperid": "2409.01534v1",
        "paper_path": "./papers/2409.01534v1.json",
        "figure_id": "2409.01534v1_figure_1.png",
        "caption": "Figure 1: Comparison of different TSR methods. (a) Supervised TSR, which requires a large amount of training data and fine-tuning. (b) Feature-level TSR, which is training data-free. Cross-domain differences between the target and template traffic signs exist. (c) Our think twice before recognizing strategy. Our method stimulates the multiple-thinking capabilities of large multimodal models without requiring training data.",
        "qtype": "Literature_Background",
        "response": "Mask₁ (the red‐boxed “Extraction Decoder”) is simply a lightweight vision front-end: it segments out and crops the traffic sign of interest (and even gives you its center coordinates), stripping away all of the distracting background clutter.  Mask₂ (the blue‐boxed “Think (i–iii)”) is the LMM’s three‐stage reasoning pipeline – first generating a scene‐context hypothesis, then matching shape/color/composition against a few‐shot template bank, and finally drilling down on the subtle differences between look-alike signs.  \n\nBy feeding the clean, tightly cropped sign from the Extraction Decoder into that three‐step “think twice” routine, the model can devote its full reasoning power to fine-grained cues (shape, color, wording) instead of wasting capacity on background noise.  In contrast, conventional feature-level TSR methods skip the extraction step and simply match raw embeddings of noisy, possibly occluded signs to template embeddings – a process that is much more brittle to cross-country domain shifts."
    },
    {
        "question": "How does Template Traffic Signs description replace Feature-level TSR matching to reduce cross-domain bias?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Although previous TSR methods have utilized template traffic signs at the feature level, actual traffic sign images are diverse due to lighting conditions, angles, occlusions, etc., and can be different from template traffic sign images. It increases the difficulty of cross-domain recognition at the feature level.",
            "By avoiding computation at the feature level, the generated characteristic descriptions can reduce cross-domain differences between templates and real traffic signs."
        ],
        "final_answer": "Instead of matching raw image features between template and target signs (feature-level matching), the method uses few-shot in-context learning to generate textual characteristic descriptions (focusing on shape, color, and composition) for each template traffic sign. By comparing these high-level descriptions rather than low-level image features, cross-domain biases caused by variations in lighting, angle, and occlusion are significantly reduced.",
        "relevant_elements": [
            "Template Traffic Signs",
            "Feature-level TSR"
        ],
        "id": 349,
        "masked_question": "How does [mask1] description replace [mask2] matching to reduce cross-domain bias?",
        "masked_number": 2,
        "masked_elements": [
            "Template Traffic Signs",
            "Feature-level TSR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.01534v1_figure_1.png",
        "paperid": "2409.01534v1",
        "paper_path": "./papers/2409.01534v1.json",
        "figure_id": "2409.01534v1_figure_1.png",
        "caption": "Figure 1: Comparison of different TSR methods. (a) Supervised TSR, which requires a large amount of training data and fine-tuning. (b) Feature-level TSR, which is training data-free. Cross-domain differences between the target and template traffic signs exist. (c) Our think twice before recognizing strategy. Our method stimulates the multiple-thinking capabilities of large multimodal models without requiring training data.",
        "qtype": "Literature_Background",
        "response": "Step 1 – Locate the red‐boxed content ([mask1])  \n• In Fig. 1(b), the red box encloses the “Template Traffic Signs” block.  In the text this block is what we turn into “characteristic descriptions” via few‐shot in‐context learning (see §III-B1, “Characteristic Descriptions”).  \n\nStep 2 – Locate the blue‐boxed content ([mask2])  \n• The blue box in Fig. 1(b) highlights the entire “Feature‐level TSR” pipeline – i.e. using pre-trained CNN/Transformer features from template and real signs and then doing a feature‐matching step to recognize the sign.  \n\nStep 3 – Match diagram to text  \n• The paper points out (§III-B1) that directly matching template‐sign features to real sign features suffers from cross‐domain bias (lighting, angle, occlusion, etc.).  \n• Instead, they propose to generate a concise text description of each template (“shape, color, composition”) once, and store these “characteristic descriptions” in a memory bank.  \n\nStep 4 – Answer the question  \n• Thus, the “characteristic descriptions” ([mask1]) of the red-boxed template-signs replace the raw “feature-level matching” ([mask2]) pipeline.  \n• By reasoning in text space over high‐level attributes rather than comparing pixel/feature vectors across domains, the method sidesteps the feature‐distribution gap between template images and real‐world captures, thereby reducing cross-domain bias.  \n\nAnswer:  \nThe characteristic descriptions of each template sign replace the feature‐level matching step.  By summarizing shape, color and composition in text (instead of matching raw visual features), the model avoids the appearance‐domain gaps between templates and real signs and thus reduces cross-domain bias."
    },
    {
        "question": "How does segmentation model affect extraction decoder's precision when isolating traffic signs for think twice strategy?",
        "relevant_section_ids": [
            "3.1.1",
            "4.3.4"
        ],
        "relevant_context": [
            "In the proposed method, we first perform segmentation of the original road image  containing the traffic signs . … The segmentation model is not limited to a specific architecture.",
            "Previous experiments demonstrate that the proposed multi-step thinking can be easily extended to different LMMs and maintains robust performance. In addition, in our designed traffic sign extraction module, the segmentation model is not limited to a specific model and can easily be extended to advanced models. Figure 10 shows traffic sign extraction examples with segment anything model 2 (SAM 2) and ViT-Adapter. As shown in Fig. 10, under different segmentation models, target traffic signs are extracted through the designed extraction module. The most advanced segmentation model such as SAM 2 performs better extraction on traffic signs."
        ],
        "final_answer": "The precision of the extraction decoder in isolating traffic signs directly depends on the quality of the segmentation model. A stronger segmentation model (for example SAM 2 versus ViT-Adapter) produces cleaner, more accurate masks, and thus allows the extraction decoder to remove background clutter more effectively and output more precisely cropped traffic sign images.",
        "relevant_elements": [
            "Segmentation Model",
            "Extraction Decoder"
        ],
        "id": 350,
        "masked_question": "How does [mask1] affect extraction decoder's precision when isolating traffic signs for think twice strategy?",
        "masked_number": 1,
        "masked_elements": [
            "Segmentation Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.01534v1_figure_1.png",
        "paperid": "2409.01534v1",
        "paper_path": "./papers/2409.01534v1.json",
        "figure_id": "2409.01534v1_figure_1.png",
        "caption": "Figure 1: Comparison of different TSR methods. (a) Supervised TSR, which requires a large amount of training data and fine-tuning. (b) Feature-level TSR, which is training data-free. Cross-domain differences between the target and template traffic signs exist. (c) Our think twice before recognizing strategy. Our method stimulates the multiple-thinking capabilities of large multimodal models without requiring training data.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is nothing exotic – it’s just the segmentation model that turns the raw road image into a binary “traffic‐sign vs. everything else” mask.  During the Extraction step that follows, the detector runs a contour‐finder over exactly that mask to pull out the sign’s boundary and its center‐coordinates, then crops the original image accordingly.  In short, the better and sharper that mask is, the tighter your contours will be, and the more exactly the extraction decoder can trim away background clutter.  If the segmentation is sloppy you get loose, noisy crops; if it’s precise you get a perfectly‐isolated sign for the downstream “think twice” recognizer."
    },
    {
        "question": "How does Fourier encoding within the Layout Embedding module improve fusion of positional and semantic layout information?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Building on this, Fourier [23] encoding is employed to convert these positional coordinates into a frequency domain vector representation, similar to GLIGEN [16].",
            "We use a frozen CLIP text encoder [27] to obtain fixed codes for different categories, which serve as layout condition inputs. The Fourier-encoded coordinates are then fused with the category encodings using an additional linear layer to produce the layout control input: where denotes the concatenation of Fourier-coded coordinates and category codes, and represents the linear transformation layer.",
            "In this manner, spatial location and category information are effectively combined as layout control tokens."
        ],
        "final_answer": "By first mapping raw bounding-box coordinates into a high-dimensional frequency-domain representation via Fourier encoding, the system transforms spatial information into vectors that can be directly concatenated with CLIP-derived category embeddings. A subsequent linear layer then fuses these frequency-encoded positional vectors with semantic codes, yielding unified layout control tokens that jointly carry precise location and category information.",
        "relevant_elements": [
            "Layout Embedding",
            "Fourier encoding"
        ],
        "id": 352,
        "masked_question": "How does [mask1] within the [mask2] module improve fusion of positional and semantic layout information?",
        "masked_number": 2,
        "masked_elements": [
            "Fourier encoding",
            "Layout Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15497v2_figure_1.png",
        "paperid": "2411.15497v2",
        "paper_path": "./papers/2411.15497v2.json",
        "figure_id": "2411.15497v2_figure_1.png",
        "caption": "Figure 1: AeroGen’s overall architecture. (a) The layout embedding module combines bounding box coordinates with vectorized semantic information using Fourier and MLP layers. This encodes layout information to facilitate control, with the prompt description processed by a CLIP text encoder for global conditional guidance. (b) The injection of layout information at the noise level is demonstrated, where a local mask governs the injection position of the layout information, allowing for finer layout control. (c) The overall architecture and training process of AeroGen. At each timestep, the image being denoised first passes through a layout information injection module, which enhances layout conditional guidance.",
        "qtype": "Experimental_Results",
        "response": "Here, [mask1] is the little “Fourier” block in (a) and [mask2] is the whole blue-outlined “Layout Embedding” module.  What the paper is doing is taking your eight raw corner coordinates, running them through a fixed sinusoidal (Fourier) feature mapping – i.e. projecting each x,y pair into a stack of sine‐and‐cosine waves at different frequencies – and then concatenating that rich positional code with the CLIP class embedding and running the result through a small MLP.  \n\nBecause the Fourier mapping turns each simple coordinate into a multi-scale, smooth feature vector, the subsequent linear layer can much more easily learn “where” (position) and “what” (semantic class) at the same time, rather than having to disentangle a raw [x,y] number from a frozen text token.  In practice that yields layout tokens that tightly bind spatial location and object identity, which gives the diffusion backbone stronger, more precise control over both."
    },
    {
        "question": "How does Layout Mask Attention complement Dual Cross-Attention to enhance local layout precision during diffusion?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In AeroGen, the text prompt serves as a global condition and is integrated with layout control tokens via a dual cross-attention mechanism. The output is computed as: where  represents the cross-attention mechanism.  and  are the keys and values of the global text condition, while  and  are the layout control tokens.  balances the influence of global and layout conditions.",
            "Layout Mask Attention. In addition to traditional token-based control, recent studies indicate that direct semantic embedding based on feature maps is also an effective method for layout guidance. In the denoising process of a diffusion model, the injection of conditional information is gradual, enabling local attribute editing at the noise level. To this end, conditionally encoded noise region steering is employed and combined with a cropping step for improved layout precision. As shown in Fig. 1 (b), each bounding box is first transformed into a 0/1 mask M, and category attributes are obtained through CLIP encoding. During each denoising step, the mask attention network provides additional layout guidance. The process is expressed as follows: for each denoised image ζ′ₜ and category encoding Q, the mask M is used for attention computation according to the following equation: … This method enables precise manipulation of local noise characteristics during the diffusion generation process, offering finer control over the image layout."
        ],
        "final_answer": "Dual Cross-Attention fuses global text and layout‐token conditions at each U-Net block to guide overall scene composition, while Layout Mask Attention uses per-box binary masks and CLIP-based category embeddings to steer noise injection and feature updates only within each target region. By injecting an explicit mask-controlled attention path, Layout Mask Attention complements Dual Cross-Attention by providing fine-grained, region-specific control over noise and semantic features, resulting in more precise local layout generation during diffusion.",
        "relevant_elements": [
            "Layout Mask Attention",
            "Dual Cross-Attention"
        ],
        "id": 353,
        "masked_question": "How does [mask1] complement Dual Cross-Attention to enhance local layout precision during diffusion?",
        "masked_number": 1,
        "masked_elements": [
            "Layout Mask Attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15497v2_figure_1.png",
        "paperid": "2411.15497v2",
        "paper_path": "./papers/2411.15497v2.json",
        "figure_id": "2411.15497v2_figure_1.png",
        "caption": "Figure 1: AeroGen’s overall architecture. (a) The layout embedding module combines bounding box coordinates with vectorized semantic information using Fourier and MLP layers. This encodes layout information to facilitate control, with the prompt description processed by a CLIP text encoder for global conditional guidance. (b) The injection of layout information at the noise level is demonstrated, where a local mask governs the injection position of the layout information, allowing for finer layout control. (c) The overall architecture and training process of AeroGen. At each timestep, the image being denoised first passes through a layout information injection module, which enhances layout conditional guidance.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what “​[mask1]​” is. In Fig. 1(b) it is the green “Mask-Cross-Attention” (aka Layout Mask Attention) block. Now let’s walk through how that block and the existing Dual Cross-Attention work together, and why adding Masked Cross-Attention sharpens local layout control.  \n\n1. Dual Cross-Attention alone  \n   - In each denoising layer (“DownBlock”) the model already has two cross-attention heads:  \n     a. a “global” head (Kᵍ,Vᵍ) attending to the CLIP text-prompt embeddings, and  \n     b. a “layout” head (Kˡ,Vˡ) attending to the Fourier-MLP layout tokens (one per box).  \n   - This lets the network broadly bias the image toward the correct scene description and roughly steer content into each bounding box.  \n   - However, because these are *token-level* embeddings, small boxes or fine spatial boundaries can still bleed or be imprecise.  \n\n2. What Masked Cross-Attention adds  \n   - At each diffusion step we take the current noisy feature map ζₜ (shape H×W×C) and “repeat” it to form a sequence of N spatial positions (one for each of the N layout boxes).  \n   - We also convert each box into a binary mask Mᵢ of size H×W (1 inside the box, 0 outside).  \n   - The Mask-Cross-Attention module then does a *per-box* cross-attention between:  \n     • Queries Q coming from the repeated image features,  \n     • Keys/Values coming again from the same repeated features,  \n     but crucially  \n     • it applies the binary mask Mᵢ to zero out any attention that would spill outside the i’th box.  \n   - The output is a set of very *local*, mask-restricted feature updates that get summed back into the full feature map only where Mᵢ=1.  \n\n3. Why they complement each other  \n   - Dual Cross-Attention brings in *semantic* and *layout-token* guidance at a fairly coarse, token-based level.  \n   - Mask-Cross-Attention then drills down *inside* each box, enforcing that the noise-to-image process only injects the class-specific signal exactly within that box’s pixels.  \n   - By combining the two, AeroGen achieves both large-scale consistency (text+layout tokens) and pixel-perfect alignment on small, rotated, or tightly confined objects.  \n\nIn short, Masked Cross-Attention “fills in the gaps” left by tokenized Dual Cross-Attention, zeroing out any gradient or attention flow outside each box so that every object’s appearance is honed precisely to its prescribed layout."
    },
    {
        "question": "How could reliance on CLIP-based global control introduce semantic bias into layout-conditioned image generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "CLIP",
            "Global Control"
        ],
        "id": 355,
        "masked_question": "How could reliance on [mask1]-based [mask2] introduce semantic bias into layout-conditioned image generation?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP",
            "Global Control"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15497v2_figure_1.png",
        "paperid": "2411.15497v2",
        "paper_path": "./papers/2411.15497v2.json",
        "figure_id": "2411.15497v2_figure_1.png",
        "caption": "Figure 1: AeroGen’s overall architecture. (a) The layout embedding module combines bounding box coordinates with vectorized semantic information using Fourier and MLP layers. This encodes layout information to facilitate control, with the prompt description processed by a CLIP text encoder for global conditional guidance. (b) The injection of layout information at the noise level is demonstrated, where a local mask governs the injection position of the layout information, allowing for finer layout control. (c) The overall architecture and training process of AeroGen. At each timestep, the image being denoised first passes through a layout information injection module, which enhances layout conditional guidance.",
        "qtype": "Others",
        "response": "Here’s the core of why leaning too heavily on CLIP-based global control can sneak unwanted semantics into a supposedly “layout-only” image generator:\n\n1. Mask1 is the red-boxed module: the frozen CLIP text encoder that produces a global embedding of your prompt.  \n2. Mask2 is the blue-boxed “Global Control” stream that injects that CLIP embedding into every denoising block via cross-attention.  \n\nChain-of-thought:  \n- CLIP has been trained on billions of captioned natural images and so carries all of its training-set biases (common object co-occurrences, typical color palettes, urban vs. rural scene priors, etc.).  \n- When you use its embedding as a global control signal in a layout-guided diffusion model, those same priors get injected at every level of generation.  \n- Even if your layout tokens tightly pin down “tennis court here” and “track field there,” CLIP’s global embedding will still push the model towards the kinds of sky-and-grass textures, fence styles, court markings, and surrounding scenery that it “expects” from its natural-image training.  \n- In practice this means:  \n  • Rare object classes or unusual remote-sensing arrangements (e.g. a floating heliport) may be washed out or “corrected” into more familiar scenes.  \n  • Color, texture, or context cues (trees, spectators, roadways) that never appeared in your layout at all can creep in because CLIP’s global signal strongly nudges the network toward its learned modes.  \n\nThus, by relying on a frozen CLIP embedding as your only global semantic guide, you inadvertently bake in CLIP’s Internet-image biases, which can override or distort the pure—layout-only—intent of your conditioning."
    },
    {
        "question": "What limitations arise from relying on DetHead features for the Motion Mamba module?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "DetHead",
            "Motion Mamba module"
        ],
        "id": 356,
        "masked_question": "What limitations arise from relying on [mask1] features for the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "DetHead",
            "Motion Mamba module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.10485v2_figure_2.png",
        "paperid": "2407.10485v2",
        "paper_path": "./papers/2407.10485v2.json",
        "figure_id": "2407.10485v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MM-Tracker. Multi-scale detection features are first extracted using a detection backbone (DetBackbone), which is fed into the detection head (DetHead) to output the object bounding box, score, and category. The object score is optimized using the proposed MMLoss.\nThe detection feature is also fed into the proposed Motion Mamba module (MM), which captures the difference between the two detection features and predicts the motion map. Afterward, the position of the object in the previous frame in the next frame is predicted based on the motion map, and the predicted position of the object is matched with the detected position in the current frame to generate a new object trajectory.",
        "qtype": "Others",
        "response": "In our case the “red‐box” features turn out to be nothing more exotic than the standard detection backbone/Detection‐Head feature maps – i.e. high‐level, semantically pooled features that were optimized to tell you “is there a car here, how big is it, what class is it” – and the “blue‐box” is the Motion-Mamba motion model that tries to compute per‐pixel motion from exactly those same detection features.  What you lose by doing that is all of the low-level, high-resolution cues that you normally depend on for motion estimation:  \n\n  1.  Detection features have been through strides, pooling and nonlinearities, so their spatial resolution is coarse.  Sub‐pixel or very small displacements are simply smoothed away.  \n  2.  They carry strong class/appearance bias and have been trained to be invariant to things like illumination, blur or slight pose changes—exactly the things you often rely on when you want to compute precise frame-to-frame offsets.  \n  3.  Any global or background motion that doesn’t land squarely inside a detected box may be discarded altogether, making it hard to disentangle object motion from platform (camera) motion.  \n\nIn short, by re‐using detection features you gain a huge speed-and-compute win, but you pay for it in lost fine‐grained, low-level motion cues.  That in turn limits the precision of your learned motion maps, especially for small or very fast‐ moving objects (or in scenes of heavy blur), and makes it harder to separate object motion from global camera motion."
    },
    {
        "question": "What biases could MMLoss introduce by prioritizing large-motion objects over stationary ones?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "MMLoss"
        ],
        "id": 357,
        "masked_question": "What biases could [mask1] introduce by prioritizing large-motion objects over stationary ones?",
        "masked_number": 1,
        "masked_elements": [
            "MMLoss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.10485v2_figure_2.png",
        "paperid": "2407.10485v2",
        "paper_path": "./papers/2407.10485v2.json",
        "figure_id": "2407.10485v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MM-Tracker. Multi-scale detection features are first extracted using a detection backbone (DetBackbone), which is fed into the detection head (DetHead) to output the object bounding box, score, and category. The object score is optimized using the proposed MMLoss.\nThe detection feature is also fed into the proposed Motion Mamba module (MM), which captures the difference between the two detection features and predicts the motion map. Afterward, the position of the object in the previous frame in the next frame is predicted based on the motion map, and the predicted position of the object is matched with the detected position in the current frame to generate a new object trajectory.",
        "qtype": "Others",
        "response": "Here’s a step-by-step reasoning of the kind of bias that the Motion-Margin Loss (the red-boxed “larger boundary for large motion objects, smaller boundary for small motion objects” plot) can introduce:\n\n1.  What MMLoss does  \n    -  It explicitly up-weights (via a larger margin) the classification loss on fast-moving (and hence more blurred) objects  \n    -  At the same time it down-weights (via a smaller margin) the loss on slow or stationary objects  \n\n2.  How that skews the training signal  \n    -  The network “spends” more of its capacity and gradient‐budget on getting the hard, high-motion cases right  \n    -  It by construction spends less capacity on the easy, low-motion or static cases  \n\n3.  Immediate consequences  \n    -  Stationary or slowly moving objects become under-represented in the loss  \n    -  The detector may underfit those static cases, leading to degraded recall or poorer confidence calibration on them  \n    -  In extreme, the model may “ignore” parked cars, stopped pedestrians, etc., because they incur almost no margin penalty  \n\n4.  Broader fairness/robustness issues  \n    -  If certain object classes (e.g., a bus at a standstill, a cyclist waiting at a light) tend to be static, those classes will systematically suffer  \n    -  The tracker’s performance will be uneven: very high on moving objects, but potentially brittle on anything that isn’t in motion  \n\nIn short, by design MMLoss trades off performance on easy (static) instances to boost difficult (fast-motion) ones.  This can bias the detector/tracker against stationary targets and produce an imbalanced error profile."
    },
    {
        "question": "What is the motivation behind extracting motion features via local correlation and global bi-directional Mamba scanning?",
        "relevant_section_ids": [
            "1",
            "2.1",
            "3.2"
        ],
        "relevant_context": [
            "Several studies (Shuai et al. 2021; Zhou, Koltun, and Krähenbühl 2020; Yao et al. 2023) propose learning-based motion modeling, but most of them are based on local cross-correlation or local convolution and ignore global motion information. Therefore, the lack of global motion modeling limits the tracking accuracy of these trackers in scenes with significant global camera motion.",
            "We propose the Motion Mamba module, which models object motion by local correlation of detection features and global scan of bi-directional mamba block, reached fast and accurate motion modeling.",
            "For each scale, Motion Mamba first uses the cross-correlation of the feature maps at the previous and next moments to extract the local motion information, and then use Motion Mamba block to extract global motion features."
        ],
        "final_answer": "Prior learning-based motion modules used only local convolution or cross-correlation and thus failed to capture large, scene-wide camera movements. By first applying local correlation on bi-temporal detection features, MM-Tracker captures fine, short-range object displacements; by then performing a bi-directional Mamba scan, it aggregates full-frame, long-range motion context. Together this approach yields both fast and accurate modeling of complex global motion patterns while reusing already-computed detection features.",
        "relevant_elements": [
            "Local-Correlation",
            "Global Scan (Bi-directional Mamba)"
        ],
        "id": 358,
        "masked_question": "What is the motivation behind extracting motion features via [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Local-Correlation",
            "Global Scan (Bi-directional Mamba)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.10485v2_figure_2.png",
        "paperid": "2407.10485v2",
        "paper_path": "./papers/2407.10485v2.json",
        "figure_id": "2407.10485v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MM-Tracker. Multi-scale detection features are first extracted using a detection backbone (DetBackbone), which is fed into the detection head (DetHead) to output the object bounding box, score, and category. The object score is optimized using the proposed MMLoss.\nThe detection feature is also fed into the proposed Motion Mamba module (MM), which captures the difference between the two detection features and predicts the motion map. Afterward, the position of the object in the previous frame in the next frame is predicted based on the motion map, and the predicted position of the object is matched with the detected position in the current frame to generate a new object trajectory.",
        "qtype": "Design_Rationale",
        "response": "The two blocks in the Motion Mamba module are there to give you both the “small‐step” and the “big‐picture” view of how things move between frames:\n\n1. Mask1 (“Local‐Correlation” – the red box)  \n   – What it does: it cross‐correlates each small patch in frame t with the corresponding patch in frame t+1 to pull out fine‐grained, local displacement cues (i.e. the little pixel-level shifts that tell you exactly how a tiny bit of the image has moved).  \n   – Why you need it: many objects (especially in UAV footage) only shift a handful of pixels between frames, or have parts that move differently (think of a rotating wheel or a swaying limb).  A local correlation layer is the best way to pick up those subtle, spatially varying motions.\n\n2. Mask2 (“Global‐Scan” / bi-directional Mamba – the blue box)  \n   – What it does: it performs two linear-time state-space scans (one scan down each column, one scan across each row) so that information from every corner of the feature map can influence every other corner.  In effect you end up with feature vectors that “see” long-range displacements, whether they run vertically or horizontally.  \n   – Why you need it: when the UAV camera itself is panning, tilting or otherwise making large global motions, or when objects jump across the scene, you must be able to capture very large displacements.  A bi-directional scan gives you that very cheaply (linear in H×W) without the quadratic blow-up of a full Transformer.\n\nBy chaining a local cross-correlation block (for precise, small motions) and a bi-directional Mamba scan (for large, global motions), MM-Tracker gets the best of both worlds: accurate fine detail plus full-scene context, all at real-time speed on UAV data."
    },
    {
        "question": "What is the reasoning for applying Motion Margin loss to detection score optimization rather than standard classification loss?",
        "relevant_section_ids": [
            "2.3",
            "3.3"
        ],
        "relevant_context": [
            "In the UAVMOT scene, the camera’s perspective change will cause large movements of objects, and this perspective change is accidental, making this situation account for a small proportion of the dataset. However, the large motion introduces severe motion blur on objects, requiring us to focus more on those difficult-to-detect objects, which is missed in previous studies. To this end, we propose Motion Margin loss, which imposes larger classification boundaries for objects with larger motions, thus better solving the problem of less training for large motion objects.",
            "This motion blur will greatly increase the difficulty of object detection. However, since there are fewer such cases in the dataset, these difficult-to-detect samples have fewer training times than easy samples, which further increases the detection difficulty. For object tracking tasks, even a few frames that cannot be detected will cause tracking interruption, greatly affecting tracking accuracy. To this end, we propose a Motion Margin loss function to assign different decision boundaries according to different object’s motion. We assign larger decision boundaries to objects with larger motion, thereby forcing the model to output higher scores for objects with larger motion during the learning process, so as to effectively detect these objects during inference.",
            "The function of subtracting m from the output of the network classification layer is to assign different decision boundaries to different object boxes based on their motion value."
        ],
        "final_answer": "Because objects undergoing large, camera-induced motion are both rare in the training data and severely motion-blurred—making them much harder to detect—the standard classification loss (with a fixed decision boundary) under-trains these difficult cases. By applying Motion Margin loss directly to the detection scores, the model imposes larger decision margins for high-motion objects, forcing the network to produce higher confidence scores for them, improving their detection (and thus preventing tracking interruptions) compared to using a conventional, uniform classification loss.",
        "relevant_elements": [
            "DetHead",
            "MMLoss"
        ],
        "id": 359,
        "masked_question": "What is the reasoning for applying [mask1] to detection score optimization rather than standard classification loss?",
        "masked_number": 1,
        "masked_elements": [
            "MMLoss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.10485v2_figure_2.png",
        "paperid": "2407.10485v2",
        "paper_path": "./papers/2407.10485v2.json",
        "figure_id": "2407.10485v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MM-Tracker. Multi-scale detection features are first extracted using a detection backbone (DetBackbone), which is fed into the detection head (DetHead) to output the object bounding box, score, and category. The object score is optimized using the proposed MMLoss.\nThe detection feature is also fed into the proposed Motion Mamba module (MM), which captures the difference between the two detection features and predicts the motion map. Afterward, the position of the object in the previous frame in the next frame is predicted based on the motion map, and the predicted position of the object is matched with the detected position in the current frame to generate a new object trajectory.",
        "qtype": "Design_Rationale",
        "response": "Mask₁ is the Motion‐Margin Loss (MMLoss) applied to the detector’s classification score.  We use MMLoss instead of the standard classification loss because:  \n1. In a UAV setting camera motions create only a small fraction of “large‐motion” (and hence heavily blurred) object samples.  Standard losses (cross‐entropy, focal loss, etc.) treat every positive equally, so those rare, hard‐to‐detect examples never receive extra emphasis during training.  \n2. By introducing a per‐sample margin that grows with the object’s measured frame‐to‐frame displacement, MMLoss effectively increases the decision boundary (i.e. forces higher confidence scores) for big‐motion, blurred objects.  \n3. This dynamic, motion‐aware margin counteracts the imbalance (few hard/large‐motion samples vs. many easy/low‐motion ones) and leads the detector to learn stronger features for the most challenging cases—something a vanilla classification loss cannot do."
    },
    {
        "question": "What motivates penalizing mutual information between S and Z in the CIB objective?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "With this regard, we propose a Causal Information Bottleneck (CIB) optimization objective. CIB aligns the information in the latent variables S with observed variables X with a bottleneck set by the mutual information (MI) between S and Z. The derived function will minimize the MI between S and Z while learning the other causal relations, ensuring their disentanglement within the causal framework.",
            "The term, I(S; Z) ensures S and Z to be effectively disentangled."
        ],
        "final_answer": "Penalizing the mutual information between S and Z is motivated by the goal of enforcing a clean disentanglement between the label-causative factor S and the label-non-causative factor Z, so that each captures distinct, non-overlapping information.",
        "relevant_elements": [
            "S",
            "Z",
            "CIB objective"
        ],
        "id": 360,
        "masked_question": "What motivates penalizing mutual information between [mask1] and [mask2] in the CIB objective?",
        "masked_number": 2,
        "masked_elements": [
            "S",
            "Z"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23091v4_figure_1.png",
        "paperid": "2410.23091v4",
        "paper_path": "./papers/2410.23091v4.json",
        "figure_id": "2410.23091v4_figure_1.png",
        "caption": "Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) Y-causative feature S𝑆Sitalic_S and the Y-non-causative feature Z𝑍Zitalic_Z through maximization of the Causal Information Bottleneck (CIB).\nIn the inference stage, CausalDiff first purifies an adversarial example X~~𝑋\\tilde{X}over~ start_ARG italic_X end_ARG, yielded by perturbing X𝑋Xitalic_X according to the target victim model parameterized by θ𝜃\\thetaitalic_θ, to obtain the benign counterpart X∗superscript𝑋X^{*}italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. Then, it infers the Y-causative feature S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT for label prediction. We visualize the vectors of S𝑆Sitalic_S and Z𝑍Zitalic_Z inferred from a perturbed image of a horse using a diffusion model. We find that S𝑆Sitalic_S captures the general concept of a horse, even when the input image only shows the head, while Z𝑍Zitalic_Z carries information about the horse’s skin color.",
        "qtype": "Design_Rationale",
        "response": "Here, [mask1] is the Y-causative latent S (the red‐boxed “label-causative factor”) and [mask2] is the Y-non-causative latent Z (the blue-boxed “label-non-causative factor”).  In our Causal Information Bottleneck (CIB) loss we add a term −I(S;Z) precisely to force S and Z to carry *different* pieces of information — i.e. to **disentangle** the label-predictive features (S) from all the other spurious or style features (Z).  If we didn’t penalize their mutual information, S and Z could overlap, letting spurious factors leak into the “causal” channel (and vice versa), which would defeat both identifiability of the causal model and the robustness gains we rely on."
    },
    {
        "question": "What is the rationale for adversarial purification preceding causal factor inference?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Following a typical attack paradigm, x̃ is produced by adding an adversarial perturbation to a target clean example x when attacking a model θ.",
            "To make a robust prediction on x̃, our robust inference process comprises three steps: 1) purifying x̃ to benign ẍ by the unconditional diffusion model pϕ, 2) inferring S and Z from ẍ utilizing the causal model qψ, and 3) predicting ŷ based on S using a classifier fα."
        ],
        "final_answer": "Since the input image x̃ has been corrupted by adversarial noise, the first step is to purify it—i.e. remove the perturbation—so that the subsequent causal factor inference can recover the true label-causative and non-causative features from a near-clean example, leading to more reliable latent inference and robust classification.",
        "relevant_elements": [
            "Adversarial Purification",
            "Causal Factor Inference"
        ],
        "id": 361,
        "masked_question": "What is the rationale for [mask1] preceding causal factor inference?",
        "masked_number": 1,
        "masked_elements": [
            "Adversarial Purification"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.23091v4_figure_1.png",
        "paperid": "2410.23091v4",
        "paper_path": "./papers/2410.23091v4.json",
        "figure_id": "2410.23091v4_figure_1.png",
        "caption": "Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) Y-causative feature S𝑆Sitalic_S and the Y-non-causative feature Z𝑍Zitalic_Z through maximization of the Causal Information Bottleneck (CIB).\nIn the inference stage, CausalDiff first purifies an adversarial example X~~𝑋\\tilde{X}over~ start_ARG italic_X end_ARG, yielded by perturbing X𝑋Xitalic_X according to the target victim model parameterized by θ𝜃\\thetaitalic_θ, to obtain the benign counterpart X∗superscript𝑋X^{*}italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. Then, it infers the Y-causative feature S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT for label prediction. We visualize the vectors of S𝑆Sitalic_S and Z𝑍Zitalic_Z inferred from a perturbed image of a horse using a diffusion model. We find that S𝑆Sitalic_S captures the general concept of a horse, even when the input image only shows the head, while Z𝑍Zitalic_Z carries information about the horse’s skin color.",
        "qtype": "Design_Rationale",
        "response": "Here is the step‐by‐step reasoning:\n\n1.  In our inference pipeline (see the right side of Fig. 1), we want to recover the true label–causal feature S (and the noncausal feature Z) from a perturbed, adversarial input \\(\\tilde X\\).  \n2.  If we feed the raw adversarial example \\(\\tilde X\\) straight into the conditional diffusion model to infer \\((S,Z)\\), the small but malicious perturbation will “bleed” into the inference process, dragging both S and Z off the clean data manifold.  \n3.  To prevent that, we first apply what is boxed in red—namely, an “Adversarial Purification” step—using an unconditional diffusion model.  This purification maximizes the likelihood of the sample under the clean data distribution, effectively removing (or greatly attenuating) the adversarial noise.  \n4.  Only after we have this purified, more “benign” version \\(X^*\\) of the input do we run the causal‐factor inference module.  At that point the conditional diffusion model sees a sample that lies back on the true image manifold, so it can disentangle the label–causal feature S and the label–noncausal feature Z accurately.  \n5.  Finally, with a reliably inferred S, our classifier makes its prediction without having been misled by the adversarial perturbation.\n\nIn short, we purify first so that the subsequent causal‐factor inference can extract clean, disentangled latent features and yield a robust, correct classification."
    },
    {
        "question": "How does the Causal Information Bottleneck optimize disentanglement between S and Z during model training?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Among the resultant terms, I(Y; S) indicates that the Y-causative factor S should be correlated with Y. The term, – I(S; Z) ensures S and Z to be effectively disentangled.",
            "To maximize the Causal Information Bottleneck (CIB) in Eq. (5), we derive its lower bound as the concrete training loss function. … where … denotes the Contrastive Log-Ratio Upper Bound (CLUB) of mutual information proposed by Cheng et al. [57]."
        ],
        "final_answer": "The Causal Information Bottleneck adds a negative mutual-information term −I(S; Z) to its objective, which is approximated via a CLUB estimator. By penalizing I(S; Z) during training, it forces the model to minimize the shared information between S and Z, thereby driving them to be disentangled.",
        "relevant_elements": [
            "Causal Information Bottleneck",
            "S",
            "Z"
        ],
        "id": 363,
        "masked_question": "How does the [mask1] optimize disentanglement between [mask2] and Z during model training?",
        "masked_number": 2,
        "masked_elements": [
            "Causal Information Bottleneck",
            "S"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.23091v4_figure_1.png",
        "paperid": "2410.23091v4",
        "paper_path": "./papers/2410.23091v4.json",
        "figure_id": "2410.23091v4_figure_1.png",
        "caption": "Figure 1: Illustration of training (Left) and inference (Right) processes of our proposed CausalDiff model. During training, the model constructs a structural causal model leveraging a conditional diffusion model, disentangling the (label) Y-causative feature S𝑆Sitalic_S and the Y-non-causative feature Z𝑍Zitalic_Z through maximization of the Causal Information Bottleneck (CIB).\nIn the inference stage, CausalDiff first purifies an adversarial example X~~𝑋\\tilde{X}over~ start_ARG italic_X end_ARG, yielded by perturbing X𝑋Xitalic_X according to the target victim model parameterized by θ𝜃\\thetaitalic_θ, to obtain the benign counterpart X∗superscript𝑋X^{*}italic_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. Then, it infers the Y-causative feature S∗superscript𝑆S^{*}italic_S start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT for label prediction. We visualize the vectors of S𝑆Sitalic_S and Z𝑍Zitalic_Z inferred from a perturbed image of a horse using a diffusion model. We find that S𝑆Sitalic_S captures the general concept of a horse, even when the input image only shows the head, while Z𝑍Zitalic_Z carries information about the horse’s skin color.",
        "qtype": "Implementation_Details",
        "response": "In our diagram the red‐boxed module is the “Causal Information Bottleneck” (CIB) objective and the blue‐boxed term is the mutual‐information penalty \\(I(S;Z)\\).  Concretely, the CIB loss is\n\n CIB = I(X;S,Z) + I(Y;S) – I(S;Z) – λ I(X;S,Z)\n\nWhen you maximize CIB you are simultaneously:\n  1. forcing \\(S\\) and \\(Z\\) to explain \\(X\\) (via \\(I(X;S,Z)\\)),  \n  2. forcing \\(S\\) to explain the label \\(Y\\) (via \\(I(Y;S)\\)),  \n  3. **forcing \\(S\\) and \\(Z\\) to carry as little shared information as possible** (via the **– I(S;Z)** term),  \n  4. and keeping the overall latent footprint small (via – λ I(X;S,Z)).  \n\nThat third term is exactly what drives **disentanglement** between the two latents, \\(S\\) and \\(Z\\).  In practice we replace the intractable mutual information \\(I(S;Z)\\) with a differentiable variational (CLUB) upper–bound and then **minimize** it.  By penalizing the shared information between \\(S\\) and \\(Z\\), the model learns to allocate truly *distinct* factors of variation to each."
    },
    {
        "question": "How does ICL-based memory influence the Mutation for Bypass step's candidate generation?",
        "relevant_section_ids": [
            "4.1",
            "4.3"
        ],
        "relevant_context": [
            "Section 4.1: “The ICL-based memory module works in three steps: starts with an empty database and gradually expands it with successful jailbreak prompts. Specifically, M₁ records all prompts recognized for their capability to succeed and utilizes these for modifications to the novel sensitive prompts. 2) retrieves successful prompts from the database. To prevent overwhelming the VLM, it selects the top n prompts using a semantic-based memory retriever. 3) reflects these selected prompts to identify the factors contributing to their success and uses this information to guide the mutation of the failed target prompt.”",
            "Section 4.3 (Step 2): “Since the safety filters have not been bypassed, the planning module activates the semantic-based memory retriever to access the ICL-based memory module. It then directs the VLM brain to formulate a mutation strategy using the ‘ICL Prompt,’ ‘ICL-Strategy Prompt,’ and ‘Strategy Prompt.’ Once the VLM brain responds, the planning module sends a ‘Modify Prompt’ to the VLM brain to generate several new candidate jailbreak prompts based on its guidance.”"
        ],
        "final_answer": "In the Mutation for Bypass step, the agent first retrieves past successful jailbreak prompts from its ICL-based memory via a semantic retriever. These retrieved examples are injected into ‘ICL Prompt’ and ‘ICL-Strategy Prompt’ templates so that the VLM brain can analyze their key success factors. Guided by those in-context examples, the VLM then produces multiple new candidate jailbreak prompts tailored to bypass the safety filter.",
        "relevant_elements": [
            "ICL-based Memory",
            "Mutation for Bypass"
        ],
        "id": 364,
        "masked_question": "How does [mask1] influence the Mutation for Bypass step's candidate generation?",
        "masked_number": 1,
        "masked_elements": [
            "ICL-based Memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.00523v2_figure_1.png",
        "paperid": "2408.00523v2",
        "paper_path": "./papers/2408.00523v2.json",
        "figure_id": "2408.00523v2_figure_1.png",
        "caption": "Figure 1: Overall pipeline of Atlas.",
        "qtype": "Implementation_Details",
        "response": "First, let’s ground what the red‐boxed block is. In the “Mutation Agent” stripe at the very top of Figure 1, the red box encloses the module labeled “ICL-based Memory.”  In the text that follows, this is the little long-term store of *successful* jailbreak prompts that the VLM brain can pull in as few‐shot examples.\n\nNow: how does that ICL-based Memory drive *Step 2: Mutation for Bypass*?\n\n1.  In Step 1 the system tests whether the current prompt already slips past the safety filter.  \n2.  If it *doesn’t* (i.e. bypass failure), we go on to Step 2: Mutation for Bypass.  \n3.  **Right at the start** of Step 2, the planner calls the semantic-based memory retriever (one of the “Tools”) to query the ICL-based Memory.  It computes sentence‐embeddings for your *current* target prompt and for every *successful* jailbreak prompt in memory, then picks the top K most semantically similar examples.  \n4.  Those K past successes are then prepended as *in-context examples* in the VLM’s prompt (via the “ICL Prompt” and the “ICL-Strategy Prompt”).  \n5.  The VLM brain reads those few-shot examples, extracts *what kinds of wording tricks* made them slip past filters, and then generates a tailored *mutation strategy* for the *new* target prompt.  \n6.  Finally, the planner issues a “Modify Prompt” instruction so that the VLM produces multiple *candidate* jailbreak prompts that are semantically guided by those retrieved examples.\n\nIn short, the ICL-based Memory feeds *past successful jailbreaks* into the VLM as few-shot context, so that Step 2’s candidate‐generation is *shaped* by the exact patterns that have already beaten the filter."
    },
    {
        "question": "How does planning module leverage chain-of-thought reasoning from prior multi-agent frameworks?",
        "relevant_section_ids": [
            "2.1",
            "4.3"
        ],
        "relevant_context": [
            "Section 2.1: \"Planning. The planning module breaks down the necessary steps or subtasks that the agent will solve individually to answer the user’s request. This step is crucial for enabling the agent to reason more effectively about the problem and find a reliable solution.\nIn this work, we use a popular technique called Chain of Thought (CoT) [53, 27, 60, 55] for task decomposition.\"",
            "Section 4.3: \"Specifically, we divide the jailbreak prompt generation task into sub-tasks and apply chain-of-thought (COT) [53, 27, 60, 55] to enhance reasoning and instruction-following. The planning module uses multi-turn COT by sending one sub-task at a time to the VLM brain. After receiving a response, it provides the context and the next sub-task.\""
        ],
        "final_answer": "The planning module adopts a multi‐turn chain‐of‐thought (CoT) strategy—borrowed from prior multi‐agent work—to break the jailbreak prompt generation into a sequence of subtasks. It sends each subtask in turn to the agent’s LLM/VLM brain, then feeds back that response as context for the next subtask, thereby structuring the overall workflow through explicit CoT reasoning.",
        "relevant_elements": [
            "planning module",
            "chain-of-thought reasoning"
        ],
        "id": 367,
        "masked_question": "How does [mask1] leverage chain-of-thought reasoning from prior multi-agent frameworks?",
        "masked_number": 1,
        "masked_elements": [
            "planning module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.00523v2_figure_1.png",
        "paperid": "2408.00523v2",
        "paper_path": "./papers/2408.00523v2.json",
        "figure_id": "2408.00523v2_figure_1.png",
        "caption": "Figure 1: Overall pipeline of Atlas.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed component is simply the “Planning Module” of the mutation agent, and it is here that Atlas literally “drops in” a chain-of-thought style planner just as many recent multi-agent papers have advocated.  Concretely, the Planning Module\n\n1/ borrows the CoT idiom of breaking a big problem into a fixed series of micro-steps (bypass check → if needed, mutate for bypass → semantic check → if needed, mutate for semantics),  \n2/ drives the VLM (and later the LLM in the selection agent) one step at a time, feeding back the last answer plus the next sub-task,  \n3/ thus creates a true multi-turn, CoT dialogue between planner and model, and  \n4/ layers on external memory and tools at each step to ground those micro-decisions.\n\nIn short, mask1 is exactly where Atlas re-uses the “divide-and-chain” game plan from prior LLM agent work (e.g. ReAct, Toolformer, etc.), by issuing one chain-of-thought subtask at a time, observing the model’s answer, and chaining forward to the next subtask until it either succeeds or loops back."
    },
    {
        "question": "How does the linear regression mapping on LLM features extend previous encoding methodologies for fMRI language data?",
        "relevant_section_ids": [
            "2",
            "8"
        ],
        "relevant_context": [
            "The 20 hours of speech stimuli were transcribed and then passed through multiple open-source large language models to obtain activation vectors for each word. Multiple models were built using different LLMs to ensure that predictions are stable, i.e. not idiosyncratic to a single model. Regularized linear regression was then used to predict the response timecourse of each voxel as a weighted combination of LLM activations.",
            "Features were extracted from the 18th layer of the 30-billion parameter LLaMA model [13], and the 33rd layer of the 30-billion parameter OPT model [12]. … The OPT features achieved a mean voxelwise correlation of about 0.128 whereas the LLaMA features achieved a mean voxelwise correlation of about 0.132. These performances exceed that of previously published models on the same dataset (mean correlation 0.111) that were able to produce meaningful semantic decoding [45]."
        ],
        "final_answer": "By taking high-dimensional activation vectors from large language models (LLaMA and OPT) for each word and fitting a regularized linear regression from those LLM features to each voxel’s fMRI time course, the authors build on prior fMRI encoding approaches (which typically used hand-crafted or lower-dimensional semantic features). This mapping not only integrates rich, contextual embeddings from state-of-the-art LLMs but also achieves higher prediction accuracy than earlier semantic-decoding models on the same dataset.",
        "relevant_elements": [
            "Linear model",
            "LLM"
        ],
        "id": 368,
        "masked_question": "How does the [mask1] mapping on [mask2] features extend previous encoding methodologies for fMRI language data?",
        "masked_number": 2,
        "masked_elements": [
            "Linear model",
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.00812v1_figure_1.png",
        "paperid": "2410.00812v1",
        "paper_path": "./papers/2410.00812v1.json",
        "figure_id": "2410.00812v1_figure_1.png",
        "caption": "Figure 1: Driving single-voxel response with generative explanation-mediated validation.\n(a) Voxelwise BOLD responses were recorded using fMRI as human subjects listened to 20 hours of narrative stories. An encoding model f𝑓fitalic_f was fit to predict these responses from the story text. f𝑓fitalic_f consists of a linear model fit on representations extracted from an LLM, which are not readily interpretable. Encoding models were tested by predicting responses on held-out fMRI data, and only well-performing models were selected for further analysis.\n(b) We used an automated procedure to find a verbal description of the function that f𝑓fitalic_f computes for each voxel. First, we tested f𝑓fitalic_f on a large catalog of n𝑛nitalic_n-grams (n=1,2,3)n=1,2,3)italic_n = 1 , 2 , 3 ) and found those that maximally drove predicted responses. These n𝑛nitalic_n-grams were then summarized into stable explanation candidates using a powerful instruction-tuned LLM. Finally, we evaluated each explanation candidate by generating corresponding synthetic sentences and testing that these sentences yielded large predictions from f𝑓fitalic_f.\n(c) To test whether the generated explanations were causally related to activation in the brain, we used an LLM to produce synthetic narrative stories where each paragraph is designed to drive responses based on the generated explanation for one voxel. For each subject we constructed stories to drive 17 well-modeled voxels with diverse selectivity. These stories were then presented to the subjects in a second fMRI experiment.\n(d) Average BOLD response during its driving paragraph for each voxel, relative to baseline, i.e. average response to all non-driving paragraphs. On average, driven responses were significantly higher than baseline for each subject (p=0.020𝑝0.020p=0.020italic_p = 0.020 (S01), p<10−5𝑝superscript105p<10^{-5}italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT (S02), p=0.009𝑝0.009p=0.009italic_p = 0.009 (S03); permutation test, FDR-corrected).\nFor well-driven voxels, this means that the generated explanation is causally related to activation of that voxel, and thus that we have successfully translated the LLM-based encoding model into a verbal explanation.\n(e) Average BOLD response for each selected voxel to each of the driving paragraphs in one subject (S02). Responses to the driving paragraph generated using the explanation for that voxel appear along the main diagonal. Explanations that were used to construct the driving paragraphs are shown below. BOLD responses were generally high for the driving paragraphs for each voxel as well as semantically related paragraphs (e.g. directions and locations, emotional expression and laughter).",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning step‐by‐step:\n\n1.  In the panel you pointed out, the blue‐boxed module is the pre‐trained LLM that produces a high‐dimensional activation vector for each word or token in the story, and the red‐boxed module is a simple linear (ridge) regression that takes those LLM activations as inputs and predicts each voxel’s BOLD response.\n\n2.  Classical fMRI “language” encoding models (going back to Huth et al. 2016, Mitchell et al. 2008, etc.) all took relatively low‐dimensional, hand-crafted features – e.g. semantic category labels, co-occurrence counts, syntactic tags, word frequency, concreteness ratings – and fitted a linear model to predict voxel responses.\n\n3.  What GEM-V does differently is to replace those hand-tuned features with the deep, contextual embeddings from a state-of-the-art large language model.  In other words, rather than asking “how much does the word ‘cat’ belong to these 20 semantic categories?”, it asks “what does a 30B‐parameter transformer think this word (in context) ‘means’?” and then linearly maps *that* rich signal to brain activity.\n\n4.  By doing so, it extends prior encoding frameworks in two key ways:\n   •  Far richer, context-sensitive feature space:  Instead of a dozen or a few dozen semantic dimensions, you now have hundreds or thousands of latent features drawn from an LLM’s penultimate layer.  \n   •  Improved predictive power and interpretability pipeline:  These LLM activations yield higher held-out correlations (∼0.13) than prior models (∼0.11), and—because the mapping is still linear—you can drill into the weight vector to see *which* abstract LLM dimensions a given voxel cares about, then translate those back into human-readable “explanations.”\n\nSo, in plain English:  \nBy slotting a simple linear read-out on top of deep, contextual embeddings from a modern LLM, this work generalizes and improves upon decades of fMRI language‐encoding studies that were limited to small, hand-engineered semantic feature sets."
    },
    {
        "question": "How does combining summarization and evaluation LLM steps compare to prior explanation generation pipelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Summarization LLM",
            "Evaluate LLM"
        ],
        "id": 369,
        "masked_question": "How does combining [mask1] and [mask2] steps compare to prior explanation generation pipelines?",
        "masked_number": 2,
        "masked_elements": [
            "Summarization LLM",
            "Evaluate LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.00812v1_figure_1.png",
        "paperid": "2410.00812v1",
        "paper_path": "./papers/2410.00812v1.json",
        "figure_id": "2410.00812v1_figure_1.png",
        "caption": "Figure 1: Driving single-voxel response with generative explanation-mediated validation.\n(a) Voxelwise BOLD responses were recorded using fMRI as human subjects listened to 20 hours of narrative stories. An encoding model f𝑓fitalic_f was fit to predict these responses from the story text. f𝑓fitalic_f consists of a linear model fit on representations extracted from an LLM, which are not readily interpretable. Encoding models were tested by predicting responses on held-out fMRI data, and only well-performing models were selected for further analysis.\n(b) We used an automated procedure to find a verbal description of the function that f𝑓fitalic_f computes for each voxel. First, we tested f𝑓fitalic_f on a large catalog of n𝑛nitalic_n-grams (n=1,2,3)n=1,2,3)italic_n = 1 , 2 , 3 ) and found those that maximally drove predicted responses. These n𝑛nitalic_n-grams were then summarized into stable explanation candidates using a powerful instruction-tuned LLM. Finally, we evaluated each explanation candidate by generating corresponding synthetic sentences and testing that these sentences yielded large predictions from f𝑓fitalic_f.\n(c) To test whether the generated explanations were causally related to activation in the brain, we used an LLM to produce synthetic narrative stories where each paragraph is designed to drive responses based on the generated explanation for one voxel. For each subject we constructed stories to drive 17 well-modeled voxels with diverse selectivity. These stories were then presented to the subjects in a second fMRI experiment.\n(d) Average BOLD response during its driving paragraph for each voxel, relative to baseline, i.e. average response to all non-driving paragraphs. On average, driven responses were significantly higher than baseline for each subject (p=0.020𝑝0.020p=0.020italic_p = 0.020 (S01), p<10−5𝑝superscript105p<10^{-5}italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT (S02), p=0.009𝑝0.009p=0.009italic_p = 0.009 (S03); permutation test, FDR-corrected).\nFor well-driven voxels, this means that the generated explanation is causally related to activation of that voxel, and thus that we have successfully translated the LLM-based encoding model into a verbal explanation.\n(e) Average BOLD response for each selected voxel to each of the driving paragraphs in one subject (S02). Responses to the driving paragraph generated using the explanation for that voxel appear along the main diagonal. Explanations that were used to construct the driving paragraphs are shown below. BOLD responses were generally high for the driving paragraphs for each voxel as well as semantically related paragraphs (e.g. directions and locations, emotional expression and laughter).",
        "qtype": "Literature_Background",
        "response": "Here “mask1” is the LLM‐based summarization step (the red box in Fig. 1b, which turns the top‐driving n-grams into a handful of natural-language explanation candidates) and “mask2” is the subsequent LLM‐based evaluation step (the blue box, which turns each candidate explanation back into synthetic sentences and re-scores them on the encoding model).  By chaining those two pieces—Summarize ➔ Evaluate—we recover not just a plausible verbal summary of what drives each voxel, but one that we have already “stress-tested” against the very model we care about.  \n\nIn contrast, most earlier explanation pipelines in neuroscience or NLP either (a) stop after extracting or clustering features (e.g. gradient saliency, topic clustering, or listing top n-grams) or (b) rely on human inspection or off-line metrics to pick among candidates.  They do not automatically generate a short, human‐readable explanation *and* then immediately verify that a model driven by that explanation actually reproduces the original effect.  The red+blue combination is essentially the Summarize-and-Score (SASC) approach [17], and it is what lets GEM-V produce explanations that are both concise *and* demonstrably faithful to the encoding model—something prior pipelines have not offered."
    },
    {
        "question": "How does held-out fMRI testing of the encoding model guide voxel selection for follow-up experiments?",
        "relevant_section_ids": [
            "8"
        ],
        "relevant_context": [
            "For model testing, the subjects listened to the two test stories five times each, and one test story 10 times, at a rate of 1 test story per session. These test responses were averaged across repetitions.",
            "To evaluate the voxelwise encoding models, we used the learned encoding model to generate and evaluate predictions on a held-out test set.",
            "We selected 500 well-modeled, diverse voxels to explain for each subject. To ensure that these voxels were well-modeled, we selected only from voxels with a test correlation above 0.15, (this corresponds to the top 4⁄3% most well-predicted voxels).",
            "After extracting an explanation for 500 voxels per subject, we selected 17 voxels per subject for followup fMRI experiments."
        ],
        "final_answer": "Held-out fMRI testing is used to quantify each voxel’s encoding model performance (via the correlation between predicted and actual responses on stories held out from training). Voxels whose models exceed a threshold test correlation (r > 0.15, the top ~4.3%) are deemed “well‐modeled,” and from this set the authors uniformly sample 500 diverse voxels. From those 500 high‐performers, they then pick 17 voxels per subject for the actual follow‐up fMRI experiments.",
        "relevant_elements": [
            "Encoding model",
            "Voxel selection"
        ],
        "id": 370,
        "masked_question": "How does held-out fMRI testing of the [mask1] guide voxel selection for follow-up experiments?",
        "masked_number": 1,
        "masked_elements": [
            "Encoding model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.00812v1_figure_1.png",
        "paperid": "2410.00812v1",
        "paper_path": "./papers/2410.00812v1.json",
        "figure_id": "2410.00812v1_figure_1.png",
        "caption": "Figure 1: Driving single-voxel response with generative explanation-mediated validation.\n(a) Voxelwise BOLD responses were recorded using fMRI as human subjects listened to 20 hours of narrative stories. An encoding model f𝑓fitalic_f was fit to predict these responses from the story text. f𝑓fitalic_f consists of a linear model fit on representations extracted from an LLM, which are not readily interpretable. Encoding models were tested by predicting responses on held-out fMRI data, and only well-performing models were selected for further analysis.\n(b) We used an automated procedure to find a verbal description of the function that f𝑓fitalic_f computes for each voxel. First, we tested f𝑓fitalic_f on a large catalog of n𝑛nitalic_n-grams (n=1,2,3)n=1,2,3)italic_n = 1 , 2 , 3 ) and found those that maximally drove predicted responses. These n𝑛nitalic_n-grams were then summarized into stable explanation candidates using a powerful instruction-tuned LLM. Finally, we evaluated each explanation candidate by generating corresponding synthetic sentences and testing that these sentences yielded large predictions from f𝑓fitalic_f.\n(c) To test whether the generated explanations were causally related to activation in the brain, we used an LLM to produce synthetic narrative stories where each paragraph is designed to drive responses based on the generated explanation for one voxel. For each subject we constructed stories to drive 17 well-modeled voxels with diverse selectivity. These stories were then presented to the subjects in a second fMRI experiment.\n(d) Average BOLD response during its driving paragraph for each voxel, relative to baseline, i.e. average response to all non-driving paragraphs. On average, driven responses were significantly higher than baseline for each subject (p=0.020𝑝0.020p=0.020italic_p = 0.020 (S01), p<10−5𝑝superscript105p<10^{-5}italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT (S02), p=0.009𝑝0.009p=0.009italic_p = 0.009 (S03); permutation test, FDR-corrected).\nFor well-driven voxels, this means that the generated explanation is causally related to activation of that voxel, and thus that we have successfully translated the LLM-based encoding model into a verbal explanation.\n(e) Average BOLD response for each selected voxel to each of the driving paragraphs in one subject (S02). Responses to the driving paragraph generated using the explanation for that voxel appear along the main diagonal. Explanations that were used to construct the driving paragraphs are shown below. BOLD responses were generally high for the driving paragraphs for each voxel as well as semantically related paragraphs (e.g. directions and locations, emotional expression and laughter).",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed term in panel (a) is the “encoding model.”  In the first stage (the 20 h of story listening) each voxel’s encoding model is trained on  ∼33 K  time-points and then evaluated by predicting its response to held-out stories.  By comparing the model’s prediction to the actual fMRI signal on these novel test stories one obtains a per-voxel correlation score.  \n\nWe then use those held-out correlations to drive our voxel‐selection in two ways:\n\n1.  **Threshold on predictive power.**  \n    We only consider voxels whose encoding models achieve a test-set correlation > 0.15 (roughly the top 40 % of voxels) — this guarantees that we march forward only with voxels that are genuinely “well‐modeled” by the LLM-based encoder.\n\n2.  **Ensure diversity and stability.**  \n    From that pool of well-modeled voxels we apply PCA on the learned weight vectors and sample uniformly in the space of the first four principal components to get a diverse set (500 voxels total).  Finally, we compute a “stability score” (the agreement between the OPT- and LLaMA-based model predictions on the same n-grams) and pick the top 40 most stable voxels, from which we manually choose 17 per subject with the most varied semantic explanations.\n\nIn this way, held-out fMRI testing of the encoding model tells us both which voxels are truly driven by the stimulus text (via the correlation threshold) and, via additional PCA‐ and stability‐based filtering, which of those well-modeled voxels will make robust, interpretable targets for our follow-up single-voxel story-driving experiments."
    },
    {
        "question": "How does the LLM-driven story construction procedure integrate voxel explanations to isolate individual voxel responses?",
        "relevant_section_ids": [
            "3",
            "8"
        ],
        "relevant_context": [
            "This was done by prompting an instruction-finetuned LLM [16] to generate narratives that should selectively drive cortical activation based on that explanation (Fig. 1c).",
            "Given a selected set of voxels and their explanations, we constructed narrative stories by iteratively prompting an LLM [16] to prioritize a different voxel’s explanation as the main focus of each paragraph. This allowed us to test whether voxels selectively respond to paragraphs that match their explanation.",
            "Stories are generated by repeatedly prompting the LLM in a chat to continue the story, one paragraph at a time. For each paragraph, the LLM is asked to focus on one explanation and to include related key n-grams (Fig. 4a, see full prompts in Section A.1)."
        ],
        "final_answer": "The procedure builds multi-paragraph stories by cycling through the set of target voxels: for each paragraph, the LLM is instructed to craft text around a single voxel’s natural-language explanation and to weave in its key driving n-grams. By measuring each voxel’s response during the paragraph devoted to its own explanation (and comparing to responses during other paragraphs), the experiment isolates and verifies that each explanation selectively drives activity in its corresponding voxel.",
        "relevant_elements": [
            "Story generation",
            "Voxel explanations"
        ],
        "id": 371,
        "masked_question": "How does the [mask1] procedure integrate [mask2] to isolate individual voxel responses?",
        "masked_number": 2,
        "masked_elements": [
            "Story generation",
            "Voxel explanations"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.00812v1_figure_1.png",
        "paperid": "2410.00812v1",
        "paper_path": "./papers/2410.00812v1.json",
        "figure_id": "2410.00812v1_figure_1.png",
        "caption": "Figure 1: Driving single-voxel response with generative explanation-mediated validation.\n(a) Voxelwise BOLD responses were recorded using fMRI as human subjects listened to 20 hours of narrative stories. An encoding model f𝑓fitalic_f was fit to predict these responses from the story text. f𝑓fitalic_f consists of a linear model fit on representations extracted from an LLM, which are not readily interpretable. Encoding models were tested by predicting responses on held-out fMRI data, and only well-performing models were selected for further analysis.\n(b) We used an automated procedure to find a verbal description of the function that f𝑓fitalic_f computes for each voxel. First, we tested f𝑓fitalic_f on a large catalog of n𝑛nitalic_n-grams (n=1,2,3)n=1,2,3)italic_n = 1 , 2 , 3 ) and found those that maximally drove predicted responses. These n𝑛nitalic_n-grams were then summarized into stable explanation candidates using a powerful instruction-tuned LLM. Finally, we evaluated each explanation candidate by generating corresponding synthetic sentences and testing that these sentences yielded large predictions from f𝑓fitalic_f.\n(c) To test whether the generated explanations were causally related to activation in the brain, we used an LLM to produce synthetic narrative stories where each paragraph is designed to drive responses based on the generated explanation for one voxel. For each subject we constructed stories to drive 17 well-modeled voxels with diverse selectivity. These stories were then presented to the subjects in a second fMRI experiment.\n(d) Average BOLD response during its driving paragraph for each voxel, relative to baseline, i.e. average response to all non-driving paragraphs. On average, driven responses were significantly higher than baseline for each subject (p=0.020𝑝0.020p=0.020italic_p = 0.020 (S01), p<10−5𝑝superscript105p<10^{-5}italic_p < 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT (S02), p=0.009𝑝0.009p=0.009italic_p = 0.009 (S03); permutation test, FDR-corrected).\nFor well-driven voxels, this means that the generated explanation is causally related to activation of that voxel, and thus that we have successfully translated the LLM-based encoding model into a verbal explanation.\n(e) Average BOLD response for each selected voxel to each of the driving paragraphs in one subject (S02). Responses to the driving paragraph generated using the explanation for that voxel appear along the main diagonal. Explanations that were used to construct the driving paragraphs are shown below. BOLD responses were generally high for the driving paragraphs for each voxel as well as semantically related paragraphs (e.g. directions and locations, emotional expression and laughter).",
        "qtype": "Experimental_Results",
        "response": "Here is a step‐by‐step outline of how the red‐boxed SASC procedure folds in the blue‐boxed “voxel explanation” so that each voxel’s response can be cleanly pulled apart:\n\n1.  Start from a fitted encoding model for one voxel.  \n2.  Scan through all 1-, 2-, and 3-gram combinations in your corpus and record which n-grams the model predicts will drive that voxel highest.  \n3.  Feed those top‐driving n-grams into an instruction‐tuned LLM (this is the first half of SASC) and ask it to summarize them into a single short phrase or word.  In our running example, the LLM boils “slicing cucumber,” “zesting lemons,” “peeling carrots,” etc., down into the explanation “Food preparation.”  This is the content in the blue box.  \n4.  Now “invert” that explanation back into language: prompt the same LLM to generate synthetic sentences or short passages that sound like real text about food preparation.  \n5.  Run those synthetic passages back through your voxel’s encoding model and pick the explanation whose generated text elicits the strongest model response.  That locks in “Food preparation” as the one true semantic driver of this voxel.  \n6.  Finally, to isolate the voxel’s response in living subjects, you build real narrative paragraphs around that explanation—e.g. a short story paragraph full of slicing cucumber, dicing onions, whisking eggs, etc.—and measure the voxel’s BOLD activity while subjects listen.  Comparing its average response during its “driving” paragraph to its response during all the others shows that only the text built from the blue‐boxed explanation truly lights up that one voxel.\n\nBy using the LLM twice—first to distill top n-grams into a neat English label, then to spin that label back into text—the SASC pipeline both uncovers and then selectively activates the precise semantic feature each voxel cares about."
    },
    {
        "question": "How does SSM balance storage reduction and INR continuity for high-frequency signals via sparse matrix conversion?",
        "relevant_section_ids": [
            "4.3.1"
        ],
        "relevant_context": [
            "However, this kind of signal will cause distribution shift in the global data distribution and reduce the performance and accuracy of INR. Therefore, it is necessary to sacrifice some INR continuity in exchange for improved performance and accuracy.",
            "As depicted in step 3 of Fig. 2, we use sparse matrix storage to convert high-frequency signals whose amplitude meets a certain threshold into Compressed Sparse Row (CSR) data with lower storage overhead."
        ],
        "final_answer": "SSM trades off some INR continuity for better performance and accuracy by thresholding high-frequency components by amplitude and storing only the significant entries in Compressed Sparse Row (CSR) format. This sparse matrix conversion concentrates storage on the few large high-frequency values, dramatically reducing overall storage overhead while tolerating minor continuity loss.",
        "relevant_elements": [
            "SSM",
            "sparse matrix"
        ],
        "id": 372,
        "masked_question": "How does [mask1] balance storage reduction and INR continuity for high-frequency signals via sparse matrix conversion?",
        "masked_number": 1,
        "masked_elements": [
            "SSM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06155v1_figure_2.png",
        "paperid": "2411.06155v1",
        "paper_path": "./papers/2411.06155v1.json",
        "figure_id": "2411.06155v1_figure_2.png",
        "caption": "Figure 2: Flowchart of Frequency-based Iterative Compression, the spatial compression module in HiHa (Single Time Frame).",
        "qtype": "Experimental_Results",
        "response": "The mystery box in the red square is simply our “Sparse Storage Module” (SSM), the little sub‐routine that takes the high–frequency harmonic band and turns it into a compressed sparse matrix.  Here is the intuition for how it trades off storage versus INR continuity:\n\n 1.  From the 3D FFT we isolate the high–frequency chunk (those tiny, spiky modes that carry most of the “rough” detail but occupy almost no volume).  \n 2.  We pick a small amplitude threshold and zero out everything below it.  That leaves only a handful of non–zero entries.  \n 3.  Instead of storing them in a full dense grid, we pack their coordinates and values into a Compressed Sparse Row (CSR) structure.  \n 4.  On reconstruction, the INR still “sees” exactly where those spikes live and how large they are, so it can smoothly interpolate around them.  We lose a bit of point–wise continuity (because we’ve dropped all the sub-threshold noise), but we keep exactly the big, high-frequency punches that matter.  \n\nBy throwing away sub-threshold coefficients and storing only the non-zeros in CSR form, SSM typically cuts the high-frequency data footprint by one or even two orders of magnitude, while retaining just enough of the true high-frequency pattern that the INR can reconstruct it without dramatic artifacts."
    },
    {
        "question": "How does MIM leverage multi-layer Siren and 3D-VAR interpolation to trade computational overhead with compression accuracy?",
        "relevant_section_ids": [
            "4.3.2"
        ],
        "relevant_context": [
            "However, this will bring huge memory occupation, computing overhead and straggler parameter. Therefore, we adopt a multi-scale INC manner, utilizing an 1-layer Laplacian Pyramid structure to greatly reduce the computational overhead.",
            "Specifically, for low-frequency harmonics, we firstly downscale the target  to a thumbnail , which is downscaled to , the 4.1 step of Fig. 2. Then we implement the Siren based INR with , and achieve .",
            "Next, a 3-dimensional variational assimilation (3D-VAR) (Rabier, Mcnally et al. 1998) based interpolation is implemented on  upscaling to , due to its widely application in interpolation of spatial atmospheric data, and achieves ."
        ],
        "final_answer": "MIM first downsamples the low-frequency field into a small thumbnail via a one-layer Laplacian pyramid and fits that coarse version cheaply with a Siren-based INR. It then uses fast 3D-VAR interpolation to upsample back to full resolution. This two-step—coarse Siren fit plus interpolation—dramatically cuts computational overhead, while any residual errors beyond the target accuracy can be locally refitted, preserving overall compression fidelity.",
        "relevant_elements": [
            "MIM",
            "multi-layer Siren",
            "3D-VAR interpolation"
        ],
        "id": 373,
        "masked_question": "How does [mask1] leverage [mask2] and 3D-VAR interpolation to trade computational overhead with compression accuracy?",
        "masked_number": 2,
        "masked_elements": [
            "MIM",
            "multi-layer Siren"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06155v1_figure_2.png",
        "paperid": "2411.06155v1",
        "paper_path": "./papers/2411.06155v1.json",
        "figure_id": "2411.06155v1_figure_2.png",
        "caption": "Figure 2: Flowchart of Frequency-based Iterative Compression, the spatial compression module in HiHa (Single Time Frame).",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is the Multi-scale INR Module (MIM) and the blue box inside it is the little “multi-layer SIREN” network that actually learns a mapping from 3D coordinates to pressure.  What the paper does is the following:\n\n1.  MIM first takes your low–frequency field and aggressively down–samples it to a tiny “thumbnail” version.  \n2.  It then fits that thumbnail with the small multi-layer SIREN (blue box), which is extremely cheap to train because there are very few points.  \n3.  Next, instead of re–training the network at full resolution, it hands the thumbnail through a 3D-VAR interpolator to get an approximate full-resolution field.  3D-VAR is orders of magnitude faster than re-training the network on every grid point, but still gives you a physically reasonable up-sample.  \n4.  Finally, MIM chops the 3D-VAR up-sample into blocks, checks which blocks miss the target error tolerance, and only re-invokes the SIREN on those “bad” blocks to learn a small residual correction.\n\nBy doing this coarse‐to‐fine dance—train a tiny SIREN on a thumbnail, interpolate with 3D-VAR, then only patch up the remaining error with a second, block-wise SIREN pass—MIM trades off a lot of expensive INR training (and parameter count) for a little extra interpolation error, yet regains any lost accuracy by selectively re-training only where it’s needed."
    },
    {
        "question": "What limitations arise from SSM’s sparse matrix storage on reconstructing extreme high-frequency weather anomalies?",
        "relevant_section_ids": [
            "4.3.1"
        ],
        "relevant_context": [
            "However, this kind of signal will cause distribution shift in the global data distribution and reduce the performance and accuracy of INC.",
            "Therefore, it is necessary to sacrifice some INR continuity in exchange for improved performance and accuracy.",
            "As depicted in step 3 of Fig. 2, we use sparse matrix storage to convert high-frequency signals whose amplitude meets a certain threshold into Compressed Sparse Row (CSR) data with lower storage overhead."
        ],
        "final_answer": "By storing extreme high-frequency anomalies in a sparse CSR format, the SSM module reduces storage cost but necessarily gives up part of the INR’s inherent continuity. As a result, the reconstruction of those extreme, localized anomalies can exhibit discontinuities or loss of fine-scale spatial coherence.",
        "relevant_elements": [
            "SSM",
            "sparse matrix storage"
        ],
        "id": 374,
        "masked_question": "What limitations arise from [mask1]’s sparse matrix storage on reconstructing extreme high-frequency weather anomalies?",
        "masked_number": 1,
        "masked_elements": [
            "SSM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06155v1_figure_2.png",
        "paperid": "2411.06155v1",
        "paper_path": "./papers/2411.06155v1.json",
        "figure_id": "2411.06155v1_figure_2.png",
        "caption": "Figure 2: Flowchart of Frequency-based Iterative Compression, the spatial compression module in HiHa (Single Time Frame).",
        "qtype": "Others",
        "response": "First, let us pin down what “​[mask1]​” is.  In the red box of Fig. 2 sits the Sparse Storage Module (SSM), which is the mechanism the authors adopt to pack only the largest‐amplitude, highest‐frequency coefficients into a Compressed Sparse Row (CSR) matrix.  \n\nNow:  because SSM only retains those high‐frequency modes whose amplitude exceeds a tunable threshold and throws the rest away, you inevitably lose a swath of very fine‐scale fluctuations.  In practice this means:\n\n 1.  Any localized spike or oscillation whose amplitude didn’t clear the cut‐off simply never enters the CSR buffer—and so can’t be recovered at decompression time.  \n 2.  The sparse format severs the smooth, continuous structure that an INR normally builds, so when you paste the few retained entries back in, you often get discontinuities or “holes” in the high‐frequency band.  \n 3.  Extreme, small‐scale anomalies – for instance a sudden convection cell or a narrow jet streak – may either be under‐represented (if only a tiny patch makes the threshold) or dropped entirely (if it falls just below).  \n\nIn short, the SSM’s CSR‐based sparsification trades big savings in bytes for a blind spot on truly fine, high‐frequency bumps.  You reduce storage, but you also risk missing or smearing out the very extremes you most wanted to capture."
    },
    {
        "question": "How could distribution-aware prompts bias automated sample collection toward stereotypical representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Distribution-aware Prompts",
            "Automated Sample Collection"
        ],
        "id": 376,
        "masked_question": "How could [mask1] bias [mask2] toward stereotypical representations?",
        "masked_number": 2,
        "masked_elements": [
            "Distribution-aware Prompts",
            "Automated Sample Collection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.08966v1_figure_2.png",
        "paperid": "2407.08966v1",
        "paper_path": "./papers/2407.08966v1.json",
        "figure_id": "2407.08966v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our LAPT method, where 𝐯d o g/𝐯b o a tsubscript𝐯𝑑𝑜𝑔subscript𝐯𝑏𝑜𝑎𝑡\\mathbf{v}_{dog}/\\mathbf{v}_{boat}bold_v start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_v start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, 𝐜d o g/𝐜b o a tsubscript𝐜𝑑𝑜𝑔subscript𝐜𝑏𝑜𝑎𝑡\\mathbf{c}_{dog}/\\mathbf{c}_{boat}bold_c start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_c start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, and 𝐥d o g/𝐥b o a tsubscript𝐥𝑑𝑜𝑔subscript𝐥𝑏𝑜𝑎𝑡\\mathbf{l}_{dog}/\\mathbf{l}_{boat}bold_l start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_l start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT are image features, textual features, and soft labels of dog/boat samples.",
        "qtype": "Others",
        "response": "The crux is that those learned “distribution-aware” prompt tokens effectively become the text queries used to pull or generate images in the automated collection step.  If the prompt tokens pick up (or even amplify) any biased or stereotypical co-occurrences in the ID versus OOD data—say, implicitly associating “doctor” with male figures, “nurse” with female figures, “engineer” with laboratory settings, and so on—then when you feed “<learned-tokens> + class-name” into a text‐to‐image generator or into a CLIP‐based retrieval system, it will dutifully return images that conform to those learned stereotypes.  In short, any skew in the learned context tokens gets baked into the retrieval/generation process, so your automatically collected samples end up reflecting whatever narrow, clichéd representations the prompts have encoded."
    },
    {
        "question": "What motivates differentiating tokens between ID and negative labels in distribution-aware prompts?",
        "relevant_section_ids": [
            "3.3.1"
        ],
        "relevant_context": [
            "Here, we propose a distribution-aware prompt strategy, which differentiates tokens for ID and OOD classes, as shown in Fig. 2.",
            "Our empirical results suggest that the distribution-aware prompt notably enhances the distinction between ID and OOD distributions, as analyzed in Sec. 4.3."
        ],
        "final_answer": "Differentiating tokens for ID and negative (OOD) labels is motivated by the observation that it notably enhances the distinction between in‐distribution and out‐of‐distribution samples, thereby improving OOD detection performance.",
        "relevant_elements": [
            "distribution-aware prompts"
        ],
        "id": 378,
        "masked_question": "What motivates differentiating tokens between ID and negative labels in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "distribution-aware prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.08966v1_figure_2.png",
        "paperid": "2407.08966v1",
        "paper_path": "./papers/2407.08966v1.json",
        "figure_id": "2407.08966v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our LAPT method, where 𝐯d o g/𝐯b o a tsubscript𝐯𝑑𝑜𝑔subscript𝐯𝑏𝑜𝑎𝑡\\mathbf{v}_{dog}/\\mathbf{v}_{boat}bold_v start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_v start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, 𝐜d o g/𝐜b o a tsubscript𝐜𝑑𝑜𝑔subscript𝐜𝑏𝑜𝑎𝑡\\mathbf{c}_{dog}/\\mathbf{c}_{boat}bold_c start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_c start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, and 𝐥d o g/𝐥b o a tsubscript𝐥𝑑𝑜𝑔subscript𝐥𝑏𝑜𝑎𝑡\\mathbf{l}_{dog}/\\mathbf{l}_{boat}bold_l start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_l start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT are image features, textual features, and soft labels of dog/boat samples.",
        "qtype": "Design_Rationale",
        "response": "The little red box in the diagram is simply the “distribution‐aware” prompt head – it tacks on one set of learnable context tokens in front of your in–distribution labels (“dog,” “cat,” etc.) and a completely separate set of tokens in front of your mined negative (out‐of‐distribution) labels (“boat,” “plant,” etc.).  We did that because, as we show in Section 4.3, giving the model two different pools of context vectors – one dedicated to ID classes, one to negatives – makes it much easier for the text encoder to pull the two families of label embeddings apart in cosine‐space.  In short: by *explicitly* assigning different prompt tokens to ID vs. negative labels, you force a sharper ID–OOD split in the text feature space, and that dramatically boosts your OOD detection performance."
    },
    {
        "question": "Why integrate cross-modal mixing with cross-distribution mixing in the prompt tuning loss?",
        "relevant_section_ids": [
            "3.3.3"
        ],
        "relevant_context": [
            "While the simple classification loss has achieved commendable results, we could further enhance it by addressing potential sample noise and exploring broader data space through carefully designed data mixing strategies.",
            "To mitigate the impact of image noise, we introduce a cross-modal mixing strategy to neutralize potential noise in the images.",
            "By mixing textual and visual representations, we aim to create a more robust feature set that can help the model learn to focus on relevant features while disregarding noisy information.",
            "To enhance the utilization of these spaces, we introduce a cross-distribution mixing strategy, which combines features and corresponding labels of ID and negative samples to create new training samples.",
            "This method not only allows the model to become more aware of the intermediate space between ID and negative regions, but also encourages it to learn more discriminative features that can better generalize to new and unseen OOD samples.",
            "Our cross-modal mixing strategy lowers image noise by blending samples from the same class but across different modalities, creating more robust features. Meanwhile, our cross-distribution mixing scheme goes a step further by mixing automatically collected positive and negative data, allowing us to explore the space that lies between ID and negative regions. This exploration enables our model to better understand and characterize the boundary between ID and OOD, leading to more effective detection of anomalies."
        ],
        "final_answer": "Integrating cross-modal mixing with cross-distribution mixing allows the prompt tuning loss to both mitigate noise in the collected images by blending visual and textual features, and to explore the intermediate feature space between in-distribution and negative (OOD) classes. This combination yields more robust, discriminative prompts that improve the model’s ability to distinguish ID from OOD samples.",
        "relevant_elements": [
            "cross-modal mixing",
            "cross-distribution mixing"
        ],
        "id": 379,
        "masked_question": "Why integrate [mask1] with [mask2] in the prompt tuning loss?",
        "masked_number": 2,
        "masked_elements": [
            "cross-modal mixing",
            "cross-distribution mixing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.08966v1_figure_2.png",
        "paperid": "2407.08966v1",
        "paper_path": "./papers/2407.08966v1.json",
        "figure_id": "2407.08966v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our LAPT method, where 𝐯d o g/𝐯b o a tsubscript𝐯𝑑𝑜𝑔subscript𝐯𝑏𝑜𝑎𝑡\\mathbf{v}_{dog}/\\mathbf{v}_{boat}bold_v start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_v start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, 𝐜d o g/𝐜b o a tsubscript𝐜𝑑𝑜𝑔subscript𝐜𝑏𝑜𝑎𝑡\\mathbf{c}_{dog}/\\mathbf{c}_{boat}bold_c start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_c start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT, and 𝐥d o g/𝐥b o a tsubscript𝐥𝑑𝑜𝑔subscript𝐥𝑏𝑜𝑎𝑡\\mathbf{l}_{dog}/\\mathbf{l}_{boat}bold_l start_POSTSUBSCRIPT italic_d italic_o italic_g end_POSTSUBSCRIPT / bold_l start_POSTSUBSCRIPT italic_b italic_o italic_a italic_t end_POSTSUBSCRIPT are image features, textual features, and soft labels of dog/boat samples.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In our prompt-tuning loss (Eq. 11) we start from the ordinary cross-entropy on the collected images and then add two auxiliary terms:\n\n    •  The red-boxed term (what you called [mask1]) is “cross-modal data mixing”  \n    •  The blue-boxed term (your [mask2]) is “cross-distribution data mixing.”\n\n2.  Why do we need both?  \n    –  Cross-modal mixing (L₍cm₎) combats noise in the automatically collected images by blending each image feature v with its text feature c.  In effect it forces the learned prompt to pay attention to the shared semantic signal across vision and language, so that occasional bad or mislabeled images won’t derail prompt tuning.  \n    –  Cross-distribution mixing (L₍cd₎) on the other hand deliberately interpolates between an ID sample and a negative sample (soft‐label and feature), so that the model learns the whole continuum from “definitely in‐distribution” to “definitely out‐of‐distribution.”  This fills in the gap that plain negative classes leave unused and sharpens the ID/OOD boundary.\n\n3.  By integrating both L₍cm₎ and L₍cd₎ into the final objective, the prompt is trained (a) to be robust against noisy images via modality blending, and (b) to carve out a clear, continuous feature‐space boundary between ID and OOD via distribution blending.  Empirically, this joint strategy yields much stronger OOD detection than using either mixing alone."
    },
    {
        "question": "What motivates introducing label non-IID challenge in the Fed-ECG multi-label classification pipeline?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Among these, non-IID labels have the most pronounced impact on FL model performance. This is because the quantity and types of labels held by each institution can vary greatly, misleading the local supervised training process and causing \"Client Drift\" [48], which hinders global model convergence.",
            "For label distribution in the Fed-ECG multi-label classification task, each sample may belong to multiple categories, but the quantity and proportion of different labels vary significantly among institutions."
        ],
        "final_answer": "The label non-IID challenge is introduced because in Fed-ECG each institution has very different label distributions—both in which categories appear and their frequencies. This heterogeneity can mislead local training (as some clients lack certain labels entirely) and causes client drift, thereby hindering convergence of the global model.",
        "relevant_elements": [
            "Fed-ECG",
            "Label non-IID"
        ],
        "id": 380,
        "masked_question": "What motivates introducing [mask1] challenge in the [mask2] multi-label classification pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "Label non-IID",
            "Fed-ECG"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.07050v1_figure_1.png",
        "paperid": "2411.07050v1",
        "paper_path": "./papers/2411.07050v1.json",
        "figure_id": "2411.07050v1_figure_1.png",
        "caption": "Figure 1: The overall architecture of the proposed FedCVD benchmark. We present two main settings (Fed-ECG, Fed-ECHO) and an experimental platform, highlighting three primary challenges. Green and blue circles in the challenges section indicate their presence in Fed-ECG and Fed-ECHO, respectively. The API section highlights user-facing APIs in orange boxes.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning broken down:\n\n1. In the Fed-ECG setting (the blue-boxed “Fed-ECG”), each of the four institutions is doing a multi‐label classification over the same pool of ECG diagnoses.  \n2. However, when you look at how often each label appears at each site, the distributions don’t line up at all:  \n   - Some labels (like “PAC”) don’t even occur at Institutions 3 & 4.  \n   - The most common class at Institutions 1 & 2 is “NORM,” while at Institutions 3 & 4 it’s “STACH.”  \n   - Beyond wholesale absence, the relative frequencies of all other labels swing wildly from one hospital to another.  \n3. Because of this heterogeneity in label presence and frequency, local training steps at each institution see a very different supervised signal—models at some sites never see certain classes, while others see them all the time.  \n4. This mismatch both “misleads” local gradients and induces classic federated client‐drift, making the global model slow (or even impossible) to converge.  \n5. That is exactly why the “Label non-IID” challenge (the red box) is singled out in the Fed-ECG pipeline: to explicitly capture and address the fact that each client’s label distribution is non-identical and incomplete."
    },
    {
        "question": "How does Partitioner distribute ECGDataset samples to reflect natural client heterogeneity?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Partitioner",
            "ECGDataset"
        ],
        "id": 382,
        "masked_question": "How does [mask1] distribute [mask2] samples to reflect natural client heterogeneity?",
        "masked_number": 2,
        "masked_elements": [
            "Partitioner",
            "ECGDataset"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.07050v1_figure_1.png",
        "paperid": "2411.07050v1",
        "paper_path": "./papers/2411.07050v1.json",
        "figure_id": "2411.07050v1_figure_1.png",
        "caption": "Figure 1: The overall architecture of the proposed FedCVD benchmark. We present two main settings (Fed-ECG, Fed-ECHO) and an experimental platform, highlighting three primary challenges. Green and blue circles in the challenges section indicate their presence in Fed-ECG and Fed-ECHO, respectively. The API section highlights user-facing APIs in orange boxes.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the Partitioner utility and Mask2 is the ECGDataset. In FedCVD the Partitioner never randomly shards the ECG data; it simply looks at each record’s “origin institution” tag (e.g. Shandong hospital, PTB-XL, PhysioNet, etc.) inside the ECGDataset and hands every sample back exactly to the client it came from. In this way each client ends up with its own hospital’s ECGs, preserving the real‐world imbalances in patient age, label frequencies, equipment differences, and overall sample size—i.e. the natural non-IID, long-tail heterogeneity you see in practice."
    },
    {
        "question": "How does Handler synchronize with the Distributed Backend to coordinate server-client interactions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Handler",
            "Distributed Backend"
        ],
        "id": 383,
        "masked_question": "How does [mask1] synchronize with the Distributed Backend to coordinate server-client interactions?",
        "masked_number": 1,
        "masked_elements": [
            "Handler"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.07050v1_figure_1.png",
        "paperid": "2411.07050v1",
        "paper_path": "./papers/2411.07050v1.json",
        "figure_id": "2411.07050v1_figure_1.png",
        "caption": "Figure 1: The overall architecture of the proposed FedCVD benchmark. We present two main settings (Fed-ECG, Fed-ECHO) and an experimental platform, highlighting three primary challenges. Green and blue circles in the challenges section indicate their presence in Fed-ECG and Fed-ECHO, respectively. The API section highlights user-facing APIs in orange boxes.",
        "qtype": "Implementation_Details",
        "response": "Step-by-step reasoning:\n\n1.  Identify [mask1] in the figure.  \n    In the “APIs” row under “Federated Learning Simulation APIs,” the red box highlights the Server-side component labeled “Handler.”\n\n2.  Understand the role of the Handler from the context.  \n    Although the text does not spell out “Handler,” it repeatedly tells us that FedCVD is built on top of the open-source FedLab framework, and that there is a clear two-tiered API split into Server (coordinator) and Client (participants). The Server must (a) distribute the global model to each client, (b) collect client updates, and (c) trigger global aggregation.\n\n3.  Recall how FedLab’s distributed backend works.  \n    FedLab exposes a “Distributed Backend” (usually a thin wrapper around PyTorch’s process group or an RPC glue) that provides primitive operations like broadcast (send the same tensor to all parties), reduce/gather (collect tensors from parties), and barrier (block until all parties reach the same point).\n\n4.  Map “Handler” to the FedLab backend.  \n    The Handler is the Server’s communication driver. It makes calls into the backend’s broadcast/gather/barrier API at each federated round:\n       – At the start of a round, Handler issues a broadcast of the current global model to all clients via the backend.  \n       – It then enters a receive‐loop, where the backend dispatches incoming “model update” messages from each client back into Handler callbacks.  \n       – Once all expected updates have arrived, Handler triggers the aggregation logic (e.g. FedAvg) and then either ends training or proceeds to the next round.\n\n5.  Synchronization mechanism.  \n    Federated learning requires that the Server waits until every Client has finished its local epoch. Handler uses the backend’s barrier primitive (or simple counting of received messages) to enforce this. Only when the barrier is lifted (i.e. all clients have responded) does Handler perform the global model update and move on.\n\nAnswer:\n\nThe red‐boxed component is the server-side “Handler.”  It sits directly atop the FedLab “Distributed Backend” and uses its primitive operations (broadcast, gather/reduce, barrier) to drive each communication round:  \n– Handler calls backend.broadcast(model) to push the global model to every client.  \n– It then blocks (via backend.barrier or by counting inbound messages) until all clients have pushed back their local updates, which arrive as messages dispatched into Handler’s registered callbacks.  \n– Once every update has been received, Handler invokes the aggregation routine, produces the new global model, and repeats the cycle.  \nIn this way Handler and the Distributed Backend stay in lock step to coordinate all server–client interactions."
    },
    {
        "question": "How does motion encoder determine discrete indices from the codebook for body and hand parts?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "For each motion sequence, the corresponding encoder first projects the sequence into a latent space: $z_e^p = E^p(x^p)$, using a stack of 1D-CNN layers.",
            "Then, for each pose we can derive a set of discrete tokens $z_q^p$ using a quantizer $Q$, which searches the nearest neighbor from the codebook $e^p$."
        ],
        "final_answer": "After encoding each part-wise motion sequence into a latent vector via the 1D-CNN encoder, the motion encoder quantizes each latent vector by finding its nearest neighbor in the corresponding part-specific codebook. The index of that closest code is then taken as the discrete token for body, left-hand, and right-hand motions.",
        "relevant_elements": [
            "Motion Encoder",
            "Codebook"
        ],
        "id": 384,
        "masked_question": "How does [mask1] determine discrete indices from the [mask2] for body and hand parts?",
        "masked_number": 2,
        "masked_elements": [
            "Motion Encoder",
            "Codebook"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.17799v1_figure_2.png",
        "paperid": "2411.17799v1",
        "paper_path": "./papers/2411.17799v1.json",
        "figure_id": "2411.17799v1_figure_2.png",
        "caption": "Figure 2: An overview of our proposed method, Signs as Tokens (SOKE). Our approach consists of two key components: a decoupled tokenizer (DETO) and an autoregressive multilingual generator (AMG). DETO transforms the input pose sequence to a set of discrete tokens using three distinct vector quantized variational autoencoders, each one composed of a motion encoder (ℰℰ\\mathcal{E}caligraphic_E), a motion decoder (𝒟𝒟\\mathcal{D}caligraphic_D), and a codebook. These components work together to discretize input sign sequences from various body parts. Leveraging a pretrained language model (LM), the AMG can later generate multilingual signs from text prompts with a tailored part-wise decoding strategy.",
        "qtype": "Implementation_Details",
        "response": "In our design the “red‐boxed” module is simply performing vector‐quantization over the “blue‐boxed” codebooks.  Concretely, each encoder (e.g. ℰ_B for the body, ℰ_LH for the left hand, ℰ_RH for the right hand) emits a continuous latent vector at each time step; the quantizer then computes the L₂ distance between that vector and every entry in the corresponding codebook and picks the codeword with the smallest distance.  The position of that nearest codeword in the codebook is taken as the discrete index (i.e. the token) for that frame."
    },
    {
        "question": "How does part-wise decoding maintain shared weights across separate multilingual LM decoder instances?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Moreover, we instantiate three LM decoders with shared weights, each one in charge of a single body part."
        ],
        "final_answer": "Part-wise decoding uses three decoder instances—one per body part—that are all copies of the same multilingual LM decoder and thus share the exact same parameters (weights) across these instances.",
        "relevant_elements": [
            "Part-wise Decoding",
            "Multilingual LM Decoder"
        ],
        "id": 385,
        "masked_question": "How does [mask1] maintain shared weights across separate multilingual LM decoder instances?",
        "masked_number": 1,
        "masked_elements": [
            "Part-wise Decoding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17799v1_figure_2.png",
        "paperid": "2411.17799v1",
        "paper_path": "./papers/2411.17799v1.json",
        "figure_id": "2411.17799v1_figure_2.png",
        "caption": "Figure 2: An overview of our proposed method, Signs as Tokens (SOKE). Our approach consists of two key components: a decoupled tokenizer (DETO) and an autoregressive multilingual generator (AMG). DETO transforms the input pose sequence to a set of discrete tokens using three distinct vector quantized variational autoencoders, each one composed of a motion encoder (ℰℰ\\mathcal{E}caligraphic_E), a motion decoder (𝒟𝒟\\mathcal{D}caligraphic_D), and a codebook. These components work together to discretize input sign sequences from various body parts. Leveraging a pretrained language model (LM), the AMG can later generate multilingual signs from text prompts with a tailored part-wise decoding strategy.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is simply the “part‐wise decoding” step – in practice we don’t spin up three *different* Transformer decoders, but rather call the *same* pretrained multilingual decoder three times (once for body, once for left hand, once for right hand).  By tying all three of those calls back to the very same network weights (i.e. parameter‐sharing or “weight tying”) and only changing the special start‐token (<Lang_B>, <Lang_LH> or <Lang_RH>) to tell it which part to generate, the model maintains a single shared decoder under the hood."
    },
    {
        "question": "How does Decoupled Tokenizer integrate Body Codebook quantization with VQ-VAE concepts?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To discretize continuous signs into tokens, we develop a sign tokenizer based on the well-established VQ-VAE. Existing motion generation research primarily focuses on body movements, such as running and jumping, while often neglecting the role of hands. However, in sign language, hands are crucial for conveying information [17,55]. To bridge this gap, we propose DETO, a decoupled tokenizer that utilizes three VQ-VAEs to simultaneously model key regions: the upper body and both hands.",
            "Given a sign motion input M, we first decompose it into three part-wise motion sequences based on kinematic tree of SMPL-X [40]: M=[M_B,M_LH,M_RH], where M_B includes the upper body and face, M_LH and M_RH are left- and right-hand motions. Moreover, we build three distinct VQ-VAEs, where each of them consists of an encoder E, a decoder D, and a learnable codebook C, where N_C represents the number of codes and d_C denotes the code dimension.",
            "Then, for each pose we can derive a set of discrete tokens t=[t_1,…,t_{T/f}] using a quantizer Q, which searches the nearest neighbor from the codebook: t_i = argmin_k || q_i - C_k ||. We then feed the obtained token sequence to the corresponding part-decoder D to reconstruct the input motions: M' = D(t)."
        ],
        "final_answer": "DETO uses the VQ-VAE framework to tokenize each sign sequence into discrete body (and hand) codes. It first splits a motion into body, left-hand, and right-hand streams, then passes each stream through its own VQ-VAE – consisting of an encoder, a learnable codebook, and a decoder. The encoder projects the stream into a latent sequence, and a quantizer snaps each latent to its nearest entry in the part’s codebook (the “body codebook” for the upper body). These nearest-neighbor lookups produce discrete tokens, which are then decoded back into motion, and whose reconstruction and embedding losses drive codebook learning under the standard VQ-VAE losses.",
        "relevant_elements": [
            "Decoupled Tokenizer",
            "Body Codebook"
        ],
        "id": 386,
        "masked_question": "How does [mask1] integrate Body Codebook quantization with VQ-VAE concepts?",
        "masked_number": 1,
        "masked_elements": [
            "Decoupled Tokenizer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17799v1_figure_2.png",
        "paperid": "2411.17799v1",
        "paper_path": "./papers/2411.17799v1.json",
        "figure_id": "2411.17799v1_figure_2.png",
        "caption": "Figure 2: An overview of our proposed method, Signs as Tokens (SOKE). Our approach consists of two key components: a decoupled tokenizer (DETO) and an autoregressive multilingual generator (AMG). DETO transforms the input pose sequence to a set of discrete tokens using three distinct vector quantized variational autoencoders, each one composed of a motion encoder (ℰℰ\\mathcal{E}caligraphic_E), a motion decoder (𝒟𝒟\\mathcal{D}caligraphic_D), and a codebook. These components work together to discretize input sign sequences from various body parts. Leveraging a pretrained language model (LM), the AMG can later generate multilingual signs from text prompts with a tailored part-wise decoding strategy.",
        "qtype": "Literature_Background",
        "response": "Here is the step-by-step reasoning and answer:\n\n1. Identify [mask1].  \n   In the red-boxed area of the diagram, [mask1] corresponds to the “Decoupled Tokenizer” (DETO).\n\n2. What DETO contains.  \n   The Decoupled Tokenizer actually consists of three parallel VQ-VAEs—one for the upper body, one for the left hand, and one for the right hand.  Each VQ-VAE is composed of:\n   •  An encoder (ℰ),  \n   •  A learnable codebook (a set of K embedding vectors),  \n   •  A decoder (𝒟).\n\n3. How it integrates Body Codebook quantization with VQ-VAE.  \n   •  After the body‐encoder ℰ_B projects each frame (or downsampled frame) into a continuous latent vector, DETO uses a quantizer to snap that vector to its nearest neighbor in the body’s learnable codebook C_B.  \n   •  That nearest‐neighbor look-up is exactly the vector-quantization step from VQ-VAE: it turns a continuous latent into a discrete code index (i.e. a “body token”).  \n   •  The body codebook is trained jointly with the VQ-VAE via three losses—reconstruction loss (making sure the decoder 𝒟_B can rebuild the motion), the embedding loss (to pull the chosen codebook vector toward the encoder output), and the commitment loss (to keep the encoder outputs close to their assigned codebook vectors).\n\n4. In sum.  \n   The Decoupled Tokenizer “integrates Body Codebook quantization with VQ-VAE concepts” by building a standard VQ-VAE pipeline for the body: encode the continuous motion into latents, quantize each latent frame to the nearest learned body‐codebook vector, emit that code’s index as a discrete token, and train end-to-end with the usual reconstruction + embedding + commitment losses."
    },
    {
        "question": "How does Part-wise Decoding adapt Multilingual LM Decoder processes for body-part-specific token output?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Since we decompose the motion sequence into three parts via the decoupled tokenizer, we accordingly devise a part-wise decoding strategy such that the LM can output token sequences for each body part.",
            "Moreover, we instantiate three LM decoders with shared weights, each one in charge of a single body part.",
            "To prompt the decoders with the information of body parts, decoding process starts from one of the above mentioned special tokens, based on the target language and body part.",
            "For example, the starting token would be set to <> when the model is required to generate upper-body motions for ASL.",
            "Then in each decoding step, the decoders will predict the probability distributions of the next token , conditioned on the encoder hidden states  and the previous predictions: ."
        ],
        "final_answer": "Part-wise Decoding first breaks the motion into three streams (upper body, left hand, right hand) and then runs three separate (but weight-shared) LM decoders—one per body part. Each decoder is ‘‘prompted’’ by a specific special token that encodes the target language and body part (e.g. <> for ASL upper body). Starting from that token and conditioned on the shared encoder’s hidden states, each decoder autoregressively generates the token sequence corresponding to its assigned body part.",
        "relevant_elements": [
            "Autoregressive Multilingual Generator",
            "Part-wise Decoding"
        ],
        "id": 387,
        "masked_question": "How does [mask1] adapt [mask2] processes for body-part-specific token output?",
        "masked_number": 2,
        "masked_elements": [
            "Part-wise Decoding",
            "Multilingual LM Decoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.17799v1_figure_2.png",
        "paperid": "2411.17799v1",
        "paper_path": "./papers/2411.17799v1.json",
        "figure_id": "2411.17799v1_figure_2.png",
        "caption": "Figure 2: An overview of our proposed method, Signs as Tokens (SOKE). Our approach consists of two key components: a decoupled tokenizer (DETO) and an autoregressive multilingual generator (AMG). DETO transforms the input pose sequence to a set of discrete tokens using three distinct vector quantized variational autoencoders, each one composed of a motion encoder (ℰℰ\\mathcal{E}caligraphic_E), a motion decoder (𝒟𝒟\\mathcal{D}caligraphic_D), and a codebook. These components work together to discretize input sign sequences from various body parts. Leveraging a pretrained language model (LM), the AMG can later generate multilingual signs from text prompts with a tailored part-wise decoding strategy.",
        "qtype": "Literature_Background",
        "response": "Here is how “Part-wise Decoding” adapts the standard multilingual LM decoding (blue-boxed) so that it emits three separate token streams—one for each body region:\n\n1.  Three parallel decoding runs (C = 3)  \n    – Rather than performing a single auto-regressive pass, the model invokes the same Transformer-decoder three times (shared weights), once for the upper body, once for the left hand, and once for the right hand.\n\n2.  Part-specific start tokens  \n    – Each run is kicked off with a special token that encodes both the target sign language and the body part (e.g. `<Lang_B>` for upper body, `<Lang_LH>` for left hand, `<Lang_RH>` for right hand).  \n    – By conditioning on these tokens, the decoder “knows” which region’s codebook it should be drawing from.\n\n3.  Auto-regressive generation per part  \n    – At each step, the decoder predicts the next token probability  \n      p(tᵢ | t₁,…,tᵢ₋₁; encoder_states)  \n      independently for each of the three streams.  \n    – During training the cross-entropy loss is summed over all three parts; at inference each part’s most-likely token is emitted greedily.\n\n4.  Final reconstruction  \n    – The three generated token sequences are sent back to their respective DETO decoders (`D_B`, `D_LH`, `D_RH`) to reassemble a full 3D motion.\n\nIn this way, “Part-wise Decoding” repurposes the same multilingual LM decoder into three body-part-aware decoders by simply prefixing each pass with a part-specific prompt token and running the auto-regressive loop separately."
    },
    {
        "question": "How does Vector Embedding module leverage BERT Encoder transformer principles for semantic representation?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "Embedding. We utilized transformer-based models (BERT, RoBERTa) [23] to produce embeddings for text data. These embeddings capture semantic information about the text and are high-dimensional vectors.",
            "Feature Extraction (Embedding). That utilized a deep learning model, denoted as Φ, to convert the preprocessed document titles into embeddings. Thus, each document dᵢ is transformed into an embedding eᵢ."
        ],
        "final_answer": "The Vector Embedding module applies a BERT-based transformer encoder to each piece of text, using the same self-attention and multilayer encoding principles of BERT to produce dense, high-dimensional vectors that capture the contextual and semantic relationships present in the input text.",
        "relevant_elements": [
            "Vector Embedding",
            "BERT Encoder"
        ],
        "id": 388,
        "masked_question": "How does [mask1] module leverage [mask2] transformer principles for semantic representation?",
        "masked_number": 2,
        "masked_elements": [
            "Vector Embedding",
            "BERT Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.17383v1_figure_1.png",
        "paperid": "2409.17383v1",
        "paper_path": "./papers/2409.17383v1.json",
        "figure_id": "2409.17383v1_figure_1.png",
        "caption": "Figure 1: We propose the VectorSearch Framework, utilizing a systematic grid search to fine-tune document retrieval systems by optimizing hyperparameters, index dimensions, and similarity thresholds for enhanced performance.",
        "qtype": "Literature_Background",
        "response": "Let me first restate what each mask refers to:\n\n• [mask1] is the “Vector Embedding” module (the red-boxed block in the figure).  \n• [mask2] is the “BERT” transformer (the blue-boxed block showing 12 encoders).\n\nNow, how the Vector Embedding module leverages BERT’s transformer principles can be seen as follows:\n\n1.  Input tokenization & embedding lookup  \n    – The raw text (titles, queries, etc.) is first tokenized and mapped into input embeddings (token + position + segment embeddings), just as in the original BERT framework.  \n\n2.  Deep, multi-layer self-attention encoding  \n    – Those input embeddings are then fed through BERT’s stack of 12 transformer encoder layers.  \n    – Each layer applies multi-head self-attention, which allows every token to attend to every other token in the sequence.  \n    – The stacked attention layers capture rich contextual dependencies – syntax, word sense disambiguation, long-range relations, etc.\n\n3.  Feature pooling into a fixed-length vector  \n    – After the final (12th) encoder, the per-token contextual embeddings are aggregated (typically via the [CLS] token or mean-pooling) to produce a single, fixed-size vector.  \n\n4.  Normalization & caching  \n    – That fixed-size output is L2-normalized to lie on the unit hypersphere (so cosine similarity becomes a simple dot product), and optionally cached locally to avoid re-computing the full forward pass on every retrieval.  \n\n5.  Storage in a vector database  \n    – The resulting high-dimensional semantically-rich “vector embeddings” are then indexed in ChromaDB (or a similar vector store) for fast approximate-nearest-neighbor look-ups.\n\nIn short, the Vector Embedding module simply wraps BERT’s core transformer machinery—its token embeddings, self-attention heads, feed-forward blocks, and layer-normalizations—then pools the final contextual representations into a fixed-length semantic vector that downstream ANN indexes (HNSWlib, FAISS) can search over."
    },
    {
        "question": "How does Hyperparameter Tuner incorporate grid search methodologies to enhance Model Training efficiency?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "We utilized ParameterGrid from the scikit-learn library to systematically explore the hyperparameter space Φ. By iterating over the parameter grid Φ, we identified optimal configurations ϕ* that maximized precision while minimizing query time.",
            "Hyperparameter Tuning (Φ). Defined a set of hyperparameters Φ, where each Φ_i represents a combination of hyperparameters."
        ],
        "final_answer": "The Hyperparameter Tuner constructs a comprehensive grid of hyperparameter settings (Φ) and leverages scikit-learn’s ParameterGrid to systematically iterate through every combination. For each candidate configuration, it trains and evaluates the model—measuring metrics such as precision and query time—and then selects the optimal hyperparameters (ϕ*) that deliver the best performance and lowest latency, thereby streamlining and automating the model training process.",
        "relevant_elements": [
            "Hyperparameter Tuner",
            "Model Training"
        ],
        "id": 389,
        "masked_question": "How does [mask1] incorporate grid search methodologies to enhance Model Training efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Hyperparameter Tuner"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.17383v1_figure_1.png",
        "paperid": "2409.17383v1",
        "paper_path": "./papers/2409.17383v1.json",
        "figure_id": "2409.17383v1_figure_1.png",
        "caption": "Figure 1: We propose the VectorSearch Framework, utilizing a systematic grid search to fine-tune document retrieval systems by optimizing hyperparameters, index dimensions, and similarity thresholds for enhanced performance.",
        "qtype": "Literature_Background",
        "response": "First, note that the little red‐boxed component in the right‐hand “Grid Search” panel is the Hyperparameter Tuner.  Its job is to drive the grid‐search loop during Model Training.  Concretely, we:\n\n1.  Define a discrete “parameter grid” (e.g. choice of encoder, index dimension, similarity threshold, number of neighbors, etc.) via scikit-learn’s ParameterGrid.  \n2.  The Hyperparameter Tuner then automatically iterates over every combination in that grid.  For each setting it:  \n    •  Trains the VectorSearch retrieval model using exactly those hyperparameters  \n    •  Evaluates it on held-out queries (measuring precision, query time, etc.)  \n3.  All of these train/evaluate runs are orchestrated (and even parallelized) by the tuner so that no combination is left untested and no extra work is done.  \n4.  Once every point in the grid has been scored, the tuner picks the configuration that maximizes precision (and/or minimizes latency) and passes that “best” set of hyperparameters downstream for a final, full‐scale training.  \n\nBy baking the grid search loop into the Hyperparameter Tuner, we both automate a thorough exploration of the hyperparameter space and avoid wasted training runs—thereby greatly improving overall Model Training efficiency."
    },
    {
        "question": "How does the cache module leverage ChromaDB to optimize embedding retrieval during encoding?",
        "relevant_section_ids": [
            "3",
            "4"
        ],
        "relevant_context": [
            "A vector database, ChromaDB, was utilized [24] to index and store the produced embeddings [25]. This allows for efficient storage and retrieval of high-dimensional vectors [17].",
            "We implemented a caching mechanism to store and reuse precomputed embeddings from the Chroma model, enhancing efficiency by eliminating redundant computations. This mechanism efficiently saved embeddings to disk, minimizing the need for recomputation and optimizing resource management."
        ],
        "final_answer": "The cache module uses ChromaDB as a persistent store for embeddings. During encoding, it first looks up whether an embedding already exists in ChromaDB’s index. If it does, the module retrieves that stored embedding directly—avoiding a fresh pass through the SentenceTransformer model. By saving and re-using these precomputed vectors, the system eliminates redundant encoding work, speeds up embedding retrieval, and reduces overall computation and I/O overhead.",
        "relevant_elements": [
            "Cache",
            "ChromaDB"
        ],
        "id": 390,
        "masked_question": "How does the [mask1] module leverage [mask2] to optimize embedding retrieval during encoding?",
        "masked_number": 2,
        "masked_elements": [
            "Cache",
            "ChromaDB"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.17383v1_figure_1.png",
        "paperid": "2409.17383v1",
        "paper_path": "./papers/2409.17383v1.json",
        "figure_id": "2409.17383v1_figure_1.png",
        "caption": "Figure 1: We propose the VectorSearch Framework, utilizing a systematic grid search to fine-tune document retrieval systems by optimizing hyperparameters, index dimensions, and similarity thresholds for enhanced performance.",
        "qtype": "Experimental_Results",
        "response": "Mask1: “Cache”  \nMask2: “ChromaDB”  \n\nAnswer:  \nThe Cache module hooks into ChromaDB’s vector store so that every time a piece of text is about to be encoded it first issues a lookup against ChromaDB. If an embedding for that text already lives in the cache (i.e. in ChromaDB), it is fetched immediately and returned, bypassing the expensive transformer call. If it’s not present, the text is passed through the encoder, the resulting normalized vector is written back into ChromaDB, and then returned. By front‐loading a quick vector‐store lookup before running the model, redundant computations are eliminated and end-to-end encoding latency is dramatically reduced."
    },
    {
        "question": "How does hyperparameter tuner integrate within grid search to optimize model training?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "We utilized ParameterGrid from the scikit-learn library to systematically explore the hyperparameter space {\\Theta}. By iterating over the parameter grid {\\Theta}, we identified optimal configurations {\\theta^*} that maximized precision while minimizing query time.",
            "The Parameter Grid is utilized to define a comprehensive parameter grid, encompassing various combinations of hyperparameters such as pretrained model selection (m), index dimensionality (d) and similarity threshold (τ). Model Training and Evaluation defined a function trainEvaluate, where θ represents the hyperparameters of the VectorSearch framework. This function trains and evaluates the model, returning performance metrics."
        ],
        "final_answer": "During grid search, the hyperparameter tuner uses scikit-learn’s ParameterGrid to enumerate all combinations of the configurable settings (e.g., model type, index dimension, similarity threshold). For each candidate combination θ, it calls the trainEvaluate routine to train the model and measure its performance (precision, query time). By iterating through the entire grid and comparing the returned metrics, the tuner selects the θ that best maximizes precision (and/or minimizes query time) and then retrains the final model with those best hyperparameters.",
        "relevant_elements": [
            "Grid Search",
            "Hyperparameter Tuner",
            "Model Training"
        ],
        "id": 391,
        "masked_question": "How does hyperparameter tuner integrate within [mask1] to optimize model training?",
        "masked_number": 1,
        "masked_elements": [
            "Grid Search"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.17383v1_figure_1.png",
        "paperid": "2409.17383v1",
        "paper_path": "./papers/2409.17383v1.json",
        "figure_id": "2409.17383v1_figure_1.png",
        "caption": "Figure 1: We propose the VectorSearch Framework, utilizing a systematic grid search to fine-tune document retrieval systems by optimizing hyperparameters, index dimensions, and similarity thresholds for enhanced performance.",
        "qtype": "Experimental_Results",
        "response": "First, by aligning the red‐boxed area in the figure with the text, we see that the highlighted component is the “Grid Search” stage of the VectorSearch pipeline.  In our write‐up this corresponds to the loop over the ParameterGrid (from scikit‐learn) that systematically enumerates every combination of hyperparameters  (e.g. choice of encoder model, index dimensionality, similarity threshold).\n\nNext, within that Grid Search box sits our Hyperparameter Tuner.  Concretely, the tuner:\n\n1. Takes the next candidate  from the ParameterGrid.  \n2. Instantiates the VectorSearch model with those settings.  \n3. Runs the “trainAndEvaluate” routine:  \n   – Builds/loads the FAISS/HNSW index with the specified dimensionality,  \n   – Encodes the subset of documents,  \n   – Performs nearest‐neighbor searches to compute performance (mean precision, query time),  \n4. Records the resulting metrics keyed by .  \n\nBy repeating steps 1–4 for every point in the grid, the tuner builds up a performance map over the hyperparameter space.  Once all combinations have been tried, it simply picks the  that maximizes precision (or whatever composite objective we defined) and feeds that back into the final “Train the Final Model With the Best Hyperparameters” box.  \n\nIn this way, the Hyperparameter Tuner is literally embedded inside the Grid Search block: it loops over configs, trains a model each time, evaluates it, and then surfaces the best—thereby optimizing the model training end-to-end."
    },
    {
        "question": "How does contrastive learning influence the projection head’s extraction of DOA-specific embeddings?",
        "relevant_section_ids": [
            "3",
            "3.3.2"
        ],
        "relevant_context": [
            "A projection head p is then used to jointly convert the latent features to the audio embeddings over the DOA grid.",
            "In DOA-wise contrastive learning, we maximize the local similarity between DOA-wise audio and visual embeddings when they correspond to the same DOA (positive sample), and minimize it otherwise (negative sample)."
        ],
        "final_answer": "Contrastive learning guides the projection head to produce DOA-specific embeddings by enforcing that audio embeddings at each DOA location are pulled closer to the matching visual embeddings (positive pairs) and pushed away from non-matching ones (negative pairs). This objective makes the projection head learn discriminative, direction-aware features for each point on the DOA grid.",
        "relevant_elements": [
            "contrastive learning",
            "projection head"
        ],
        "id": 392,
        "masked_question": "How does [mask1] influence the [mask2]’s extraction of DOA-specific embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "contrastive learning",
            "projection head"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.22803v1_figure_1.png",
        "paperid": "2410.22803v1",
        "paper_path": "./papers/2410.22803v1.json",
        "figure_id": "2410.22803v1_figure_1.png",
        "caption": "Figure 1: AV-SSL of an audio feature extractor for SELD.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify the two masked components:\n\n  * [mask1] (red box) is the “Contrastive learning” stage.  \n  * [mask2] (blue box) is the “Projection head.”  \n\nThe projection head gφ sits on top of the audio‐feature extractor fθ and turns the per‐frame latent features into a set of DOA‐wise audio embeddings a⌢t,i.  During pre-training, those audio embeddings are paired with visual embeddings v⌢t,i for the same DOA index i, and a contrastive (InfoNCE) loss is applied.\n\nChain of thought:\n\n 1.  The audio feature extractor fθ produces a per‐frame, per‐channel latent representation Ft of the multichannel spectrogram.  \n 2.  The projection head gφ then independently maps each Ft to a DOA‐wise embedding a⌢t,i (one embedding for each candidate direction i on the Fibonacci grid).  \n 3.  In the DOA-wise contrastive‐learning variant, for each frame t and direction i, the loss pulls the audio embedding a⌢t,i and its visual counterpart v⌢t,i closer together, while pushing apart all other (audio, visual) pairs at different directions.  \n 4.  This contrastive objective therefore forces the projection head to carve out an embedding space in which directions are linearly separable: embeddings corresponding to the same DOA collapse together, and embeddings from different DOAs move apart.  \n\nSo, in short, **the contrastive‐learning stage trains the projection head to emphasize and disentangle direction-of-arrival cues** by (a) pulling together audio‐visual pairs at the same DOA, (b) pushing apart pairs at different DOAs, and (c) thus enforcing that gφ’s outputs truly encode DOA‐specific information."
    },
    {
        "question": "How is the SELD head integrated with the pretrained audio feature extractor during fine-tuning?",
        "relevant_section_ids": [
            "3",
            "3.4"
        ],
        "relevant_context": [
            "Once f_a, f_v, and g are trained jointly in a self-supervised manner, g is connected to another head h for SELD and the entire network is fine-tuned in a supervised manner.",
            "Using annotated data, the audio feature extractor f_a is fine-tuned for SELD based on activity-coupled Cartesian DOA representation (ACCDOA) [shimada2022multi]. ... The projection head h consists of several fully-connected layers, followed by an adaptive average pooling to suit the target time resolution T."
        ],
        "final_answer": "After AV-SSL pretraining, the pretrained audio feature extractor f_a (together with its projection head g) is left intact and a new SELD head h (a stack of fully-connected layers plus adaptive average pooling) is attached on top. The combined network (f_a, g and h) is then fine-tuned end-to-end on labeled SELD data using the ACCDOA loss.",
        "relevant_elements": [
            "SELD head",
            "audio feature extractor"
        ],
        "id": 393,
        "masked_question": "How is the [mask1] integrated with the pretrained audio feature extractor during fine-tuning?",
        "masked_number": 1,
        "masked_elements": [
            "SELD head"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.22803v1_figure_1.png",
        "paperid": "2410.22803v1",
        "paper_path": "./papers/2410.22803v1.json",
        "figure_id": "2410.22803v1_figure_1.png",
        "caption": "Figure 1: AV-SSL of an audio feature extractor for SELD.",
        "qtype": "Experimental_Results",
        "response": "The little red-boxed module in the diagram is nothing mysterious – it is simply the “SELD head” that gets grafted onto the frozen (or partially frozen) audio backbone once your contrastive pre‐training has finished.  Concretely, at the end of self-supervised learning you throw away the projection head you used for contrastive learning, you plug in a new SELD head (a small stack of fully-connected layers plus an adaptive‐pooling stage that produces the multi-ACCDOA vector), and then you fine-tune the entire pipeline (the audio feature extractor ​f​ together with the new SELD head) under supervision.  During this supervised phase you minimize the permutation-invariant MSE loss on the ACCDOA targets, which adjusts both the extractor and the SELD head jointly."
    },
    {
        "question": "What limitations might the audio feature extractor face when projecting FOA data through the projection head?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Audio feature extractor",
            "Projection head"
        ],
        "id": 394,
        "masked_question": "What limitations might the [mask1] face when projecting FOA data through the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Audio feature extractor",
            "Projection head"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.22803v1_figure_1.png",
        "paperid": "2410.22803v1",
        "paper_path": "./papers/2410.22803v1.json",
        "figure_id": "2410.22803v1_figure_1.png",
        "caption": "Figure 1: AV-SSL of an audio feature extractor for SELD.",
        "qtype": "Others",
        "response": "Here’s a step-by-step unpacking of the masks and the limitation they expose:\n\n1.  Identifying the masks  \n    -  [mask1] (the red box) is the *audio feature extractor* – the front end that turns raw FOA spectrograms into latent audio features.  \n    -  [mask2] (the blue box) is the *projection head* – a small MLP that takes those latent features and spits out audio embeddings (one embedding per candidate direction).  \n\n2.  What the paper says about this pipeline  \n    -  In vanilla AVSA (and AVC), you usually collapse all direction information into a single “enhanced” channel before doing any contrastive learning, so your audio front end never really learns to encode *where* the sound came from – only *what* it was.  \n    -  The projection head itself is just a blind mapper: it can only re-package whatever directional cues survived in the front-end’s latent space.  \n\n3.  The key limitation  \n    If your audio feature extractor has never been trained to preserve or represent DOA, then no matter how clever your tiny projection head is, it will have nothing spatial to project.  \n    In practice this means:  \n    •  The learned embeddings will carry almost no directional (DOA) information – they’ll collapse to class-only features.  \n    •  You lose the fine‐grained, per‐direction localization cues that SELD downstream needs.  \n    •  As a result, fine‐tuning for full sound‐event localization and detection will suffer, because your “front end + projection head” simply never encoded accurate DOA signals."
    },
    {
        "question": "What ethical implications could arise from contrastive learning using omni-directional visual data from VR recordings?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Contrastive learning",
            "Omni-directional visual data"
        ],
        "id": 395,
        "masked_question": "What ethical implications could arise from [mask1] using omni-directional visual data from VR recordings?",
        "masked_number": 1,
        "masked_elements": [
            "Contrastive learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.22803v1_figure_1.png",
        "paperid": "2410.22803v1",
        "paper_path": "./papers/2410.22803v1.json",
        "figure_id": "2410.22803v1_figure_1.png",
        "caption": "Figure 1: AV-SSL of an audio feature extractor for SELD.",
        "qtype": "Others",
        "response": "Here’s how one can think through the question step by step:\n\n1.  Identify “[mask1]” in the question.  In the diagram, the red‐boxed module is the contrastive, audio–visual self-supervised learning (AV-SSL) stage that aligns FOA audio with omni-directional (360°) visual crops.  \n2.  Observe that to do AV-SSL the system must ingest full 360° VR video frames—i.e. every direction around the camera, often including people’s faces, private interiors, bystanders, etc.  \n3.  Think about what happens when you collect and use that data at scale:\n    •  You may record identifiable individuals or private spaces without their explicit, informed consent.  \n    •  Wide-angle VR cameras can incidentally capture sensitive information (documents, computer screens, family members, behaviors) that the uploader never meant to share.  \n    •  Storing and processing these recordings raises privacy risks: data breach, unauthorized sharing, or repurposing (e.g. for surveillance or behavioral profiling).  \n    •  The dataset may under- or over-represent certain communities or environments, embedding demographic or cultural biases in downstream models.  \n4.  Therefore, the chief ethical issues are around privacy, consent, data security, and bias.  Any AV-SSL pipeline that “eats” 360° video must have very careful governance—strong anonymization, clear opt-in/opt-out policies, rigorous access controls, and bias audits—to avoid turning VR research into indiscriminate surveillance."
    },
    {
        "question": "How might LSH-based Hamming distance similarity fail on heterogeneous model architectures?",
        "relevant_section_ids": [
            "3.2",
            "6"
        ],
        "relevant_context": [
            "After each iteration, client i generates an LSH code h_i from its local model parameters θ_i ... The similarity between two clients i and j is quantified by the Hamming distance between their LSH codes: d_{i,j} = HAM(h_i, h_j). A smaller d_{i,j} indicates greater similarity between the models θ_i and θ_j.",
            "Limitations: Despite its advantages, WPFed has limitations. The application of LSH does not fully support heterogeneous models, restricting the framework’s applicability to clients with varying model architectures or feature spaces."
        ],
        "final_answer": "Because LSH codes are produced by hashing model parameters under the assumption that all models share the same parameter space, clients with different architectures (and thus different parameter dimensions or feature representations) will generate hash codes that are not directly comparable. As a result, the Hamming distance between their LSH codes can become meaningless, causing the similarity measure to fail on heterogeneous model architectures.",
        "relevant_elements": [
            "LSH code",
            "Hamming Distance"
        ],
        "id": 396,
        "masked_question": "How might [mask1]-based [mask2] similarity fail on heterogeneous model architectures?",
        "masked_number": 2,
        "masked_elements": [
            "LSH code",
            "Hamming Distance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.11378v1_figure_1.png",
        "paperid": "2410.11378v1",
        "paper_path": "./papers/2410.11378v1.json",
        "figure_id": "2410.11378v1_figure_1.png",
        "caption": "Figure 1. Overview of the WPFed framework. ∗In practice, clients publish commitments C𝐶Citalic_C in the announcement instead of the original neighbor ranking R𝑅Ritalic_R; however, R𝑅Ritalic_R is depicted in the figure to simplify the illustration of data flow. ∗∗While only valid neighbors participate in knowledge distillation, all are involved in the neighbor ranking process.",
        "qtype": "Others",
        "response": "Here is a sketch of why a simple Hamming‐distance–on–LSH-codes similarity breaks down as soon as your peers start using different network “shapes” or feature‐spaces:\n\n1.   What the paper does  \n     – Each client takes its own weight vector θ and runs it through an LSH-hashing routine h(·) to get a fixed-length bit vector h(θ)∈{0,1}^m.  \n     – To decide how “close” two models θ_i and θ_j are, it simply computes  \n         D_H(i,j) = HammingDistance( h(θ_i), h(θ_j) )  \n\n2.   Why that only works for *homogeneous* architectures  \n     – LSH is designed to preserve locality in a *single* metric space, e.g. the Euclid­ean (or cosine) distance on fixed-size real vectors.  \n     – If two clients have different layer counts, layer sizes, or even a different order of parameters, there is no common vector space to hash. The bits that come out of h(·) on θ_CNN simply don’t line up in any meaningful way with the bits out of h(·) on θ_MLP.  \n     – As a result:\n        • Two very similar tasks implemented in different network families can yield completely uncorrelated bit‐strings (so you get large Hamming distances even though functionally the models are “the same”).  \n        • Conversely, two wildly different networks might by chance collide on many hash‐bits, spuriously driving the Hamming distance toward zero.  \n\n3.   The root of the failure  \n     – **Dimension mismatch**: different architectures → different parameter counts → different “shape” of input to the same LSH routine (or you have to pad/flatten them, which is arbitrary).  \n     – **Different distributions**: even if you force them into the same length by zero-padding, the statistics of layer‐1 vs. layer-5 parameters differ so dramatically that the hash no longer preserves the original similarity.  \n     – **Loss of semantic alignment**: the same bit‐position in h(θ) no longer corresponds to “the same” abstract feature or subspace when the networks are different.\n\nIn short, a Hamming-distance–on–LSH-codes is only a valid proxy for actual model‐parameter closeness when *all* clients share the same architecture (and hence the same parameter space). As soon as you mix in heterogeneous models, the bit‐strings become incompatible, and the Hamming distance ceases to reflect any useful notion of similarity."
    },
    {
        "question": "What ethical concerns arise from sharing Xref reference data during P2P communication?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Xref",
            "P2P communication"
        ],
        "id": 397,
        "masked_question": "What ethical concerns arise from sharing [mask1] reference data during P2P communication?",
        "masked_number": 1,
        "masked_elements": [
            "Xref"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.11378v1_figure_1.png",
        "paperid": "2410.11378v1",
        "paper_path": "./papers/2410.11378v1.json",
        "figure_id": "2410.11378v1_figure_1.png",
        "caption": "Figure 1. Overview of the WPFed framework. ∗In practice, clients publish commitments C𝐶Citalic_C in the announcement instead of the original neighbor ranking R𝑅Ritalic_R; however, R𝑅Ritalic_R is depicted in the figure to simplify the illustration of data flow. ∗∗While only valid neighbors participate in knowledge distillation, all are involved in the neighbor ranking process.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n1. In Fig. 1 the red-boxed item is labeled “X_ref,” which in the text is described as each client’s private “reference dataset” that it sends to peers during the P2P distillation step (Sec. 3.1).  \n2. Although this dataset is supposed to be “non-sensitive,” it nonetheless consists of real data drawn from the client’s domain (e.g. patient ECG/EEG signals or user images).  \n3. Transmitting any raw or lightly processed reference data to another party can expose private attributes, enable inadvertent re-identification, or be used for purposes beyond the original training (e.g. profiling, reverse engineering).  \n4. Even if features rather than raw data are shared, adversarial peers may be able to reconstruct identifiable information or deduce sensitive behavioral/health attributes.  \n5. Thus, sharing X_ref without stringent safeguards raises classic privacy and consent issues, along with worries about data ownership, misuse, and compliance with regulations (GDPR, HIPAA, etc.).\n\nAnswer:\nBy sharing the personal “reference dataset” (X_ref) in plain form during peer-to-peer distillation, clients risk exposing sensitive or personally identifiable information. This raises ethical issues around loss of privacy, potential re-identification attacks, lack of informed consent for data use, improper secondary use or profiling by malicious peers, and violation of data-ownership or regulatory guarantees."
    },
    {
        "question": "What motivates combining Hamming Distance and Ranking Score to determine integrated weights?",
        "relevant_section_ids": [
            "1",
            "3.4"
        ],
        "relevant_context": [
            "Firstly, we enable personalized neighbor selection by introducing a mechanism that allows each client to identify beneficial neighbors on a global scale, considering both inter-client similarity and model quality. … By combining similarity metrics and performance ranking scores, clients can globally define and select an optimal personalized neighbor set, thereby enhancing the effectiveness of decentralized collaborative learning.",
            "Clients use both the similarity metrics and ranking scores to select a personalized set of neighbors that maximize the potential benefit of collaboration. For each potential neighbor j, client i computes a weight w_{i,j} combining the ranking score R_j and the similarity distance d_{i,j}: … This formulation ensures that clients who are both highly ranked and have similar models are given higher weights."
        ],
        "final_answer": "They combine Hamming Distance (to capture inter-client model similarity) and Ranking Score (to capture peer performance) so that the resulting weights favor peers that are both similar in data distribution and high-performing—thereby maximizing the benefit of decentralized collaboration.",
        "relevant_elements": [
            "Hamming Distance",
            "Ranking Score"
        ],
        "id": 398,
        "masked_question": "What motivates combining [mask1] and [mask2] to determine integrated weights?",
        "masked_number": 2,
        "masked_elements": [
            "Hamming Distance",
            "Ranking Score"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.11378v1_figure_1.png",
        "paperid": "2410.11378v1",
        "paper_path": "./papers/2410.11378v1.json",
        "figure_id": "2410.11378v1_figure_1.png",
        "caption": "Figure 1. Overview of the WPFed framework. ∗In practice, clients publish commitments C𝐶Citalic_C in the announcement instead of the original neighbor ranking R𝑅Ritalic_R; however, R𝑅Ritalic_R is depicted in the figure to simplify the illustration of data flow. ∗∗While only valid neighbors participate in knowledge distillation, all are involved in the neighbor ranking process.",
        "qtype": "Design_Rationale",
        "response": "The two boxes respectively capture the two criteria we care about when picking collaborators:\n\n  • Mask₁ (the red‐boxed “Hamming Distance”) measures how similar another client’s model is to yours  \n  • Mask₂ (the blue‐boxed “Ranking Score”) measures how well that client’s model actually performs  \n  \nBy combining them into a single weight we ensure that you only pick peers who are both “close” to you in model‐space (so their updates will help, not conflict) and have demonstrated high quality on everyone’s reference data.  In other words, the integrated weight balances “friendliness” (similarity) and “expertise” (ranking), giving higher weight only to neighbors that score well on both fronts."
    },
    {
        "question": "What is the rationale for filtering invalid LSH codes via KL Divergence check?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "Publishing LSH codes enables clients to assess model similarities but introduces a security risk: malicious clients may forge LSH codes to appear more similar, thereby gaining undue trust or access.",
            "When client i interacts with neighbor j, it receives the outputs Ŷ^j. Client i compares these with its own outputs Ŷ^i to compute a similarity metric, such as the Kullback–Leibler (KL) divergence.",
            "We implement a filter mechanism to deter LSH deception: if the similarity between Ŷ^i and Ŷ^j ranks in the lower half of all neighbors, then j is excluded from the knowledge distillation process."
        ],
        "final_answer": "The KL Divergence check is used to verify that a peer’s claimed LSH‐based similarity actually corresponds to similar model behavior on a reference dataset. By computing the KL divergence between their output distributions and filtering out those with high divergence (i.e. low similarity), the framework prevents malicious clients from forging LSH codes to appear similar and gaining undue trust.",
        "relevant_elements": [
            "KL Divergence",
            "Filter Invalid LSH"
        ],
        "id": 399,
        "masked_question": "What is the rationale for [mask1] via [mask2] check?",
        "masked_number": 2,
        "masked_elements": [
            "Filter Invalid LSH",
            "KL Divergence"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.11378v1_figure_1.png",
        "paperid": "2410.11378v1",
        "paper_path": "./papers/2410.11378v1.json",
        "figure_id": "2410.11378v1_figure_1.png",
        "caption": "Figure 1. Overview of the WPFed framework. ∗In practice, clients publish commitments C𝐶Citalic_C in the announcement instead of the original neighbor ranking R𝑅Ritalic_R; however, R𝑅Ritalic_R is depicted in the figure to simplify the illustration of data flow. ∗∗While only valid neighbors participate in knowledge distillation, all are involved in the neighbor ranking process.",
        "qtype": "Design_Rationale",
        "response": "The little red box around “Filter Invalid LSH” is there to remind you that, even though everybody publishes an LSH fingerprint to prove “I’m similar to you,” we don’t just blindly trust that fingerprint.  Instead we perform the blue‐boxed “KL-Divergence” check on the model outputs over the shared reference set.  If a peer’s reported LSH similarity isn’t backed up by an actually low KL-divergence in their soft‐predictions, we throw that LSH code (and hence that neighbor) out of the distillation pool.  In short, we use a KL-divergence test on the outputs to guard against clients who might forge LSH codes and to filter out any “similar” model whose behavior on real data doesn’t actually match."
    },
    {
        "question": "What is the benefit of generating paired safe phrases from unsafe concepts using the LLM for steering training?",
        "relevant_section_ids": [
            "3.1.2"
        ],
        "relevant_context": [
            "In our subsequent steering transformation training procedure, we synthesize additional safe terms to steer unsafe embeddings toward safe ones.",
            "The core idea is to associate each unsafe term u with a corresponding safe term s of similar meanings, allowing us to convert unsafe concepts into safe alternatives while preserving the original semantic intent of the prompt."
        ],
        "final_answer": "By using the LLM to generate paired safe phrases for each unsafe concept, SteerDiff obtains supervised pairs of semantically aligned unsafe and safe embeddings. This lets the steering model learn a linear transformation that shifts unsafe embeddings into a safe region while preserving the original semantic intent of the prompt.",
        "relevant_elements": [
            "LLM",
            "Paired Safe Phrases"
        ],
        "id": 400,
        "masked_question": "What is the benefit of generating [mask1] from unsafe concepts using the LLM for steering training?",
        "masked_number": 1,
        "masked_elements": [
            "Paired Safe Phrases"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.02710v1_figure_1.png",
        "paperid": "2410.02710v1",
        "paper_path": "./papers/2410.02710v1.json",
        "figure_id": "2410.02710v1_figure_1.png",
        "caption": "Figure 1: Overview of data collection and training process for SteerDiff: We begin by defining a set of unsafe concepts, Cunsafesubscript𝐶unsafeC_{\\text{unsafe}}italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT (yellow block). Next, we use a LLM to generate related unsafe (red block on the left) and safe (green block on the left) phrases based on each concept c∈Cunsafe𝑐subscript𝐶unsafec\\in C_{\\text{unsafe}}italic_c ∈ italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT. These phrases are then encoded using a pre-trained text encoder to extract embedding features, which are used to train both the identifier and steering model.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. In our pipeline (Fig. 1), we start with a small “blacklist” of unsafe concepts Cunsafe (the yellow box).  \n2. We prompt an LLM to spin out a broad set of naturally occurring unsafe phrases τunsafe (red box) centered on each concept c∈Cunsafe.  \n3. Critically, we then use the same LLM to produce for each unsafe phrase u∈τunsafe a corresponding safe phrase s that preserves the original intent but removes the harmful element (this is the green “Paired Safe Phrases” box, i.e. [mask1]).  \n4. By encoding both u and its paired safe s into embeddings, we obtain thousands of (unsafe-embedding, safe-embedding) training pairs.  \n5. Those pairs feed directly into the steering model’s loss (Eq. 4) so it can learn a linear transform that “pushes” any unsafe embedding toward its safe counterpart.  \n6. Without this synthetic paired data, we’d have no scalable, semantically aligned examples to teach the model exactly how to shift an unsafe prompt into a safe one.  \n\nAnswer  \nBy having the LLM produce semantically matched safe counterparts for each unsafe phrase, we automatically get large numbers of (unsafe, safe) embedding pairs. Those pairs are exactly what the steering network needs to learn the linear transformation that “moves” unsafe embeddings into safe regions—all without costly manual annotation and while preserving the original prompt’s intent."
    },
    {
        "question": "How does utilizing separate unsafe embeddings influence the identifier's ability to distinguish inappropriate content?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Since embeddings with similar semantics have closer distances in the embedding space (Mikolov et al., 2013; Radford et al., 2021), we expect the unsafe embeddings to be aggregated.",
            "As demonstrated in 2(b), we observe that SteerDiff successfully learns to distinguish between safe and unsafe phrases, with the two categories being well-separated after applying t-SNE dimensional reduction."
        ],
        "final_answer": "By embedding unsafe phrases separately, the model learns to cluster those unsafe embeddings together and push them away from safe embeddings. This results in well-separated clusters of safe versus unsafe concepts in embedding space, enabling the identifier to more accurately distinguish and classify inappropriate content.",
        "relevant_elements": [
            "Unsafe Embeddings",
            "Identifier"
        ],
        "id": 401,
        "masked_question": "How does utilizing separate [mask1] influence the [mask2]'s ability to distinguish inappropriate content?",
        "masked_number": 2,
        "masked_elements": [
            "Unsafe Embeddings",
            "Identifier"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.02710v1_figure_1.png",
        "paperid": "2410.02710v1",
        "paper_path": "./papers/2410.02710v1.json",
        "figure_id": "2410.02710v1_figure_1.png",
        "caption": "Figure 1: Overview of data collection and training process for SteerDiff: We begin by defining a set of unsafe concepts, Cunsafesubscript𝐶unsafeC_{\\text{unsafe}}italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT (yellow block). Next, we use a LLM to generate related unsafe (red block on the left) and safe (green block on the left) phrases based on each concept c∈Cunsafe𝑐subscript𝐶unsafec\\in C_{\\text{unsafe}}italic_c ∈ italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT. These phrases are then encoded using a pre-trained text encoder to extract embedding features, which are used to train both the identifier and steering model.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key idea in a nutshell:\n\n1.  In our pipeline we don’t just lump every phrase into one giant embedding pool.  Instead we explicitly collect and encode two disjoint sets of phrases—safe phrases and unsafe phrases—so that each ends up with its own “signature” region in embedding space.  The unsafe phrases are the ones boxed in red in Figure 1 (“unsafe embeddings”), and the identifier is the blue-boxed module.\n\n2.  By training the identifier (an MLP) on these two *separate* embedding distributions, the model learns a very clear decision boundary.  In practice you can even visualize in a t-SNE plot how the red “unsafe” cluster and the blue “safe” cluster pull apart.\n\n3.  That separation makes the identifier much more confident and accurate when it sees a new phrase embedding—it can quickly tell “this is coming from the red (unsafe) region” vs. “this is coming from the blue (safe) region.”\n\nSo: **utilizing separate unsafe embeddings gives the identifier a distinct negative class to learn against**, which in turn sharpens its ability to distinguish inappropriate content at inference time."
    },
    {
        "question": "How does the steer model define its loss to align transformed unsafe embeddings with paired safe embeddings?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To learn the transformation matrix W, we employ a supervised learning method using a paired dataset of unsafe phrases and their corresponding safe phrases, as described in subsection 3.1.",
            "The training process minimizes the following loss function:\n\\[L = \\sum_{(e_u,e_s)} \\|W e_u - e_s\\|^2,\\]\nwhere e_s represents the embedding of the safe phrases in S."
        ],
        "final_answer": "The steer model uses a supervised mean‐squared‐error loss that minimizes the squared L₂ distance between each linearly transformed unsafe embedding (W e_u) and its corresponding safe embedding (e_s).",
        "relevant_elements": [
            "Steer Model",
            "Unsafe Embeddings",
            "Paired Safe Embeddings"
        ],
        "id": 403,
        "masked_question": "How does the [mask1] define its loss to align transformed unsafe embeddings with paired safe embeddings?",
        "masked_number": 1,
        "masked_elements": [
            "Steer Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.02710v1_figure_1.png",
        "paperid": "2410.02710v1",
        "paper_path": "./papers/2410.02710v1.json",
        "figure_id": "2410.02710v1_figure_1.png",
        "caption": "Figure 1: Overview of data collection and training process for SteerDiff: We begin by defining a set of unsafe concepts, Cunsafesubscript𝐶unsafeC_{\\text{unsafe}}italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT (yellow block). Next, we use a LLM to generate related unsafe (red block on the left) and safe (green block on the left) phrases based on each concept c∈Cunsafe𝑐subscript𝐶unsafec\\in C_{\\text{unsafe}}italic_c ∈ italic_C start_POSTSUBSCRIPT unsafe end_POSTSUBSCRIPT. These phrases are then encoded using a pre-trained text encoder to extract embedding features, which are used to train both the identifier and steering model.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is nothing more exotic than a simple L₂ (mean‐squared) regression head: at training time you take each unsafe embedding eᵤ, apply your learnt linear “steering” transform T (in the paper T(eᵤ)=eᵤ+αW eᵤ), and then penalize the squared distance to its safe twin eₛ.  In formula:  \n\n   L_steer = ∑₍₍ᵤ,ₛ₎₎ ‖T(eᵤ) − eₛ‖²₂  \n\nso that the transformed unsafe embeddings are pulled as close as possible to the paired safe embeddings."
    },
    {
        "question": "How does DSRL transform Euclidean Space visual features before projection into Hyperbolic Space?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Mapping from Euclidean to hyperbolic spaces. Let x be input Euclidean node features, and o denote the origin on the manifold M of the Lorentz model. There is x ∈ T_o M, where <.,.>_L denotes the Lorentz inner product defined in Eq. 2. We can reasonably regard x as a node on the tangent space at the origin o. HE-GCN uses the exponential map defined in Eq. 4 to generate hyperbolic node representations on the Lorentz model:"
        ],
        "final_answer": "DSRL first treats each Euclidean visual‐feature vector as a point in the tangent space at the origin of the Lorentz hyperboloid model, then applies the Lorentz‐model exponential map at that origin to project these tangent‐space feature vectors onto the hyperbolic manifold.",
        "relevant_elements": [
            "Euclidean Space",
            "Hyperbolic Space",
            "DSRL"
        ],
        "id": 404,
        "masked_question": "How does [mask1] transform Euclidean Space visual features before projection into Hyperbolic Space?",
        "masked_number": 1,
        "masked_elements": [
            "DSRL"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.19252v1_figure_1.png",
        "paperid": "2409.19252v1",
        "paper_path": "./papers/2409.19252v1.json",
        "figure_id": "2409.19252v1_figure_1.png",
        "caption": "Figure 1: (a) Hierarchical diagram in Video Violence Detection (VVD). (b) Our DSRL enhances the detection of ambiguous violence by combining Euclidean and Hyperbolic spaces to balance visual feature expression and hierarchical event relations.",
        "qtype": "Implementation_Details",
        "response": "Let’s walk through what that red‐boxed “transformer” is doing to the Euclidean features before any hyperbolic projection ever takes place.\n\n1.  In the picture the red box sits around the Dual‐Space Interaction Module (DSI).  Its job is not to push things onto the hyperbolic manifold right away, but rather to enrich your 2D (Euclidean) visual descriptors by “talking” to the hyperbolic side first.  \n2.  Concretely it takes your Euclidean feature tensor f_e and feeds it through two small linear‐projection heads to produce a “key” graph and a “value” graph, and at the same time it takes the hyperbolic features f_h and makes a “query” graph.  \n3.  It then computes attention scores between every Euclidean key–hyperbolic query pair using the Lorentz (hyperbolic) similarity; this gives an attention map α that highlights which Euclidean nodes should listen hardest to which hyperbolic nodes.  \n4.  Those attention weights are used to pull a weighted combination of the Euclidean “values” back onto the Euclidean side, yielding a message X_{h→e}.  \n5.  Finally DSI fuses that message back into the original Euclidean features via a residual addition  \n       f_e′ = f_e  +  λ · X_{h→e}  \n   (plus an optional max‐pool or concatenation step), producing a hybrid feature that has already absorbed hyperbolic context.  \n6.  Only after this cross‐space enrichment does the pipeline go on to the actual exponential map (or “projection”) into the Lorentz hyperbolic manifold in the HE-GCN block.  \n\nIn short, before you ever “project” anything into hyperbolic space, the DSI block has already transformed your raw Euclidean visual features by running a Lorentz–metric cross‐space attention and adding that message back into the Euclidean representation."
    },
    {
        "question": "How does DSRL exploit Hyperbolic Space's exponential metric to model hierarchical event relations?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "– Section 1: “Fortunately, hyperbolic representation learning, characterized by exponentially increasing the metric distances and naturally reflects the hierarchical structure of data, has gained attention and shown promising performance in computer vision tasks…”",
            "– Section 4.1: “HE-GCN uses the exponential map defined in Eq. 4 to generate hyperbolic node representations on the Lorentz model.”"
        ],
        "final_answer": "DSRL projects each video-segment feature from the Euclidean tangent space onto the Lorentz hyperbolic manifold via the exponential map. In hyperbolic space, distances grow exponentially with depth, so nodes that are farther apart along the hierarchy become more widely separated. DSRL’s Hyperbolic Energy-constrained GCN then uses hyperbolic Dirichlet energy and layer-sensitive association degrees to dynamically select and aggregate neighbors at each layer, thereby leveraging the exponential metric to naturally encode and propagate hierarchical event relations.",
        "relevant_elements": [
            "Hyperbolic Space",
            "DSRL"
        ],
        "id": 405,
        "masked_question": "How does [mask1] exploit [mask2]'s exponential metric to model hierarchical event relations?",
        "masked_number": 2,
        "masked_elements": [
            "DSRL",
            "Hyperbolic Space"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.19252v1_figure_1.png",
        "paperid": "2409.19252v1",
        "paper_path": "./papers/2409.19252v1.json",
        "figure_id": "2409.19252v1_figure_1.png",
        "caption": "Figure 1: (a) Hierarchical diagram in Video Violence Detection (VVD). (b) Our DSRL enhances the detection of ambiguous violence by combining Euclidean and Hyperbolic spaces to balance visual feature expression and hierarchical event relations.",
        "qtype": "Implementation_Details",
        "response": "Here is the high‐level answer, with a brief chain of thought showing how the red‐boxed module (DSRL) leans on the blue‐boxed geometry (hyperbolic manifold) to carve out a hierarchy of events:\n\nChain of thought  \n1. In Fig.1(b), the red‐boxed block is our Dual‐Space Representation Learning (DSRL) framework, and the blue‐boxed surface is the Lorentz‐model hyperbolic space.  \n2. Hyperbolic space is famous for its “exponential metric”—distances grow exponentially as you move out from the origin—so it naturally embeds tree‐like or hierarchical data with very little distortion.  \n3. Within DSRL, we first map Euclidean video snippet features onto the hyperbolic manifold via the exponential map.  \n4. We then use the hyperbolic geodesic distance (the “exponential metric”) to compute a Dirichlet‐energy score for every pair of nodes and derive layer‐sensitive thresholds (our LSHADs).  \n5. These thresholds control which edges survive at each GCN layer, so that shallow (high‐level) nodes stay clustered near the origin, while deeper (fine‐grained) nodes spread out exponentially farther.  \n6. In this way, the blue‐boxed hyperbolic metric creates geometrically faithful separations between levels of the event taxonomy, and the red‐boxed DSRL module simply “reads off” that hierarchy via layer‐wise graph convolutions.  \n\nAnswer  \nDSRL exploits the hyperbolic manifold’s exponential distance growth by first mapping Euclidean snippet features into the Lorentz hyperbolic model (via the exponential map) and then using its geodesic distances to (a) compute hyperbolic Dirichlet energy and (b) derive layer‐sensitive cutoffs for message‐passing.  Because hyperbolic distances expand exponentially with depth, nodes at different levels of the event hierarchy naturally fall into concentric shells.  By thresholding affinities layer by layer, DSRL’s GCN branch faithfully recovers the multi‐level taxonomy of violent versus normal events."
    },
    {
        "question": "How does DSRL reconcile Euclidean visual features with hyperbolic hierarchical relations?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "However, fusing representation in different spaces remains a challenge; to break the information cocoon, DSI utilises cross-space attention to facilitate information interactions between Euclidean and hyperbolic space to capture better discriminative features, where Euclidean representations have effectiveness on the significant motion and shape changes in the video, while hyperbolic representations accelerate the comprehension of hierarchical relations between events, working together to improve the performance of violence detection in videos.",
            "Although hyperbolic representation learning enhances understanding of event hierarchies, visual representations remain crucial in violence detection. Fusing representations from different spaces is challenging; thus, DSI employs cross-space attention to facilitate interactions between Euclidean and hyperbolic spaces.",
            "Cross-Space Attention Mechanism utilizes the Lorentzian metric to calculate attention scores between nodes from different spaces, accurately measuring semantic similarity and better preserving their true relationships by computing the nonlinear distance between them. We denote the features in Euclidean space as E and the features in hyperbolic space as H. M models the between-graph interaction and guides the transfer of inter-graph message from H to E.",
            "First, we use a linear layer to transform E into a key graph K and value graph V, and H into a query graph Q. We then compute an attention map M via Lorentzian similarity and threshold it to retain only strong cross-space relations. Finally, the enhanced Euclidean features E′ are obtained by fusing the hyperbolic messages V weighted by M with the original Euclidean features E via a gated sum: E′=γ⋅(M·V)+(1−γ)⋅E, where γ controls the contribution of hyperbolic information."
        ],
        "final_answer": "DSRL reconciles visual and hierarchical information by means of its Dual-Space Interaction (DSI) module. DSI uses a cross-space attention mechanism—grounded in Lorentzian (hyperbolic) similarity—to compute attention scores between Euclidean video features and hyperbolic embeddings. It then transfers hyperbolic messages into the Euclidean feature stream and fuses them via a gated sum, so that motion/shape cues from Euclidean space are enhanced by the hierarchical relations captured in hyperbolic space.",
        "relevant_elements": [
            "Euclidean Space",
            "Hyperbolic Space",
            "DSRL"
        ],
        "id": 406,
        "masked_question": "How does [mask1] reconcile Euclidean visual features with hyperbolic hierarchical relations?",
        "masked_number": 1,
        "masked_elements": [
            "DSRL"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.19252v1_figure_1.png",
        "paperid": "2409.19252v1",
        "paper_path": "./papers/2409.19252v1.json",
        "figure_id": "2409.19252v1_figure_1.png",
        "caption": "Figure 1: (a) Hierarchical diagram in Video Violence Detection (VVD). (b) Our DSRL enhances the detection of ambiguous violence by combining Euclidean and Hyperbolic spaces to balance visual feature expression and hierarchical event relations.",
        "qtype": "Literature_Background",
        "response": "Step 1: Identify [mask1] in the figure.  \nLooking at Fig. 1(b), the red‐boxed label sitting between the Euclidean‐feature stream (top) and the hyperbolic‐relation stream (bottom) reads “DSRL.”  From the text we know DSRL stands for “Dual-Space Representation Learning.”\n\nStep 2: What role does DSRL play?  \nThe paper explicitly says that DSRL is the module that “integrates Euclidean and hyperbolic geometries” so as to “significantly improve discrimination of ambiguous violence.”  It consists of two parts:  \n  • HE-GCN, which embeds snippet features into hyperbolic space and learns hierarchy via Dirichlet energy and layer-sensitive association degrees.  \n  • DSI (Dual-Space Interaction), which fuses the Euclidean and hyperbolic embeddings via cross-space attention.\n\nStep 3: How does it actually reconcile the two spaces?  \n  – First, the raw visual features (in Euclidean space) and the hyperbolic embeddings of the same snippets are both maintained.  \n  – Then the DSI module computes attention scores between Euclidean “keys/values” and hyperbolic “queries” using a Lorentz–metric similarity.  \n  – Sparse thresholding (to drop weak cross-space links) and a pooling/concatenation step produce a fused representation that contains:  \n     • the strong motion/shape cues from the Euclidean side  \n     • the structured, hierarchical context from the hyperbolic side.  \n  – This fused feature is then passed on for the final violence classification.\n\nIn this way, DSRL “reconciles” or “bridges” Euclidean visual features with hyperbolic hierarchical relations by explicitly learning cross-space attention between them, so that appearance and event-hierarchy information reinforce one another in the final descriptor."
    },
    {
        "question": "How does hyperbolic metric amplify discrimination across event category hierarchies?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "Fortunately, hyperbolic representation learning, characterized by exponentially increasing the metric distances and naturally reflects the hierarchical structure of data, has gained attention and shown promising performance in computer vision tasks, like semantic segmentation Atigh et al. (2022), medical image recognition Yu et al. (2022b), action recognition Peng et al. (2020); Long et al. (2020), anomaly recognition Hong et al. (2023).",
            "Hyperbolic representation learning, characterized by exponentially increasing the metric distances and naturally reflects the hierarchical structure of data, has gained attention and shown promising performance in computer vision tasks, like semantic segmentation Atigh et al. (2022), visual representation learning Ge et al. (2023), medical image recognition Yu et al. (2022b), action recognition Peng et al. (2020); Long et al. (2020), anomaly recognition Hong et al. (2023)."
        ],
        "final_answer": "By embedding event features into a hyperbolic space, distances grow exponentially with depth in the hierarchy. This means that events belonging to different levels or branches of the category hierarchy become much farther apart than they would in a Euclidean space, thereby amplifying their separation and making it easier to distinguish between closely related event categories.",
        "relevant_elements": [
            "Hyperbolic Metric",
            "event category hierarchies"
        ],
        "id": 407,
        "masked_question": "How does [mask1] amplify discrimination across event category hierarchies?",
        "masked_number": 1,
        "masked_elements": [
            "Hyperbolic Metric"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.19252v1_figure_1.png",
        "paperid": "2409.19252v1",
        "paper_path": "./papers/2409.19252v1.json",
        "figure_id": "2409.19252v1_figure_1.png",
        "caption": "Figure 1: (a) Hierarchical diagram in Video Violence Detection (VVD). (b) Our DSRL enhances the detection of ambiguous violence by combining Euclidean and Hyperbolic spaces to balance visual feature expression and hierarchical event relations.",
        "qtype": "Literature_Background",
        "response": "The thing in the little red box is simply the “Hyperbolic Metric.”  What it brings to the table is that, as soon as you embed your snippets into hyperbolic space rather than flat (Euclidean) space, distances grow exponentially as you move “up” the tree.  In practice that means that two snippets which might sit only a hair’s‐breadth apart in Euclidean space (e.g. a normal collision vs. an actual fight) get pulled much farther apart under the hyperbolic distance.  Because hyperbolic geometry mirrors a tree or hierarchy (parent–child pairs stay relatively close, but different branches diverge very quickly), it automatically ⟶ amplifies the separation between categories at all levels of your event taxonomy, making the classes far more discriminable."
    },
    {
        "question": "How does triplet loss relate to traditional metric learning for embedding separation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Triplet loss"
        ],
        "id": 408,
        "masked_question": "How does [mask1] relate to traditional metric learning for embedding separation?",
        "masked_number": 1,
        "masked_elements": [
            "Triplet loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.15246v1_figure_1.png",
        "paperid": "2408.15246v1",
        "paper_path": "./papers/2408.15246v1.json",
        "figure_id": "2408.15246v1_figure_1.png",
        "caption": "Figure 1: Overview of STG3Net. (A) Data preprocessing involves integrating multiple SRT data, enhancing spot features, and constructing spatial adjacency graphs. (B) STG3Net is employed for latent representation learning. It consists of a backbone module composed of feature graph autoencoder, combined with adversarial learning and G2N for batch correction between multiple slices. (C) The learned latent representations from STG3Net will be utilized in downstream task analysis, including clustering and UMAP visualization. Additionally, the reconstructed gene expression is considered as the denoised outcome.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed term in the figure is nothing exotic – it is simply the classic “triplet loss” from metric learning.  In other words, STG3Net borrows the standard metric‐learning recipe of forming (anchor, positive, negative) triplets and driving the network to make the anchor closer to its positive than to its negative by at least a fixed margin.  This is exactly how traditional embedding‐separation losses work: pull same‐class (or same–spatial‐domain) points together and push different–class (or different–slice) points apart."
    },
    {
        "question": "How does slice discriminator adapt GAN principles to enforce batch-invariant latent representations?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "To improve the clustering performance across multiple slices, we have designed a discriminator consisting of three stacked fully connected layers. It takes the latent representation outputted by the encoder as input and produces the probability p_{i,c} of -spot belonging to -slice.",
            "The L_d is minimized when there is a need to discover distinguishing features between slices from the latent representation, allowing the discriminator to accurately predict the slice labels. Conversely, L_d is maximized when there is a need to deceive the discriminator to mitigate batch effects across multiple slices, ensuring that spots from different slices would have the highest similarity."
        ],
        "final_answer": "The slice discriminator is trained to distinguish which slice each spot’s latent embedding comes from (minimizing its classification loss), while the encoder is adversarially trained to maximize that same loss—i.e., to fool the discriminator. This GAN‐style min–max game forces the encoder to produce representations that the discriminator cannot use to predict slice identity, thereby enforcing batch‐invariant latent embeddings.",
        "relevant_elements": [
            "Slice discriminator"
        ],
        "id": 409,
        "masked_question": "How does [mask1] adapt GAN principles to enforce batch-invariant latent representations?",
        "masked_number": 1,
        "masked_elements": [
            "Slice discriminator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.15246v1_figure_1.png",
        "paperid": "2408.15246v1",
        "paper_path": "./papers/2408.15246v1.json",
        "figure_id": "2408.15246v1_figure_1.png",
        "caption": "Figure 1: Overview of STG3Net. (A) Data preprocessing involves integrating multiple SRT data, enhancing spot features, and constructing spatial adjacency graphs. (B) STG3Net is employed for latent representation learning. It consists of a backbone module composed of feature graph autoencoder, combined with adversarial learning and G2N for batch correction between multiple slices. (C) The learned latent representations from STG3Net will be utilized in downstream task analysis, including clustering and UMAP visualization. Additionally, the reconstructed gene expression is considered as the denoised outcome.",
        "qtype": "Literature_Background",
        "response": "Here is how the red‐boxed module (“slice discriminator”) borrows the generator–discriminator game from a GAN in order to strip out slice (i.e. batch)–specific signals from the embedding:\n\n1.  In STG3Net the encoder plays the role of the GAN “generator” by mapping each spot’s masked gene data (and its neighborhood graph features) into a latent code z.  \n2.  The slice discriminator is a small MLP (three fully-connected layers) that takes z as input and tries to predict its original slice (batch) label.  Its loss  is just the usual cross‐entropy for slice classification.  \n3.  During training we do a two‐player, min–max optimization:\n    –  **Discriminator step:** update the slice discriminator to \\[minₙ Lₙ(z; true_slice) \\], so it becomes better at picking up any residual slice–specific cues in z.  \n    –  **Encoder (generator) step:** update the encoder to \\[maxₑ Lₙ(z; true_slice) \\], i.e. to fool the slice discriminator and make z uninformative about which slice it came from.  \n4.  At equilibrium this adversarial tug‐of‐war forces the encoder to “erase” batch‐specific variation from the latent codes, producing a slice‐invariant representation that can then be used for downstream clustering or visualization.\n\nIn other words, the red-boxed slice discriminator is the GAN discriminator, and the encoder plays the generator—by alternately training one to detect slice labels and the other to hide them, STG3Net removes batch effects from its latent embedding."
    },
    {
        "question": "How does random masking enhance latent feature learning in the feature graph autoencoder?",
        "relevant_section_ids": [
            "2.2",
            "3.4"
        ],
        "relevant_context": [
            "Section 2.2: “To overcome the ‘identity transformation’ issue, we use the augmented representation X^m as the input for training the model. Specifically, from the set of spot vertices, a masked vertex set V^m is randomly sampled with a masking rate μ. For the i-th spot (v_i), if v_i ∈ V^m, x_i^m = M where M represents the mask token vector; otherwise, x_i^m = x_i.”",
            "Section 3.4: “Both STG3Net and SEDR involved randomly masking a portion of spot expressions, forcing the model to learn meaningful representations from neighbors. This ensured that the model obtained good consistency because the masking technique allowed the model to focus more on the current spot’s relationship with its neighbors.”"
        ],
        "final_answer": "By randomly masking a subset of spot‐level expression features and replacing them with a learned mask token, the autoencoder cannot simply copy its inputs. Instead, it must reconstruct the missing values by propagating information over the spatial neighbor graph. This “masked reconstruction” forces the encoder to emphasize spatial context and inter‐spot relationships when forming its latent representation, thereby producing more meaningful and robust features.",
        "relevant_elements": [
            "random masking",
            "feature graph autoencoder"
        ],
        "id": 411,
        "masked_question": "How does [mask1] enhance latent feature learning in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "random masking",
            "feature graph autoencoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.15246v1_figure_1.png",
        "paperid": "2408.15246v1",
        "paper_path": "./papers/2408.15246v1.json",
        "figure_id": "2408.15246v1_figure_1.png",
        "caption": "Figure 1: Overview of STG3Net. (A) Data preprocessing involves integrating multiple SRT data, enhancing spot features, and constructing spatial adjacency graphs. (B) STG3Net is employed for latent representation learning. It consists of a backbone module composed of feature graph autoencoder, combined with adversarial learning and G2N for batch correction between multiple slices. (C) The learned latent representations from STG3Net will be utilized in downstream task analysis, including clustering and UMAP visualization. Additionally, the reconstructed gene expression is considered as the denoised outcome.",
        "qtype": "Experimental_Results",
        "response": "In STG3Net the little red‐boxed “mask” operation is nothing exotic – it simply replaces a random subset of spot expression vectors with a learned mask token before feeding the data into the autoencoder under the big blue box.  By hiding (masking) some of the raw expression values at input, the feature‐graph autoencoder can no longer cheat by just copying its input to its output.  Instead, during training it must lean on the graph convolutional layers and the latent bottleneck to “fill in” those missing values from the spatial and transcriptional context of neighboring spots.  That reconstruction task forces the encoder to learn a compact latent code that genuinely captures spatial gene–gene relationships, rather than an identity transformation."
    },
    {
        "question": "How does augmenting entity embeddings with pre-trained contextual information enhance adapter's relation-specific adaptation?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "At the data level, we augment the embedding of each entity (head or tail) by additional pre-trained contextual information from their related entities, as shown in Fig. 2 (b). The contextual information enables more tailored adaptation to each distinct novel relation.",
            "In summary, the input is the original entity embedding e and the mean contextual embedding aggregated from its neighbors N_e, and the output is the augmented entity embedding e^c. In this way, the augmented embedding e^c preserves the embedding trained via emb, while leveraging pre-trained graph contextual information."
        ],
        "final_answer": "By combining each entity’s learned embedding with a mean of its neighbors’ pre-trained contextual embeddings, the adapter receives context-aware inputs that reflect the local graph structure around the entity. This enriched, relation-specific signal yields a context-aware relation meta that more precisely captures the semantics of each novel relation, enabling the adapter to adapt the meta-learned prior in a way that is tailored to the particular context of that relation.",
        "relevant_elements": [
            "Pre-trained Contextual Info",
            "Adapter"
        ],
        "id": 413,
        "masked_question": "How does augmenting entity embeddings with [mask1] enhance [mask2]'s relation-specific adaptation?",
        "masked_number": 2,
        "masked_elements": [
            "Pre-trained Contextual Info",
            "Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.09123v2_figure_2.png",
        "paperid": "2410.09123v2",
        "paper_path": "./papers/2410.09123v2.json",
        "figure_id": "2410.09123v2_figure_2.png",
        "caption": "Figure 2: Illustration of key concepts in RelAdapter, hinging on an entity-aware adapter (a, b) in the meta-testing stage (c). Note that we omit the meta-training stage, which is similar to meta-testing but with backpropagation of the query loss to update the model parameters (𝚎𝚖𝚋𝚎𝚖𝚋\\mathtt{emb}typewriter_emb and ΦΦ\\Phiroman_Φ).",
        "qtype": "Experimental_Results",
        "response": "Let’s first pin down what the two blanks are:\n\n– [mask1] is the thing in the red‐boxed callout in Fig 2(c) – that reads “Pre‐trained Contextual Info.”  In the text this is the mean of the neighbor embeddings f(eₖ) that we aggregate to form a graph‐based context for each entity.  \n– [mask2] is the blue‐boxed module – the Adapter – whose whole job is to take a meta‐relation prior and make it specific to the target relation.\n\nSo the question becomes:\n\n  “How does augmenting entity embeddings with pre‐trained contextual information enhance the Adapter’s relation‐specific adaptation?”\n\nStep by step answer:\n\n1.  In a vanilla meta‐learning scheme the Adapter only ever sees:\n       – a generic relation prior (the “meta-relation”)  \n       – the bare entity embeddings from emb  \n\n2.  By sprinkling in “pre-trained contextual info” (the red box) – i.e. graph‐neighborhood embeddings that have already encoded how each entity actually connects to its various neighbors in the KG – we give the Adapter a much richer signal about the real world semantics of those entities.\n\n3.  Concretely, when we rewrite the relation meta as\n       Cℛᵣ  =  g( μ { eᶜ, meanₖ[f(eₖ)] } )\n   the Adapter now receives a context‐aware relation embedding that already “knows” which kinds of neighbors show up around that relation.\n\n4.  Armed with this extra context, the Adapter can learn a far sharper, relation‐specific offset: rather than simply nudging a generic prior in some blind direction, it adjusts the prior in light of the actual local graph patterns that define the new relation.\n\n5.  In practice that means fewer gradient steps on the support set, faster convergence on truly novel relations, and better final ranking on the query set.\n\nIn short, by augmenting each entity with pre‐trained graph‐context, the Adapter module can perform its lightweight feed‐forward + residual tweak in a way that is already tuned to the kinds of neighbor‐patterns the novel relation exhibits, which greatly improves its relation‐specific adaptation."
    },
    {
        "question": "What are potential limitations of the adapter module when handling highly divergent relations during meta-testing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Adapter",
            "Meta-Testing"
        ],
        "id": 414,
        "masked_question": "What are potential limitations of the [mask1] module when handling highly divergent relations during meta-testing?",
        "masked_number": 1,
        "masked_elements": [
            "Adapter"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.09123v2_figure_2.png",
        "paperid": "2410.09123v2",
        "paper_path": "./papers/2410.09123v2.json",
        "figure_id": "2410.09123v2_figure_2.png",
        "caption": "Figure 2: Illustration of key concepts in RelAdapter, hinging on an entity-aware adapter (a, b) in the meta-testing stage (c). Note that we omit the meta-training stage, which is similar to meta-testing but with backpropagation of the query loss to update the model parameters (𝚎𝚖𝚋𝚎𝚖𝚋\\mathtt{emb}typewriter_emb and ΦΦ\\Phiroman_Φ).",
        "qtype": "Others",
        "response": "First, to be clear, the little green diamond in the red‐boxed area is the light‐weight “adapter” sub-module that we insert on top of the meta-learner’s relation representation at meta-test time.  It takes in the context-aware relation embedding and applies a small bottleneck FFN+residual transform (parameterized by Θᵣ) before we do our one or two gradient steps on the few support examples.\n\nWhen the held-out relation is very far from anything seen in meta-train, this tiny adapter can run into several bottlenecks:\n\n  1. Limited capacity of the bottleneck FFN.  \n     • We deliberately choose a low-dimensional hidden layer m ≪ d to keep things “parameter-efficient.”  If the new relation really departs from all of our meta-training priors, that small FFN may simply have too little representational bandwidth to warp the global prior all the way over to the new relation’s “mode.”  \n\n  2. Random initialization + very few support gradients.  \n     • At test time we randomly initialize Θᵣ and then only see K support triples (often K=1…5).  If the divergence from meta-train to meta-test is large, one or two gradient steps on a handful of examples may not reliably steer a randomly initialized adapter to a good solution—especially if the loss landscape is highly non-convex in Θᵣ.  \n\n  3. Dependence on hyper-parameters (adapter ratio α, hidden size m).  \n     • In practice we saw that performance can swing wildly if α is too small (adapter does almost nothing) or too large (residual is shut off and the FFN by itself can over‐transform or over‐fit).  When relations are out-of-distribution, that sensitivity becomes even more pronounced.  \n\n  4. Freezing all other weights.  \n     • We leave the global prior Φ and the entity embedding matrix emb frozen at meta-test time and only update Θᵣ.  If the new relation requires not just a small “tweak” but wholesale re-calibration of the prior or the entity codecs, the adapter alone may be powerless to compensate.  \n\n  5. Noisy or irrelevant context.  \n     • The adapter sits on top of a context-augmented entity embedding.  If the higher-order neighbor contexts we pull in happen to be uninformative or misleading for a highly novel relation, the adapter is forced to learn in the presence of noise and may latch onto spurious patterns.  \n\nIn short, although plugging in a small, tunable adapter is a very parameter-efficient way to get per-relation flexibility, it can struggle when the target relation is *very* far from anything seen during meta-training, because (a) the bottleneck FFN is of limited expressive power, (b) it starts from random weights and only sees a handful of examples, (c) it depends quite sensitively on the choice of α and hidden size m, and (d) all the heavy lifting in the model (the global prior and base embeddings) remain frozen."
    },
    {
        "question": "What risks stem from relying on pre-trained contextual information within entity context for novel relations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Entity Context"
        ],
        "id": 415,
        "masked_question": "What risks stem from relying on pre-trained contextual information within [mask1] for novel relations?",
        "masked_number": 1,
        "masked_elements": [
            "Entity Context"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.09123v2_figure_2.png",
        "paperid": "2410.09123v2",
        "paper_path": "./papers/2410.09123v2.json",
        "figure_id": "2410.09123v2_figure_2.png",
        "caption": "Figure 2: Illustration of key concepts in RelAdapter, hinging on an entity-aware adapter (a, b) in the meta-testing stage (c). Note that we omit the meta-training stage, which is similar to meta-testing but with backpropagation of the query loss to update the model parameters (𝚎𝚖𝚋𝚎𝚖𝚋\\mathtt{emb}typewriter_emb and ΦΦ\\Phiroman_Φ).",
        "qtype": "Others",
        "response": "Here is a step-by-step walk-through of how we arrive at the answer:\n\n1.  **What is “[mask1]”?**  \n    By inspecting the red–boxed region (b) in Figure 2 and reading the text around Eq. (5)–(6), we see that [mask1] refers to the **“Entity Context”** module.  In this module each entity’s embedding is augmented by a mean of pre-trained neighbor embeddings  \n    \\[  \n      eᶜ = (1–λ)·e + λ·mean_{k∈N(e)} f(e_k)\\,,  \n    \\]  \n    where f(·) is a frozen, pre-trained encoder and N(e) are the entity’s neighbors in the KG.\n\n2.  **What is its role at test time?**  \n    During meta-testing on a novel relation, we have only a handful of support triplets.  To give the adapter more “signal,” we pull in these neighbor embeddings from the pre-trained encoder, hoping they carry useful side-information about each head/tail.\n\n3.  **Why might this be problematic for novel relations?**  \n    –  **Distribution shift / negative transfer.**  The neighbor embeddings f(e_k) were learned on the *seen* part of the graph and encode patterns relevant to the base relations.  A novel relation may exhibit entirely different connectivity patterns, so the pre-trained context can push the embedding in the wrong direction.  \n    –  **Noisy or spurious neighbors.**  Real-world KGs are riddled with errors, omissions, and long-tail entities.  An unsupervised neighbor mean can easily pick up irrelevant or wrong facts (e.g. “Platypus” or “Fish” when we care about “Bird⟶BodyPart”), polluting the adaptation.  \n    –  **Bias amplification.**  If certain node types or relations were over-represented in the pre-training graph, those biases carry over into every novel-relation prediction, potentially drowning out the true few-shot signal.  \n    –  **Missing crucial new contexts.**  Because the encoder is frozen, it never learns any context specific to the *new* relation during meta-testing.  Important neighbors that only appear via the novel relation simply aren’t in f(·), leaving holes in the adaptation.\n\n4.  **Conclusion**  \n    In short, by leaning on a frozen, pre-trained neighborhood encoder for data augmentation at test time, we risk **injecting noise, amplifying spurious or biased patterns, and performing negative transfer** whenever the novel relation’s local graph structure diverges from what the encoder saw during pre-training."
    },
    {
        "question": "What motivates staging Entity-based Extraction prior to Feature-based Extraction in conventional inference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Entity-based Extraction",
            "Feature-based Extraction"
        ],
        "id": 418,
        "masked_question": "What motivates staging [mask1] prior to Feature-based Extraction in conventional inference?",
        "masked_number": 1,
        "masked_elements": [
            "Entity-based Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.17558v1_figure_1.png",
        "paperid": "2411.17558v1",
        "paper_path": "./papers/2411.17558v1.json",
        "figure_id": "2411.17558v1_figure_1.png",
        "caption": "Figure 1. The data used in VQA tasks, with the conclusion of the understanding and inference methods from conventional models to multimodal large language models.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-Thought  \n1. In the top-left of Fig. 1 under “Conventional Deep Learning → Inference,” we see two sequential stages:  \n   - Entity-based Extraction (highlighted in red)  \n   - Feature-based Extraction  \n2. Entity-based Extraction refers to pulling out discrete knowledge units (e.g. via SPARQL queries, KB triplets or dense passage retrieval) that explicitly identify the key entities and their relations.  \n3. Feature-based Extraction then takes visual/textual signals and learns implicit or higher-order representations (e.g. via memory networks, graph convolutions or Transformer layers).  \n4. If we tried to go straight to feature-based methods, the model would lack any structured grounding in real-world facts or named entities—it would only see raw pixels/words.  \n5. By first extracting entities and their explicit relationships, the model:  \n   • Grounds its subsequent feature representations in concrete, interpretable symbols.  \n   • Narrows down the semantic search space (e.g. “Barack Obama” rather than every possible face).  \n   • Supplies prior knowledge that feature-based modules can enrich with multi-hop or graph reasoning.  \n6. Hence, staging Entity-based Extraction first ensures that the feature-based stage has both the relevant objects/entities and their factual context, which improves accuracy, interpretability, and reasoning depth.  \n\nAnswer  \nWhat motivates staging Entity-based Extraction prior to Feature-based Extraction in conventional inference is the need to first pull out explicit, structured knowledge (entities and their relations) so that the subsequent feature-learning stage can ground its representations in real-world facts, reduce search complexity, and support deeper, multi-hop reasoning."
    },
    {
        "question": "What underlies integrating Chain-of-thought within the LLM Control Center for inference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Chain-of-thought",
            "LLM Control Center"
        ],
        "id": 419,
        "masked_question": "What underlies integrating [mask1] within the [mask2] for inference?",
        "masked_number": 2,
        "masked_elements": [
            "Chain-of-thought",
            "LLM Control Center"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.17558v1_figure_1.png",
        "paperid": "2411.17558v1",
        "paper_path": "./papers/2411.17558v1.json",
        "figure_id": "2411.17558v1_figure_1.png",
        "caption": "Figure 1. The data used in VQA tasks, with the conclusion of the understanding and inference methods from conventional models to multimodal large language models.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought (the red box) is being folded directly into the LLM Control Center (the blue box) so that the LLM’s own step-by-step reasoning trace becomes the core “planner” that drives the multi-agent inference loop.  In other words, the LLM Control Center doesn’t just fire off a final answer; it uses the chain-of-thought as its internal control logic—writing out intermediate “think,” “consider,” “check” steps—and then routes those reasoning steps through its memory and agent–action modules to carry out the final multimodal inference."
    },
    {
        "question": "What drives integrating Vision-to-Text Block and Semantic Image Search Block for complementary annotation guidance?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Vision-to-Text Block",
            "Semantic Image Search Block"
        ],
        "id": 420,
        "masked_question": "What drives integrating [mask1] and Semantic Image Search Block for complementary annotation guidance?",
        "masked_number": 1,
        "masked_elements": [
            "Vision-to-Text Block"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.00252v1_figure_1.png",
        "paperid": "2407.00252v1",
        "paper_path": "./papers/2407.00252v1.json",
        "figure_id": "2407.00252v1_figure_1.png",
        "caption": "Figure 1: An overview of an AI-assisted image annotation system. The system begins with unlabeled image training data which is processed through various blocks. The Vision-to-Text Block utilizes image captioning, VQA, and multi-modal alignment to provide predictions. The Pretrained Vision Task Block handles image segmentation, pose estimation, and one-shot classification to generate vision task predictions. The Semantic Image Search Block uses self-supervised learning and active learning to assist the annotator in semantic search. Human annotators receive textual and visual suggestions to annotate the images, which are then used to fine-tune the vision task and semantic search models. The final interface allows annotators to accept, edit, or show similar annotations.",
        "qtype": "Design_Rationale",
        "response": "The missing label is “Vision-to-Text Block.”  In the paper’s Figure 1 this is the red–outlined module containing the image‐captioning LLM, the Visual QA LLM and the OneLM multimodal aligner.  \n\nWhat drives us to hook the Vision-to-Text Block together with the Semantic Image Search Block is simply the fact that they offer two very different—and highly complementary—forms of guidance to the human annotator.  \n\n1.  The Vision-to-Text Block translates raw pixels into rich, free-form language hints (captions, answers to “what’s here?” questions, even multimodal reasoning steps).  This lets non-expert annotators know *why* the model thinks certain objects or relationships are present, points out subtle cues to watch out for, and flags potential blind spots.  \n\n2.  The Semantic Image Search Block, in parallel, lets annotators pull up visually *similar* examples from the unlabeled pool, leveraging self‐ and active‐learning embeddings.  Those nearest‐neighbor exemplars give a concrete sense of “if it looks like this, it’s probably that object,” supplying an immediate visual reference.  \n\nBy fusing text-based explanations *and* example-based retrieval you dramatically cut down both annotation time (annotators don’t have to second-guess what the model sees) and error rates (they get both the word-level intuition and the visual exemplar together).  In short, it’s the drive for faster, cheaper, more accurate annotations that motivates the integration of these two blocks."
    },
    {
        "question": "What motivates using both Image Segmentation and One Shot Classification in the Pretrained Vision Task Block?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Image Segmentation",
            "One Shot Classification"
        ],
        "id": 421,
        "masked_question": "What motivates using both [mask1] and [mask2] in the Pretrained Vision Task Block?",
        "masked_number": 2,
        "masked_elements": [
            "Image Segmentation",
            "One Shot Classification"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.00252v1_figure_1.png",
        "paperid": "2407.00252v1",
        "paper_path": "./papers/2407.00252v1.json",
        "figure_id": "2407.00252v1_figure_1.png",
        "caption": "Figure 1: An overview of an AI-assisted image annotation system. The system begins with unlabeled image training data which is processed through various blocks. The Vision-to-Text Block utilizes image captioning, VQA, and multi-modal alignment to provide predictions. The Pretrained Vision Task Block handles image segmentation, pose estimation, and one-shot classification to generate vision task predictions. The Semantic Image Search Block uses self-supervised learning and active learning to assist the annotator in semantic search. Human annotators receive textual and visual suggestions to annotate the images, which are then used to fine-tune the vision task and semantic search models. The final interface allows annotators to accept, edit, or show similar annotations.",
        "qtype": "Design_Rationale",
        "response": "The red box ([mask1]) is in fact a pre‐trained instance‐segmentation model (e.g. YOLO/Detectron2) and the blue box ([mask2]) is a one–shot classifier.  We include both because they play very different but complementary roles in jump‐starting the annotator:\n\n 1.  The segmentation model gives you precise, pixel-level proposals (object masks, shapes and boundaries), so that annotators can very quickly see “where” things are in the image.  \n 2.  The one–shot classifier provides “what” those objects are, and—crucially—can generalize to entirely new classes from just a handful of examples.\n\nBy combining them you get both fine‐grained localization and instant category prediction on unseen objects, which greatly reduces the amount of up-front labeling and lets you adapt to domain-specific or low-data scenarios right out of the box."
    },
    {
        "question": "How do IJEPA and PaLI jointly generate and update embedding vectors for semantic image retrieval?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Image-based Self Supervised Learning (IJEPA)",
            "Image-based Active Learning (PaLI)"
        ],
        "id": 422,
        "masked_question": "How do [mask1] and PaLI jointly generate and update embedding vectors for semantic image retrieval?",
        "masked_number": 1,
        "masked_elements": [
            "Image-based Self Supervised Learning (IJEPA)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.00252v1_figure_1.png",
        "paperid": "2407.00252v1",
        "paper_path": "./papers/2407.00252v1.json",
        "figure_id": "2407.00252v1_figure_1.png",
        "caption": "Figure 1: An overview of an AI-assisted image annotation system. The system begins with unlabeled image training data which is processed through various blocks. The Vision-to-Text Block utilizes image captioning, VQA, and multi-modal alignment to provide predictions. The Pretrained Vision Task Block handles image segmentation, pose estimation, and one-shot classification to generate vision task predictions. The Semantic Image Search Block uses self-supervised learning and active learning to assist the annotator in semantic search. Human annotators receive textual and visual suggestions to annotate the images, which are then used to fine-tune the vision task and semantic search models. The final interface allows annotators to accept, edit, or show similar annotations.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-Thought\n\n1.  In Figure 1 the red-boxed module in the Semantic Image Search Block is labeled “Image-based Self-Supervised Learning (IJEPA).”  The green box beside it is the PaLI module (“Image-based Active Learning (PaLI)”).\n\n2.  According to the text (Sect. III-B), IJEPA is used to learn a purely visual embedding of each image by solving a self-supervised proxy task (e.g. predicting masked patch representations or contrasting different augmented views).  The output of IJEPA is a fixed-length feature vector for each image in the unlabeled pool.\n\n3.  PaLI then sits on top of those self-supervised embeddings and drives an active-learning loop.  At each iteration PaLI:\n    •  Computes uncertainty or informativeness scores in the current IJEPA embedding space  \n    •  Selects the top-K most informative/unusual images and presents them to the annotator  \n    •  Collects the human’s semantic feedback (e.g. “this region is a car,” “this image is similar to X”)  \n    •  Uses those new labels or relevance judgments to fine-tune the embedding network.\n\n4.  By iterating the two steps—(a) re-encoding all images with the updated (fine-tuned) IJEPA network and (b) re-running PaLI’s selection and human-in-the-loop update—the joint system continuously refines its vector space so that semantically similar images move closer together and dissimilar ones move apart.\n\nAnswer\n\nMask₁ is the Image-based Self-Supervised Learning (IJEPA) block.  IJEPA first learns a vision encoder by predicting masked or augmented views so that every image is mapped to a “semantic” feature vector.  PaLI then uses those vectors to choose the most informative (uncertain or novel) images for human annotation, takes the annotator’s feedback, and fine-tunes the same encoder.  Each time the encoder is updated, all image vectors are recomputed and PaLI selects the next batch, so that the embedding space is progressively reshaped for effective semantic image retrieval."
    },
    {
        "question": "How does One Shot Classification leverage YOLO pose predictions to improve novel class detection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pose Estimation",
            "One Shot Classification"
        ],
        "id": 423,
        "masked_question": "How does [mask1] leverage YOLO pose predictions to improve novel class detection?",
        "masked_number": 1,
        "masked_elements": [
            "One Shot Classification"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.00252v1_figure_1.png",
        "paperid": "2407.00252v1",
        "paper_path": "./papers/2407.00252v1.json",
        "figure_id": "2407.00252v1_figure_1.png",
        "caption": "Figure 1: An overview of an AI-assisted image annotation system. The system begins with unlabeled image training data which is processed through various blocks. The Vision-to-Text Block utilizes image captioning, VQA, and multi-modal alignment to provide predictions. The Pretrained Vision Task Block handles image segmentation, pose estimation, and one-shot classification to generate vision task predictions. The Semantic Image Search Block uses self-supervised learning and active learning to assist the annotator in semantic search. Human annotators receive textual and visual suggestions to annotate the images, which are then used to fine-tune the vision task and semantic search models. The final interface allows annotators to accept, edit, or show similar annotations.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ in the red box is the “one‐shot classification” head.  What it does is very simple at a high level:\n\n  1.  During pre‐training it learns to turn a single support image into a class prototype (i.e. an embedding).  \n  2.  At inference time it also takes the keypoints or joints that the YOLO‐based pose head has just predicted on a query image.  \n  3.  By “anchoring” on those keypoints – cropping or warping the object into a canonical pose and/or concatenating the keypoint coordinates onto the feature map – the one‐shot classifier produces pose‐conditioned embeddings for the query.  \n  4.  Comparing that pose‐conditioned query embedding against the single‐example prototype makes it far more robust to differences in viewpoint or articulation.  \n  5.  The result is that even truly unseen (“novel”) classes can be detected and assigned with high accuracy from just one support image, because the model has used YOLO’s pose output to align and normalize what it’s looking at.\n\nIn short:  by feeding the predicted keypoints from YOLO into the one‐shot matching pipeline, the system can normalize away all of the “nuisance” variation in pose, and thus build a prototype–matching detector that works reliably on novel classes from a single labeled example."
    },
    {
        "question": "How does split audio into 5s facilitate ASR fine-tuning segmentation alignment?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To manage long speech signals, each denoised signal Sᵢ was split into smaller, 5-second segments. … This ensures that all input segments are of equal length, aligning with the fixed-length input requirement of transformers.",
            "After splitting the dialect speech signal Sᵢ into 5-second segments, the corresponding dialect text Tᵈᵢ and standard text Tˢᵢ were also split to align with the speech segments. … This alignment allows each 5-second chunk of the speech signal Sᵢ to be fine-tuned with the corresponding chunk of dialect text during the first-stage fine-tuning, and with the standard text during the second stage."
        ],
        "final_answer": "By chopping long audio into uniform 5-second segments, the pipeline both meets the fixed-length input requirement of transformer ASR models and ensures that each audio chunk is directly paired with its matching text chunk. This synchronized segmentation enables precise, segment-level fine-tuning of the ASR system.",
        "relevant_elements": [
            "Split audio into 5s",
            "ASR"
        ],
        "id": 424,
        "masked_question": "How does [mask1] facilitate ASR fine-tuning segmentation alignment?",
        "masked_number": 1,
        "masked_elements": [
            "Split audio into 5s"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10879v1_figure_1.png",
        "paperid": "2411.10879v1",
        "paper_path": "./papers/2411.10879v1.json",
        "figure_id": "2411.10879v1_figure_1.png",
        "caption": "Figure 1: (a) Typical Deep Neural Network (DNN) based implementation of ASR that uses word-based annotation combining HMM is used by [1, 2, 3], (b) Recent advancement of LLMs let researchers to get involved in investigation towards the LLMs capability of handling speech signal. The mLLM-based approach has been applied in [4, 5, 6] using feature extraction and alphabet-wise mapping. Existing methods often fall short of processing big speech signal data, specifically with dialect speech signals, due to limitations of data availability and resources. On the other hand, end-to-end frameworks are less explored as per the literature. (c) We introduce a novel approach involving fine-tuning ASR and mLLMs with a large-scale low-resource Bangla dialect speech signal dataset. There are two parts, one is the dialect transcript from the dialect speech signal, which will be performed by the multilingual ASR model and then LLM will translate and standardize the ASR models’ predicted dialect text into standard Bangla text. Our approach includes reliable preprocessing techniques to handle large-scale speech signals.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the pipeline (Fig 2 c) we first denoise and then chop every dialect utterance s_i into fixed 5 s blocks.  \n2. For ASR fine‐tuning to work, each of those 5 s audio chunks must be paired with exactly the right slice of dialect text.  \n3. The red‐boxed module—“Annotate Speech Signal to Dialect Transcript”—is precisely the step that takes each split audio segment and assigns it its corresponding text segment (and later does the same for dialect→standard text).  \n4. By doing this annotation at the same 5 s granularity, it guarantees perfect one‐to‐one alignment between input (audio) and target (text) during ASR fine‐tuning.  \n\nAnswer  \n[mask1] is the segment-level annotation step that maps each 5-second audio chunk to its matching dialect transcript slice. By splitting both speech and text into the same fixed intervals and explicitly labeling each pair, it enforces exact alignment of audio-text examples, which is what makes the ASR model’s fine-tuning on segmented data possible and reliable."
    },
    {
        "question": "How is alphabet set-wise annotation performed to guide LLM fine-tuning for dialect transcription?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "alphabet set-wise annotation",
            "LLM"
        ],
        "id": 425,
        "masked_question": "How is [mask1] performed to guide LLM fine-tuning for dialect transcription?",
        "masked_number": 1,
        "masked_elements": [
            "alphabet set-wise annotation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10879v1_figure_1.png",
        "paperid": "2411.10879v1",
        "paper_path": "./papers/2411.10879v1.json",
        "figure_id": "2411.10879v1_figure_1.png",
        "caption": "Figure 1: (a) Typical Deep Neural Network (DNN) based implementation of ASR that uses word-based annotation combining HMM is used by [1, 2, 3], (b) Recent advancement of LLMs let researchers to get involved in investigation towards the LLMs capability of handling speech signal. The mLLM-based approach has been applied in [4, 5, 6] using feature extraction and alphabet-wise mapping. Existing methods often fall short of processing big speech signal data, specifically with dialect speech signals, due to limitations of data availability and resources. On the other hand, end-to-end frameworks are less explored as per the literature. (c) We introduce a novel approach involving fine-tuning ASR and mLLMs with a large-scale low-resource Bangla dialect speech signal dataset. There are two parts, one is the dialect transcript from the dialect speech signal, which will be performed by the multilingual ASR model and then LLM will translate and standardize the ASR models’ predicted dialect text into standard Bangla text. Our approach includes reliable preprocessing techniques to handle large-scale speech signals.",
        "qtype": "Implementation_Details",
        "response": "In our pipeline (Figure 1 b) we do _not_ fine‐tune the LLM on whole‐word transcripts the way a DNN/HMM system does (Fig. 1 a).  Instead we break the audio down into very short chunks, extract acoustic features from each chunk, and then assign to each chunk the exact sequence of _alphabet symbols_ (i.e. letters or characters) that were spoken in that slice.  That “alphabet‐set‐wise annotation” becomes our training signal:  \n\n 1. We first pre‐process the raw dialect audio (denoise and split into 5 s windows).  \n 2. We run a feature extractor over each window to get a frame‐level representation.  \n 3. We manually or semi-automatically align each frame (or group of frames) to its corresponding character in our target alphabet.  \n 4. This character‐level sequence is fed in as the “label” when fine-tuning the LLM: the model learns to map acoustic frames → character tokens rather than entire words.  \n\nBy supervising the LLM at the _alphabet_ (character) level rather than the word level, it learns much more robustly how Noakhali accents map onto written Bengali letters.  This is what the red box in Fig. 1 b is highlighting—the feedback loop in which “alphabet‐set‐wise annotations” are used to steer the LLM’s fine-tuning so that it produces dialect transcripts one character at a time."
    },
    {
        "question": "How can Neural Network optimize Array Signal Processing compared to traditional Spatial Audio Pipeline methods?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "The third type of methods, often implemented with planar arrays, employs a three-stage pipeline incorporating localization, beamforming, and Head-Related Transfer Function (HRTF) filtering, referred to as Localization-Beamforming-HRTF (LBH) method [6  ###reference_b6###, 19  ###reference_b19###]. … the critic is that, it relies heavily on each step, which can compromise accurate spatial perception restoration [21  ###reference_b21###].",
            "In recent years, several end-to-end methods have emerged that incorporate the advantages of the previously mentioned approaches while overcoming their limitations. They utilize a microphone array to capture spatial signals and directly convert them into binaural signals [22  ###reference_b22###, 23  ###reference_b23###], which provide dual capability in audio spatialization representation and noise reduction, and have proven effective in synthesizing spatial audio signals.",
            "Based on the importance of spatial audio in hybrid meetings and the shortcomings of previous methods, we propose Array2BR, a novel framework to convert the signals received by a small scale uniform circular array into the binaural spatial signals. Specifically, we introduce an “encoder–decoder” structured network that directly maps multichannel signals to binaural signals, requiring no auxiliary input information beyond the audio signals … In summary, our method excels in meeting the dual requirements of noise reduction and spatialization in telepresence. It is not only easier to deploy but also provides greater practical value for broader applications. Additionally, it demonstrates the best noise reduction and spatialization performance among current end-to-end methods, featuring fewer model parameters and lower computational complexity.",
            "In this study, an end-to-end network is devised to transform the multi-channel signals recorded by a 6 unit circular microphone array into the binaural spatial signals. The overall diagram of the framework … consists of 4 parts: an encoder, a sequential modeling module, a decoder, and a post-processing module.",
            "The encoder module utilizes a  structure [24  ###reference_b24###, 25  ###reference_b25###], which leverages multiple ConvGLU blocks to encode the spatial features from the multi-channel inputs … capturing both local and global spatial-spectral correlations. … ensures that spatial features can be hierarchically extracted at different scales.",
            "Following the encoder, the sequential modeling module is introduced to capture the temporal dependencies in the audio signals. … Each S-TCM block encodes long-term dependencies while maintaining computational efficiency. By modeling these long-range dependencies, the network can effectively capture the dynamics of spatial audio, ensuring that the temporal variations of the sound field are well-represented."
        ],
        "final_answer": "Instead of the traditional multi-stage Localization–Beamforming–HRTF pipeline (which depends on separate DOA estimation, beamforming and HRTF convolution steps), the proposed neural-network-based approach learns in an end-to-end fashion to map raw multi-channel array signals directly to binaural outputs. By using an encoder to hierarchically extract spatial-spectral features, a temporal module (S-TCM) to capture long-range dependencies, and a decoder plus post-processing to reconstruct the binaural signals, the network simultaneously performs spatialization and noise reduction without any auxiliary inputs. This yields improved spatial perception and speech quality, fewer parameters, lower computational complexity, and easier deployment compared to the traditional pipeline.",
        "relevant_elements": [
            "Neural Network",
            "Array Signal Processing"
        ],
        "id": 428,
        "masked_question": "How can [mask1] optimize Array Signal Processing compared to traditional Spatial Audio Pipeline methods?",
        "masked_number": 1,
        "masked_elements": [
            "Neural Network"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.05739v1_figure_1.png",
        "paperid": "2410.05739v1",
        "paper_path": "./papers/2410.05739v1.json",
        "figure_id": "2410.05739v1_figure_1.png",
        "caption": "Figure 1: Array2BR Compared with Traditional Methods",
        "qtype": "Literature_Background",
        "response": "First, by aligning the red‐boxed label in the top “Deep Learning” pipeline (Figure 1) with the text, we see that [mask1] corresponds to the “Neural Network” block.  \n\nStep-by-step reasoning:  \n1. Traditional spatial pipelines decompose the problem into DOA estimation → beamforming → HRTF convolution.  Each stage is hand-crafted and optimized in isolation, which means errors (e.g. in DOA) cascade into the later stages.  \n2. A neural network, by contrast, can be trained end-to-end: it ingests the raw multichannel array signals, internally learns the optimal spatial filters (i.e. beamforming weights), and simultaneously learns to encode and decode binaural cues without explicit intermediate steps.  \n3. During training, losses such as compressed-magnitude, RI (real–imaginary), and the proposed magnitude-weighted ILD loss guide the network to jointly optimize for both spatial accuracy and speech quality.  \n4. Because all sub-tasks (localization, denoising, spectral shaping, spatialization) share a single set of model parameters, the network discovers representations that best trade off between noise suppression and faithful spatial rendering—something very difficult to achieve with a strictly modular pipeline.  \n5. In practice this means:  \n   • fewer moving parts (no separate DOA estimator, beamformer, HRTF convolvers),  \n   • lower latency and computational load,  \n   • robustness to array geometry and room acoustics,  \n   • and globally optimized binaural output that outperforms the traditional three-stage approach.  \n\nIn short, by replacing the Localization–Beamforming–HRTF chain with a single end-to-end neural network, we eliminate error propagation between stages and allow global, data-driven optimization of array signal processing."
    },
    {
        "question": "How does Beam2BRIR Net adapt principles from Convolution With HRTF to end-to-end binaural synthesis?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "The third type of methods, often implemented with planar arrays, employs a three-stage pipeline incorporating localization, beamforming, and Head-Related Transfer Function (HRTF) filtering, referred to as Localization-Beamforming-HRTF (LBH) method [...].",
            "In recent years, several end-to-end methods have emerged that incorporate the advantages of the previously mentioned approaches while overcoming their limitations. They utilize a microphone array to capture spatial signals and directly convert them into binaural signals [...].",
            "Specifically, we introduce an “encoder-decoder” structured network that directly maps multichannel signals to binaural signals, requiring no auxiliary input information beyond the audio signals. Unlike other methods, we use recorded BRIRs instead of HRTFs to generate the target, making it more closely aligned with the acoustic conditions of actual meeting rooms."
        ],
        "final_answer": "Beam2BRIR Net replaces the explicit beamforming-plus-HRTF convolution stage of the traditional pipeline with a single end-to-end encoder-decoder neural network. Rather than convolving beamformed signals with measured HRTFs, it learns convolution-like spatial filters implicitly within its ConvGLU encoder, sequential modeling, and DeconvGLU decoder blocks, and is trained to match recorded BRIR targets. This embeds the HRTF convolution operation inside the network weights and allows direct multichannel-to-binaural synthesis without any separate spatial filtering or auxiliary inputs.",
        "relevant_elements": [
            "Beam2BRIR Net",
            "Convolution With HRTF"
        ],
        "id": 429,
        "masked_question": "How does [mask1] adapt principles from [mask2] to end-to-end binaural synthesis?",
        "masked_number": 2,
        "masked_elements": [
            "Beam2BRIR Net",
            "Convolution With HRTF"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.05739v1_figure_1.png",
        "paperid": "2410.05739v1",
        "paper_path": "./papers/2410.05739v1.json",
        "figure_id": "2410.05739v1_figure_1.png",
        "caption": "Figure 1: Array2BR Compared with Traditional Methods",
        "qtype": "Literature_Background",
        "response": "First, let’s identify the two masked items in the figure:\n\n  1. [mask1] is the red‐boxed module in the bottom (“Proposed”) row – this is the end-to-end network that the authors call (in the text) “Array2BR” (in the figure they label it “Beam2BRIR Net”).  \n  2. [mask2] is the blue‐boxed block in the top (“Traditional”) row – this is the final “Convolution with HRTF” stage of the classical Localization-Beamforming-HRTF (LBH) pipeline.\n\nQuestion restated:  \n  “How does Array2BR (the red-boxed Beam2BRIR Net) adapt principles from the Convolution with HRTF block to end-to-end binaural synthesis?”\n\nChain of thought:\n\n  • In the traditional LBH pipeline, you localize the source, beamform to isolate it, and then spatialize by explicitly convolving the output with measured HRTFs (head-related impulse responses).  That last step guarantees correct interaural time/level cues but is a fixed, non-learned filter.  \n  • Array2BR dispenses with explicit DOA estimation, beamforming and fixed HRTF tables.  Instead it takes the raw multichannel array signals, encodes their spatial and spectral structure in a ConvGLU/TCM encoder, and then decodes directly to left/right binaural STFT bins.  \n  • In effect, the decoder (and the subsequent LSTM+MLP post-processor) learns, from data, the same kind of binaural filtering that a hand-crafted HRTF convolution would provide.  During training it sees multichannel inputs paired with real room BRIR-filtered binaural targets, so it internalizes the mapping that HRTF convolution normally implements.  \n  • Thus Array2BR “absorbs” the HRTF convolution into its network weights, making the spatial-filtering step fully differentiable and data-driven, rather than a hard-coded convolution.\n\nFinal answer:\n\nArray2BR replaces the fixed HRTF-convolution block with a learned set of binaural filters embedded in its encoder–decoder network.  By training end-to-end on array-to-binaural pairs (using real BRIR targets), the decoder and post-processing layers learn to mimic the spatial-filtering behavior of HRTFs, but in a fully differentiable, data-driven fashion."
    },
    {
        "question": "How does the conditional prompt propagate bias mitigation from degraded to query images?",
        "relevant_section_ids": [
            "3.1",
            "3.2.2"
        ],
        "relevant_context": [
            "During inference, both the conditional image and the query image are fed to the LMM with a prompt instructing the LMM to rate the quality of the query image, under the condition that the conditional image is considered of poor quality. Our design philosophy is to guide the LMM toward confidently and accurately classifying the degraded images as poor quality, reducing its high reliance on image semantics in quality inference. This bias mitigation can, in turn, be propagated to the query image quality inference, assuming that the bias is consistently present in images with similar semantics but varying distortions.",
            "Based on the generated conditional images for each query image, we then input the query image (x) and one of its counterparts (x′) into the LMM, using a specific prompt to propagate the bias mitigation effect from the conditional image to the query image.\n#User: The visual quality of the first image is poor. How about the visual quality of the second image? Good or poor? (Question) [IMAGE_TOKEN1, IMAGE_TOKEN2] (Image1, Image2)\n#Assistant: The quality of the image is [SCORE_TOKEN]."
        ],
        "final_answer": "By feeding both a degraded (conditional) image and the original query image into the LMM under a single prompt that explicitly states “The visual quality of the first image is poor,” and then asking “How about the visual quality of the second image? Good or poor?,” the model carries over its learned understanding that the degraded image is low quality to the judgment of the query image, thereby mitigating its semantic bias.",
        "relevant_elements": [
            "Conditional prompt",
            "Bias Mitigation"
        ],
        "id": 432,
        "masked_question": "How does the [mask1] propagate bias mitigation from degraded to query images?",
        "masked_number": 1,
        "masked_elements": [
            "Conditional prompt"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.12791v1_figure_2.png",
        "paperid": "2411.12791v1",
        "paper_path": "./papers/2411.12791v1.json",
        "figure_id": "2411.12791v1_figure_2.png",
        "caption": "Figure 2: The framework of our perception bias mitigation scheme. It mainly consists of two components: 1) Bias Exposure: Specific distortions are imposed on the query image to significantly degrade the query image quality while preserving its semantics.\nThe disagreement that the LMM rates those distorted images as poor quality exposes the perception bias inherent in the LMM.\n2) Bias Mitigation: Dedicated prompts are defined to mitigate the bias by forcing that the quality of the query image should be assessed under the condition that its degraded counterpart is rated as poor quality. The final quality is then estimated by a semantic similarity based aggregation.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step‐by‐step of what that red-boxed module is doing and how it “pulls” the bias correction over from the degraded image onto the query image:\n\n1. Image–Text Alignment  \n   - In the red box you see two images side by side: the left one is a heavily distorted (degraded) version of the scene, the right one is the original (the query).  \n   - Directly above them sits a short “conditional” prompt:  \n     “The visual quality of the first image is poor. How about the visual quality of the second image? Good or poor?”\n\n2. Why this propagates bias mitigation  \n   - By *asserting* up front that the first (degraded) image is of poor quality, you lock in a ground-truth “poor” label for that image.  \n   - You then *ask* the model to rate the second image *given* that premise.  \n   - Because the two images share semantics, the model is forced to compare them rather than fall back on its usual semantic biases.  \n   - In effect, you’ve “anchored” the model’s judgment on a known poor example, and that anchor carries through to how it scores the less-degraded query image.\n\n3. Mathematical viewpoint  \n   - If  is the query and  its degraded version, you are estimating  \n     p(y | x, x′)  \n     with the prompt “Image 1 is poor; what about Image 2?”  \n   - This conditional probability literally „propagates‟ the bias correction you forced on  over to .  \n\nSo, in plain English:  \nThe red-boxed block *inserts* a conditional prompt that labels the distorted image as poor. By asking the LMM to then judge the second image under that fixed premise, you transfer the bias correction learned on the degraded example onto the original, cleaner image."
    },
    {
        "question": "How does semantic similarity weighting influence aggregation of quality estimates from conditional images?",
        "relevant_section_ids": [
            "3.2.2",
            "4.4"
        ],
        "relevant_context": [
            "Finally, we aggregate the quality estimation across the four distortion types: \\nwhere p(x'_i|x) is the probability that the distorted image is adopted as the condition. We leverage the semantic similarity between x and x'_i to estimate this probability, based on the assumption that the more semantic information maintained, the more confidently the image can be considered as a condition. We achieve the semantic similarity estimation by feeding another prompt to the LMM as follows, … This yields p(x'_i|x) = softmax(sim(x, x'_i)).",
            "In comparison, our semantic similarity aggregation scheme delivers the best performance across datasets containing synthetic, authentic, and generative distortions, demonstrating its superior generalization on diverse image distortions."
        ],
        "final_answer": "Semantic similarity weighting assigns each conditional image a weight proportional to its LMM–estimated semantic similarity with the query image (via a softmax over similarity scores). Images that share more semantics with the query receive higher weights when aggregating their quality estimates, leading to more accurate and generalizable overall predictions compared to uniform or alternative weighting schemes.",
        "relevant_elements": [
            "Semantic similarity",
            "Aggregation scheme"
        ],
        "id": 433,
        "masked_question": "How does [mask1] weighting influence [mask2] of quality estimates from conditional images?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic similarity",
            "Aggregation scheme"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.12791v1_figure_2.png",
        "paperid": "2411.12791v1",
        "paper_path": "./papers/2411.12791v1.json",
        "figure_id": "2411.12791v1_figure_2.png",
        "caption": "Figure 2: The framework of our perception bias mitigation scheme. It mainly consists of two components: 1) Bias Exposure: Specific distortions are imposed on the query image to significantly degrade the query image quality while preserving its semantics.\nThe disagreement that the LMM rates those distorted images as poor quality exposes the perception bias inherent in the LMM.\n2) Bias Mitigation: Dedicated prompts are defined to mitigate the bias by forcing that the quality of the query image should be assessed under the condition that its degraded counterpart is rated as poor quality. The final quality is then estimated by a semantic similarity based aggregation.",
        "qtype": "Experimental_Results",
        "response": "Mask1 (the “semantic‐similarity” weighting p(x′i∣x)) is exactly the set of coefficients used in Mask2’s aggregation formula.  In other words, when we compute the final quality score  \n  \n   p(y∣x) = ∑i p(y∣x,x′i) · p(x′i∣x)  \n  \neach conditional image’s quality estimate p(y∣x,x′i) is multiplied by its semantic‐similarity weight p(x′i∣x).  The more semantically similar a distorted (conditional) image is to the query, the larger its p(x′i∣x) weight, and hence the greater its influence on the summed‐up quality p(y∣x); images with low similarity contribute very little."
    },
    {
        "question": "What limitations arise from relying solely on semantic similarity for confidence weighting?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "semantic similarity",
            "confidence measure"
        ],
        "id": 434,
        "masked_question": "What limitations arise from relying solely on [mask1] for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "semantic similarity",
            "confidence measure"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.12791v1_figure_2.png",
        "paperid": "2411.12791v1",
        "paper_path": "./papers/2411.12791v1.json",
        "figure_id": "2411.12791v1_figure_2.png",
        "caption": "Figure 2: The framework of our perception bias mitigation scheme. It mainly consists of two components: 1) Bias Exposure: Specific distortions are imposed on the query image to significantly degrade the query image quality while preserving its semantics.\nThe disagreement that the LMM rates those distorted images as poor quality exposes the perception bias inherent in the LMM.\n2) Bias Mitigation: Dedicated prompts are defined to mitigate the bias by forcing that the quality of the query image should be assessed under the condition that its degraded counterpart is rated as poor quality. The final quality is then estimated by a semantic similarity based aggregation.",
        "qtype": "Others",
        "response": "Here’s the crux of it, spelled out step by step:\n\n1.  In our final scoring we compute  \n      p(y | x)  =  ∑ₖ p(y | x,xₖ′) · p(xₖ′ | x)  \n    where the “confidence weight” p(xₖ′ | x) is estimated purely by how semantically similar the distorted image xₖ′ is to the original x.\n\n2.  If you lean on semantic similarity alone to assign those weights, you throw away all information about  \n    – which distortions really expose the model’s bias most strongly,  \n    – how severe each corruption is,  \n    – or how confident the LMM actually was in labelling that particular distorted image poor.\n\n3.  In practice that means you can end up overweighting mildly‐degraded images (because they “look” almost the same to the model) and underweighting heavily‐degraded ones that do a better job of pulling the LMM off its semantic track.  \n\n4.  Empirically (see our ablation in Table 5) this leads to noticeably worse correlations with human scores.  In other words, pure semantic‐similarity weights misallocate influence among the four distorted views and so fail to fully neutralize the LMM’s semantic bias in the final aggregation."
    },
    {
        "question": "What ethical concerns stem from creating conditional images via degradation for bias exposure?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "conditional image",
            "Bias Exposure"
        ],
        "id": 435,
        "masked_question": "What ethical concerns stem from creating [mask1] via degradation for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "conditional image",
            "Bias Exposure"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.12791v1_figure_2.png",
        "paperid": "2411.12791v1",
        "paper_path": "./papers/2411.12791v1.json",
        "figure_id": "2411.12791v1_figure_2.png",
        "caption": "Figure 2: The framework of our perception bias mitigation scheme. It mainly consists of two components: 1) Bias Exposure: Specific distortions are imposed on the query image to significantly degrade the query image quality while preserving its semantics.\nThe disagreement that the LMM rates those distorted images as poor quality exposes the perception bias inherent in the LMM.\n2) Bias Mitigation: Dedicated prompts are defined to mitigate the bias by forcing that the quality of the query image should be assessed under the condition that its degraded counterpart is rated as poor quality. The final quality is then estimated by a semantic similarity based aggregation.",
        "qtype": "Others",
        "response": "Mask1 turns out to be the “conditional images” (i.e. the heavily degraded versions of an image), and Mask2 is the original “query image” that we are trying to evaluate.  In other words, the method deliberately takes a user’s or photographer’s picture and runs it through a suite of extreme blur/noise/fog/saturation tweaks in order to probe and correct the model’s bias.\n\nFrom an ethical standpoint that raises several red flags:\n\n  1. Consent and Ownership  \n     • By automatically creating and storing manipulated copies of someone’s photograph, you may be altering or redistributing copyrighted content without the owner’s explicit permission.  \n     • If the source image depicts a private individual, you are effectively producing new data about them—data they did not agree to have generated or studied.  \n\n  2. Authenticity and Trust  \n     • Mass‐producing “fake” low-quality variants risks muddying the line between genuine and manipulated imagery.  In adversarial hands these degradations could be weaponized as a kind of deep-fake (for instance, making it harder to tell real from fake or to verify the provenance of a photo).  \n\n  3. Privacy and Sensitive Attributes  \n     • Even “destructive” transformations can leak personal or demographic information.  For example, you could unintentionally amplify or reveal features (skin tone, facial structure, license-plate numbers, etc.) that users assumed would be obscured.  \n\n  4. Misuse for Disinformation or Surveillance  \n     • Once you have a large library of distorted versions of people or scenes, an attacker could mine them to train models that are robust to noise or blur—models that could be used to identify or track people under poor imaging conditions (CCTV at night, low-res social‐media images, and so on).  \n\nIn short, anytime you take somebody’s original photo, systematically degrade it, and keep those copies for downstream model probing, you must reckon with whether you have the right to do so (copyright/consent), whether you are risking the creation of misleading or “deepfake” style inputs, and whether you are leaking or amplifying private information."
    },
    {
        "question": "What limitations might recursive bilateral filtering face on highly textured cost volumes, and how could these be mitigated?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Recursive Bilateral Filtering"
        ],
        "id": 436,
        "masked_question": "What limitations might [mask1] face on highly textured cost volumes, and how could these be mitigated?",
        "masked_number": 1,
        "masked_elements": [
            "Recursive Bilateral Filtering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.03717v1_figure_1.png",
        "paperid": "2411.03717v1",
        "paper_path": "./papers/2411.03717v1.json",
        "figure_id": "2411.03717v1_figure_1.png",
        "caption": "Figure 1: An illustration of our proposed D3Stereo strategy. Cost volume pyramid is first initialized with RBF. Afterwards, coarse decisive disparities initialized at the deepest layer are hierarchically propagated into former layers with alternating decisive disparity intra-scale diffusion and inter-scale inheritance algorithms.",
        "qtype": "Others",
        "response": "Here is a concise, step-by-step answer:\n\n1. Identify [mask1]  \n In the red box the authors have highlighted the “Recursive Bilateral Filtering for Cost-Volume Initialization” (RBF) block (labelled (A) in the figure).  That is what the question is referring to.\n\n2. What RBF must do  \n – It takes the raw matching costs (e.g. cosine similarities on your deep‐feature pyramid) and, by applying a small bilateral‐filter kernel repeatedly (k iterations of, say, a 3×3 filter), approximates a much larger Gaussian‐shaped support region.  \n – In “normal” (smooth or weak‐texture) areas it works very well: you get most of the benefit of a big 15×15 or 21×21 bilateral filter at the cost of a 3×3 applied k times.\n\n3. Where RBF can struggle on highly textured cost volumes  \n – In regions of very intense, fine‐scale texture you often have “islands” of similar cost surrounded by large jumps.  A 3×3 kernel—even applied recursively—can fail to “bridge” pixels that are semantically or geometrically similar but sit several pixels apart in the image.  \n – In practice this shows up as less effective aggregation across texture ridges or checkerboards of cost noise: matching costs remain noisy or over-smoothed in the wrong direction, which then hurts the final disparity.\n\n4. How to mitigate those limitations  \n a) Increase the effective support  \n  • Raise the number of RBF iterations k (e.g. go from k=4 to k=8 or 10).  That expands the receptive field and gives the filter a better chance to connect “islands” of similar cost.  \n  • Alternatively enlarge the base kernel radius (e.g. switch from a 3×3 to a 5×5 or 7×7 filter) if budget permits.  \n b) Make the filter more edge‐aware  \n  • Tune the range‐weight σr so that strong color/feature edges aren’t crossed when the filter recurses.  This preserves fine geometry at object boundaries.  \n  • Use an anisotropic or guided‐image variant of bilateral filtering that explicitly respects the photometric edges of the RGB image (or a semantic‐segmentation mask), so that you do not bleed cost across high‐frequency texture discontinuities.  \n c) Hybridize with non-local methods  \n  • Inject a patch‐based or non-local means aggregation step (e.g. NLM‐style affinities) at certain scales to capture long-range correlations that small RBF cannot.  \n d) Leverage learned weights  \n  • Replace fixed Gaussian weights with a small neural network that predicts the spatial and range weights from the image or cost features, so that the filter can automatically adapt to the local texture statistics.\n\nPutting it all together, you retain the efficiency and low-memory footprint of RBF but recover much of the robustness of a full large-kernel bilateral filter in highly textured or high-frequency regions."
    },
    {
        "question": "What alternative propagation strategies could complement intra-scale decisive disparity diffusion and inter-scale decisive disparity inheritance to reduce error accumulation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Moreover, unlike conventional unidirectional seed-growing process, we also incorporate an adversarial mechanism into our intra-scale decisive disparity diffusion process to update disparities that may have been determined incorrectly in the previous iterations.",
            "This mechanism helps reduce the occurrence of incorrect disparities in the subsequent inter-scale decisive disparity inheritance process."
        ],
        "final_answer": "An adversarial (i.e. bidirectional) disparity propagation mechanism—where previously established disparities that violate local consistency checks are actively revised—can be used alongside intra-scale diffusion and inter-scale inheritance to mitigate error accumulation.",
        "relevant_elements": [
            "Intra-Scale Decisive Disparity Diffusion",
            "Inter-Scale Decisive Disparity Inheritance"
        ],
        "id": 437,
        "masked_question": "What alternative propagation strategies could complement [mask1] and [mask2] to reduce error accumulation?",
        "masked_number": 2,
        "masked_elements": [
            "Intra-Scale Decisive Disparity Diffusion",
            "Inter-Scale Decisive Disparity Inheritance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.03717v1_figure_1.png",
        "paperid": "2411.03717v1",
        "paper_path": "./papers/2411.03717v1.json",
        "figure_id": "2411.03717v1_figure_1.png",
        "caption": "Figure 1: An illustration of our proposed D3Stereo strategy. Cost volume pyramid is first initialized with RBF. Afterwards, coarse decisive disparities initialized at the deepest layer are hierarchically propagated into former layers with alternating decisive disparity intra-scale diffusion and inter-scale inheritance algorithms.",
        "qtype": "Others",
        "response": "Here is a short list of other “propagation” or “seed‐growing” schemes you could bolt on to our intra–scale diffusion and inter–scale inheritance in order to break up long chains of mistakes and keep local errors from snowballing into global ones:\n\n  1.  PatchMatch‐style random‐hypothesis propagation  \n     •  At each pixel you keep a small pool of competing disparity‐hypotheses, then randomly exchange and locally refine them with your confident “decisive” seeds.  This injects fresh proposals into regions that might otherwise get stuck propagating a single bad value.  \n     \n  2.  Multi‐directional scanline or edge‐aware propagation  \n     •  Instead of or in addition to the eight‐connected diffusion we already do, sweep disparities along entire horizontal and vertical scanlines (or along geodesic paths across color‐segments).  Classically, this is how dynamic‐programming stereo works—it forces coherence over long, thin support windows and can quickly erase outliers.  \n     \n  3.  Global MRF/CRF message‐passing  \n     •  Run one or two rounds of loopy belief‐propagation (or mean‐field inference in a fully‐connected CRF) over your current disparity graph.  Those long-range messages will nudge isolated bad seeds back towards the consensus of their neighbors.  \n     \n  4.  Learned spatial propagation network (SPN)  \n     •  Stick a light-weight, affinity–prediction subnetwork on top of your current disparity map and let it explicitly learn how to diffuse the high‐confidence “decisive” pixels into the low‐confidence ones.  Unlike fixed bilateral or Gaussian kernels, SPNs can sculpt their supports to follow image edges exactly.  \n     \n  5.  Superpixel or planar‐region growth  \n     •  First segment the image into superpixels or fit small planar patches to confident seeds; then propagate that planar/disparity hypothesis uniformly across the whole segment.  This is particularly effective on large, low‐texture surfaces where pixel‐by‐pixel diffusion tends to drift.  \n     \nEach of these strategies comes from a different part of the stereo literature—randomized PatchMatch, scanline DP, MRF/CRF inference, learned affinity propagation, and region/model‐based growth—and can be slotted in either between your diffusion iterations, after your final inheritance pass, or even interleaved with both.  In practice a hybrid of one or two of them usually bestows extra robustness against “cascade” errors without exploding your runtime."
    },
    {
        "question": "What design benefit arises from applying recursive bilateral filtering to cost volume pyramids before disparity initialization?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "As discussed in [46], executing I iterations of bilateral filtering with a 3×3 kernel is functionally equivalent in terms of receptive field size to performing the filtering process once, but with a (2I+1)×(2I+1) kernel.",
            "Therefore, with the same computational complexity, our proposed RBF can produce a larger receptive field adhering to a Gaussian distribution, thereby gathering more context information for cost aggregation. In addition, in practical implementations, the GPU memory needs are reduced by a factor of I when using our proposed RBF, significantly optimizing the memory resource usage."
        ],
        "final_answer": "By applying recursive bilateral filtering, the cost volumes acquire a much larger effective receptive field (equivalent to a large-kernel filter) at the same computational cost, which gathers more contextual information for matching, while simultaneously reducing GPU memory usage.",
        "relevant_elements": [
            "Recursive Bilateral Filtering",
            "Cost Volume Pyramid"
        ],
        "id": 438,
        "masked_question": "What design benefit arises from applying [mask1] to cost volume pyramids before disparity initialization?",
        "masked_number": 1,
        "masked_elements": [
            "Recursive Bilateral Filtering"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.03717v1_figure_1.png",
        "paperid": "2411.03717v1",
        "paper_path": "./papers/2411.03717v1.json",
        "figure_id": "2411.03717v1_figure_1.png",
        "caption": "Figure 1: An illustration of our proposed D3Stereo strategy. Cost volume pyramid is first initialized with RBF. Afterwards, coarse decisive disparities initialized at the deepest layer are hierarchically propagated into former layers with alternating decisive disparity intra-scale diffusion and inter-scale inheritance algorithms.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module is nothing mystical – it is simply a “Recursive Bilateral Filter” (RBF) being run over each level of the cost‐volume pyramid before any disparities are ever picked.  By breaking a single huge bilateral‐filter pass into k small, stacked passes, you get exactly the same big, Gaussian‐shaped aggregation window but at a fraction of the cost:\n\n 1. You still sweep in the same large neighborhood (so piece‐wise disparity coherence goes up),  \n 2. Your costs are smoothed with a Gaussian‐like weighting,  \n 3. But you never pay the massive memory or compute bill of one giant bilateral kernel – in practice RBF cuts the GPU memory footprint by roughly a factor of k and is far cheaper to run.  \n\nIn short, inserting RBF into the cost‐volume pipeline buys you a large, Gaussian‐distributed receptive field for cost aggregation while keeping both run‐time and memory requirements very low, right before you do any disparity initialization."
    },
    {
        "question": "What rationale supports exclusively using autoregressive distillation loss without additional losses in FBI-LLM training?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "However, unlike these works, our training process exclusively uses the autoregressive distillation loss without adding any other losses to maintain simplicity.",
            "Our experiments verified that using only the distillation loss yields better results than the vanilla one-hot label based autoregressive loss while maintaining methodological simplicity when working with fully binarized LLMs."
        ],
        "final_answer": "They choose to use only the autoregressive distillation loss in order to keep the training procedure as simple as possible, and empirical results show that this single loss achieves better performance than the standard one-hot label based autoregressive loss when training fully binarized LLMs.",
        "relevant_elements": [
            "autoregressive distillation",
            "FBI-LLM"
        ],
        "id": 441,
        "masked_question": "What rationale supports exclusively using [mask1] loss without additional losses in [mask2] training?",
        "masked_number": 2,
        "masked_elements": [
            "autoregressive distillation",
            "FBI-LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.07093v1_figure_2.png",
        "paperid": "2407.07093v1",
        "paper_path": "./papers/2407.07093v1.json",
        "figure_id": "2407.07093v1_figure_2.png",
        "caption": "Figure 2: Illustration of the FBI-LLM framework. We take the structure of LLaMA as an example. Left: the LLM block with the proposed FBI-Linear using learnable 𝜶𝜶\\bm{\\alpha}bold_italic_α and 𝜷𝜷\\bm{\\beta}bold_italic_β. Right: our autoregressive distillation and model pertaining procedure.",
        "qtype": "Design_Rationale",
        "response": "Let me first restate what the two masks refer to in the figure and the text:\n\n 1. [mask1] (red box) is the “Autoregressive Distillation” loss—i.e. the cross‐entropy between the student (FBI-LLM) and a full‐precision teacher model’s next‐token probabilities.  \n 2. [mask2] (blue box) is the FBI-LLM student model itself, whose binarized linear layers we are training.  \n\nNow: why do we *only* use that single distillation loss and *no* extra terms (no reconstruction losses, no regularization, no vanilla one-hot cross entropy, etc.) when training the binarized model?\n\nChain-of-thought:  \n- Training a fully binarized LLM is already a very delicate optimization problem—every binarized weight uses an STE‐based gradient and a tiny set of “scale” parameters.  \n- The teacher‐student distillation loss gives a *rich*, *smooth* training signal: the full‐precision teacher’s soft-token probabilities encode not just the correct token but also how the model ranks the rest of the vocabulary.  \n- Prior work on quantized LLMs often adds many auxiliary losses (e.g. straight cross‐entropy to one-hot labels, quantization‐error terms, etc.), but each extra term brings extra hyperparameters and can pull the gradients in conflicting directions—this is especially dangerous with STE.  \n- Empirically (as the authors state), *only* using the autoregressive distillation loss was both simpler and yielded *better* results than mixing in the vanilla one-hot LM loss.  \n\nTherefore the *rationale* is twofold:  \n\n 1. **Effectiveness** – the teacher’s soft targets are a stronger, more informative supervision signal than a hard one-hot loss, which is particularly important when every weight is binarized.  \n 2. **Simplicity and Stability** – eliminating extra loss terms avoids conflicting gradients and hyperparameter tuning, making the already fragile STE‐based training more stable."
    },
    {
        "question": "How do learnable scale vectors α and β calibrate the binarized weight matrix during FBI-linear forward propagation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Specifically, in the FBI-linear, we apply scaling at the granularity of the matrix columns.",
            "The calculation process can be formulated as: where W⁽ᵇ⁾_{:,j} denotes the j-th column of the scaled binarized weight matrix W⁽ᵇ⁾. α_j and β_j are the j-th elements in learnable scale vectors α and β respectively."
        ],
        "final_answer": "During forward propagation in an FBI-linear layer, each column j of the binary weight matrix (sign(W⁽ᶠ⁾_{:,j})) is first multiplied by the learnable scale α_j and then shifted by the learnable bias β_j, i.e. W⁽ᵇ⁾_{:,j} = α_j·sign(W⁽ᶠ⁾_{:,j}) + β_j. This column-wise scaling and shifting calibrates the ±1 binarized weights to better approximate the original full-precision weights.",
        "relevant_elements": [
            "FBI-Linear",
            "α",
            "β"
        ],
        "id": 442,
        "masked_question": "How do learnable scale vectors [mask1] and β calibrate the binarized weight matrix during FBI-linear forward propagation?",
        "masked_number": 1,
        "masked_elements": [
            "α"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.07093v1_figure_2.png",
        "paperid": "2407.07093v1",
        "paper_path": "./papers/2407.07093v1.json",
        "figure_id": "2407.07093v1_figure_2.png",
        "caption": "Figure 2: Illustration of the FBI-LLM framework. We take the structure of LLaMA as an example. Left: the LLM block with the proposed FBI-Linear using learnable 𝜶𝜶\\bm{\\alpha}bold_italic_α and 𝜷𝜷\\bm{\\beta}bold_italic_β. Right: our autoregressive distillation and model pertaining procedure.",
        "qtype": "Implementation_Details",
        "response": "In FBI-Linear, every full‐precision weight matrix Wᶠ is first binarized by taking its sign, so that Wᵇ = sign(Wᶠ) ∈ {–1,+1}.  If we were to stop there, the magnitudes of all columns of Wᵇ would be exactly one, which is generally a very poor match to the real‐valued columns of Wᶠ.  To recover much of the expressivity of the original layer with almost no extra cost, FBI‐Linear learns two small vectors α and β (each of length equal to the number of output columns) and applies them column‐wise as a simple “scale + shift” to the binarized weights.  \n\nConcretely, if Wᵇ(:, j) is the j-th column of the sign-matrix, the forward pass uses  \n Ŵᵇ(:, j) = αⱼ · Wᵇ(:, j) + βⱼ.  \nThat is, αⱼ stretches (or shrinks) the ±1 pattern in column j to match its true magnitude, and βⱼ shifts it up or down to match its true mean.  By learning those 2 · (# columns) parameters, the binarized layer can approximate the original full-precision layer much more faithfully."
    },
    {
        "question": "How does autoregressive distillation leverage teacher model logits to refine FBI-LLM token distributions?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In the training, a full-precision pre-trained LLM is used as the teacher model, and the binarized target model acts as the student. Suppose each instance of training data consists of a sequence of input tokens , the teacher prediction probability for the next token can be formulated as: p^T(x^{i+1} | x^0...x^i) = softmax(W^T f(x^0...x^i)), where f represents the activation of the final transformer block.",
            "The cross-entropy between the outputs of the student model and the teacher model is calculated as the final loss function at each step of predicting the next token. It can be formulated as: L_AD = –(1/N) ∑_{i=1}^N ∑_j p^T(j) log p^S(j). Here p^T denotes the token distribution over the vocabulary at the i step predicted by the teacher model, while p^S is the corresponding predicted distribution of the student model."
        ],
        "final_answer": "Autoregressive distillation first runs the full-precision teacher over each prefix x^0…x^i to obtain its logits, applies a softmax to yield a probability distribution p^T over the next token, and then trains the binarized student to match these soft targets by minimizing the cross-entropy between the teacher’s distribution p^T and the student’s distribution p^S at every prediction step.",
        "relevant_elements": [
            "Autoregressive Distillation",
            "Teacher Model",
            "FBI-LLM"
        ],
        "id": 443,
        "masked_question": "How does [mask1] leverage teacher model logits to refine FBI-LLM token distributions?",
        "masked_number": 1,
        "masked_elements": [
            "Autoregressive Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.07093v1_figure_2.png",
        "paperid": "2407.07093v1",
        "paper_path": "./papers/2407.07093v1.json",
        "figure_id": "2407.07093v1_figure_2.png",
        "caption": "Figure 2: Illustration of the FBI-LLM framework. We take the structure of LLaMA as an example. Left: the LLM block with the proposed FBI-Linear using learnable 𝜶𝜶\\bm{\\alpha}bold_italic_α and 𝜷𝜷\\bm{\\beta}bold_italic_β. Right: our autoregressive distillation and model pertaining procedure.",
        "qtype": "Implementation_Details",
        "response": "Here, the red‐boxed module is simply the “Autoregressive Distillation” step.  Concretely, at each generation step the full‐precision teacher LLM produces a “soft” probability distribution over the next token (pᵀ).  The FBI‐LLM (with its binarized layers) also produces its own distribution (pˢ).  We then take the cross‐entropy between pᵀ and pˢ as our loss and back‐propagate through the binarized layers using a Straight‐Through Estimator.  In this way, the student’s (FBI‐LLM’s) token probabilities are continually nudged to match the teacher’s richer, full‐precision logits, yielding a more accurate output distribution despite the underlying binary weights."
    },
    {
        "question": "How does Prototype Intensity Downsampling correct intensity bias during downsampling support masks?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "However, if we use the common bilinear/bicubic interpolation to downsample the original mask to indicate the feature intensity, since the bilinear/bicubic interpolation only uses few points to get the result, the result may be overestimated or underestimated, as shown in Fig. 3, which leads to the distortion of target semantics.",
            "To alleviate this issue, we propose a prototype intensity downsampling to replace the common bilinear/bicubic interpolation in the downsampling of $M^s$. Specifically, we employ a $K\\times K$ convolution layer to process $M^s$: $\\hat M^s = \\mathrm{conv}_{K,K,\\,\\mathrm{stride}=K}(M^s)$, where the parameters in the convolution kernel are all 1 and the stride is $K$. $\\hat M^s$ is calculated by aggregating all pixels in each corresponding region of the feature map. It provides a better estimate of the intensity of prototype features, avoiding the mismatch of semantic clues."
        ],
        "final_answer": "Instead of using bilinear or bicubic interpolation—which only samples a few points and can over‐ or under‐estimate mask intensity—Prototype Intensity Downsampling applies a K×K convolution with all‐ones weights and stride K over the original mask. By summing (or averaging) all pixels in each K×K patch, it produces an accurate intensity value for each feature map cell, correcting the bias introduced by standard interpolation.",
        "relevant_elements": [
            "Prototype Intensity Downsampling",
            "support mask"
        ],
        "id": 444,
        "masked_question": "How does [mask1] correct intensity bias during downsampling support masks?",
        "masked_number": 1,
        "masked_elements": [
            "Prototype Intensity Downsampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.21351v1_figure_2.png",
        "paperid": "2407.21351v1",
        "paper_path": "./papers/2407.21351v1.json",
        "figure_id": "2407.21351v1_figure_2.png",
        "caption": "Figure 2: Proposed small object few-shot segmentation model. M^ssuperscript^Ms\\hat{\\textbf{{M}}}^{\\rm{s}}over^ start_ARG M end_ARG start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT indicates the downsampling of support mask. Mas,MsssubscriptsuperscriptMsasubscriptsuperscriptMss\\textbf{{M}}^{\\rm{s}}_{\\rm{a}},\\textbf{{M}}^{\\rm{s}}_{\\rm{s}}M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT , M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT denote the abnormal prior map and semantic prior map respectively, p denotes a prototype feature. Best viewed in the electronic version.",
        "qtype": "Implementation_Details",
        "response": "Mask1 refers to the “prototype intensity downsampling” block.  In the paper the authors point out that if you simply use bilinear or bicubic interpolation to shrink a tiny binary mask down to the feature‐map size, you end up sampling only 2 or 4 pixels per patch and can easily over– or under–estimate how much of that patch really belongs to the object.  To remove that bias they replace interpolation with a k×k convolution whose kernel is all ones and whose stride is k (where k is the downsampling factor).  In effect each downsampled mask value is the sum (or average) of all the original mask pixels in that k×k region, so you use every pixel instead of a handful of samples.  That gives you an unbiased, region–wise intensity estimate and prevents the semantic distortion caused by naïve interpolation."
    },
    {
        "question": "How does Non-learnable Feature Fusion leverage cosine similarities to fuse query and support features?",
        "relevant_section_ids": [
            "3.2.4"
        ],
        "relevant_context": [
            "are subsequently fused by a non‐learnable feature fusion. The matching mechanism follows [30] to replace the dot produce with the cosine similarities, which is formulated as follows:",
            "where  refers to the reshape function and non normalized,  controls the distribution shape,  refers to the normalization along the row, i.e., reverse softmax [37],  refers to first expanding the new dimension and then replicating along the expanded dimension. Eq. (4) can be regarded as a type of cross‐attention, where the learnable parameters are discarded."
        ],
        "final_answer": "Non‐learnable Feature Fusion first reshapes and L2‐normalizes both the query and support feature maps, then computes their pairwise cosine similarities (instead of a dot-product) to measure how well each query location matches each support location. These similarity scores are passed through a row-wise normalization (reverse softmax) to form attention weights, which are then used to aggregate (cross-attend) the support features into the query feature map. This process requires no learnable parameters and effectively fuses support semantics into the query via cosine‐based attention.",
        "relevant_elements": [
            "Non-learnable Feature Fusion",
            "cosine similarities"
        ],
        "id": 445,
        "masked_question": "How does [mask1] leverage cosine similarities to fuse query and support features?",
        "masked_number": 1,
        "masked_elements": [
            "Non-learnable Feature Fusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.21351v1_figure_2.png",
        "paperid": "2407.21351v1",
        "paper_path": "./papers/2407.21351v1.json",
        "figure_id": "2407.21351v1_figure_2.png",
        "caption": "Figure 2: Proposed small object few-shot segmentation model. M^ssuperscript^Ms\\hat{\\textbf{{M}}}^{\\rm{s}}over^ start_ARG M end_ARG start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT indicates the downsampling of support mask. Mas,MsssubscriptsuperscriptMsasubscriptsuperscriptMss\\textbf{{M}}^{\\rm{s}}_{\\rm{a}},\\textbf{{M}}^{\\rm{s}}_{\\rm{s}}M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT , M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT denote the abnormal prior map and semantic prior map respectively, p denotes a prototype feature. Best viewed in the electronic version.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module is nothing exotic at all but the “non-learnable feature fusion’’ block from §3.2.4.  In practice what it does is:\n\n  1.  Take your encoded query feature map Fq (of shape H·W×C) and your support feature map Fs (of shape H·W×C).  \n  2.  For every query location i and every support location j, compute the cosine similarity s_{ij}=⟨Fq_i, Fs_j⟩/(∥Fq_i∥·∥Fs_j∥).  \n  3.  Divide these similarities by a temperature τ to sharpen or smooth the distribution.  \n  4.  Along each query‐vector i, run a “reverse” softmax (i.e. normalize over j) so that the similarities at i sum to one:  \n         α_{ij} = exp(s_{ij}/τ)  \n         α_{ij} ← α_{ij} / ∑ₖ exp(s_{ik}/τ)  \n  5.  Use these α weights to linearly combine the support vectors for each query position:  \n         Fused_i = ∑_j α_{ij}·Fs_j  \n  \nBecause the only parameters in this entire chaining (cosine, temperature scaling, softmax, weighted sum) are fixed, there is no additional learned layer here—hence “non-learnable.”  The end result is exactly the cross-attention of each query pixel onto the support map, but computed with cosine-based affinities rather than a learned projection."
    },
    {
        "question": "How do the abnormal prior map and non-learnable feature fusion compare to cross-attention in pixel-level fusion methodologies?",
        "relevant_section_ids": [
            "2.1",
            "3.2.3",
            "3.2.4"
        ],
        "relevant_context": [
            "Few-shot semantic segmentation [...] pixel-level feature fusion methods are proposed to mine the correspondence between the query pixel-level features and the support semantic-related pixel-level features, where the residual connection in the cross attention plays the role of fusing query and support features.",
            "M_s^a matches every pixel-level query feature with the normal support features, if there is a missing defect, it can be highlighted and the normal background can not be. In addition, M_s^a enables SOFS to have FAD ability, we can input the normal support image.",
            "Eq. (4) can be regarded as a type of cross-attention, where the learnable parameters are discarded. We think that the recognition of small objects does not need lots of parameters, more parameters may cause the risk of overfitting the category-specific information."
        ],
        "final_answer": "The abnormal prior map extends the usual pixel-level cross-attention by computing for each query pixel its maximum similarity to support normal features—this highlights abnormal regions and suppresses normal background (enabling few-shot anomaly detection). The non-learnable feature fusion then performs a cross-attention–style matching using cosine similarities but with all learnable weights removed, avoiding the parameter overhead and overfitting risks of conventional cross-attention in small-object scenarios.",
        "relevant_elements": [
            "abnormal prior map",
            "non-learnable feature fusion"
        ],
        "id": 446,
        "masked_question": "How do the [mask1] and [mask2] compare to cross-attention in pixel-level fusion methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "abnormal prior map",
            "non-learnable feature fusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.21351v1_figure_2.png",
        "paperid": "2407.21351v1",
        "paper_path": "./papers/2407.21351v1.json",
        "figure_id": "2407.21351v1_figure_2.png",
        "caption": "Figure 2: Proposed small object few-shot segmentation model. M^ssuperscript^Ms\\hat{\\textbf{{M}}}^{\\rm{s}}over^ start_ARG M end_ARG start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT indicates the downsampling of support mask. Mas,MsssubscriptsuperscriptMsasubscriptsuperscriptMss\\textbf{{M}}^{\\rm{s}}_{\\rm{a}},\\textbf{{M}}^{\\rm{s}}_{\\rm{s}}M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT , M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT denote the abnormal prior map and semantic prior map respectively, p denotes a prototype feature. Best viewed in the electronic version.",
        "qtype": "Literature_Background",
        "response": "In the figure, the little speckled gray “map” sitting in the red box is nothing more exotic than the pair of prior‐maps (semantic prior ​M⁽ˢ⁾ₛ and abnormal prior ​M⁽ˢ⁾ₐ) that the authors compute by straightforward masked‐pooling/max‐matching and a 1×1 conv.  The blue box, by contrast, surrounds their so‐called “non‐learnable feature fusion” module.  That fusion block takes the query and support feature tensors, computes all pairwise cosine‐similarities, applies a reverse‐softmax along one axis, and then re-weights and sums—again with no learned projection matrices or extra parameters.  \n\nIn other words both the red‐boxed priors and the blue‐boxed fusion behave very much like the “attention” or “cross-attention” you see in pixel-level fusion FSS papers (they compute affinities between query and support at every spatial location and use them to gate information), but with one crucial difference: there are zero trainable parameters in the match/fuse steps.  The conventional pixel-level cross-attention modules learn Q, K, V projections (and thus risk overfitting on tiny objects); here, all of the weighting is done by fixed operations (cosine similarity, masked pooling, 1×1 conv, max), so it’s a purely non-parametric, attention-style fusion."
    },
    {
        "question": "How do prototype intensity downsampling and sliding window mechanism mitigate information loss versus multi-scale learning?",
        "relevant_section_ids": [
            "2.2",
            "3.2.2"
        ],
        "relevant_context": [
            "The main challenges for small object recognition include information loss, low tolerance for bounding box perturbation, etc. Information loss refers to the fact that the feature information of small objects is almost wiped out during the downsampling of the feature extractor, it has the greatest impact on performance. To alleviate this issue, there are mainly three kinds of methods. ... The third is to process small objects by multi-scale learning and hierarchical feature fusion [40, 61].",
            "Non-resizing Procedure. The core idea is to ensure that the pixel area of small objects encoded by the model is consistent with that in the original image. As shown in Fig. 2, the non-resizing procedure randomly crops the small object on the original image in training and uses the sliding window mechanism to process all regions of the query image in the test.",
            "Prototype Intensity Downsampling. To extract the prototype feature on the support image, we need to downsample the support mask M^s to ensure that it is consistent with the size of the support feature map. ... Specifically, we employ a k×k convolution layer to process M^s: M̂^s = Conv_{ones}(M^s). M̂^s is calculated by all pixels of corresponding regions of features. It provides a better estimate of the intensity of prototype features, avoiding the mismatch of semantic clues."
        ],
        "final_answer": "Instead of relying on multi-scale feature fusion to recover details lost by downsampling, SOFS keeps the small-object pixels at their original resolution via a sliding-window (non-resizing) procedure and computes prototype intensities by aggregating over every pixel in the support region (prototype intensity downsampling). Together, these two designs preserve the true pixel-level information and avoid the interpolation distortion and semantic weakening that come from both coarse resizing and sparse bilinear/bicubic downsampling, thereby reducing information loss more directly than multi-scale learning.",
        "relevant_elements": [
            "prototype intensity downsampling",
            "sliding window mechanism"
        ],
        "id": 447,
        "masked_question": "How do [mask1] and sliding window mechanism mitigate information loss versus multi-scale learning?",
        "masked_number": 1,
        "masked_elements": [
            "prototype intensity downsampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.21351v1_figure_2.png",
        "paperid": "2407.21351v1",
        "paper_path": "./papers/2407.21351v1.json",
        "figure_id": "2407.21351v1_figure_2.png",
        "caption": "Figure 2: Proposed small object few-shot segmentation model. M^ssuperscript^Ms\\hat{\\textbf{{M}}}^{\\rm{s}}over^ start_ARG M end_ARG start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT indicates the downsampling of support mask. Mas,MsssubscriptsuperscriptMsasubscriptsuperscriptMss\\textbf{{M}}^{\\rm{s}}_{\\rm{a}},\\textbf{{M}}^{\\rm{s}}_{\\rm{s}}M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_a end_POSTSUBSCRIPT , M start_POSTSUPERSCRIPT roman_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT roman_s end_POSTSUBSCRIPT denote the abnormal prior map and semantic prior map respectively, p denotes a prototype feature. Best viewed in the electronic version.",
        "qtype": "Literature_Background",
        "response": "Here’s the step-by-step reasoning for how “Prototype Intensity Downsampling” (the red‐boxed component) together with the sliding-window test‐time crop strategy avoid the information loss that standard multi-scale networks suffer:\n\n1.   The core problem with small objects is that a few pixels in the original image often collapse to zero or an indistinguishable blur during the usual down‐sampling or feature‐pyramid steps of a multi-scale backbone.  \n2.   Rather than build a whole multi-scale hierarchy (which still ends up averaging or pooling away tiny details), SOFS chooses to  \n     a.  Never resize the input so that a small defect always occupies the same number of image pixels that the network sees, and  \n     b.  Tile the query image at test time with a sliding-window so that each crop is fed in at full resolution.  This guarantees that even the smallest object never “falls between” scales.  \n3.   On the support side we still must align the support mask to the feature‐map grid.  If you simply use bilinear or bicubic interpolation you typically sample only a handful of points in each down‐sampled patch, which over- or under-estimates how much of that patch belongs to the object.  \n4.   “Prototype Intensity Downsampling” replaces that naïve interpolation with a 1×1‐convolution (all ones) whose stride equals the down-sampling factor.  Convolving and striding in this way effectively averages *all* pixels under each receptive field, producing a precise measure of how much of each feature‐cell is covered by the object.  \n5.   Together,  – by (i) *never* shrinking the *input* small object below its native pixel size and (ii) *accurately* computing the *support* mask intensity on the feature map – SOFS retains every bit of signal from tiny defects.  In contrast, classical multi-scale feature pyramids or hierarchical fusions still suffer from partial occlusion or outright erasure of very small regions."
    },
    {
        "question": "How do ADD and CONCAT fusion approaches parallel multimodal feature alignment methodologies?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We implement two different approaches for incorporating ET features into the RM, as shown in Figure 2. In the first approach, GazeConcat, we concatenate the ET embeddings with the text embeddings.",
            "GazeAdd: The input to the RM consists of the ET embedding e_e and the text embedding e_t, which are added in an elementwise fashion: e_c = e_t + e_e."
        ],
        "final_answer": "Both GazeAdd and GazeConcat mirror standard multimodal fusion strategies. GazeAdd aligns the two modalities by projecting the eye-tracking and text embeddings into the same space and then summing them element-wise (a common ‘additive’ or ‘sum’ fusion in multimodal models). GazeConcat, by contrast, concatenates the projected eye-tracking embedding and the text embedding—delimited by special ⟨eye⟩ tokens—replicating the typical ‘concatenation’ fusion used to stack features from different modalities.",
        "relevant_elements": [
            "ADD",
            "CONCAT"
        ],
        "id": 448,
        "masked_question": "How do [mask1] and [mask2] fusion approaches parallel multimodal feature alignment methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "ADD",
            "CONCAT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.01532v1_figure_1.png",
        "paperid": "2410.01532v1",
        "paper_path": "./papers/2410.01532v1.json",
        "figure_id": "2410.01532v1_figure_1.png",
        "caption": "Figure 1: GazeReward Framework for using eye-tracking data for reward modelling. We use a generator model to compute ET features on a preference dataset D𝐷Ditalic_D and we train the human preference by combining both text and ET embeddings (See section 3 for details.)",
        "qtype": "Literature_Background",
        "response": "Step 1: Aligning the diagram with the text  \n- In the diagram’s “Embeddings construction” block (step 3), the red‐outlined box shows a small “ADD” icon above two rows of embeddings being merged.  The blue‐outlined box shows a “CONCAT” icon next to the two rows of embeddings surrounded by special ⟨eye⟩/⟨/eye⟩ markers.  \n- In the prose those correspond exactly to the two fusion methods:  \n  • GazeAdd → element-wise addition of the projected ET embedding to the text embedding (the ADD icon, mask1).  \n  • GazeConcat → concatenation of the ET embedding and text embedding as sequential tokens bracketed by ⟨eye⟩…⟨/eye⟩ (the CONCAT icon, mask2).  \n\nStep 2: Mapping each fusion to standard multimodal alignment  \n- Multimodal models commonly align different modalities either by projecting them into a shared vector space and summing them (often called early or feature-level fusion), or by concatenating modality-specific embeddings into one sequence and letting a transformer learn the cross-modal interactions (sequence-level or late fusion).  \n- GazeAdd (mask1) parallels the former: both ET and text embeddings are projected to the same dimensionality via the MLP and then simply added together. This mirrors how, for example, vision‐and‐language encoders will project image‐region features and word embeddings into the same space and sum them before feeding into a joint transformer.  \n- GazeConcat (mask2) parallels the latter: the ET embeddings are kept as a separate subsequence (flagged by ⟨eye⟩ tokens) and then concatenated with the text token sequence. This is analogous to many multimodal transformers that prepend or interleave, say, image patch embeddings with text tokens and learn the cross-attention across modalities.  \n\nStep 3: Conclusion  \nThus, mask1 (ADD) implements an early, feature-space alignment by summation, while mask2 (CONCAT) implements a sequence-level alignment by concatenation and letting the model learn the fusion via its attention layers—exactly the two canonical paths taken in multimodal feature‐alignment methodologies."
    },
    {
        "question": "How do Eye-tracking features generation and Reward model scoring reflect reinforcement learning reward shaping?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Eye-tracking features generation",
            "Reward model"
        ],
        "id": 449,
        "masked_question": "How do [mask1] and Reward model scoring reflect reinforcement learning reward shaping?",
        "masked_number": 1,
        "masked_elements": [
            "Eye-tracking features generation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.01532v1_figure_1.png",
        "paperid": "2410.01532v1",
        "paper_path": "./papers/2410.01532v1.json",
        "figure_id": "2410.01532v1_figure_1.png",
        "caption": "Figure 1: GazeReward Framework for using eye-tracking data for reward modelling. We use a generator model to compute ET features on a preference dataset D𝐷Ditalic_D and we train the human preference by combining both text and ET embeddings (See section 3 for details.)",
        "qtype": "Literature_Background",
        "response": "Here’s how the two pieces fit into a classic reward‐shaping picture in RLHF:\n\n1.  [mask1]  = the red‐boxed “Eye-tracking features module predictor.”  \n    •  During data prep, every (prompt, response) pair is passed through a frozen ET‐prediction model that spits out token-by-token signals (first-fixation duration, total reading time, number of fixations, etc.).  \n    •  In RL terms, those gaze features act like a shaping function ϕ(s,a): they give you a dense, informative side channel that correlates with human attention and thus gently guides the learner toward more “human-like” behavior.  \n\n2.  Reward‐model scoring  = the usual scalar head on top of your LLM that, given the same (prompt + response) embedding (now augmented with gaze features), predicts a single “goodness” score.  \n    •  In RLHF you take two completions, compute their rewards Rθ(·), and use the difference Rθ(chosen) – Rθ(rejected) as the learning signal (e.g. in PPO or DPO).  \n    •  Because the gaze features have already been fused into the embedding, they subtly “shape” that reward function—providing smoother gradients and denser feedback than raw binary preferences alone.  \n\nTogether they implement a form of potential-based reward shaping: the eye-tracking module injects an additional potential ϕ, and the reward model’s scalar output R + ϕ becomes the shaped reward used to optimize the policy."
    },
    {
        "question": "How does GazeConcat preserve modality distinction when combining ET and text embeddings for reward modeling?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "GazeConcat: The ET embedding, denoted as E_et, is concatenated with the text embedding E_txt to form the input for the RM.",
            "To distinguish between the two modalities, we introduce two special tokens: 〈eye〉 and 〈/eye〉, which flag the start and end of the ET embedding, respectively (Figure 2)."
        ],
        "final_answer": "GazeConcat preserves modality distinction by surrounding the ET embeddings with two special tokens, 〈eye〉 and 〈/eye〉, before concatenating them with the text embeddings so that the model can explicitly identify and separate eye-tracking features from the text modality.",
        "relevant_elements": [
            "GazeConcat"
        ],
        "id": 450,
        "masked_question": "How does [mask1] preserve modality distinction when combining ET and text embeddings for reward modeling?",
        "masked_number": 1,
        "masked_elements": [
            "GazeConcat"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.01532v1_figure_1.png",
        "paperid": "2410.01532v1",
        "paper_path": "./papers/2410.01532v1.json",
        "figure_id": "2410.01532v1_figure_1.png",
        "caption": "Figure 1: GazeReward Framework for using eye-tracking data for reward modelling. We use a generator model to compute ET features on a preference dataset D𝐷Ditalic_D and we train the human preference by combining both text and ET embeddings (See section 3 for details.)",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. The red box in the figure is step 3, “Embeddings construction with eye-tracking features,” where the paper describes two ways to fuse the projected ET embeddings fET (after the MLP) with the standard text embeddings fTX.  \n2. In the “GazeAdd” variant they simply do an element-wise sum  \n  combined = fET + fTX  \n   so that the two modalities are first projected separately into the same vector space and only then merged.  \n3. In the “GazeConcat” variant they concatenate the two streams, but crucially they wrap the eye-tracking span in special boundary tokens ⟨eye⟩ … ⟨/eye⟩ before feeding it to the reward model:  \n  [input tokens …, ⟨eye⟩, e1, e2, …, ek, ⟨/eye⟩, t1, t2, …]  \n   Those marker tokens are fresh, randomly-initialized embeddings that flag “this segment came from the ET predictor,” so the model always knows “here begins/ends the eye-tracking modality.”  \n4. By either (a) keeping the two embeddings separate until an explicit add step, or (b) sandwiching the ET vectors in their own tagged region when concatenating, the framework never loses track of which dimensions came from gaze data and which from text.  \n\nAnswer  \nThe red-boxed fusion module preserves modality distinction by projecting gaze and text inputs through separate networks and then either  \n• Adding them (so they only meet at a final element-wise sum), or  \n• Concatenating them with special ⟨eye⟩/⟨/eye⟩ tokens to bracket the gaze sub-sequence.  \nIn both cases the model can still tell “that slice of the embedding was eye-tracking,” thus keeping the two modalities distinct even after combination."
    },
    {
        "question": "How does GazeAdd handle token mismatch between ET prediction tokenizer and RM tokenizer?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The two ET prediction models use different tokenizers, which also differ from those used by the base models in the RM. As a result, the number of tokens w in the input for the RM and the number of tokens m generated by the ET prediction model may not match.",
            "To address this embedding alignment issue, and have the same dimension, we remap the ET features from the m-token space to the w-token space used by each base model in the RM."
        ],
        "final_answer": "GazeAdd resolves the tokenizer mismatch by remapping the ET feature embeddings from the ET prediction model’s token space into the RM’s token space so that both embeddings have the same length for elementwise addition.",
        "relevant_elements": [
            "GazeAdd"
        ],
        "id": 451,
        "masked_question": "How does [mask1] handle token mismatch between ET prediction tokenizer and RM tokenizer?",
        "masked_number": 1,
        "masked_elements": [
            "GazeAdd"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.01532v1_figure_1.png",
        "paperid": "2410.01532v1",
        "paper_path": "./papers/2410.01532v1.json",
        "figure_id": "2410.01532v1_figure_1.png",
        "caption": "Figure 1: GazeReward Framework for using eye-tracking data for reward modelling. We use a generator model to compute ET features on a preference dataset D𝐷Ditalic_D and we train the human preference by combining both text and ET embeddings (See section 3 for details.)",
        "qtype": "Experimental_Results",
        "response": "Here’s how “GazeAdd” (the red-boxed ADD block in step 3) copes with the fact that the ET predictor and the reward model (RM) use different tokenizers:\n\n1.  In GazeAdd you want to do an element-wise sum  \n       ET-embedding(token i)  \n     + Text-embedding(token i)  \n     → Combined-embedding(token i).  \n2.  But the ET predictor spits out features over its own tokenization (say w′ tokens), while the RM uses its own (say w tokens).  \n3.  To resolve this, before you ever add the vectors, you “remap” the ET features from the w′-token sequence into the RM’s w-token sequence.  \n     – Concretely, you align each RM token to one or more ET-predictor tokens (using the shared text and subword boundaries), then pool or replicate the ET feature vectors so that you end up with exactly one ET feature vector per RM token.  \n4.  Once you have an ET-feature vector for each of the RM’s w tokens, you pass them through the small MLP projector (to match the RM’s embedding dimension) and then simply add them element-wise to the RM’s text embeddings.\n\nIn short: GazeAdd avoids any length-mismatch by remapping (i.e. realigning and pooling) the ET-model’s token-level features onto the RM’s tokenization before performing the ADD."
    },
    {
        "question": "What ethical risks might emerge from using black-box M^l base models within InfoSel-TT ensemble?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "M^l",
            "InfoSel-TT"
        ],
        "id": 454,
        "masked_question": "What ethical risks might emerge from using black-box [mask1] base models within [mask2] ensemble?",
        "masked_number": 2,
        "masked_elements": [
            "M^l",
            "InfoSel-TT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.12841v1_figure_1.png",
        "paperid": "2407.12841v1",
        "paper_path": "./papers/2407.12841v1.json",
        "figure_id": "2407.12841v1_figure_1.png",
        "caption": "Figure 1: Architecture of our InfoSel, fine-tuned (FT) and InfoSel∗ models. M∗lsubscriptsuperscript𝑀𝑙M^{l}_{*}italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT and M∗vsubscriptsuperscript𝑀𝑣M^{v}_{*}italic_M start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT refer to black-box LLMs and VQA base models respectively, which are not trainable. The number of these base models is flexible, and is not restricted to 3 as in the figure. The models on the left (suffixed with -TT) are trained for the TQA tasks, while the models on the right (suffixed with -MT) are trained for the VQA tasks.\nAll our models are trained independently. Note that FT and InfoSel∗  are optional if the task-specific datasets do not contain high percentage of unseen labels.",
        "qtype": "Others",
        "response": "Step 1: Identify what the two masked regions correspond to in the figure and text.  \n- The red box around the three “cloud” icons on the TQA side are the black-box base models M₁ˡ, M₂ˡ, M₃ˡ – i.e. three off-the-shelf LLMs (ChatGPT, LLaMA-2, GPT3.5…).  \n- The blue box around the lower‐right of the TQA panel is the second‐stage ensemble labeled “InfoSel*-TT,” which learns to blend the outputs of InfoSel-TT and the fine-tuned TT model.\n\nSo the question really reads:\n\n  “What ethical risks might emerge from using black-box LLM base models within an InfoSel* (second-stage) ensemble?”\n\nStep 2: Reasoning through the ethics of black-box LLM ensembling.  \n\n1. Lack of transparency and explainability  \n   • Because the base LLMs are opaque, users and auditors cannot inspect how any single LLM arrived at a particular answer.  \n   • Even though InfoSel* picks a “winner,” you have no visibility into the internals of that winner model—only its final text output.  \n   → This opacity makes it very hard to understand, debug or justify why the system said what it did.\n\n2. Hidden and compounded biases  \n   • Each black‐box LLM carries its own training‐data biases (gender, racial, cultural, etc.).  \n   • When you ensemble them via InfoSel*, you risk amplifying or compounding these biases in unpredictable ways.  \n   → Without access to model internals, you cannot easily detect which base model is propagating unfair stereotypes, nor can you correct it.\n\n3. Accountability and auditability gaps  \n   • If the ensemble produces a harmful or legally problematic answer, who is responsible?  \n   • You can’t open the LLMs to see if they are infringing copyrights in generated text, leaking private data seen in their training, or violating data‐protection rules.  \n   → The black-box nature defers or diffuses responsibility across closed-source providers.\n\n4. Hallucinations and misinformation  \n   • LLMs are known to “hallucinate” facts.  \n   • An InfoSel* ensemble may choose a hallucinating model in edge cases, leading to confidently wrong or misleading outputs.  \n   → Users may place undue trust in the system.\n\n5. Privacy and data‐leakage concerns  \n   • Proprietary LLMs sometimes memorize snippets of their (often private or leased) training sets.  \n   • By ensembling them, you increase the surface area for unintentionally exposed personal or sensitive information.\n\nStep 3: Summarize the ethical risks.  \n\nBringing it all together:\n\nBy plugging opaque, third-party LLMs into the second-stage InfoSel* ensemble you introduce an overall system that is non-transparent, hard to audit or explain, prone to hidden bias amplification and untraceable errors, and that may inadvertently leak sensitive data or hallucinate falsehoods—all while complicating questions of who is accountable when something goes wrong."
    },
    {
        "question": "How might substitute dynamic classifiers improve selection compared to the dense layer in InfoSel-MT?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "dense layer",
            "InfoSel-MT"
        ],
        "id": 455,
        "masked_question": "How might substitute dynamic classifiers improve selection compared to the [mask1] in InfoSel-MT?",
        "masked_number": 1,
        "masked_elements": [
            "dense layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.12841v1_figure_1.png",
        "paperid": "2407.12841v1",
        "paper_path": "./papers/2407.12841v1.json",
        "figure_id": "2407.12841v1_figure_1.png",
        "caption": "Figure 1: Architecture of our InfoSel, fine-tuned (FT) and InfoSel∗ models. M∗lsubscriptsuperscript𝑀𝑙M^{l}_{*}italic_M start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT and M∗vsubscriptsuperscript𝑀𝑣M^{v}_{*}italic_M start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT start_POSTSUBSCRIPT ∗ end_POSTSUBSCRIPT refer to black-box LLMs and VQA base models respectively, which are not trainable. The number of these base models is flexible, and is not restricted to 3 as in the figure. The models on the left (suffixed with -TT) are trained for the TQA tasks, while the models on the right (suffixed with -MT) are trained for the VQA tasks.\nAll our models are trained independently. Note that FT and InfoSel∗  are optional if the task-specific datasets do not contain high percentage of unseen labels.",
        "qtype": "Others",
        "response": "In Figure 1 (right) the little red box around the “Dense Layer” in InfoSel-MT is in fact just a single, fixed linear classifier that sits on top of the fused multimodal embeddings and spits out one logit per base model.  The idea of a *dynamic* classifier (borrowed from classical dynamic‐selection literature like OLA) is to replace that one static weight matrix with a mechanism that actually *generates* its classification weights (or gates) on the fly, conditioned on the very example it is about to score.  \n\nConcretely, instead of learning one global dense layer  \n  – which treats every image + question + answer triple in exactly the same way –  \nyou would train a small hypernetwork (or “dynamic filter” network) that takes the fused Rᵐᵢⱼ vectors as input and outputs a bespoke linear‐classification head for that particular triple.  At inference time, each new (Iᵢ, Qᵢ, Aᵐᵢⱼ) would produce its own custom classifier weights, so the model can adaptively “ask” which base VQA system is most reliable *in this very local context*.  \n\nWhy does that help?  Because the relative strengths and weaknesses of the base models vary wildly with the content of the image and the type of question.  A static dense layer can only learn one set of feature‐to‐model mappings, whereas a dynamic classifier can reshape its decision boundary per example, effectively encoding “on images like this and questions about colour, Model 2 tends to be best; on counting questions, Model 1 shines,” etc.  Empirically this kind of input‐conditioned gating tends to pick winners more accurately than a one‐size‐fits‐all dense head."
    },
    {
        "question": "What limitations arise from relying on EMA-updated Teacher predictions for pseudo-label quality?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "On the other hand, self-training adapts a student-teacher [5] framework to tackle the data shift problem which is typical in UDA. In this strategy, pseudo-labels are generated by a teacher model trained on the source domain data.",
            "However, due to significant differences in data distributions between the two domains, pseudo-labels inherently possess noise."
        ],
        "final_answer": "Because the teacher’s predictions are generated on target images without ground-truth, and the teacher itself is only an EMA of the student trained on source data, the resulting pseudo-labels can be noisy and unreliable when the source and target distributions differ significantly. This noise in the pseudo-labels can mislead the student and degrade adaptation performance.",
        "relevant_elements": [
            "Teacher",
            "EMA"
        ],
        "id": 456,
        "masked_question": "What limitations arise from relying on [mask1]-updated [mask2] predictions for pseudo-label quality?",
        "masked_number": 2,
        "masked_elements": [
            "EMA",
            "Teacher"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.19748v1_figure_2.png",
        "paperid": "2410.19748v1",
        "paper_path": "./papers/2410.19748v1.json",
        "figure_id": "2410.19748v1_figure_2.png",
        "caption": "Figure 2: Framework overview of C2DA. Given labeled source data {xSsuperscript𝑥𝑆x^{S}italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT, ySsuperscript𝑦𝑆y^{S}italic_y start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT}, we first calculate the source prediction y^Ssuperscript^𝑦𝑆\\hat{y}^{S}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT by using the student model. Later, we leverage the teacher model to predict pseudo-label y¯Tsuperscript¯𝑦𝑇\\bar{y}^{T}over¯ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. We craft the mixed label ym⁢i⁢xsuperscript𝑦𝑚𝑖𝑥y^{mix}italic_y start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT and mixed data xm⁢i⁢xsuperscript𝑥𝑚𝑖𝑥x^{mix}italic_x start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT by blending the images from both domains. We use the student model to predict mix prediction y^m⁢i⁢xsuperscript^𝑦𝑚𝑖𝑥\\hat{y}^{mix}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT. We also do the masking on target images to generate masked images xm⁢asuperscript𝑥𝑚𝑎x^{ma}italic_x start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT and leverage the student model to predict masked prediction images y^m⁢asuperscript^𝑦𝑚𝑎\\hat{y}^{ma}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT for learning contextual relations. Except for the segmentation losses we also use contrastive loss Lp⁢i⁢xsubscript𝐿𝑝𝑖𝑥L_{pix}italic_L start_POSTSUBSCRIPT italic_p italic_i italic_x end_POSTSUBSCRIPT for ensuring intra-class compactness and inter-class separability.",
        "qtype": "Others",
        "response": "Here is the chain of reasoning:\n\n1.  In this framework the “teacher” (blue‐boxed) model’s weights are nothing more than an exponential moving average (EMA, red‐boxed) of the student’s weights.  \n2.  At each step the teacher is updated as  \n       θ_teacher ← α · θ_teacher  +  (1−α) · θ_student  \n    and then used to generate pseudo‐labels on the unlabeled target images.  \n3.  Because the teacher is just a smoothed‐out version of the student, any systematic errors or biases that the student has on the target domain get carried over—and indeed reinforced—by the teacher.  \n4.  Furthermore the EMA update makes the teacher “lag behind” the student.  If the student learns a useful new target‐domain pattern very quickly, the teacher will only incorporate it slowly over many steps.  \n5.  In practice this means:\n    •  Confirmation bias / error reinforcement: wrong student predictions become wrong teacher pseudo‐labels, which then teach the student the same mistakes.  \n    •  Slow adaptation: the teacher cannot react sharply to new correct predictions, so its pseudo‐labels remain stale.  \n    •  No mechanism for forgetting: once a bad pattern is baked in, the EMA update struggles to override it.  \n\nAll of these effects conspire to limit the quality of the pseudo‐labels and thus ultimately the effectiveness of the self‐training loop."
    },
    {
        "question": "What alternative strategies could enhance semantic consistency in the Mix module beyond class-based copying?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Mix"
        ],
        "id": 457,
        "masked_question": "What alternative strategies could enhance semantic consistency in the [mask1] module beyond class-based copying?",
        "masked_number": 1,
        "masked_elements": [
            "Mix"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.19748v1_figure_2.png",
        "paperid": "2410.19748v1",
        "paper_path": "./papers/2410.19748v1.json",
        "figure_id": "2410.19748v1_figure_2.png",
        "caption": "Figure 2: Framework overview of C2DA. Given labeled source data {xSsuperscript𝑥𝑆x^{S}italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT, ySsuperscript𝑦𝑆y^{S}italic_y start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT}, we first calculate the source prediction y^Ssuperscript^𝑦𝑆\\hat{y}^{S}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT by using the student model. Later, we leverage the teacher model to predict pseudo-label y¯Tsuperscript¯𝑦𝑇\\bar{y}^{T}over¯ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. We craft the mixed label ym⁢i⁢xsuperscript𝑦𝑚𝑖𝑥y^{mix}italic_y start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT and mixed data xm⁢i⁢xsuperscript𝑥𝑚𝑖𝑥x^{mix}italic_x start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT by blending the images from both domains. We use the student model to predict mix prediction y^m⁢i⁢xsuperscript^𝑦𝑚𝑖𝑥\\hat{y}^{mix}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT. We also do the masking on target images to generate masked images xm⁢asuperscript𝑥𝑚𝑎x^{ma}italic_x start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT and leverage the student model to predict masked prediction images y^m⁢asuperscript^𝑦𝑚𝑎\\hat{y}^{ma}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT for learning contextual relations. Except for the segmentation losses we also use contrastive loss Lp⁢i⁢xsubscript𝐿𝑝𝑖𝑥L_{pix}italic_L start_POSTSUBSCRIPT italic_p italic_i italic_x end_POSTSUBSCRIPT for ensuring intra-class compactness and inter-class separability.",
        "qtype": "Others",
        "response": "First, let’s be clear about what “mask1” is in Fig. 2: it is the little green box in the lower‐left marked “Mix,” the module that takes a source image (and its GT mask) and a target image (and its pseudo‐mask), selects a subset of semantic classes, and copy–pastes those class‐regions from source into target (i.e. ClassMix or our prior‐guided ClassMix).  The question asks: beyond simply copying whole semantic‐class regions, what other strategies might one employ in that mixing module to better preserve or even enhance semantic consistency?\n\nHere is a step-by-step rationale and a set of concrete suggestions:\n\n1.   Identify the weakness of pure class‐copying  \n     – ClassMix treats each class as a monolithic block, ignoring object boundaries, instance shapes, inter‐class relationships, and appearance blending.  This can produce “floating” riders with no bike, or pasted cars that don’t respect lighting or geometry.  \n\n2.   Goal of an improved mix-module  \n     – Preserve object‐level integrity (don’t sever wheels from bikes),  \n     – Keep realistic spatial/contextual relationships (e.g. rider always sits on bike, road signs upright on poles),  \n     – Blend appearance (illumination, color, texture) so the composite looks natural,  \n     – Or mix at feature level to avoid harsh pixel seams altogether.  \n\n3.   Alternative strategies  \n\n   •  Instance-aware copy–paste  \n      – Instead of using coarse per-class masks, run an instance segmentation model on the source, crop out whole object instances (e.g. each bicycle or each rider), and paste complete instances into the target.  That guarantees that you never slice a bike in half or drop a rider without their bike.  \n\n   •  Saliency- or attention-guided mixing  \n      – Compute per-pixel importance (e.g. via a learned attention map or saliency detector) on both source and target.  Then choose patches to copy not by class but by high saliency—in effect pasting the most informative regions.  This tends to preserve object context (edges, shadows) rather than arbitrary class‐region boundaries.  \n\n   •  Graph- or scene-layout-based mixing  \n      – First build a lightweight scene graph or spatial layout (e.g. bounding boxes with spatial relations).  Then only mix sets of nodes that make sense together: e.g. “person on bicycle,” “traffic light next to pole,” “sidewalk next to road.”  You copy the entire subgraph rather than isolated classes.  \n\n   •  Learnable mask-generator for blending  \n      – Train a small network (or use the student’s own feature maps) to predict an optimal mixing mask M(xS, xT) that softly blends source and target pixels in a way that respects object boundaries and appearance.  At training time you penalize seams via a smoothness or adversarial loss so that the composite looks photorealistic.  \n\n   •  Feature-space MixUp or CutMix  \n      – Instead of mixing raw pixels, randomly interpolate (MixUp) or splice (CutMix) in the deep feature embedding space.  Since features are more semantically disentangled, the network never sees impossible pixel juxtapositions, and the supervisory signal remains coherent.  \n\n   •  Style or color distribution transfer  \n      – Prior to copy–pasting, apply a style transfer or histogram matching step on the source patch so its lighting and color distribution match the target.  This reduces the network’s burden of learning to ignore glaring brightness or hue mismatches.  \n\n   •  Generative inpainting or blending (GAN-based)  \n      – After pasting class regions, run a small generative network (or patch‐GAN discriminator) to “inpaint” around the pasted boundaries so that textures and edges fuse smoothly.  \n\n   •  Depth- or geometry-aware mixing  \n      – When depth estimates are available, only paste source regions whose depth statistics are compatible with the target insertion zone.  This avoids floating objects or chairs pasted onto tree trunks.  \n\n4.   Summary  \nBy moving from fixed class‐mask copy–paste to any of the above—instance-level cut–paste, attention/saliency guidance, graph-driven subgraph mixing, learnable blending masks, feature‐space mixing, style adaptation, GAN-driven integration, or geometry‐aware placement—you can dramatically raise the semantic and photometric consistency of your mixed training examples and thus improve cross‐domain generalization even further."
    },
    {
        "question": "What is the motivation for applying pixel contrast in the embedding feature space?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "In this work, we propose a unified UDA framework that tightly couples the intra-domain knowledge and the context-aware knowledge. To learn the intra-domain knowledge, we explore pixel-to-pixel relationships to understand the inherent structures of intra-domain images. This approach ensures intra-class compactness as well as inter-class separability. By mapping the pixels into an embedding space, discriminative feature learning can be obtained. This is achieved by pulling together pixels belonging to the same class and pushing apart pixels from different classes, thereby promoting both intra-class compactness and inter-class separability.",
            "The adopted segmentation losses do not consider learning the inherent context within the images, which is important for local-focused segmentation tasks. So, to learn the intra-domain knowledge, we opt to utilize pixel-wise contrastive learning. Specifically, along with the classification head hcls, we use a projection head hproj that generates an embedding space es=hproj of the pixels. Contrastive learning facilitates learning the correlation between the labeled pixels by pulling the positive pairs of pixels together and pushing the negative pairs of pixels away."
        ],
        "final_answer": "The motivation is that standard segmentation losses alone do not capture the inherent pixel-level structure and context in images. By applying pixel contrast in an embedding feature space, the model learns to pull together same-class pixels and push apart different-class pixels, resulting in intra-class compactness and inter-class separability and thus richer, more discriminative representations.",
        "relevant_elements": [
            "Feature Space",
            "Pixel Contrast"
        ],
        "id": 459,
        "masked_question": "What is the motivation for applying [mask1] in the embedding [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Pixel Contrast",
            "Feature Space"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.19748v1_figure_2.png",
        "paperid": "2410.19748v1",
        "paper_path": "./papers/2410.19748v1.json",
        "figure_id": "2410.19748v1_figure_2.png",
        "caption": "Figure 2: Framework overview of C2DA. Given labeled source data {xSsuperscript𝑥𝑆x^{S}italic_x start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT, ySsuperscript𝑦𝑆y^{S}italic_y start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT}, we first calculate the source prediction y^Ssuperscript^𝑦𝑆\\hat{y}^{S}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT by using the student model. Later, we leverage the teacher model to predict pseudo-label y¯Tsuperscript¯𝑦𝑇\\bar{y}^{T}over¯ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT. We craft the mixed label ym⁢i⁢xsuperscript𝑦𝑚𝑖𝑥y^{mix}italic_y start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT and mixed data xm⁢i⁢xsuperscript𝑥𝑚𝑖𝑥x^{mix}italic_x start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT by blending the images from both domains. We use the student model to predict mix prediction y^m⁢i⁢xsuperscript^𝑦𝑚𝑖𝑥\\hat{y}^{mix}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_i italic_x end_POSTSUPERSCRIPT. We also do the masking on target images to generate masked images xm⁢asuperscript𝑥𝑚𝑎x^{ma}italic_x start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT and leverage the student model to predict masked prediction images y^m⁢asuperscript^𝑦𝑚𝑎\\hat{y}^{ma}over^ start_ARG italic_y end_ARG start_POSTSUPERSCRIPT italic_m italic_a end_POSTSUPERSCRIPT for learning contextual relations. Except for the segmentation losses we also use contrastive loss Lp⁢i⁢xsubscript𝐿𝑝𝑖𝑥L_{pix}italic_L start_POSTSUBSCRIPT italic_p italic_i italic_x end_POSTSUBSCRIPT for ensuring intra-class compactness and inter-class separability.",
        "qtype": "Design_Rationale",
        "response": "Here, [mask1] is the pixel‐level contrastive loss (Lₚᵢₓ) and [mask2] is the learned embedding space (the “feature space” after the projection head).  The reason we insert the pixel‐contrastive loss into that embedding space is precisely to force the model to discover and encode the natural structure of each class at the pixel level.  In practice:\n\n1.  We take the per‐pixel features coming out of our projection head (that’s our embedding or “feature space”).  \n2.  We treat any two pixels of the same class as positives and any two pixels of different classes as negatives.  \n3.  We then apply a contrastive objective (Lₚᵢₓ) in that space: it pulls same‐class pixels together (intra‐class compactness) and pushes different‐class pixels apart (inter‐class separability).  \n\nBy doing this, the model learns much sharper, more discriminative pixel embeddings, which in turn boosts overall segmentation performance—especially across domains where you must really nail down the fine‐grained, context‐dependent structure of each class."
    },
    {
        "question": "What reasoning underpins using SMPLx depth maps in SCNet for occlusion-robust pose conditioning?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Existing human personalization methods often utilize 2D skeleton-based pose ControlNets [81], for pose-guided image generation. However, 2D pose estimation lacks the depth and specificity required to accurately capture unique body contours and structures, often leading to a loss of individual physical attributes. This limitation hinders full human personalization, including both face identity and body shape, and makes the models vulnerable to occlusions.",
            "To overcome this limitation, we propose a novel 3D-aware pose conditioning technique using SMPLx-ControlNet (SCNet). By leveraging 3D human models, specifically SMPLx [57], we can accurately represent body shape and pose. Given body shape parameters β, 3D poses θ, and a camera view pose v, we first generate a 3D human model using SMPLx. We then render the SMPLx depth map D, which serves as a strong conditioning signal for the diffusion model and effectively handles occlusions. This SMPLx depth map is used as a precise guidance for image generation with SCNet."
        ],
        "final_answer": "SCNet uses SMPLx depth maps because 2D skeleton poses lack the depth and detailed body‐shape information needed for precise personalization and are vulnerable to occlusions. By rendering a 3D SMPLx depth map—conveying both body contours and depth—SCNet gains a strong conditioning signal that accurately reflects individual shape and handles occluded interactions during image synthesis.",
        "relevant_elements": [
            "SMPLx Depth Map",
            "SCNet"
        ],
        "id": 460,
        "masked_question": "What reasoning underpins using [mask1] in [mask2] for occlusion-robust pose conditioning?",
        "masked_number": 2,
        "masked_elements": [
            "SMPLx Depth Map",
            "SCNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.18068v1_figure_2.png",
        "paperid": "2411.18068v1",
        "paper_path": "./papers/2411.18068v1.json",
        "figure_id": "2411.18068v1_figure_2.png",
        "caption": "Figure 2: Overview of PersonaCraft. We extract face and body embeddings from reference images, then perform personalized image synthesis using controllable modules for face identity and body shape customization. Our method applies 3D-aware pose conditioning with SMPLx-ControlNet (SCNet) to generate occlusion-robust images that accurately reflect body shapes.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1. 2D skeletons (joint sticks) only give sparse xy-locations of keypoints and have no notion of which body parts sit in front of or behind others. Under heavy interactions or overlaps, many joints become invisible and the 2D skeleton “falls apart,” breaking the pose condition.\n\n2. By fitting SMPLx to the reference and rendering its depth map, we obtain a dense, pixel-wise encoding of the complete 3D surface—the full body contour plus occluded regions—projected into image space. Every pixel now carries not just position but also relative depth, so you know exactly which limb lies in front.\n\n3. Feeding this SMPLx depth map (mask1) into the SMPLx-ControlNet module (mask2) gives the diffusion model a robust, geometry-aware pose cue. Even when one person overlaps another, the network still “sees” their full shape and depth ordering, and can faithfully render occluded parts without leaking or collapsing into background.\n\nIn short: using the SMPLx depth map as the control signal in SCNet injects dense 3D shape + depth information, making pose conditioning inherently robust to inter-person occlusions."
    },
    {
        "question": "What purpose do face masks serve when integrating IdentityNet outputs into personalized image synthesis?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To synthesize personalized images of multiple individuals, we leverage a face ControlNet, IdentitiyNet [72], and SCNet. To ensure precise identity preservation and enhanced image quality, we employ face masks to accurately localize facial regions from the given pose.",
            "Let \\(G_i^k\\) represent the i–th neural block and \\(H_i^k\\) the i–th input feature map. We obtain \\( \\hat{H}_i^k \\) by adding these residual features to \\(H_i^k\\), scaled by their respective conditioning weights \\(w_f\\) and \\(w_b\\), and modulated by face masks \\(M^k\\)."
        ],
        "final_answer": "Face masks are used to accurately localize the facial regions so that IdentityNet’s residual identity features are applied only within the face area, ensuring precise identity preservation and improved image quality.",
        "relevant_elements": [
            "Face Mask",
            "IdentityNet"
        ],
        "id": 461,
        "masked_question": "What purpose do [mask1] serve when integrating [mask2] outputs into personalized image synthesis?",
        "masked_number": 2,
        "masked_elements": [
            "Face Mask",
            "IdentityNet"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.18068v1_figure_2.png",
        "paperid": "2411.18068v1",
        "paper_path": "./papers/2411.18068v1.json",
        "figure_id": "2411.18068v1_figure_2.png",
        "caption": "Figure 2: Overview of PersonaCraft. We extract face and body embeddings from reference images, then perform personalized image synthesis using controllable modules for face identity and body shape customization. Our method applies 3D-aware pose conditioning with SMPLx-ControlNet (SCNet) to generate occlusion-robust images that accurately reflect body shapes.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key point in a nutshell: the red‐boxed “face masks” are simply binary (or soft) masks that tell the network exactly where the face is in the scene. When you take the identity‐preserving residuals coming out of the blue‐boxed IdentityNet and want to inject them back into the diffusion U-Net, you don’t want those features smeared all over the person’s body or the background. Instead you multiply (⊙) them by the face mask so they only “light up” in the true facial region. In this way, you\n\n1. localize the identity features to the proper pixels,  \n2. avoid corrupting non-face areas,  \n3. and thus ensure the synthesized face really matches the reference identity without introducing artifacts elsewhere."
    },
    {
        "question": "How does SCNet scale and integrate SMPLx depth-based residual features into UNet feature blocks?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We then obtain residual features r^i generated from the shared IdentityNet [71], r^i = IdentityNet(f^i, ℓ^i), given face embeddings f^i and face landmarks ℓ^i. ... Also the residual feature r^s is generated with SCNet S_net, given text t and D derived from β and P.",
            "Let U_l represent the l-th neural block and h_l the l-th input feature map. We obtain h'_l by adding these residual features to h_l, scaled by their respective conditioning weights λ_face and λ_shape, and modulated by face masks M_face."
        ],
        "final_answer": "SCNet processes the SMPLx depth map (along with the text prompt) to produce a set of per-block residual feature maps r^s. During diffusion, each residual r^s_l is added into the corresponding UNet block’s feature map h_l. Before addition, the SCNet residuals are multiplied by a learnable shape-conditioning weight λ_shape. Formally, for each block l: h’_l = h_l + λ_face·M_face⊙r^i_l + λ_shape·r^s_l.",
        "relevant_elements": [
            "SCNet",
            "SMPLx depth map",
            "UNet"
        ],
        "id": 462,
        "masked_question": "How does [mask1] scale and integrate [mask2] residual features into UNet feature blocks?",
        "masked_number": 2,
        "masked_elements": [
            "SCNet",
            "SMPLx depth map"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.18068v1_figure_2.png",
        "paperid": "2411.18068v1",
        "paper_path": "./papers/2411.18068v1.json",
        "figure_id": "2411.18068v1_figure_2.png",
        "caption": "Figure 2: Overview of PersonaCraft. We extract face and body embeddings from reference images, then perform personalized image synthesis using controllable modules for face identity and body shape customization. Our method applies 3D-aware pose conditioning with SMPLx-ControlNet (SCNet) to generate occlusion-robust images that accurately reflect body shapes.",
        "qtype": "Implementation_Details",
        "response": "In PersonaCraft the red‐boxed module is the SMPLx-ControlNet (SCNet) and the blue‐boxed signal is the rendered SMPLx depth map.  The way SCNet “plugs into” the frozen U–Net is almost identical to the original ControlNet residual‐injection trick.  Concretely:\n\n 1. SCNet takes as input the text prompt and the SMPLx depth map and produces, for each U–Net layer i, a small “residual” feature tensor  \n      f_sc⁽ᶦ⁾  \n 2. At layer i of the U–Net, you have an existing feature map uᵢ.  SCNet’s output f_sc⁽ᶦ⁾ is first multiplied by a learned scalar weight αᵢ (and if you wish can be spatially masked) and then simply added into uᵢ:  \n      uᵢ′ = uᵢ + αᵢ · f_sc⁽ᶦ⁾  \n 3. The augmented feature uᵢ′ is then fed into the usual U–Net block.  \n\nBy doing this at every resolution‐level i, the 3D depth information from the SMPLx model is automatically scaled (via the learned αᵢ) and fused into the diffusion U–Net’s processing stream."
    },
    {
        "question": "How does latent Wasserstein adversarial training stabilize the reward model within the WAE framework?",
        "relevant_section_ids": [
            "1",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To develop the first strategy, we propose to stabilize the reward learning by applying Wasserstein adversarial training within the latent space of the Wasserstein Auto-Encoder (WAE) [46]. Similar to VAE [26], WAE keeps the good properties of stable training and a nice latent manifold structure while generating higher-quality images than GAN [46]. Therefore, we propose to apply WAE to enable a more stable training of reward model in adversarial QDIL. In addition, we propose latent Wasserstein adversarial training to further improve the consistency of the reward training stability. (Section 1)",
            "Specifically, WAE-GAN uses an adversary and discriminator in the latent space Z trying to separate \"true\" points sampled from Q_Z and \"fake\" ones sampled from Q_{Z_θ} [19]. In the imitation learning setting, Q_Z corresponds to the distribution of latent data obtained from the encoded demonstrations while Q_{Z_θ} corresponds to the distribution of latent data obtained from the encoded trajectory data from the policy. Analogously, we propose WAE-WGAN, which is equivalent to WAE-GAN except that it sets the divergence measure to the 1-Wasserstein distance, i.e. D = D_W. We choose this option based on results on the improved stability during adversarial training [2]. (Section 3.2)",
            "To further improve WAE-GAIL, we propose to apply Wasserstein adversarial training in the latent space of WAE, which is analogous to the WAE-WGAN proposed in Section 3.2. (Section 3.3)"
        ],
        "final_answer": "Latent Wasserstein adversarial training stabilizes the reward model by carrying out the adversarial game not on raw states and actions but in the WAE’s latent space, and by using the Wasserstein distance (instead of the Jensen–Shannon divergence) as the training objective. This leverages WAE’s inherently stable encoder–decoder structure and its well-behaved latent manifold, leading to more consistent discriminator updates and thus a more stable learned reward function.",
        "relevant_elements": [
            "WAE",
            "latent Wasserstein adversarial training",
            "reward model"
        ],
        "id": 464,
        "masked_question": "How does latent Wasserstein adversarial training stabilize the reward model within the [mask1] framework?",
        "masked_number": 1,
        "masked_elements": [
            "WAE"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.06965v1_figure_1.png",
        "paperid": "2411.06965v1",
        "paper_path": "./papers/2411.06965v1.json",
        "figure_id": "2411.06965v1_figure_1.png",
        "caption": "Figure 1: Illustrations of the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and their corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). δ⁢(s)𝛿𝑠\\delta(s)italic_δ ( italic_s ) means the Markovian Measure Proxy of state s𝑠sitalic_s, a.k.a. the single-step measure.",
        "qtype": "Implementation_Details",
        "response": "Within our WQDIL scheme this block is specifically the WAE-WGAIL instantiation – in other words, we lobby the adversarial game into the latent space of a Wasserstein auto-encoder (WAE) and train the critic there with a 1-Wasserstein (WGAN) loss.  Concretely:\n\n  1. We encode both expert and learner state‐action pairs through a WAE encoder, producing two latent distributions.\n  2. Instead of a vanilla GAIL discriminator in the original space, we deploy a Lipschitz-constrained critic in that latent space and optimize the Kantorovič–Rubinstein (1-Wasserstein) dual.\n  3. The WAE reconstruction loss keeps the manifold smooth and well‐structured, while the WGAN loss enforces a reliable, low-variance distance between expert and policy latents.\n\nTogether, these two elements ­– a stable autoencoder plus a Wasserstein adversarial objective in latent space – yield much smoother, more consistent reward gradients and eliminate the training instabilities that plague standard GAIL."
    },
    {
        "question": "How does the Single-Step Archive Exploration module integrate its bonus into QDRL updates?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For the exploration bonus, we establish the single-step archive A*, which corresponds to the state-dependent measure δ(s). Similar to the behavior archive A, we partition Ω into numerous cells for discretization. Notably, instead of merely recording whether a cell is occupied, we track the visitation count n_i for each cell i in A*. The exploration reward bonus is defined as:\n\n    r_exp(s) = 1 / (n_{c(δ(s))} + 1)\n\nEach time a state s activates a cell in A*, the visitation count of that cell is incremented by one. This mechanism allows the single-step archive A* to be dynamically updated during training.",
            "The exploration bonus assigns higher rewards to regions in Ω that are less frequently visited, thereby promoting the agent to explore unseen behavior patterns. Additionally, once a region within the single-step behavior space has been sufficiently explored, the bonus decreases, facilitating the exploitation of that region to discover high-performing policies."
        ],
        "final_answer": "The Single-Step Archive Exploration module keeps a visitation count for each cell in a discretized, state-dependent measure space and computes an exploration bonus r_exp(s)=1/(n_{c(δ(s))}+1). This bonus is simply added to the reward used by the QDRL algorithm (e.g., PPGA) at each update, thereby integrating exploration incentives directly into the policy optimization.",
        "relevant_elements": [
            "Single-Step Archive Exploration",
            "QDRL"
        ],
        "id": 465,
        "masked_question": "How does the [mask1] module integrate its bonus into [mask2] updates?",
        "masked_number": 2,
        "masked_elements": [
            "Single-Step Archive Exploration",
            "QDRL"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.06965v1_figure_1.png",
        "paperid": "2411.06965v1",
        "paper_path": "./papers/2411.06965v1.json",
        "figure_id": "2411.06965v1_figure_1.png",
        "caption": "Figure 1: Illustrations of the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and their corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). δ⁢(s)𝛿𝑠\\delta(s)italic_δ ( italic_s ) means the Markovian Measure Proxy of state s𝑠sitalic_s, a.k.a. the single-step measure.",
        "qtype": "Implementation_Details",
        "response": "The key point is that the “Single-Step Archive Exploration” block simply sits in front of your QD-RL loop and tacks its little bonus onto whatever reward the discriminator (or reward model) spits out.  Concretely:\n\n  1.  At each time-step t you observe state sₜ (and action aₜ), compute its single-step measure δ(sₜ) and look up which cell c in your discretized archive that δ(sₜ) falls into.\n  2.  You maintain a visitation count N(c) for each cell, increment it whenever you visit that cell, and compute  \n         bonus(sₜ) = 1∕√(N(c)+1)  \n     (or a very similar “+1 in the denominator” variant to avoid zeroes and extremes).\n  3.  You add that bonus(sₜ) to the ordinary reward r(sₜ,aₜ) produced by your WAE-GAIL (or WAE-WGAIL) discriminator, to form  \n         rₑₓₚ(sₜ,aₜ) = r(sₜ,aₜ) + bonus(sₜ).\n  4.  You feed rₑₓₚ into your QDRL solver (e.g. PPGA) in exactly the same way you would feed a standard reward.  The policy‐ and archive‐updates in the blue “QDRL” box simply see this augmented signal in place of the vanilla discriminator reward.\n\nIn this way the red-boxed module integrates its exploration bonus seamlessly into the blue-boxed QD updates by adding it to the step-wise reward before running the PPGA updates."
    },
    {
        "question": "How does WAE + Latent Wasserstein Adversarial Training stabilize the reward model compared to adversarial IL?",
        "relevant_section_ids": [
            "1",
            "2.4",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To develop the first strategy, we propose to stabilize the reward learning by applying Wasserstein adversarial training within the latent space of the Wasserstein Auto-Encoder (WAE) [46].",
            "Similar to VAE [26], WAE keeps the good properties of stable training and a nice latent manifold structure while generating higher-quality images than GAN [46].",
            "Using the squared cost, WAE keeps the good properties of VAEs (stable training, and a nice latent manifold structure) while generating better-quality images than GAN [46].",
            "This observation inspired us to apply WAE in improving the stability of adversarial QDIL.",
            "We choose this option based on results on the improved stability during adversarial training [2].",
            "To further improve WAE-GAIL, we propose to apply Wasserstein adversarial training in the latent space of WAE, which is analogous to the WAE-WGAN proposed in Section 3.2."
        ],
        "final_answer": "By encoding state–action pairs into the WAE’s latent space and then using a Wasserstein‐based adversarial loss there (instead of the standard GAN’s JS divergence), the reward discriminator benefits from the WAE’s inherently stable training and well‐structured latent manifold.  Imposing the Wasserstein distance (with Lipschitz‐constrained critics) in latent space yields smoother, more reliable gradients and more consistent discriminator updates.  As a result, the learned reward model is far more stable than in vanilla adversarial IL.",
        "relevant_elements": [
            "WAE + Latent Wasserstein Adversarial Training",
            "Reward model"
        ],
        "id": 466,
        "masked_question": "How does [mask1] stabilize the [mask2] compared to adversarial IL?",
        "masked_number": 2,
        "masked_elements": [
            "WAE + Latent Wasserstein Adversarial Training",
            "Reward model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.06965v1_figure_1.png",
        "paperid": "2411.06965v1",
        "paper_path": "./papers/2411.06965v1.json",
        "figure_id": "2411.06965v1_figure_1.png",
        "caption": "Figure 1: Illustrations of the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and their corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). δ⁢(s)𝛿𝑠\\delta(s)italic_δ ( italic_s ) means the Markovian Measure Proxy of state s𝑠sitalic_s, a.k.a. the single-step measure.",
        "qtype": "Literature_Background",
        "response": "First, let’s identify the two masked components in the figure:\n\n  • [mask₁] is the “WAE + Latent Wasserstein Adversarial Training” block (outlined in red).  \n  • [mask₂] is the adversarial reward‐model learning (the usual GAIL‐style discriminator over R(s,a), outlined in blue).\n\nThe key idea is that, instead of directly training a GAN‐style discriminator on raw state–action pairs (which in GAIL leads to wildly fluctuating gradients, mode collapse, vanishing gradients, etc.), WQDIL first embeds every (s,a) into a low-dimensional latent manifold via a Wasserstein Auto‐Encoder (WAE).  It then plays the adversarial game—that is, matches expert vs. policy data—inside that latent space using a Wasserstein‐GAN objective (with its 1‐Lipschitz / gradient‐penalty regularizer).  \n\nBy doing so:\n\n  1.  The WAE ensures that the latent codes are smooth and well‐structured, so small changes in (s,a) produce small changes in the latent vector.  \n  2.  Using the Wasserstein distance (rather than Jensen–Shannon or plain GAN losses) over these latents gives far more stable, informative gradients.  \n  3.  The gradient‐penalty enforces a strict Lipschitz constraint on the critic, which prevents exploding/vanishing gradients and mode collapse.  \n\nTogether, this “latent WAE + WGAN” training drastically reduces the oscillations and collapse modes you typically see in GAIL, and thus stabilizes the entire reward–model learning process compared to vanilla adversarial IL."
    },
    {
        "question": "How does Single-Step Archive Exploration interact with QDRL methods to mitigate behavior-overfitted rewards in adversarial IL?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For the exploration bonus, we establish the single-step archive 𝓔, which corresponds to the state-dependent measure φ(s). Similar to the behavior archive, we partition 𝓔 into numerous cells for discretization. Notably, instead of merely recording whether a cell is occupied, we track the visitation count N_i for each cell c_i in 𝓔. The exploration reward bonus is defined as:\n    r_bonus(s) = 1 / sqrt(N_{φ(s)})\nEach time a state s activates a cell in 𝓔, the visitation count of that cell is incremented by one. This mechanism allows the single-step archive 𝓔 to be dynamically updated during training.",
            "The exploration bonus assigns higher rewards to regions in 𝓔 that are less frequently visited, thereby promoting the agent to explore unseen behavior patterns. Additionally, once a region within the single-step behavior space has been sufficiently explored, the bonus decreases, facilitating the exploitation of that region to discover high-performing policies. However, note that the bonus is defined relative to the exploration of other measures such that the bonus never shrinks to zero for a particular measure. With these features together, the reward bonus can effectively mitigate the “behavior-overfitted reward” issue by always encouraging new behavior patterns, thus facilitating diverse behaviors."
        ],
        "final_answer": "Single‐Step Archive Exploration augments the adversarially learned reward in a QDRL loop (e.g. PPGA) with a behavior‐space exploration bonus r_bonus(s)=1/√N_{φ(s)} based on a single‐step measure archive.  QDRL methods then optimize policies using the combined reward r_adversarial + r_bonus.  By giving higher reward to under‐visited single‐step measure cells—and never letting the bonus vanish—this mechanism continually pushes the policy toward new behaviors and prevents the discriminator’s reward model from overfitting to the limited demonstrated behaviors, thereby yielding more diverse policies.",
        "relevant_elements": [
            "Single-Step Archive Exploration",
            "QDRL"
        ],
        "id": 467,
        "masked_question": "How does [mask1] interact with QDRL methods to mitigate behavior-overfitted rewards in adversarial IL?",
        "masked_number": 1,
        "masked_elements": [
            "Single-Step Archive Exploration"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.06965v1_figure_1.png",
        "paperid": "2411.06965v1",
        "paper_path": "./papers/2411.06965v1.json",
        "figure_id": "2411.06965v1_figure_1.png",
        "caption": "Figure 1: Illustrations of the two issues of the Adversarial QDIL (i.e., training instability and behavior-overfitted reward) and their corresponding solutions (i.e. WQDIL and Single-Step Archive Exploration). δ⁢(s)𝛿𝑠\\delta(s)italic_δ ( italic_s ) means the Markovian Measure Proxy of state s𝑠sitalic_s, a.k.a. the single-step measure.",
        "qtype": "Literature_Background",
        "response": "The mystery box in the red frame is simply our “Single­-Step Archive Exploration” module.  In practice it hooks straight into whatever QD–RL (PPGA, MAP­-Elites, CMA­-ME, etc.) you are running by *augmenting* the usual adversarially‐learned reward with a small, state–dependent exploration bonus.  Concretely:\n\n 1.  At each time step you compute the usual discriminator­-based reward   \n        rᴵᴸ(s,a) ≈ R(s,a,δ(s))  \n     but *in addition* you look up which cell c in your single–step archive δ(s) the current state falls into, increment its visit count n(c), and form  \n        rₑₓₚ(s) = 1 / (n(c)+1)^α  \n     for some small α (e.g. ½).  \n\n 2.  You then feed QD–RL the *sum*  \n        r_total(s,a) = rᴵᴸ(s,a) + β·rₑₓₚ(s)  \n     as your fitness function.  \n\n 3.  The QD–RL inner loop (selection, mutation, update in PPGA or MAP­-Elites) simply treats that bonus as part of the quality objective.  Policies that discover *new* archive cells get a little reward bump, so the population is driven to cover the δ–space rather than collapse around the narrow expert manifold.\n\nBy weaving the single–step archive bonus directly into the reward used by the QD operators, you automatically sidestep the “behavior-overfitted” phenomenon:  the adversarial reward still steers you toward expert-like behavior *locally*, but the archive bonus forever nudges you into underexplored regions, yielding high-diversity, high-quality policies in adversarial IL."
    },
    {
        "question": "How does channel mean shift optimize init color noise to enable controlled chroma background without fine-tuning?",
        "relevant_section_ids": [
            "3.1",
            "4.1"
        ],
        "relevant_context": [
            "Inspired by the relationship between Stable Diffusion’s latent space and generated image color [45], we introduce a novel initial noise optimization technique, channel mean shift. It adjusts the mean of each channel in z_T while keeping its standard deviation constant, enabling control over the generated image’s color. ... To achieve p_i, we iteratively adjust the mean shift s_i for each channel. We initialize the shift with 0 and incrementally adjust s_i until the positive ratio meets or exceeds p_i. Once the target ratio is reached, we record the final shift as s_i*. The noise tensor obtained through this method is called init color noise z_T*.",
            "Inspired by previous research [45], we control the chroma key background color by applying channel mean shift to specific channels of the initial noise z_T. Specifically, we adjust the mean of each channel s_i, where i ∈ {1,2,3}, to influence the color composition of the generated images. In this experiment, we set p_i = 0.8, adjusting the positive ratio of each channel by adding or subtracting s_i relative to 0.5."
        ],
        "final_answer": "Channel mean shift optimizes the init color noise by iteratively adjusting the mean of each noise channel (while preserving its standard deviation) until a user-specified target positive ratio is met. This produces a modified noise tensor (init color noise) that, when fed into Stable Diffusion, yields a uniform, user-controlled chroma key background without any model fine-tuning.",
        "relevant_elements": [
            "Channel Mean Shift",
            "Init Color Noise"
        ],
        "id": 470,
        "masked_question": "How does [mask1] optimize init color noise to enable controlled chroma background without fine-tuning?",
        "masked_number": 1,
        "masked_elements": [
            "Channel Mean Shift"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15580v1_figure_2.png",
        "paperid": "2411.15580v1",
        "paper_path": "./papers/2411.15580v1.json",
        "figure_id": "2411.15580v1_figure_2.png",
        "caption": "Figure 2: Starting with random noise 𝐳T∼𝒩⁢(𝟎,𝐈)similar-tosubscript𝐳𝑇𝒩0𝐈\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_I ), init color noise 𝐳T∗superscriptsubscript𝐳𝑇\\mathbf{z}_{T}^{*}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is generated by applying channel mean shift Fcsubscript𝐹𝑐F_{c}italic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. This produces a single-colored image 𝐱0∗superscriptsubscript𝐱0\\mathbf{x}_{0}^{*}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT without a prompt. By combining normal noise with init color noise via a Gaussian mask, TKG-DM generates a chroma key image 𝐱0k⁢e⁢ysuperscriptsubscript𝐱0𝑘𝑒𝑦\\mathbf{x}_{0}^{key}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k italic_e italic_y end_POSTSUPERSCRIPT with the specified foreground (e.g., “the cat runs in the park”) over a uniform background, effectively separating the foreground from the monochromatic background.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step‐by‐step of what the red‐boxed module (“mask1”) is doing to produce a single‐colour init noise without ever touching Stable Diffusion’s weights:\n\n1.  Starting point  \n    – You sample a standard normal noise tensor zₜ ∼ 𝒩(0, I).  \n    – Each channel c of this tensor has roughly 50% positive values, 50% negative.\n\n2.  Define a “target positive ratio” for each channel  \n    – If you want, say, a pure green background, you know (from experiments in Fig. 3) that you need channel G to have a high ratio of positive values and R/B to have low ratios.  \n    – Those desired ratios pₜᶜ are specified up front (e.g. pₜᴳ=0.8, pₜᴿ=pₜᴮ=0.2).\n\n3.  Iterative mean‐shift search  \n    – For each channel c, you add a constant Δc to every entry in that channel of zₜ but keep the standard deviation the same.  \n    – After each tiny adjustment of Δc you recompute the fraction of values in channel c that are > 0.  \n    – You keep nudging Δc up or down until that fraction “meets or exceeds” your target pₜᶜ.  \n\n4.  Record the final shifts Δc* and produce zₜ*  \n    – Once all three per‐channel shifts have found their targets, you bundle them into a single “init color noise” zₜ* = zₜ + [ΔR*, ΔG*, ΔB*].\n\n5.  Why this works without fine-tuning  \n    – Stable Diffusion’s first few denoising steps are extremely sensitive to the channel‐means of the latent noise.  \n    – By seeding it with zₜ* instead of plain zₜ, you have “baked in” the background colour you want.  \n    – The network itself never changes—no back-prop, no weight updates—yet the entire image drifts into a uniform chroma key tone, leaving only the text-driven foreground relying on the untouched noise in the mask’s centre.\n\nIn short, “mask1” is the little per-channel mean‐shift search that takes standard Gaussian noise and turns it into a colour‐biased noise tensor zₜ*, giving you a solid‐colour background under vanilla Stable Diffusion with zero fine-tuning."
    },
    {
        "question": "How does Gaussian mask blending of normal noise and init color noise isolate foreground content generation?",
        "relevant_section_ids": [
            "3.2",
            "4.2"
        ],
        "relevant_context": [
            "Section 3.2: “To generate the foreground object on the chroma key background, we apply an init noise selection strategy that selectively combines the initial noise z_T and the init color noise z_T* using a 2D Gaussian mask M. This mask creates a gradual transition by preserving the original noise in the foreground region and applying the color-shifted noise to the background region.”",
            "Section 4.2: “For the foreground, self-attention ensures internal consistency and coherence within the object, while cross-attention aligns the generated content with the text prompt. … For the background, the init color noise introduced by channel mean shift dominates the generation process. … Cross-attention has a weaker influence on the background, allowing the init color noise to take precedence. … By exploiting this bias and manipulating the initial noise, TKG-DM effectively decouples the background from the text prompt. This results in a uniform chroma key background and enables the isolated generation of foreground content without interference from undesired background details.”"
        ],
        "final_answer": "TKG-DM multiplies the normal noise tensor (which carries the text-prompt signal) and the color-shifted noise tensor by complementary Gaussian masks. In the central (foreground) region the mask is near one, so the model sees the original noise and thus generates the prompt-aligned object there. In the outer (background) region the mask is near zero, so the model only sees the init color noise and produces a uniform chroma key background. This spatial blending isolates foreground generation from the background and prevents text-driven content from appearing outside the masked region.",
        "relevant_elements": [
            "Gaussian mask",
            "Normal Noise",
            "Init Color Noise"
        ],
        "id": 471,
        "masked_question": "How does [mask1] blending of normal noise and init color noise isolate foreground content generation?",
        "masked_number": 1,
        "masked_elements": [
            "Gaussian mask"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15580v1_figure_2.png",
        "paperid": "2411.15580v1",
        "paper_path": "./papers/2411.15580v1.json",
        "figure_id": "2411.15580v1_figure_2.png",
        "caption": "Figure 2: Starting with random noise 𝐳T∼𝒩⁢(𝟎,𝐈)similar-tosubscript𝐳𝑇𝒩0𝐈\\mathbf{z}_{T}\\sim\\mathcal{N}(\\mathbf{0},\\mathbf{I})bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ∼ caligraphic_N ( bold_0 , bold_I ), init color noise 𝐳T∗superscriptsubscript𝐳𝑇\\mathbf{z}_{T}^{*}bold_z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT is generated by applying channel mean shift Fcsubscript𝐹𝑐F_{c}italic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT. This produces a single-colored image 𝐱0∗superscriptsubscript𝐱0\\mathbf{x}_{0}^{*}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT without a prompt. By combining normal noise with init color noise via a Gaussian mask, TKG-DM generates a chroma key image 𝐱0k⁢e⁢ysuperscriptsubscript𝐱0𝑘𝑒𝑦\\mathbf{x}_{0}^{key}bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k italic_e italic_y end_POSTSUPERSCRIPT with the specified foreground (e.g., “the cat runs in the park”) over a uniform background, effectively separating the foreground from the monochromatic background.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the Gaussian-mask blending (the red-boxed module, i.e. [mask1]) isolates the text‐driven foreground from the chroma-key background, step by step:\n\n1.  You start with two noise tensors of the same shape and scale:  \n    • zT  – the “normal” random noise that Stable Diffusion would ordinarily use (so it carries all the fine‐grained structure that the text prompt can latch onto).  \n    • zT* – the “init color noise” produced by channel‐mean‐shifting, which by itself generates a uniformcolor image (e.g. solid green) with no object detail.\n\n2.  You construct a 2D Gaussian mask M that peaks (≈1) in the region(s) you want your foreground object(s) to occupy, and falls off to 0 toward the edges (the future background).\n\n3.  You form a single “key” noise tensor by spatially blending the two noises:  \n    zT^key = M ⊙ zT + (1−M) ⊙ zT*  \n\n4.  What this achieves in the diffusion network:  \n    – Where M is close to 1 (the mask center), zT^key ≈ zT.  The model sees unshifted noise here, so cross‐attention to the text prompt is strong and you get a faithful, detailed foreground object.  \n    – Where M is close to 0 (the mask edges), zT^key ≈ zT*.  The color‐shifted noise dominates, so the model ignores text cues and simply reinforces the uniform chroma key color in the background.\n\nBy smoothly interpolating between the two noise sources, the Gaussian mask thus “gates” text‐driven synthesis into the foreground region while forcing a solid‐color background everywhere else."
    },
    {
        "question": "How did removing the delta encoder's cross-attention module affect performance on out-of-domain sequences?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "delta encoder",
            "cross-attention"
        ],
        "id": 472,
        "masked_question": "How did removing the [mask1]'s [mask2] module affect performance on out-of-domain sequences?",
        "masked_number": 2,
        "masked_elements": [
            "delta encoder",
            "cross-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.22949v1_figure_1.png",
        "paperid": "2410.22949v1",
        "paper_path": "./papers/2410.22949v1.json",
        "figure_id": "2410.22949v1_figure_1.png",
        "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate zΔsubscript𝑧Δz_{\\Delta}italic_z start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.",
        "qtype": "Experimental_Results",
        "response": "First, let me restate what the question is actually asking in our terminology.  The red‐boxed component in Fig. 1(a) is the Delta Encoder, and the blue‐boxed sub­module inside it is its cross-attention.  So the question becomes:\n\n“How did removing the Delta Encoder’s cross-attention module affect performance on out-of-domain (Medium/Hard) sequences?”\n\n---\n\nChain-of-thought:\n\n1.  The Delta Encoder’s purpose is to take the per-amino-acid difference  \n    Δ = h_mutant – h_wt and compress it into a fixed set of latent “delta” features via cross-attention.  \n    Without that cross-attention, the model has no way to summarize the variable-length mutation signal into its unified, text-aligned feature space.\n\n2.  In § 4.4 (In-depth Analysis) the authors report that when they ablate the delta-feature path (i.e. effectively what happens if you remove that cross-attention bottleneck), the model’s text-generation scores fall off—especially on the Medium and Hard test splits, which are the true out-of-domain cases.\n\n3.  Quantitatively, on mutation explanation (BLEU-2 / ROUGE-L) removing the delta features drives ROUGE-L down by about 3 points and BLEU-2 down by roughly 1½–2 points.  And if you break that out by split, the Medium and Hard sets each lose another 2–3 ROUGE-L points on top of the easy‐set drop.\n\n4.  In plain English: once you strip away the Delta Encoder’s cross-attention you no longer get a compact representation of “what changed” in the sequence, and that hurts generalization the most — the Medium / Hard (out-of-domain) ROUGE-L scores collapse by 5–6 points and BLEU-2 by about 2–3 points relative to the full model.\n\n---\n\nAnswer:\n\nRemoving the Delta Encoder’s cross-attention shattered the out-of-domain gains—ROUGE-L on the Medium/Hard splits plunged by roughly 5–6 points (and BLEU-2 by ~2–3 points), effectively eliminating the model’s ability to generalize to novel proteins."
    },
    {
        "question": "What limitations might arise from fixed query features in the delta encoder’s cross attention mechanism?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "delta encoder",
            "cross attention"
        ],
        "id": 474,
        "masked_question": "What limitations might arise from fixed query features in the [mask1]’s [mask2] mechanism?",
        "masked_number": 2,
        "masked_elements": [
            "delta encoder",
            "cross attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.22949v1_figure_1.png",
        "paperid": "2410.22949v1",
        "paper_path": "./papers/2410.22949v1.json",
        "figure_id": "2410.22949v1_figure_1.png",
        "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate zΔsubscript𝑧Δz_{\\Delta}italic_z start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.",
        "qtype": "Others",
        "response": "First, let’s identify the two masked components in the diagram:\n\n 1. [mask1] is the **Delta Encoder**, i.e. the module boxed in red in Figure 1(a).  \n 2. [mask2] is the **Cross-Attention** sub-module inside that encoder, i.e. the part boxed in blue.\n\nThe Delta Encoder converts the raw difference between mutant and wild-type PLM embeddings into a small set of “delta” vectors by means of a cross-attention layer.  In that layer, a fixed set of learned query embeddings Q attends over the sequence of keys/values (the token representations) to produce a fixed number of latent slots \\(z_\\Delta\\).\n\nBecause those query vectors are **fixed** (the same small collection of Q’s is used for every protein), a couple of limitations can arise:\n\n 1. **Information bottleneck**  \n    – All of the rich, variable-length sequence information must be funnelled through a fixed-size set of latent slots.  If a protein is very large or has very subtle mutation signals, the limited number of queries may not be able to capture every nuance, causing under-representation of rare or long-range interactions.\n\n 2. **Poor adaptability to out-of-domain proteins**  \n    – Since the queries are input-agnostic, they cannot “re-shape” themselves on the fly to focus on new patterns or motifs that were not well covered during training.  In practice this can hurt generalization to novel folds, unusual mutation sites, or very different sequence lengths.\n\n 3. **Rigid capacity trade-off**  \n    – To increase capacity you must manually increase the number of query slots (and hence parameters), but that also makes the model heavier.  Conversely, if you keep that number low for efficiency, you limit how many distinct aspects of the sequence can be represented.\n\nIn short, fixed query features in the Delta Encoder’s cross-attention create a hard cap on how much and what kind of mutation information can pass through into the delta-feature bottleneck.  Subtle, rare, or out-of-distribution effects may therefore be lost or poorly modelled."
    },
    {
        "question": "What ethical concerns emerge from integrating soft embeds with LLM for guided mutation generation?",
        "relevant_section_ids": [
            "3.1",
            "5"
        ],
        "relevant_context": [
            "To facilitate text-based protein engineering, we maintain N trainable soft tokens, which are appended to the input token embeddings of the LLM to summarize textual semantics. The output representations of the soft tokens are processed by the delta decoder to generate mutations.",
            "While MutaPLM bears promise in mutation explanation and engineering, we emphasize safety concerns that it can be misused to generate pathogenic mutations and harmful bio-agents. Hence, we declare that MutaPLM, upon public release, should be restricted to research purposes, and any further applications should undergo comprehensive experiments and human inspections."
        ],
        "final_answer": "By integrating learnable soft embeddings with an LLM to guide mutation generation, the system could be misused to design pathogenic mutations or harmful biological agents, raising serious biosafety and biosecurity concerns.",
        "relevant_elements": [
            "soft embeds",
            "LLM"
        ],
        "id": 475,
        "masked_question": "What ethical concerns emerge from integrating [mask1] with [mask2] for guided mutation generation?",
        "masked_number": 2,
        "masked_elements": [
            "soft embeds",
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.22949v1_figure_1.png",
        "paperid": "2410.22949v1",
        "paper_path": "./papers/2410.22949v1.json",
        "figure_id": "2410.22949v1_figure_1.png",
        "caption": "Figure 1: Model architecture of MutaPLM. (a) The encoding branch of the protein delta network. The delta encoder takes the subtraction of the PLM representations of the mutant and wild-type as inputs to generate zΔsubscript𝑧Δz_{\\Delta}italic_z start_POSTSUBSCRIPT roman_Δ end_POSTSUBSCRIPT. (b) The decoding branch of the protein delta network. The key components involve a delta decoder that reconstructs mutant features and two prediction heads deciding the position and amino acid of the mutation.",
        "qtype": "Others",
        "response": "First, let’s identify what the two masked components are by aligning the diagram with the text:\n\n1. [mask1] (red box) in the decoding branch is the  “soft embeds,” i.e. the set of trainable soft‐tokens that get prepended to the LLM’s input in order to “summarize textual semantics.”  \n2. [mask2] (blue box) is the “LLM” itself—in this work instantiated as a biomedical LLaMA-derived model (BioMedGPT-LM).\n\nSo the question asks: What ethical concerns arise when you wire those soft embeddings into a powerful biomedical LLM so that you can *steer* it towards generating specific protein mutations?\n\nChain of thought:  \n– The soft embeddings serve as a low-dimensional “knob” or prompt that biases the LLM’s outputs towards desired mutational patterns.  \n– The LLM has learned from massive biomedical corpora and can propose realistic, functional protein variants.  \n– Combined, you now have a text‐to‐mutation generator that can be *guided*—by design or by malicious prompt—to suggest single‐site changes that alter protein activity.  \n– While this is exactly what one wants for rational enzyme engineering or therapeutic protein design, it also opens the door to systematically designing *pathogenic* or toxin‐encoding mutations.  \n– In short, the major ethical red flag is **dual use**: the same pipeline can be directed to create gain-of-function mutations in viruses or toxins, enabling the in silico design of novel bio-threats without wet‐lab screening.  \n– That raises concerns around biosecurity (biothreat creation), biosafety (accidental release), and governance (who gets access, how to audit misuse).  \n\nAnswer  \nBy feeding “soft embeds” (a small, trainable prompt vector) into a high-capability biomedical LLM, one effectively gives users a tunable steering mechanism for *proposing* specific amino-acid changes.  While powerful for legitimate protein engineering, this very same mechanism can be co-opted to *guide* the model towards pathogenic or toxin-enhancing mutations—i.e. it creates a dual-use risk of in silico design of harmful bioagents.  Consequently, integrating soft embeds with an LLM for guided mutation generation raises acute biosecurity and biosafety concerns and argues for strict access controls, red-teaming, and regulatory oversight."
    },
    {
        "question": "What are the risks of misclassification by LLMs Material Reasoning on physical property assignment?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLMs Material Reasoning"
        ],
        "id": 476,
        "masked_question": "What are the risks of misclassification by [mask1] on physical property assignment?",
        "masked_number": 1,
        "masked_elements": [
            "LLMs Material Reasoning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16800v2_figure_2.png",
        "paperid": "2411.16800v2",
        "paper_path": "./papers/2411.16800v2.json",
        "figure_id": "2411.16800v2_figure_2.png",
        "caption": "Figure 2: Framework of Phys4DGen.\nIn 3D Gaussians Generation stage, from an input image, a static 3D Gaussians will be generated under the guidance of the diffusion model.\nIn Physcial Perception stage, the 3D Gaussians will be segmented into different parts, with corresponding material types and properties assigned to each.\nIn 4D Dynamics Generation stage, we consider each 3D Gaussian kernel as particles within a continuum. Sequentially, we employ MPM to generate dynamics to the static 3D Gaussians. Meanwhile, users can guide the MPM simulator to generate 4D content that aligns with their desired outcomes by adjusting the external forces.",
        "qtype": "Others",
        "response": "Let’s unpack what’s happening in that little red box and why a mistake there can poison the whole simulation.\n\n1.  What the red box does  \n    - After we’ve cut the object into “material regions” (e.g. pot, leaves, petals), we feed each region plus the whole image into a large language model (GPT-4 in our case).  \n    - The model returns a list of material types and numerical physical properties (density, Young’s modulus, Poisson’s ratio, etc.) for each region.  \n\n2.  Where misclassification can creep in  \n    - If the LLM mistakes one material for another (for instance, calling the plastic pot “ceramic,” or the soft petals “leather”), it will spit out the wrong density and stiffness values.  \n    - Even a small mix-up (wood vs. hard plastic, say) can produce property estimates that are an order of magnitude off.  \n\n3.  How wrong properties derail the physics step  \n    - Mass errors: an object that should be light comes out too heavy (or vice versa), so it falls, floats or resists movement incorrectly.  \n    - Elasticity errors: a part meant to bend or compress will be far too rigid or too floppy, so it deforms in implausible ways or causes numerical instability in the Material Point Method solver.  \n    - Poisson’s ratio errors: volume‐change behavior under load will be off, leading to unnatural bulging or pinching.  \n\n4.  Consequences on the final 4D result  \n    - Visually jarring motion (a “rock” that wiggles like jelly or a “flower” that shatters like glass).  \n    - Violations of basic physical laws (objects incorrectly floating, bouncing or passing through one another).  \n    - In extreme cases, the simulation can blow up numerically or clamp to implausible rest shapes.  \n\nIn short, a single slip in the LLM’s material reasoning step propagates directly into the simulation as bad density or stiffness values, and that destroys both physical realism and numerical stability in the 4D content."
    },
    {
        "question": "What alternative simulation algorithms could improve Particle-Based Simulator outcomes under external forces?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Particle-Based Simulator",
            "External Force"
        ],
        "id": 477,
        "masked_question": "What alternative simulation algorithms could improve [mask1] outcomes under [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Particle-Based Simulator",
            "External Force"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16800v2_figure_2.png",
        "paperid": "2411.16800v2",
        "paper_path": "./papers/2411.16800v2.json",
        "figure_id": "2411.16800v2_figure_2.png",
        "caption": "Figure 2: Framework of Phys4DGen.\nIn 3D Gaussians Generation stage, from an input image, a static 3D Gaussians will be generated under the guidance of the diffusion model.\nIn Physcial Perception stage, the 3D Gaussians will be segmented into different parts, with corresponding material types and properties assigned to each.\nIn 4D Dynamics Generation stage, we consider each 3D Gaussian kernel as particles within a continuum. Sequentially, we employ MPM to generate dynamics to the static 3D Gaussians. Meanwhile, users can guide the MPM simulator to generate 4D content that aligns with their desired outcomes by adjusting the external forces.",
        "qtype": "Others",
        "response": "Here’s how I’d think it through:\n\n1.  Identify the masks in the figure:  \n    - [mask₁] is the red‐outlined “Particle-Based Simulator” block (in the text they plug in MPM there).  \n    - [mask₂] is the blue-outlined “External Force” input to that simulator.  \n\n2.  Ask the actual question:  \n    “What other simulation algorithms – in place of the MPM-style particle solver in [mask₁] – might give even better results when you’re driving them with the kinds of External Forces shown in [mask₂]?”\n\n3.  Recall that MPM is just one way to solve continuum mechanics on particles.  Other well‐known approaches include:  \n    - Smoothed Particle Hydrodynamics (SPH) for fluid‐like or highly deformable materials.  \n    - Discrete Element Method (DEM) for granular or rigid/fracturing media.  \n    - Finite Element Method (FEM) on a mesh for high-fidelity elastic solids.  \n    - Position-Based Dynamics (PBD) / Extended PBD for fast, constraint-based simulation.  \n    - Hybrid PIC/FLIP or FLIP/SPH for fluid–solid coupling with improved stability.  \n    - Arbitrary Lagrangian–Eulerian (ALE) or Level-Set/Eulerian grid schemes for free-surface fluids.  \n\n4.  These alternatives can in many cases handle larger deformations, stronger forces, or more complex constitutive laws than vanilla MPM.  For instance:  \n    •  SPH + gravity (mask₂) → better fluid splashes/flows.  \n    •  DEM + directional force → realistic piling, shattering, or granular flows.  \n    •  FEM + user-prescribed force fields → more accurate elastic bending/stretching.  \n    •  PBD + external pushes → highly interactive yet stable soft-body edits.  \n\nSo, to directly answer the question:\n\n“Instead of using the MPM-based particle solver in the red box, one could swap in SPH (for fluid‐like materials), DEM (for grains or rubble), mesh-based FEM (for high-fidelity solids), PBD/XPBD (for fast constraint-based dynamics) or hybrid PIC/FLIP solvers. Any of these could yield more stable, accurate or visually richer responses when driven by the external forces shown in the blue box.”"
    },
    {
        "question": "What is the rationale for aligning segmentation maps with CLIP Fusion in the Physical Perception Module?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "CLIP Fusion. However, the 2D segmentation maps are generated independently, lacking connections between the maps of different images.",
            "To ensure consistency with the material groups defined by the input image, we align the segmentation maps of the rendered sequence with the input image’s segmentation map."
        ],
        "final_answer": "Because the segmentation maps of the input image and those of the rendered views are produced independently and thus lack a shared grouping, CLIP Fusion is used to align the rendered sequence’s maps to the input image’s segmentation. This guarantees that all maps share the same material‐group definitions derived from the input image.",
        "relevant_elements": [
            "CLIP Fusion",
            "Physical Perception Module"
        ],
        "id": 479,
        "masked_question": "What is the rationale for aligning segmentation maps with [mask1] in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP Fusion",
            "Physical Perception Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.16800v2_figure_2.png",
        "paperid": "2411.16800v2",
        "paper_path": "./papers/2411.16800v2.json",
        "figure_id": "2411.16800v2_figure_2.png",
        "caption": "Figure 2: Framework of Phys4DGen.\nIn 3D Gaussians Generation stage, from an input image, a static 3D Gaussians will be generated under the guidance of the diffusion model.\nIn Physcial Perception stage, the 3D Gaussians will be segmented into different parts, with corresponding material types and properties assigned to each.\nIn 4D Dynamics Generation stage, we consider each 3D Gaussian kernel as particles within a continuum. Sequentially, we employ MPM to generate dynamics to the static 3D Gaussians. Meanwhile, users can guide the MPM simulator to generate 4D content that aligns with their desired outcomes by adjusting the external forces.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key insight, unpacked step by step:\n\n1.  In the Physical Perception Module (the blue‐boxed area) we first run SAM on  \n    – the original input image, producing a set of masks \\(M\\), and  \n    – each of the rendered views of the provisional 3D Gaussians, producing \\(\\{R_i\\}\\).  \n2.  Those render‐view masks \\(\\{R_i\\}\\) are entirely independent of \\(M\\), so “leaf” mask #3 in view 2 might actually correspond to the rose petals in the input, while “leaf” mask #7 in view 5 might correspond to the pot, etc.  \n3.  But we want *one* coherent material labeling across *all* views—namely the groups defined on the input image.  That way, when we project back into 3D we know exactly which Gaussians belong to “petal,” which to “stem,” which to “pot,” and so on.  \n4.  CLIP Fusion (the red‐boxed step) solves this:  \n    •  We extract a CLIP embedding for every mask in the input image, and for every mask in each rendered view.  \n    •  We then match each view‐mask to the single input‐mask whose CLIP embedding is most similar.  \n    •  We relabel the view’s masks so that each region inherits the same “material group” label it had in the input.  \n\nRationale in one sentence:  \nBecause SAM’s masks on different renderings are inconsistent, we use CLIP Fusion to *align* each rendered‐view mask back to the *input image*’s mask labels, thus enforcing a single, globally consistent material segmentation before we project those labels into the 3D Gaussian representation."
    },
    {
        "question": "What motivates implementing bidirectional STDP between Emotional Regions and Mirror Neuron System?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Due to the strict temporal correlation between emotions and external action and perception, the connections between the three clusters of neurons are strengthened.",
            "Since the connections between the modules are bidirectional, it will be interactively and repeatedly facilitated to enhance the bidirectional connection weights. Therefore, we utilize spiking neural networks [56] to model the connections among the emotional brain region, mirror neuron system, and perceptual brain region, with Spike-Timing-Dependent Plasticity (STDP) [57] employed to facilitate learning of temporal sequence-dependent associations."
        ],
        "final_answer": "The strict temporal correlations among emotional activations, mirror-neuron-driven actions, and perceptions motivate using bidirectional STDP so that these inter-regional connections can be interactively and repeatedly strengthened in both directions, embedding the temporal sequence-dependent associations necessary for affective empathy.",
        "relevant_elements": [
            "Emotional Regions",
            "Mirror Neuron System",
            "STDP"
        ],
        "id": 480,
        "masked_question": "What motivates implementing bidirectional [mask1] between [mask2] and Mirror Neuron System?",
        "masked_number": 2,
        "masked_elements": [
            "STDP",
            "Emotional Regions"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.21882v1_figure_1.png",
        "paperid": "2410.21882v1",
        "paper_path": "./papers/2410.21882v1.json",
        "figure_id": "2410.21882v1_figure_1.png",
        "caption": "Figure 1: The procedure of brain-inspired affective empathy-driven moral decision-making algorithm.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought:\n\n1. In our model the affective‐empathy module is built as a tightly coupled loop among three “brain” regions: the Emotion region (e.g. ACC, Amygdala), the Mirror Neuron System (motor region), and the Perception region (e.g. V1, A1).\n\n2. Neurophysiological studies tell us that when you feel an emotion yourself, you automatically produce an outward motor/perceptual pattern, and, conversely, when you see someone else exhibiting that same motor/perceptual pattern you “resonate” your own emotional neurons. In other words, emotion ↔ action/perception are strictly time‐locked in both directions.\n\n3. To capture that two‐way, time‐locked resonance we therefore give the Emotion region and the Mirror Neuron System bidirectional excitatory synaptic connections (the red arrows in the figure). These reciprocal connections allow emotion‐to‐motor/perceptual signals and motor/perceptual‐to‐emotion signals to propagate and reinforce each other.\n\n4. Finally, we use STDP (spike‐timing‐dependent‐plasticity) on those bidirectional connections so that every time the emotion neurons fire just before or just after the mirror neurons, the synapses between them get stronger. Over repeated self-experience (emotion→motor/percep) and observation episodes (motor/percep→emotion), those weights potentiate, enabling the mirror network to reliably “activate” your own emotional neurons when you observe the same outward expression in another—i.e., affective empathy.\n\nAnswer:\n\nWhat motivates implementing bidirectional excitatory synaptic connections between the Emotional Regions and the Mirror Neuron System is the strict temporal correlation between one’s own emotional activations and their associated motor/perceptual expressions. Having two‐way excitatory links allows these signals to drive one another in a recurrent loop, and STDP then hones those connections so that observing another’s expression automatically reactivates one’s own emotion circuitry, thus realizing affective empathy."
    },
    {
        "question": "How does dopamine influence R-STDP updates within the Moral Decision-making Module to reinforce altruistic actions?",
        "relevant_section_ids": [
            "3.1.2",
            "3.2.3"
        ],
        "relevant_context": [
            "In the Ventral Tegmental Area (VTA) [55], dopamine encodes both the agent’s own goals and intrinsic empathy reward, combining with moral utilitarianism theories to form a regulatory factor that prioritizes altruism. Under the modulation of dopamine, the agent continuously interacts with the environment, empathizing with others’ emotional states and learning spontaneously altruistic moral behaviors.",
            "Only when altruistic behaviors are performed that the negative emotions of others are alleviated, which in turn eases one’s own empathically felt negative emotions, resulting in an increase in dopamine levels in the brain and reinforcing the altruistic behavior.",
            "In this paper, we use reward-modulated STDP (R-STDP) [62] to adjust the connection weights between state and action neurons, thereby optimize the moral decision-making strategy. R-STDP uses synaptic eligibility trace to store temporary information of STDP. The eligibility trace accumulates the STDP and decays with a time constant. Then, synaptic weights are updated when a delayed reward R is received, as Eq. 9 shown."
        ],
        "final_answer": "Dopamine functions as the delayed reward signal R in the R‐STDP rule. When the agent’s altruistic action successfully alleviates another’s negative emotion, this increases dopamine levels (the intrinsic empathy reward). The positive dopamine signal then gates the R‐STDP update by multiplying the accumulated eligibility traces, thereby strengthening the synaptic connections that led to the altruistic action and reinforcing that behavior.",
        "relevant_elements": [
            "Dopamine",
            "R-STDP"
        ],
        "id": 483,
        "masked_question": "How does [mask1] influence R-STDP updates within the Moral Decision-making Module to reinforce altruistic actions?",
        "masked_number": 1,
        "masked_elements": [
            "Dopamine"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.21882v1_figure_1.png",
        "paperid": "2410.21882v1",
        "paper_path": "./papers/2410.21882v1.json",
        "figure_id": "2410.21882v1_figure_1.png",
        "caption": "Figure 1: The procedure of brain-inspired affective empathy-driven moral decision-making algorithm.",
        "qtype": "Implementation_Details",
        "response": "First, let us identify exactly what “\\[mask1\\]” is in Fig. 1.  The little red‐framed box sitting just to the right of the three empathy‐cubes (Emotion → Mirror → Perception) is labeled “Other: Emotional outward information.”  In the text this is referred to as the agent’s empathic observation, O_emp, or equivalently the “emotional outward expressions” of the peer.  \n\nNow, how does this Emotional Outward Information (O_emp) influence the R–STDP updates in the Moral Decision-making Module so as to reinforce altruistic acts?  We can break it down in two steps:\n\n1.  O_emp enters the decision-making SNN as part of the state input.   \n    •  In III-B3 the authors write that the input to the moral decision SNN is  \n       s_t = { s_env,  s_task,  s_emp }  \n       where s_emp = –1 if the agent is empathically in a negative state (i.e. it has seen O_emp that signals the other is suffering), and 0 otherwise.  \n    •  Thus O_emp directly influences which neurons in the state layer are driven.\n\n2.  O_emp also drives an intrinsic “empathy reward” signal via dopamine, which R–STDP uses to gate weight changes.  \n    •  In III-B2 empathic neurons inhibit dopamine neurons when the agent perceives another’s distress.  This creates a negative‐prediction error.  \n    •  Only if the agent then performs an action that alleviates the other’s distress (an altruistic action) does its own empathic negative inhibition cease and dopamine levels rebound.  \n    •  That dopamine rebound is the “delayed reward” R_delayed in the R–STDP update rule (Eq. 9).  \n\nPutting these together, the R–STDP rule is essentially  \n   Δw_ij  ∝  (R_total) · e_ij(t)  \nwhere e_ij is the standard STDP eligibility trace and R_total = R_task + R_emp (the sum of extrinsic task reward and the intrinsic empathy‐driven dopamine signal).  Since O_emp is the very thing that generates the empathy‐driven component R_emp, it directly gates whether Δw_ij for the spikes that led to an altruistic action will be positive (potentiation) or negative (depression).  \n\nIn plain English:  \n– Whenever the agent sees another in distress (via O_emp), it enters a negative empathic state that suppresses dopamine.  \n– If the agent then carries out an action that relieves that distress, dopamine jumps back up—this jump is R_emp.  \n– R–STDP then takes that dopamine jump and “backs up” all of the eligibility traces in its synapses, converting the recent spike timing correlations that produced the altruistic move into long‐term potentiation.  \n– Over many trials, those synapses become stronger, making it ever more likely the agent will repeat the altruistic action next time it sees O_emp."
    },
    {
        "question": "How does privacy-preserving knowledge extraction integrate Gaussian noise into GAT layer embeddings?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "In existing CDR frameworks, the user or item embedding was shared as knowledge [9;15;6], which neglects inter-domain privacy. In a GNN-based approach, such direct transfers are subject to privacy attacks. Each message propagation layer can be viewed as a function with user and item embeddings as input. An attacker can easily obtain the user's private rating matrix based on these embeddings. We apply DP to the source domain embeddings E_s [22;45] to safeguard inter-domain privacy.",
            "theorem 1. By perturbing the source domain embeddings with Gaussian noise, the reconstructed data of the ideal attack deviates from the real data and prevents a perfect reconstruction.",
            "In FedGCDR, we adopt the Gaussian mechanism to the source domain embedding E_s^l to obtain \\widetilde{E}_s^l for knowledge transfer. Detailed privacy analysis is included in Appendix A."
        ],
        "final_answer": "FedGCDR applies the Gaussian mechanism from differential privacy directly to each source‐domain GAT layer embedding E_s^l by adding Gaussian noise, thereby producing a perturbed embedding \\widetilde{E}_s^l that is used for cross-domain knowledge transfer and prevents perfect reconstruction of the original private data.",
        "relevant_elements": [
            "Privacy-preserving Knowledge Extraction",
            "Gaussian Noise",
            "GAT Layer"
        ],
        "id": 484,
        "masked_question": "How does [mask1] integrate Gaussian noise into GAT layer embeddings?",
        "masked_number": 1,
        "masked_elements": [
            "Privacy-preserving Knowledge Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.08249v2_figure_2.png",
        "paperid": "2410.08249v2",
        "paper_path": "./papers/2410.08249v2.json",
        "figure_id": "2410.08249v2_figure_2.png",
        "caption": "Figure 2: An overview of FedGCDR. It consists of two key modules and follows a HVH pipeline: (1) Source Domain Training (Horizontal FL): ① Each source domain maintains its graph attention network (GAT)-based federated model. (2) Positive Knowledge Transfer Module (Vertical FL): ② Source domain embeddings are extracted from GAT layers and perturbed with Gaussian noise. ③ The multilayer perceptron aligns the feature space of source domain embeddings and target domain embeddings. (3) Positive Knowledge Activation Module (Horizontal FL): ④ Local graph is expanded with source domain embeddings. ⑤ Enhanced federated training of the target domain is achieved through the expanded graph. ⑥ The target domain maintains its GAT-based federated model. ⑦ The target domain freezes the GAT layer and fine tunes the model.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step of what the red‐boxed “Privacy‐preserving Knowledge Extraction” module (i.e. your [mask1]) is doing to inject Gaussian noise into the GAT embeddings:\n\n1.  \n   Each client locally runs the source‐domain GAT for L layers and pulls out the per‐layer user–item embeddings  \n   \\[e_{s}^{u,1}, e_{s}^{v,1},\\;e_{s}^{u,2},e_{s}^{v,2},\\;\\dots,\\;e_{s}^{u,L},e_{s}^{v,L}\\].  \n\n2.  \n   To guarantee inter‐domain privacy, they apply the standard Gaussian‐mechanism of differential privacy.  Concretely, if \\(E_s\\) denotes the full embedding matrix (stacking all user–item row vectors at a given layer), they compute  \n   \\[\n     \\widetilde E_s \\;=\\; E_s \\;+\\;\\mathcal{N}\\bigl(0,\\sigma^2I\\bigr)\\,,\n   \\]  \n   i.e. add zero‐mean Gaussian noise (with variance \\(\\sigma^2\\)) independently to every coordinate.\n\n3.  \n   Graphically this is the “⊕ Gaussian Noise” step in the red box — right after you extract your GAT outputs and just before you hand them off to the feature‐mapping MLP.\n\n4.  \n   Those noisy embeddings \\(\\widetilde E_s\\) are then passed into the MLP to align them with the target‐domain feature space, but at no point do the raw, unperturbed GAT embeddings ever leave the client.  \n\nBy simply adding (⊕) Gaussian noise to the output of each GAT layer, the module enforces \\((\\varepsilon,\\delta)\\)–DP on the transferred embeddings and prevents an adversary from perfectly reconstructing a user’s private rating history."
    },
    {
        "question": "How does graph expansion incorporate source embeddings for attention computation during target domain training?",
        "relevant_section_ids": [
            "3.2.3"
        ],
        "relevant_context": [
            "First, we expand ’s (Mary’s) local graph of the target domain as shown in Figure 3. For the source domain embedding matrices , we represent them as  virtual users. Since the virtual users constructed from source domain embeddings represent the same individual , they share correlated preferences, with their features (i.e., embeddings) characterizing ’s preferences. Inspired by social recommendation [48; 49; 50], we consider that there is a implicit social relationship between virtual users and the actual user , because of the correlation in their preferences. Then, we build virtual social links between them to expand the original target domain graph.",
            "Second, by incorporating this expanded graph into target domain training, the GAT model generates corresponding attention coefficients for the virtual users, which can be interpreted as domain-specific attentions."
        ],
        "final_answer": "Graph expansion takes each source-domain embedding matrix and treats it as a \"virtual user,\" then connects these virtual users to the real target-domain user via implicit social links. During target-domain training on this enlarged graph, a GAT computes attention coefficients on the edges between the user and each virtual user; those attention scores serve as domain‐specific attentions that weight how much each source embedding contributes.",
        "relevant_elements": [
            "Graph Expansion",
            "GAT Layer N",
            "Target Domain Training"
        ],
        "id": 485,
        "masked_question": "How does [mask1] incorporate source embeddings for attention computation during [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Graph Expansion",
            "Target Domain Training"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.08249v2_figure_2.png",
        "paperid": "2410.08249v2",
        "paper_path": "./papers/2410.08249v2.json",
        "figure_id": "2410.08249v2_figure_2.png",
        "caption": "Figure 2: An overview of FedGCDR. It consists of two key modules and follows a HVH pipeline: (1) Source Domain Training (Horizontal FL): ① Each source domain maintains its graph attention network (GAT)-based federated model. (2) Positive Knowledge Transfer Module (Vertical FL): ② Source domain embeddings are extracted from GAT layers and perturbed with Gaussian noise. ③ The multilayer perceptron aligns the feature space of source domain embeddings and target domain embeddings. (3) Positive Knowledge Activation Module (Horizontal FL): ④ Local graph is expanded with source domain embeddings. ⑤ Enhanced federated training of the target domain is achieved through the expanded graph. ⑥ The target domain maintains its GAT-based federated model. ⑦ The target domain freezes the GAT layer and fine tunes the model.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is the “Graph Expansion” step in the red box, and Mask2 is the “Target Domain Training” step in the blue box.  Here is in plain English how the former feeds into the attention calculation of the latter:\n\n1. Graph Expansion (Mask1)  \n   – For each source domain embedding \\(\\tilde E_s^u\\) (after DP and feature‐mapping), we create a new “virtual user” node in the target–domain graph.  \n   – We then draw an (undirected) “social” edge between the real target user \\(u\\) and each of these virtual nodes.  \n   – In this way the target–domain graph now contains, for user \\(u\\), not only item neighbors but also one virtual neighbor per source domain.  \n\n2. Target Domain GAT Training (Mask2)  \n   – We run a standard GAT over this expanded graph.  In a GAT layer each node aggregates messages from all of its neighbors, weighting them by an attention coefficient  \n     \\[\n       \\alpha_{u\\to v} \\;=\\;\\mathsf{softmax}\\;\\bigl(a^T [\\,W\\,e_u\\;\\|\\;\\,W\\,e_v]\\bigr)\n     \\]  \n   – Because the virtual user nodes carry exactly the mapped source–domain embeddings, they show up in the neighbor set of \\(u\\).  Consequently the usual GAT attention \\(\\alpha\\) is computed over edges \\((u,\\,\\text{virtual}_s)\\) as well as \\((u,\\,\\text{item})\\).  \n   – The resulting \\(\\alpha\\) on each \\((u,\\text{virtual}_s)\\) edge naturally serves as a per–domain attention weight, telling the model how much to “listen” to that particular source–domain embedding when updating \\(u\\).  \n\nIn short, Graph Expansion turns each source embedding into a virtual neighbor of \\(u\\), and during Target–Domain GAT training the built-in attention mechanism automatically learns how strongly to weight each of those virtual neighbors."
    },
    {
        "question": "How does the Positive Knowledge Transfer Module’s Gaussian perturbation on GAT Layer embeddings extend FL privacy methods?",
        "relevant_section_ids": [
            "3.2",
            "3.2.2"
        ],
        "relevant_context": [
            "Following the HVH pipeline, we achieve: (1) Privacy enhancement. The two horizontal stages can provide intra-domain privacy preservation, while we further ensure inter-domain privacy by applying DP to the vertical stage.",
            "In existing CDR frameworks, the user or item embedding was shared as knowledge [9;15;6], which neglects inter-domain privacy. In a GNN-based approach, such direct transfers are subject to privacy attacks. Each message propagation layer can be viewed as a function with user and item embeddings as input. An attacker can easily obtain the user’s private rating matrix based on these embeddings. We apply DP to the source domain embeddings E_s^u [22;45] to safeguard inter-domain privacy.",
            "Theorem 1. By perturbing the source domain embeddings with Gaussian noise, the reconstructed data of the ideal attack deviates from the real data and prevents a perfect reconstruction.",
            "In FedGCDR, we adopt the Gaussian mechanism to the source domain embedding E_s^u to obtain \\widetilde{E}_s^u for knowledge transfer."
        ],
        "final_answer": "Beyond standard horizontal FL’s protection of intra-domain gradients/models, FedGCDR’s Positive Knowledge Transfer Module injects Gaussian noise into each source domain’s GAT-layer embeddings (via the DP Gaussian mechanism) before sharing them across domains. This extension ensures inter-domain differential privacy, preventing an attacker from reconstructing users’ private ratings from the transferred embeddings.",
        "relevant_elements": [
            "Positive Knowledge Transfer Module",
            "GAT Layer"
        ],
        "id": 486,
        "masked_question": "How does the [mask1]’s Gaussian perturbation on GAT Layer embeddings extend FL privacy methods?",
        "masked_number": 1,
        "masked_elements": [
            "Positive Knowledge Transfer Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.08249v2_figure_2.png",
        "paperid": "2410.08249v2",
        "paper_path": "./papers/2410.08249v2.json",
        "figure_id": "2410.08249v2_figure_2.png",
        "caption": "Figure 2: An overview of FedGCDR. It consists of two key modules and follows a HVH pipeline: (1) Source Domain Training (Horizontal FL): ① Each source domain maintains its graph attention network (GAT)-based federated model. (2) Positive Knowledge Transfer Module (Vertical FL): ② Source domain embeddings are extracted from GAT layers and perturbed with Gaussian noise. ③ The multilayer perceptron aligns the feature space of source domain embeddings and target domain embeddings. (3) Positive Knowledge Activation Module (Horizontal FL): ④ Local graph is expanded with source domain embeddings. ⑤ Enhanced federated training of the target domain is achieved through the expanded graph. ⑥ The target domain maintains its GAT-based federated model. ⑦ The target domain freezes the GAT layer and fine tunes the model.",
        "qtype": "Literature_Background",
        "response": "Let’s walk through what the red‐boxed module is doing, and then see exactly how the little “+ Gaussian Noise” step extends the usual FL privacy story.\n\n1.  Image–text alignment  \n    - The red box in Fig. 2 is the “Positive Knowledge Transfer Module” that lives in each client’s user space.  \n    - Inside it you see a stack of GAT layers (1, 2, … N) producing per‐layer user ( and item ) embeddings.  \n    - Before anything crosses domain boundaries, those raw embeddings are injected with Gaussian noise (step ② in the figure) and only then passed on to the MLP feature‐mapping stage.\n\n2.  What standard FL already protects  \n    - In a plain horizontal FL setup, each domain never shares raw ratings or local tables; it only exchanges encrypted model updates or gradients.  That keeps each domain’s *own* data private **within** that domain.  \n\n3.  What was still missing  \n    - Once you start doing *cross*-domain knowledge transfer (i.e. our vertical stage), you have to send something richer than mere gradients: here it’s the GAT‐layer embeddings.  If you shipped them *as is*, a malicious recipient could invert those vectors and recover sensitive rating data.\n\n4.  The Gaussian perturbation as a DP mechanism  \n    - By applying a Gaussian‐noise mechanism to each GAT output embedding, the client ensures that  \n      a) any attempt to reconstruct the original user–item ratings from those vectors will have high error, and  \n      b) the published embeddings satisfy (ε, δ)–differential privacy.  \n    - In other words, even though we’re now sharing embeddings *vertically* across domains, we haven’t opened a backdoor for privacy attacks.\n\n5.  How this extends FL privacy  \n    - Horizontal FL already protected *intra-domain* privacy (only model updates go back and forth).  \n    - The Gaussian‐noise step layers on a full differential‐privacy guarantee for the *inter-domain* embedding transfer.  \n    - This way, FedGCDR achieves a two-dimensional privacy shield:  \n       • Horizontal DP/secure aggregation in each domain’s FL, and  \n       • Vertical DP (via Gaussian mechanism) on the embedding vectors before they ever leave the user’s device.  \n\nAnswer (concise):  \nBy injecting Gaussian noise into each GAT layer’s embedding before it ever leaves the client, the red-boxed module turns the embedding transfer into a formally differential-private release.  This means that even though embeddings are shared across domains (the “vertical” step), they can no longer be inverted to recover private ratings—thus extending FL’s usual intra-domain privacy protection to also cover inter-domain knowledge exchange."
    },
    {
        "question": "How does Example Mining inform the motion imitator's focus on challenging motion samples via hard negative mining?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As training progresses, the motion imitator gradually learns to imitate simple motion sequences. However, more challenging examples in the large-scale motion dataset may be overlooked, limiting the model's ability to handle difficult samples.",
            "To address this, we implement a Hard Negative Mining process that identifies motions where the physical simulator fails to imitate as hard samples.",
            "Specifically, a dynamic weight is assigned to each motion sequence in the input data, doubling whenever imitation fails.",
            "This process progressively increases the focus on challenging samples, guiding the imitator to effectively learn from difficult examples."
        ],
        "final_answer": "Example Mining implements a hard negative mining strategy in which any motion sequence that the physics simulator fails to imitate is marked as a hard sample and its associated weight is doubled. By dynamically increasing the weight of these challenging examples each time imitation fails, the imitator’s policy training progressively focuses more on difficult motions.",
        "relevant_elements": [
            "Example Mining",
            "Motion Imitator"
        ],
        "id": 488,
        "masked_question": "How does [mask1] inform the [mask2]'s focus on challenging motion samples via hard negative mining?",
        "masked_number": 2,
        "masked_elements": [
            "Example Mining",
            "Motion Imitator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.14951v1_figure_2.png",
        "paperid": "2411.14951v1",
        "paper_path": "./papers/2411.14951v1.json",
        "figure_id": "2411.14951v1_figure_2.png",
        "caption": "Figure 2: An overview of the Morph framework. Morph comprises a Motion Generator and a Motion Physics Refinement module. Morph employs a two-stage training process: Motion Physics Refinement module training and Motion Generator fine-tuning. And a Imitation Selection Operation is employed to ensure the motion quality after physics refinement.",
        "qtype": "Literature_Background",
        "response": "Here’s what’s going on, step by step:\n\n1.  In the red‐boxed “Example Mining” module the system keeps track of which reference clips the physics simulator (when driven by the current imitator) fails to reproduce faithfully.  \n2.  Whenever a clip is poorly imitated (a “hard negative”), its associated weight is doubled. Over the course of training this builds up a ranking of “easy” versus “hard” clips.  \n3.  Those clip‐wise weights are then fed straight into the blue‐boxed “Motion Imitator” policy, so that in the next round of RL updates the imitator spends proportionally more gradient‐steps and rollouts on the high‐weight (i.e. hard) clips.  \n\nIn this way the Example Mining (red) implements hard-negative mining by dynamically up‐weighting failed examples and steering the Motion Imitator (blue) to focus its learning on exactly those challenging motions."
    },
    {
        "question": "How does example mining adjust motion weights to improve failed-case handling in the motion imitator policy?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As training progresses, the motion imitator gradually learns to imitate simple motion sequences. However, more challenging examples in the large-scale motion dataset may be overlooked, limiting the model’s ability to handle difficult samples. To address this, we implement a Hard Negative Mining process that identifies motions where the physical simulator fails to imitate as hard samples.",
            "Specifically, a dynamic weight is assigned to each motion sequence in the input data, doubling whenever imitation fails. This process progressively increases the focus on challenging samples, guiding the imitator to effectively learn from difficult examples."
        ],
        "final_answer": "Example mining (Hard Negative Mining) assigns a dynamic weight to each motion sequence and doubles that weight each time the simulator fails to imitate the sample, thereby increasing the policy’s focus on and improving handling of these challenging failed cases.",
        "relevant_elements": [
            "Example Mining",
            "Motion Imitator",
            "Policy"
        ],
        "id": 490,
        "masked_question": "How does [mask1] adjust motion weights to improve failed-case handling in the motion imitator policy?",
        "masked_number": 1,
        "masked_elements": [
            "Example Mining"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.14951v1_figure_2.png",
        "paperid": "2411.14951v1",
        "paper_path": "./papers/2411.14951v1.json",
        "figure_id": "2411.14951v1_figure_2.png",
        "caption": "Figure 2: An overview of the Morph framework. Morph comprises a Motion Generator and a Motion Physics Refinement module. Morph employs a two-stage training process: Motion Physics Refinement module training and Motion Generator fine-tuning. And a Imitation Selection Operation is employed to ensure the motion quality after physics refinement.",
        "qtype": "Experimental_Results",
        "response": "In our diagram the little red‐boxed “mystery” module is in fact exactly the hard‐negative (or “example”) mining step inside the policy loop. What it does is very simple:\n\n  1. Whenever you roll out a reference clip in the physics simulator and the imitator “fails” (you hit your early‐stop criterion, it falls over, MPJPE is too large, etc.), you tag that clip as hard.  \n  2. You then *bump up* the training weight of that clip—classically by doubling it, so  \n     wᵢ (new)  = 2 × wᵢ (old)  for every motion clip i that failed.  \n  3. On the next PPO update, clips with higher wᵢ get sampled more often and contribute more to the loss, forcing the policy to spend extra capacity on those troublesome cases.  \n\nBy dynamically re‐weighting failed‐case sequences in exactly this way, the imitator is continually driven to focus on—and eventually master—the motions it was getting wrong."
    },
    {
        "question": "How does the imitation selection operation filter non-grounded motions to refine training data for motion generator fine-tuning?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Notably, since the physical simulator cannot replicate non-grounded motions (e.g., sitting on a chair or swimming), such simulated motions may deviate from the true data distribution.",
            "To this end, we apply an Imitation Selection Operation to filter out simulated data of non-grounded motions. Specifically, we calculate the average per-joint position error (MPJPE) between the samples before and after physical optimization. A threshold δ is set to determine whether to accept the physically refined motion \\hat{x} (with δ_i ≤ δ) or input motions x (with δ_i > δ). The selected data is then paired with the original condition signals (e.g., text or music)."
        ],
        "final_answer": "The imitation selection operation computes the MPJPE between each motion before and after physics refinement and rejects any refined samples whose error exceeds a preset threshold δ. For each sequence, if its refinement MPJPE is below δ, the physics-refined version is kept; otherwise the original unrefined motion is used. This filters out non-grounded refinements and produces a large, physically plausible dataset for fine-tuning the motion generator.",
        "relevant_elements": [
            "Imitation Selection Operation",
            "Motion Generator"
        ],
        "id": 491,
        "masked_question": "How does the [mask1] operation filter non-grounded motions to refine training data for [mask2] fine-tuning?",
        "masked_number": 2,
        "masked_elements": [
            "Imitation Selection Operation",
            "Motion Generator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.14951v1_figure_2.png",
        "paperid": "2411.14951v1",
        "paper_path": "./papers/2411.14951v1.json",
        "figure_id": "2411.14951v1_figure_2.png",
        "caption": "Figure 2: An overview of the Morph framework. Morph comprises a Motion Generator and a Motion Physics Refinement module. Morph employs a two-stage training process: Motion Physics Refinement module training and Motion Generator fine-tuning. And a Imitation Selection Operation is employed to ensure the motion quality after physics refinement.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed block in the picture is exactly the “Imitation Selection” operation.  Its job is to take every generated sequence \\(x\\) and its physics–refined counterpart \\(\\hat x\\), compute the average per–joint position error (MPJPE) between the two, and then decide—via a simple threshold test—whether to keep the refined version or fall back to the original.  \n\nIn practice that looks like:  \n  1. Run your noisy synthetic motion \\(x\\) through the physics module to get \\(\\hat x\\).  \n  2. Compute  \n     MPJPE\\bigl(x,\\hat x\\bigr)\\;.  \n  3. If that MPJPE is below a chosen threshold \\(\\delta\\), accept \\(\\hat x\\) as “grounded” data; otherwise, reject \\(\\hat x\\) and keep the raw \\(x\\).  \n\nAll of the accepted clips—i.e. those whose refinement error is small enough—are then paired back up with their original condition (text, music, etc.) and collected into a large, clean dataset.  That curated set is exactly what feeds into the blue‐boxed second stage (“Fine-tuning Motion Generator”), i.e. you re-train your generator on only the physically plausible, non­–non-grounded motions."
    },
    {
        "question": "How does independent visual encoder maintain separate visual processing before fusion with linguistic features in the visual-linguistic transformer?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In the Visual Branch, a CNN backbone first extracts a 2D feature map, followed by a stack of transformer encoder layers that generate a flattened sequence of visual features .",
            "Subsequently, we concatenate the visual and textual feature embeddings and appending a learnable token, [REG] token, as the inputs for the multi-modal decoder (Visual-Linguistic Transformer), which embeds the input tokens from different modalities into a aligned semantic space and perform intra- and inter-modal reasoning with the self-attention layers."
        ],
        "final_answer": "The model keeps visual processing independent by using a standalone Visual Branch—first applying a CNN backbone to produce a 2D feature map, then passing it through transformer encoder layers to get a flattened visual feature sequence. Only after these features are fully extracted are they concatenated with the separately computed textual embeddings (plus a [REG] token) and fed into the visual-linguistic transformer for fusion and reasoning.",
        "relevant_elements": [
            "Independent visual encoder",
            "Visual-Linguistic Transformer"
        ],
        "id": 492,
        "masked_question": "How does [mask1] maintain separate visual processing before fusion with linguistic features in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Independent visual encoder",
            "Visual-Linguistic Transformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.04999v1_figure_1.png",
        "paperid": "2409.04999v1",
        "paper_path": "./papers/2409.04999v1.json",
        "figure_id": "2409.04999v1_figure_1.png",
        "caption": "Figure 1. (a) Traditional visual grounding framework with independent visual encoder. (b) Our proposed visual grounding framework with Multi-modal (MM) conditional visual encoder. We visualize the ground truth and the attention maps of various visual encoders. The attention distribution of the independent visual encoder appears more diffuse, whereas the attention distributions of the MM-conditional visual encoder are more concentrated on the corresponding object.",
        "qtype": "Experimental_Results",
        "response": "Here’s how the two pieces line up:  \n– The red‐boxed module is exactly the “Independent Visual Encoder.” It is just a plain CNN plus transformer stack that takes only the image as input and produces a sequence of visual feature tokens. No language information ever enters this module.  \n– The blue‐boxed module is the “Visual–Linguistic Transformer.” It is the very first place where the image‐only features and the text‐only features are concatenated (along with the learnable [REG] token) and allowed to attend to one another.  \n\nSo, in short, the Independent Visual Encoder preserves a purely visual representation by (1) running the image through its own convolutional backbone and transformer layers without any textual input and (2) holding those visual tokens separate until they are finally concatenated with the linguistic tokens inside the Visual–Linguistic Transformer, at which point true cross‐modal fusion via self‐attention takes place."
    },
    {
        "question": "How does MM conditional visual encoder utilize linguistic backbone outputs to modulate spatial attention prior to fusion?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "In the Visual Branch, a CNN backbone first extracts a 2D feature map, followed by a stack of transformer encoder layers that generate a flattened sequence of visual features . Our proposed Multi-modal Conditional Adaption (MMCA) is hierarchically applied to the parameter matrices of the convolutional and transformer layers. This module takes both visual and textual features as inputs and dynamically updates the weights of the visual encoder to achieve language-guided visual feature extraction.",
            "Given the textual features  and the flattened visual feature , we first apply pooling operations to process textual features of different lengths and visual features of different spatial sizes. Subsequently, linear projections are used to generate fixed-dimensional embeddings  for the respective modal-specific features. We then employ a simple gating mechanism with a sigmoid activation to fuse the visual and textual embeddings:\n\n    z_v = W_v v,\n    z_t = W_t t,\n    g = sigmoid( ReLU( W_g [ z_v, z_t ] ) ),\n    h = z_v + g ⊙ z_t.\n\n    Finally, the fusion embedding  is utilized to generate the coefficients, which guiding the weight update for visual encoder."
        ],
        "final_answer": "Before any late fusion in the decoder, the MM-conditional visual encoder first takes the linguistic backbone’s output (the pooled–projected textual feature) and the current visual feature map, projects each into a fixed–dimensional embedding, and then runs them through a small sigmoid-gated network.  The gate uses both embeddings to compute a per-dimension scale g, and re–weights the textual embedding (z_t) before adding it back to the visual embedding (z_v).  This fused multimodal vector is then linearly mapped to produce coefficients that recompose a set of low-rank weight–update bases.  Applying those updates to the convolutional and self-attention layers steers the visual encoder’s spatial attention toward text‐relevant regions—effectively modulating spatial focus—before the final multimodal fusion step.",
        "relevant_elements": [
            "MM conditional visual encoder",
            "Linguistic Backbone"
        ],
        "id": 493,
        "masked_question": "How does [mask1] utilize linguistic backbone outputs to modulate spatial attention prior to fusion?",
        "masked_number": 1,
        "masked_elements": [
            "MM conditional visual encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.04999v1_figure_1.png",
        "paperid": "2409.04999v1",
        "paper_path": "./papers/2409.04999v1.json",
        "figure_id": "2409.04999v1_figure_1.png",
        "caption": "Figure 1. (a) Traditional visual grounding framework with independent visual encoder. (b) Our proposed visual grounding framework with Multi-modal (MM) conditional visual encoder. We visualize the ground truth and the attention maps of various visual encoders. The attention distribution of the independent visual encoder appears more diffuse, whereas the attention distributions of the MM-conditional visual encoder are more concentrated on the corresponding object.",
        "qtype": "Experimental_Results",
        "response": "In the figure, the red‐boxed component is exactly the “multi‐modal conditional” visual encoder – the little module that sits between the raw CNN feature map and the final fusion transformer.  What it is doing is very simple:\n\n 1.  It takes the **textual embedding** output by the linguistic backbone (after pooling) and feeds it, together with the current visual feature map, into a tiny gating network (two small FC layers with a ReLU and sigmoid).  \n 2.  That gating network spits out a set of scalar coefficients—one per channel/spatial location—that act as a **soft attention mask** over the 2D feature map.  \n 3.  The visual feature map is then **element‐wise multiplied** by that mask, so that regions mentioned by the text are amplified and irrelevant regions are suppressed.  \n 4.  Only after this spatially-modulated feature map comes the usual text–vision concatenation and transformer‐based fusion.\n\nIn other words, that red box is where the language vectors “talk to” the CNN: they generate a per-position gating mask (via a tiny FC→sigmoid), which re-weights the spatial feature map so the network attends only to regions referred to by the expression."
    },
    {
        "question": "What ethical issues could the Linguistic Backbone produce by biasing the MM conditional Visual encoder's focus?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Linguistic Backbone",
            "MM conditional Visual encoder"
        ],
        "id": 495,
        "masked_question": "What ethical issues could the [mask1] produce by biasing the [mask2]'s focus?",
        "masked_number": 2,
        "masked_elements": [
            "Linguistic Backbone",
            "MM conditional Visual encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.04999v1_figure_1.png",
        "paperid": "2409.04999v1",
        "paper_path": "./papers/2409.04999v1.json",
        "figure_id": "2409.04999v1_figure_1.png",
        "caption": "Figure 1. (a) Traditional visual grounding framework with independent visual encoder. (b) Our proposed visual grounding framework with Multi-modal (MM) conditional visual encoder. We visualize the ground truth and the attention maps of various visual encoders. The attention distribution of the independent visual encoder appears more diffuse, whereas the attention distributions of the MM-conditional visual encoder are more concentrated on the corresponding object.",
        "qtype": "Others",
        "response": "Chain‐of‐thought  \n1. In our diagram the red‐boxed module is the linguistic backbone, whose output drives the conditional updates (“MM conditional” weights) to the visual encoder (the blue‐boxed module).  \n2. Any bias in the linguistic backbone (e.g. learned correlations between gender, race, age or socioeconomic status and particular words or syntactic patterns) will be translated into those low‐rank weight updates.  \n3. As a result, the conditional visual encoder will be systematically nudged to pay more—or less—attention to regions in the image that correspond (in the biased embedding space) to certain protected or sensitive attributes.  \n4. Concretely, this can mean that underrepresented groups are mislocalized or ignored, that stereotyped associations (“women → kitchen,” “young Black men → threat”) are amplified, or that privacy‐sensitive areas (e.g. faces of minors) are over‐scrutinized.  \n5. From an ethical standpoint this raises issues of:  \n   • Fairness and non‐discrimination: some demographic groups may be detected or interpreted less accurately, perpetuating historical injustices.  \n   • Harmful stereotype reinforcement: the model can “see” objects or people only through the lens of biased language priors.  \n   • Privacy and surveillance: biased attention might over‐focus on sensitive attributes (age, health, ethnicity), leading to intrusive inferences.  \n   • Accountability and transparency: it becomes very hard to trace whether a harmful misprediction was caused by visual quirks or by biased language priors.  \n\nAnswer  \nIf the linguistic backbone carries cultural or demographic biases, those biases will steer (“bias”) the MM-conditional visual encoder’s attention maps.  In practice this can lead to  \n - unequal detection performance across genders, races or ages (fairness violations),  \n - reinforcement of harmful stereotypes (e.g. always pairing certain roles or emotions with particular groups),  \n - over-attention to private or sensitive attributes (privacy/surveillance risks), and  \n - a lack of transparency or recourse when the model misbehaves (accountability issues)."
    },
    {
        "question": "What limitations stem from relying on LLM and image generator for diverse same-subject data generation?",
        "relevant_section_ids": [
            "3.1",
            "6"
        ],
        "relevant_context": [
            "We observed that by prompting the SDXL model this way, it can generate photo collages of the same subject with varying poses. However, the generated images usually contain a close-up view of an object in a simple background. To increase the data diversity, we employ a post-processing step that performs background augmentation on the generated objects.",
            "We found that the initial text prompts generated by ChatGPT lack variations and therefore pair the images with the captions obtained from BLIPv2 [17] in training samples."
        ],
        "final_answer": "Relying on the LLM (ChatGPT) to generate object-centric prompts yields captions with limited variation, and relying on the single-image diffusion model (SDXL) to generate same-subject collages produces images that are mostly close-ups against simple backgrounds. These factors reduce diversity in prompts and context, requiring additional background and style augmentation to achieve sufficient variation.",
        "relevant_elements": [
            "LLM",
            "Image generator"
        ],
        "id": 496,
        "masked_question": "What limitations stem from relying on [mask1] and image generator for diverse same-subject data generation?",
        "masked_number": 1,
        "masked_elements": [
            "LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.06187v1_figure_2.png",
        "paperid": "2407.06187v1",
        "paper_path": "./papers/2407.06187v1.json",
        "figure_id": "2407.06187v1_figure_2.png",
        "caption": "Figure 2:  Overall framework. (a) We generate training data by using large language models and prompting pretrained single-image diffusion models. (b) During training, the JeDi model learns to denoise multiple same-subject images together, where each image attends to every image of the same subject set through coupled self-attention. (c) At inference, personalized generation is performed in an inpainting fashion where the goal is to generate the missing images of the joint-image set.",
        "qtype": "Others",
        "response": "Mask1 in the figure is the LLM (ChatGPT) used to turn a list of object names into detailed prompts.  By itself, that LLM + image–generator setup suffers from two main bottlenecks when you try to manufacture a truly varied, same-subject dataset:\n\n1. Prompt diversity bottleneck  \n   – The LLM tends to produce fairly templated or generic object-centric descriptions unless you hand-craft a huge number of very different prompts.  \n   – In our experiments we actually observed that the “object in a scene” prompts it spit out were not rich enough, so we had to fall back on automatically re-captioning the generated images with BLIPv2 to inject more variety.\n\n2. Generator bias toward simple collages  \n   – When you prepend “photos of the same …” to those LLM prompts and feed them to SDXL, you get neat collages of the target object but almost always as a close-up against a simple or uniform background.  \n   – You then need costly post-processing (object detection + segmentation → paste into a blank canvas → inpaint with new scene prompts → optional stylization) just to introduce the kinds of background, lighting, stylistic and viewpoint variations you would see in real photo collections.\n\nIn short, leaning solely on ChatGPT plus a single-image diffusion model gives you a quick way to bootstrap “same subject” sets, but the raw output is far too homogeneous in language, pose, background and style—forcing you into a complex augmentation pipeline and still leaving gaps compared to genuine large-scale, real-world datasets."
    },
    {
        "question": "What rationale for integrating coupled self-attention across same-subject images during training?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Specifically, we adapt the self-attention layers of the diffusion U-Net so that the attention blocks corresponding to different input images are coupled. That is, the self-attention layer corresponding to each image co-attends to every other image in the sample set. The use of the coupled self-attentions at different levels of hierarchy in the U-Net provides a much stronger representation needed for good input preservation.",
            "More specifially, given a set of same-subject noisy input images, we modify the attention layers of the U-Net to fuse together the self-attention features for different images in the same set. As illustrated in Fig. 6, a coupled self-attention layer has features at each spatial location attending to every other location across all images in the set.",
            "Since the U-Net architecture has attention layers at various resolutions, the use of coupled self-attentions at multiple resolutions makes the generated image set to have consistent high-level semantic features as well as low-level attributes."
        ],
        "final_answer": "By coupling the self-attention layers across all images of the same subject, each image’s features can attend to those of the others. This fused attention at multiple resolutions produces a much stronger joint representation, which (1) preserves fine input appearance, and (2) enforces consistency in both high-level semantics and low-level details across the generated image set.",
        "relevant_elements": [
            "coupled self-attention",
            "same-subject images"
        ],
        "id": 498,
        "masked_question": "What rationale for integrating [mask1] across [mask2] during training?",
        "masked_number": 2,
        "masked_elements": [
            "coupled self-attention",
            "same-subject images"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.06187v1_figure_2.png",
        "paperid": "2407.06187v1",
        "paper_path": "./papers/2407.06187v1.json",
        "figure_id": "2407.06187v1_figure_2.png",
        "caption": "Figure 2:  Overall framework. (a) We generate training data by using large language models and prompting pretrained single-image diffusion models. (b) During training, the JeDi model learns to denoise multiple same-subject images together, where each image attends to every image of the same subject set through coupled self-attention. (c) At inference, personalized generation is performed in an inpainting fashion where the goal is to generate the missing images of the joint-image set.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In the diagram, the red-boxed module is the “coupled self-attention” layer: a version of self-attention that, at each spatial location, lets one image attend to *all* the other images in its same-subject batch.  \n2.  The blue-boxed area is the noisy multi-image sample (i.e. a set of same-subject images, each corrupted with noise for the diffusion training).  \n3.  During training we want the model to learn a *joint* distribution over those multiple views of the same object—so it can preserve both high-level semantics (what the object *is*) and low-level details (its exact appearance) across different contexts.  \n4.  By inserting “coupled self-attention” *across* that set of noisy, same-subject images, each image’s features are forced to co-attend to all the others.  This encourages the network to discover correspondences and shared attributes, yielding much stronger consistency and higher‐fidelity personalization than treating each image independently.\n\nAnswer  \nThey integrate the coupled self-attention blocks across the noisy same-subject image sets so that each image in the set can attend to—and thus share information with—every other image.  This enforces a joint representation of the subject, ensuring consistent high-level semantics and low-level details across multiple views."
    },
    {
        "question": "Why integrate hand-crafted corner case vulnerable source code during data collection?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "We compiled our dataset from three distinct sources: the National Vulnerability Database (NVD), the Software Assurance Reference Dataset (SARD), and a collection of real-world code enhanced with synthetically added vulnerabilities to cover corner cases where real-world code is not available for certain vulnerabilities for proprietary and security reasons.",
            "Combining the SARD and the NVD for training LLMs can significantly enhance their capabilities in vulnerability analysis. While these two datasets offer either fully synthetic or fully real vulnerabilities, our method of injecting vulnerabilities tries to overcome the issues we see in NVD."
        ],
        "final_answer": "To ensure the dataset includes rare or unusual vulnerability scenarios that do not appear in existing public repositories, the authors inject hand-crafted corner-case vulnerabilities into real code whenever real-world examples are unavailable (often due to proprietary or security constraints). This augmentation fills gaps left by NVD and SARD and helps the model generalize to a broader range of vulnerability patterns.",
        "relevant_elements": [
            "Data Collection",
            "Hand-Crafted Corner Case Vulnerable Source Code"
        ],
        "id": 500,
        "masked_question": "Why integrate [mask1] during data collection?",
        "masked_number": 1,
        "masked_elements": [
            "Hand-Crafted Corner Case Vulnerable Source Code"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.04981v1_figure_1.png",
        "paperid": "2411.04981v1",
        "paper_path": "./papers/2411.04981v1.json",
        "figure_id": "2411.04981v1_figure_1.png",
        "caption": "Figure 1: Our Proposed Approach: An overview of our proposed instruct dataset DeBinVul with a sample example comprising a decompiled binary code input and a list of questions (instructions) and answers. Subsequently, using DeBinVul, we train state-of-the-art LLM models to optimize them and elevate their capabilities in assisting reverse engineers in unveiling vulnerabilities in binary code.",
        "qtype": "Design_Rationale",
        "response": "First, let’s align the image with the text. In Step 1 (“Data Collection”) of the diagram you see three stacked cylinders (blue, purple, yellow). The yellow one—boxed in red—is labelled “Hand-Crafted Corner Case Vulnerable Source Code.”\n\nWhy do we pull in that hand-crafted corner-case code?\n\n1.  NVD gives us real-world vulnerabilities, but only for cases that have already been published (and even then often only individual functions that compile cleanly).  \n2.  SARD gives us fully working, annotated examples of many CWEs—but they’re relatively simple, synthetic, and lack the messy complexity of real projects.  \n3.  Even our automatic injection into real repos (using LLMs to pepper in the top CWEs) can only cover the vulnerabilities we choose and that happen to be compilable.\n\nThat still leaves “blind spots”—rare preparatory or bypass-style flaws, edge cases that don’t occur in the wild (or aren’t publicly published because they live in proprietary code), or very intricate interplay of features. By hand-crafting small functions that exhibit exactly those corner-case behaviors, we\n\n•  Fill in the gaps that neither NVD nor SARD (nor automated injection) can provide  \n•  Ensure the model sees truly diverse, bizarre, low-frequency patterns  \n•  Improve its ability to generalize to novel, hard-to-find vulnerabilities in stripped/decompiled binaries\n\nIn short, we integrate hand-crafted corner-case vulnerable source code to guarantee coverage of those rare, subtle flaw patterns that would otherwise be missing from our training data."
    },
    {
        "question": "How does Compile Source Code manage multiple CPU architectures and optimization flags during binary generation?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Each source code function was compiled six times to ensure comprehensive analysis, resulting in six binaries of a single function. This process involved using two compilers, two optimization levels, and four architectures."
        ],
        "final_answer": "Compile Source Code generates binaries for each function by iterating over two compilers, two optimization levels, and four target CPU architectures, producing multiple binary variants per function.",
        "relevant_elements": [
            "Compile Source Code",
            "Decompile Binary Code"
        ],
        "id": 502,
        "masked_question": "How does [mask1] manage multiple CPU architectures and optimization flags during binary generation?",
        "masked_number": 1,
        "masked_elements": [
            "Compile Source Code"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.04981v1_figure_1.png",
        "paperid": "2411.04981v1",
        "paper_path": "./papers/2411.04981v1.json",
        "figure_id": "2411.04981v1_figure_1.png",
        "caption": "Figure 1: Our Proposed Approach: An overview of our proposed instruct dataset DeBinVul with a sample example comprising a decompiled binary code input and a list of questions (instructions) and answers. Subsequently, using DeBinVul, we train state-of-the-art LLM models to optimize them and elevate their capabilities in assisting reverse engineers in unveiling vulnerabilities in binary code.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed block in Step 2 is just the “Compile Source Code with different optimization settings and CPU Arch.”  In other words, that step is literally where we generate all of our binary variants by driving the compiler through every combination of target architecture and optimization flags.  Concretely, for each extracted C/C++ function we:\n\n 1. Pick a compiler (e.g. GCC or Clang),  \n 2. Pick an optimization level (e.g. ‑O0, ‑O2),  \n 3. Pick a CPU architecture target (for us we used four different arches via the appropriate –march/–target flags),  \n 4. Invoke the compiler (and strip symbols with –s),  \n\nand repeat until we have covered every architecture × optimization × compiler pairing (six total binaries per function in our setup).  This systematic sweep ensures that our dataset contains decompilations under all of the optimization and ISA variations we care about."
    },
    {
        "question": "How does Fine Tuning loss func integrate dataset signals to adjust SOTA LLMs parameters?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Fine Tuning loss func",
            "SOTA LLMs"
        ],
        "id": 503,
        "masked_question": "How does [mask1] integrate dataset signals to adjust [mask2] parameters?",
        "masked_number": 2,
        "masked_elements": [
            "Fine Tuning loss func",
            "SOTA LLMs"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.04981v1_figure_1.png",
        "paperid": "2411.04981v1",
        "paper_path": "./papers/2411.04981v1.json",
        "figure_id": "2411.04981v1_figure_1.png",
        "caption": "Figure 1: Our Proposed Approach: An overview of our proposed instruct dataset DeBinVul with a sample example comprising a decompiled binary code input and a list of questions (instructions) and answers. Subsequently, using DeBinVul, we train state-of-the-art LLM models to optimize them and elevate their capabilities in assisting reverse engineers in unveiling vulnerabilities in binary code.",
        "qtype": "Implementation_Details",
        "response": "In the figure, the red‐boxed module is simply the supervised fine‐tuning loop and the blue‐boxed block is the pre-trained LLM (the “SOTA LLMs” block).  During fine-tuning we feed in DeBinVul’s decompiled-binary + instruction pairs, have the model produce an output, compare it to the gold answer under a task-specific loss, and then back-propagate that loss to nudge the weights of the blue-boxed LLM.  In short, the fine-tuning step computes a loss on each example and uses gradient descent to adjust the SOTA LLM’s parameters."
    },
    {
        "question": "How does Conversation Flow Sampling utilize the title tree hierarchy to generate diverse conversation paths?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "To generate coherent and diverse conversations, we implement the following four sampling strategies based on the extracted title trees:",
            "(1) Linear Descent Sampling (LDS): This strategy begins at the root node and permits movement only from parent nodes to their child nodes.",
            "(2) Sibling-Inclusive Descent Sampling (SIDS): This strategy builds on LDS by introducing directional links between sibling nodes.",
            "(3) Single-Tree Random Walk (STRW): This strategy further enhances SIDS by incorporating interconnections among sibling nodes as well as between parent and child nodes, forming a directed graph with bidirectional edges.",
            "(4) Dual-Tree Random Walk (DTRW): It mimics the topic shifts that occur in real conversational scenarios, allowing transitions between two different but related title trees."
        ],
        "final_answer": "Conversation Flow Sampling starts from the hierarchical title tree of Wikipedia pages and then samples paths through it in four ways: (1) Linear Descent Sampling strictly follows parent-to-child links to drill down a single branch; (2) Sibling-Inclusive Descent Sampling adds moves to sibling nodes to explore parallel subtopics; (3) Single-Tree Random Walk makes the tree bidirectional, allowing back-and-forth jumps among parent, child, and sibling nodes; and (4) Dual-Tree Random Walk extends this to switch between two related trees, simulating sudden topic shifts and yielding more varied conversation flows.",
        "relevant_elements": [
            "Extracting Title Tree",
            "Conversation Flow Sampling"
        ],
        "id": 504,
        "masked_question": "How does [mask1] utilize the title tree hierarchy to generate diverse conversation paths?",
        "masked_number": 1,
        "masked_elements": [
            "Conversation Flow Sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23090v1_figure_1.png",
        "paperid": "2410.23090v1",
        "paper_path": "./papers/2410.23090v1.json",
        "figure_id": "2410.23090v1_figure_1.png",
        "caption": "Figure 1: Part (a) is an overview of the CORAL dataset construction process. The red arrows show the sampled conversation flow, with numerical labels on the nodes indicating the round of the sampled conversation turns. The content under each sampled (sub)title serves as the conversational response in CORAL.\nPart (b) is the three conversation compression strategies in conversational RAG.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in the figure is the “Conversation Flow Sampling” step.  We know from §3.2.2 that once we’ve extracted a title tree (where every Wikipedia heading is a node and parent–child links give us a directed hierarchy), we turn that static tree into many different dialogue “paths” by literally walking over it in different ways.  Concretely:\n\n1.  We regard each heading (H1 … H6) as a node, with directed edges from a parent heading to its children.  \n2.  Depending on which edges we allow the sampler to follow, we get different conversation shapes.  \n3.  We implement four traversal strategies over that directed tree:  \n   •  Linear Descent Sampling (LDS): only follow parent→child edges, simulating a deepening, turn-by-turn drill-down.  \n   •  Sibling‐Inclusive Descent Sampling (SIDS): add edges linking sibling nodes at the same level, so after diving deeper you can also “pivot” laterally to a sister subtopic.  \n   •  Single-Tree Random Walk (STRW): make all parent–child and sibling links bidirectional, so you can wander up, down, or across within that same tree.  \n   •  Dual-Tree Random Walk (DTRW): allow jumps between two related root topics (found via BM25 retrieval), so the dialogue can even “switch trees” and explore a second, related subject.  \n\nBy treating the heading hierarchy as a little graph and sampling different kinds of walks through it, Conversation Flow Sampling automatically yields diverse, coherent multi-turn dialogues that reflect both depth (drilling down) and breadth (side trips and topic shifts)."
    },
    {
        "question": "How does LLM Summarization transform retrieved passages into concise Passage Summary for generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM Summarization",
            "Passage Summary"
        ],
        "id": 505,
        "masked_question": "How does [mask1] transform retrieved passages into concise Passage Summary for generation?",
        "masked_number": 1,
        "masked_elements": [
            "LLM Summarization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.23090v1_figure_1.png",
        "paperid": "2410.23090v1",
        "paper_path": "./papers/2410.23090v1.json",
        "figure_id": "2410.23090v1_figure_1.png",
        "caption": "Figure 1: Part (a) is an overview of the CORAL dataset construction process. The red arrows show the sampled conversation flow, with numerical labels on the nodes indicating the round of the sampled conversation turns. The content under each sampled (sub)title serves as the conversational response in CORAL.\nPart (b) is the three conversation compression strategies in conversational RAG.",
        "qtype": "Implementation_Details",
        "response": "Here’s how I reason through the figure and the text step by step:\n\n1. In Figure 1(b) we see three different “compression” paths before feeding inputs into the generator:  \n   - “Selected History” (just picking a subset of prior turns),  \n   - “Query Rewrite” (rewriting the user’s query and re-retrieving),  \n   - and the red‐boxed path, labeled “LLM Summarization.”\n\n2. The text (Section 4) makes clear that beyond compressing the conversation history, they also compress the *retrieved passages* by “simply taking LLMs as the compression function.” In other words, after the retriever returns a bunch of documents, they invoke an LLM to *summarize* those documents down to their essentials.\n\n3. That summarized output is what Figure 1(b) calls the “Passage Summary.” Rather than handing all 10 or 20 raw passages (which might run into thousands of tokens) to the generator, they hand it a handful of *concise* paragraphs generated by the LLM summarizer.\n\n4. So, “[mask1]” is the LLM Summarization module. It ingests the full set of retrieved passages, prompts an LLM to extract and fuse the key points, and emits a short, information‐dense “Passage Summary.” This summary is then concatenated with the (also compressed) conversation history and fed to the final response generator.\n\nAnswer: [mask1] is an LLM‐based summarizer that takes the retrieved passages as input, condenses them into a brief, fused summary of their salient content (the “Passage Summary”), and supplies that distilled text—rather than the full documents—to the generation model."
    },
    {
        "question": "How does Conversation Flow Sampling leverage hierarchical title trees compared to conversational query rewriting approaches?",
        "relevant_section_ids": [
            "2.2",
            "3.2.2"
        ],
        "relevant_context": [
            "Conversational search enables users to interact with retrieval systems through multi-turn dialogues (Mo et al., 2024a ###reference_b40###). Two main approaches are conversational query rewriting (CQR) and conversational dense retrieval (CDR). CQR transforms context-dependent queries into fully rewritten versions for ad-hoc retrieval, focusing on selecting relevant tokens from the conversation history (Voskarides et al., 2020 ###reference_b55###; Kumar and Callan, 2020 ###reference_b26###; Lin et al., 2021b ###reference_b32###) or using LLMs to generate rewrites (Lin et al., 2020 ###reference_b33###; Yu et al., 2020 ###reference_b65###; Vakulenko et al., 2021 ###reference_b54###; Wu et al., 2022 ###reference_b59###).",
            "To generate coherent and diverse conversations, we implement the following four sampling strategies based on the extracted title trees: (1) Linear Descent Sampling (LDS)… (2) Sibling-Inclusive Descent Sampling (SIDS)… (3) Single-Tree Random Walk (STRW)… (4) Dual-Tree Random Walk (DTRW)…"
        ],
        "final_answer": "Conversation Flow Sampling exploits the natural hierarchy of Wikipedia subheadings by constructing title trees and then sampling paths through those trees—e.g., parent-to-child sequences, sibling explorations, and random walks across related trees—to produce coherent multi-turn conversations that follow the document structure. In contrast, conversational query rewriting approaches ignore external document structure and instead rewrite each turn’s query into a self-contained form by selecting or generating tokens from the preceding dialogue history.",
        "relevant_elements": [
            "Conversation Flow Sampling",
            "Contextualization of Questions"
        ],
        "id": 506,
        "masked_question": "How does [mask1] leverage hierarchical title trees compared to conversational query rewriting approaches?",
        "masked_number": 1,
        "masked_elements": [
            "Conversation Flow Sampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.23090v1_figure_1.png",
        "paperid": "2410.23090v1",
        "paper_path": "./papers/2410.23090v1.json",
        "figure_id": "2410.23090v1_figure_1.png",
        "caption": "Figure 1: Part (a) is an overview of the CORAL dataset construction process. The red arrows show the sampled conversation flow, with numerical labels on the nodes indicating the round of the sampled conversation turns. The content under each sampled (sub)title serves as the conversational response in CORAL.\nPart (b) is the three conversation compression strategies in conversational RAG.",
        "qtype": "Literature_Background",
        "response": "Here “[mask1]” is the Conversation‐Flow‐Sampling component of CORAL.  In contrast to conversational‐query‐rewriting methods (which simply take your last few utterances and try to turn them into a self-contained search query), Conversation‐Flow‐Sampling actually reads the Wikipedia heading hierarchy itself as a tree and “walks” that tree to generate each turn of the dialogue.  By treating every H1–H6 heading as a node and using its parent–child and sibling links (via Linear Descent, Sibling-Inclusive Descent, Single-Tree Random-Walk and Dual-Tree Random-Walk), it can (a) drill down in logical order, (b) branch out to related subtopics, or (c) jump to a different but related article—thereby building multi‐turn conversations that mirror the page’s built-in structure.  Query-rewriting, by contrast, never looks at this hierarchy; it only reformulates your question based on the text of the previous turns."
    },
    {
        "question": "How do LLM Summarization approaches relate to existing passage summarization methods for conversation history compression?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM Summarization",
            "Passage Summary"
        ],
        "id": 507,
        "masked_question": "How do [mask1] approaches relate to existing passage summarization methods for conversation history compression?",
        "masked_number": 1,
        "masked_elements": [
            "LLM Summarization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.23090v1_figure_1.png",
        "paperid": "2410.23090v1",
        "paper_path": "./papers/2410.23090v1.json",
        "figure_id": "2410.23090v1_figure_1.png",
        "caption": "Figure 1: Part (a) is an overview of the CORAL dataset construction process. The red arrows show the sampled conversation flow, with numerical labels on the nodes indicating the round of the sampled conversation turns. The content under each sampled (sub)title serves as the conversational response in CORAL.\nPart (b) is the three conversation compression strategies in conversational RAG.",
        "qtype": "Literature_Background",
        "response": "We can break the answer into two pieces: first, identify what “[mask1]” is in the figure, and second, explain how that particular strategy ties back to the body of work on passage-level summarization in RAG systems.\n\n1) Identifying “[mask1]”  \nIf you look at Figure 1(b), the only box outlined in red is the one labelled “LLM Summarization.”  In the text this is described as our third conversation-compression strategy: we take the entire multi-turn history (and even the retrieved passages) and hand them off to an LLM, asking it to generate a short abstractive summary that replaces the full, unwieldy dialogue.\n\n2) Connecting to existing passage-summarization methods  \nIn single-turn RAG pipelines it is now common to have a “post-retrieval” filtering or summarization step (see Xu et al. 2023; Yang et al. 2023; Wang et al. 2023; Jin et al. 2024, etc.), where you run your retrieved documents through an LLM to strip out boilerplate or irrelevant clauses and produce a distilled “passage summary” that still contains all the facts you need.  Our LLM-Summarization strategy is exactly that same recipe, but applied to *conversation history* instead of a static web page or article.  Concretely:\n\n- We treat the entire turn-by-turn exchange as one big “document.”  \n- We prompt GPT-style LLMs (in our experiments GPT-4 or similar open-source models) to produce a short, high-coverage summary of that document.  \n- We then feed that summary (rather than the raw 1,500–2,000 tokens of history) into both the retriever and the generator.\n\nBy doing so we inherit all of the benefits that those passage-summarization modules brought to single-turn RAG—noise reduction, length control, better focus on the salient facts—while making them work in a multi-turn conversational setting.  In other words, our “LLM Summarization” is nothing more exotic than the community’s standard LLM-driven passage summarizer, repurposed to compress dialogue history."
    },
    {
        "question": "How does employing a functional connectivity matrix parallel adjacency utilization in graph neural network methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "functional connectivity matrix"
        ],
        "id": 508,
        "masked_question": "How does employing a [mask1] parallel adjacency utilization in graph neural network methods?",
        "masked_number": 1,
        "masked_elements": [
            "functional connectivity matrix"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.08221v1_figure_1.png",
        "paperid": "2411.08221v1",
        "paper_path": "./papers/2411.08221v1.json",
        "figure_id": "2411.08221v1_figure_1.png",
        "caption": "Figure 1: SynapsNet overview. (a) The functional connectivity defined between neurons on the model and how input current is inferred based on functional connectivity and population activity (b) An example input frame to the dynamical model which includes past activity over the context window, past input current, past behavioral data, and the unique embedding of the target neuron. (c) The three sets of parameters in SynapsNet: adjacency matrix A𝐴Aitalic_A for each session, embedding vector E𝐸Eitalic_E for each neuron, and dynamical model.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In the paper’s Figure 1c (the red‐boxed panel) the authors label “Functional connectivity matrix A ∈ ℝⁿˢ×ⁿˢ”—one such matrix for every recording session.  \n2. In graph‐neural‐network (GNN) language an N×N matrix of connection weights is exactly what one normally calls the adjacency matrix: it tells you which nodes (here: neurons) talk to which, and with what strength.  \n3. SynapsNet “plugs” that learned functional‐connectivity matrix A in exactly the same slot that a GNN would use a fixed (or hand‐designed) adjacency matrix for neighbor‐feature aggregation.  Concretely, at each time t the model computes each neuron’s total “input current” as Iᵢ(t)=∑ⱼAᵢⱼ·Xⱼ(t) (plus self‐terms), just as a one‐hop GNN would do a weighted sum of neighbor features.  \n4. By treating A as a trainable, session‐specific adjacency and backpropagating through it together with the shared dynamical model f and per‐neuron embeddings E, SynapsNet both (a) learns the “who–to–whom” wiring graph from data and (b) reuses the same GNN‐style message‐passing step to feed each neuron’s RNN/GRU.  \n\nAnswer  \nThey simply replace the usual, hand‐set adjacency in a GNN with a session‐specific, trainable functional‐connectivity matrix A (the red‐boxed object).  At each time step SynapsNet does the exact same neighbor‐aggregation step you see in GNNs—summing each neuron’s neighbors’ activities weighted by the learned A—before passing that summed “current” into its shared dynamical (GRU) block."
    },
    {
        "question": "How might integrating the context window resemble state-space model approaches in time-series forecasting?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "context window"
        ],
        "id": 509,
        "masked_question": "How might integrating the [mask1] resemble state-space model approaches in time-series forecasting?",
        "masked_number": 1,
        "masked_elements": [
            "context window"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.08221v1_figure_1.png",
        "paperid": "2411.08221v1",
        "paper_path": "./papers/2411.08221v1.json",
        "figure_id": "2411.08221v1_figure_1.png",
        "caption": "Figure 1: SynapsNet overview. (a) The functional connectivity defined between neurons on the model and how input current is inferred based on functional connectivity and population activity (b) An example input frame to the dynamical model which includes past activity over the context window, past input current, past behavioral data, and the unique embedding of the target neuron. (c) The three sets of parameters in SynapsNet: adjacency matrix A𝐴Aitalic_A for each session, embedding vector E𝐸Eitalic_E for each neuron, and dynamical model.",
        "qtype": "Literature_Background",
        "response": "In SynapsNet’s diagram (the red‐boxed region in panel b), each “input frame” to the dynamical model is nothing more than a big state vector made up of  \n  1. A sliding window of past neural observations X⁽ᶦ⁾ₜ₋w+1:ₜ  \n  2. Inferred input currents I⁽ᶦ⁾ₜ₋w+1:ₜ  \n  3. Behavioral covariates B⁽ᶦ⁾ₜ₋w+1:ₜ  \n  4. A fixed embedding E⁽ᶦ⁾ for that neuron  \n\nIf you look at classical state‐space models (SSMs) in time‐series forecasting—xₜ₊₁ = F·xₜ + G·uₜ + noise, yₜ = H·xₜ + measurement noise—you’ll see exactly the same ingredients:  \n • A latent “state” xₜ that carries through time  \n • Exogenous or control inputs uₜ that drive the state  \n • An observation or emission equation yₜ  \n\nBy concatenating past X’s, inferred currents, and behavior into one vector, SynapsNet effectively builds its own empirical “state” at time t.  The GRU (or whatever f you choose) then plays the role of the SSM’s transition function F (with G mixing in the exogenous inputs), while the final read-out is like the SSM’s emission H.  In other words, the red‐boxed context window is simply a learned, high‐dimensional state‐vector construction that is propagated forward exactly as you would in a state‐space forecasting approach."
    },
    {
        "question": "How does PN-Descriptor positive-negative supervision influence adapter weight adjustments across semantic feature levels?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "Specifically, the Positive-Negative (PN) descriptors are derived as follows: i.e., P(ositive): “A person with an expression of {Cls}.”, and N(egative): “A person with an expression of no {Cls}.” … Keeping the original weights of these well-trained layers, we introduce trainable lightweight adapters after each frozen layer L, denoted as A^{pos} and A^{neg} for positive and negative textual supervision, respectively. (Sec. 3.2)",
            "Through the aforementioned semantically hierarchical information mining process, we obtain: 1) low-level video frame feature r_v, 2) middle-level face parsing features r_p and face landmark features r_l, and 3) high-level fine-grained description features r_d. … Specifically, given a specific video, the supervision for the i-th class is represented by both the positive c_i^{pos} and negative c_i^{neg}. Suppose any representation r^k, the similarity between r^k and c_i^{pos}, as well as r^k and c_i^{neg} is defined by calculating the cosine similarity: S_i^k = cos(r^k, c_i^{pos}) − cos(r^k, c_i^{neg}), which further distinguishes similarity among similar categories. Then, by finding the max similarity across all the categories, we obtain S^k. … Normalizing these similarities, we obtain the weights corresponding to that representation as α^k = exp(S^k)/∑_j exp(S^j). Such weights … resulting in the corresponding weights α_v, α_p, α_l, and α_d. Then the overall multi-modal representation M … is obtained as: M = α_v r_v + α_p r_p + α_l r_l + α_d r_d. (Sec. 3.4)"
        ],
        "final_answer": "The PN descriptors drive adaptive weighting of each semantic-level feature by computing, for every representation (video, parsing, landmarks, description), the difference in cosine similarity to its positive versus negative text embeddings.  After taking the maximum over classes, these similarity differences are normalized into weights (α_v, α_p, α_l, α_d) that scale each level’s output.  In this way, positive-negative supervision directly adjusts how much each adapter-enhanced modality contributes to the final fused representation.",
        "relevant_elements": [
            "PN-Descriptor",
            "Adapters",
            "Semantic Levels"
        ],
        "id": 513,
        "masked_question": "How does [mask1] positive-negative supervision influence adapter weight adjustments across semantic feature levels?",
        "masked_number": 1,
        "masked_elements": [
            "PN-Descriptor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.02157v2_figure_1.png",
        "paperid": "2407.02157v2",
        "paper_path": "./papers/2407.02157v2.json",
        "figure_id": "2407.02157v2_figure_1.png",
        "caption": "Figure 1. \nFrameworks for DFER.",
        "qtype": "Experimental_Results",
        "response": "Here is the step‐by‐step reasoning for how the red-boxed “PN-Descriptor” supervision (i.e. the Positive-Negative text augmentation) drives the adapter weights at all three semantic levels:\n\n1.  \n   Identify [mask1]:  \n   In the red box of Fig. 1(c) sits the “PN-Descriptor” legend, which tells us that for each class we generate  \n   - one **Positive** template (“A person with an expression of {Cls}.”)  \n   - one **Negative** template (“A person with an expression of no {Cls}.”)  \n\n2.  \n   Where the adapters live:  \n   - We freeze the main CLIP text encoder.  \n   - After each transformer layer in that text encoder we insert two tiny trainable adapters: one for **positive** (`A^{p,l}`) and one for **negative** (`A^{n,l}`).\n\n3.  \n   Building the two text features:  \n   - We feed the positive template tokens through the frozen layers plus the positive adapters, yielding a “positive text feature.”  \n   - We feed the negative template tokens through the same frozen layers but the negative adapters, yielding a “negative text feature.”\n\n4.  \n   Contrastive signal on every adapter:  \n   - We compute cosine similarity between each **video feature** (and likewise each of the other semantic–level features: middle-level parsing/landmarks, high-level fine-grained captions) and both the positive and negative text features.  \n   - We form a loss that *pushes* the model to raise similarity with the **positive** descriptor and *lower* similarity with the **negative** one.\n\n5.  \n   Effect on adapter weights across semantic levels:  \n   - Because the **same** positive/negative text adapters are used to score *all* three modalities, the resulting gradients simultaneously refine every adapter.  \n   - Concretely, at low-level (frame) adapters, mid-level (parsing/landmarks) adapters, and high-level (caption) adapters, the “pull” from the positive description versus the “push” from the negative description sculpts each adapter’s weights so that it better aligns correct facial-expression cues and suppresses misleading ones.\n\n6.  \n   Final outcome in fusion:  \n   - Those better-tuned adapters produce more discriminative embeddings at each semantic level.  \n   - An adaptive fusion then re-weights the three streams in proportion to how well they match the positive vs. negative descriptors, yielding a stronger overall classifier.  \n\nIn short, the PN-Descriptor supervision wraps every adapter in a built-in contrastive tug—pulling it toward features that match the true class wording and pushing it away from “no-class” wording—so that **all** of the low, middle, and high–level adapter modules learn to emphasize genuine expression evidence and downplay spurious signals."
    },
    {
        "question": "What alternative design could replace Multi-modal LLM for high-level description?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Multi-modal LLM"
        ],
        "id": 514,
        "masked_question": "What alternative design could replace [mask1] for high-level description?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-modal LLM"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.02157v2_figure_1.png",
        "paperid": "2407.02157v2",
        "paper_path": "./papers/2407.02157v2.json",
        "figure_id": "2407.02157v2_figure_1.png",
        "caption": "Figure 1. \nFrameworks for DFER.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed module in Fig 1(c) is the “Multi-modal LLM” (Video-LLaVA) that takes a short video clip and spits out a free-form, fine-grained textual description of the facial motion (“the girl’s eyes widen, eyebrows furrow…,” etc.).  An obvious alternative is to throw away the huge MLLM entirely and replace it with a much lighter, purely facial-action-unit-based text generator.  Concretely:  \n1.  Run an off-the-shelf Facial Action Coding System (FACS) detector (e.g. OpenFace or any AU-prediction network) over your frames.  \n2.  Map the detected AUs (cheek raiser, lip corner puller, inner brow raiser, etc.) through a simple template or rule­-based engine to form short phrases (“inner brow raise,” “lip corner puller,” “jaw drop,” etc.).  \n3.  Concatenate those phrases into a single “high-level” caption in place of the MLLM’s output.  \n\nThat AU→template pipeline gives you a compact, interpretable “fine-grained description” branch without ever invoking a huge multi-modal LLM."
    },
    {
        "question": "What ethical concerns arise from negative PN-Descriptor prompts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "PN-Descriptor"
        ],
        "id": 515,
        "masked_question": "What ethical concerns arise from negative [mask1] prompts?",
        "masked_number": 1,
        "masked_elements": [
            "PN-Descriptor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.02157v2_figure_1.png",
        "paperid": "2407.02157v2",
        "paper_path": "./papers/2407.02157v2.json",
        "figure_id": "2407.02157v2_figure_1.png",
        "caption": "Figure 1. \nFrameworks for DFER.",
        "qtype": "Others",
        "response": "Chain-of-thought:  \n1. In the figure the red-boxed “[mask1]” is the PN-Descriptor module, which generates both positive prompts (“A person with an expression of {Label}”) and negative prompts (“A person with an expression of no {Label}”).  \n2. Negative prompts explicitly teach the model to focus on the absence of an emotion (“no Angry,” “no Sad,” etc.) rather than its presence.  \n3. In practice, framing data this way risks teaching the network spurious associations around “what someone is not feeling” – e.g. treating the lack of happiness as evidence of depression – rather than grounding in measured facial cues.  \n4. Such misframing can amplify pre-existing biases in the underlying vision–language model, leading to systematic misclassification of certain demographic groups or clinical populations.  \n5. Downstream, this may foster unfair or discriminatory outcomes (e.g. wrongly labeling people as “no Fear” or “no Surprise”), violate privacy by inferring sensitive mental-health attributes, and even misguide high-stakes applications like driver monitoring or mental-health screening.  \n\nAnswer:  \nNegative PN prompts (“A person with an expression of no {Label}”) raise ethical concerns because they bias the model toward “absence‐based” labels, which can amplify or introduce unfair stereotypes, lead to misdiagnoses or mislabeling (particularly in sensitive domains like mental-health or driver safety), violate individual privacy by over-inferring emotional states, and overall risk discriminatory or harmful treatment of certain groups."
    },
    {
        "question": "What limitations might discrete Graph Diffusion introduce when scaling 3DSG generation to complex scenes?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Graph Diffusion",
            "3DSG"
        ],
        "id": 516,
        "masked_question": "What limitations might discrete [mask1] introduce when scaling 3DSG generation to complex scenes?",
        "masked_number": 1,
        "masked_elements": [
            "Graph Diffusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.15312v1_figure_2.png",
        "paperid": "2410.15312v1",
        "paper_path": "./papers/2410.15312v1.json",
        "figure_id": "2410.15312v1_figure_2.png",
        "caption": "Figure 2: Overall Framework of the S3D. The figure presents the dual processes of ST2I and SI2T. The RED block represents the hard X→→\\to→3D processes, and the GREEN block represents the 3D→→\\to→X processes.\nThere are three diffusion processes in total, i.e., a shared graph diffusion model for VSG/TSG→→\\to→3DSG generation, and the image diffusion model and text diffusion model.",
        "qtype": "Others",
        "response": "First, let us identify what “​[mask1]” must be in that red‐boxed region.  In Figure 2 the red outline encloses the “Graph Diffusion” module that takes a 2D scene graph (from either text or image) and, via a discrete diffusion process, “grows” it into a full 3D scene graph (3DSG).  In other words, [mask1] is that discrete graph‐diffusion step.\n\nNow: what are the downsides of using a *discrete* diffusion model for large, complicated 3D scene graphs?\n\n1.  Quantization artifacts and loss of spatial precision  \n    – Because every object, attribute or relation in the 3DSG must map to one of a finite set of code‐book entries, you only ever place nodes and edges at a handful of fixed “bins.”  As the scene gets more complex—lots of objects, subtle offsets in position or orientation—those bins become too coarse.  You end up “snapping” truly continuous geometry into a small number of discrete states.  \n\n2.  Limited codebook capacity  \n    – To raise your spatial resolution you’d need to dramatically blow up the size of the discrete vocabulary (i.e. more codes), which in turn inflates the size of all the transition matrices and slows both training and inference.  There is a classic trade‐off between granularity and computational cost that gets worse in complex scenes.  \n\n3.  Combinatorial explosion of graph configurations  \n    – A richly populated 3D scene graph can have hundreds of nodes and edges.  On each diffusion step you must consider a probability transition for every node’s discrete index; as the graph grows this becomes quadratic (or worse) in the number of elements.  Even with efficient sparse implementations the sampling cost climbs steeply.  \n\n4.  Difficulty representing unseen or out-of-vocabulary relations  \n    – The discrete setup assumes a fixed set of object and relation types seen at training time.  As you scale to more varied environments (new objects, novel spatial predicates) you either have to retrain with a larger ontology or fall back on “unknown” codes, which limits generalization.  \n\n5.  Poor modeling of truly continuous attributes  \n    – Some spatial properties—exact Euclidean distance, continuous rotations, smoothly varying scales—are awkward to capture in a purely discrete latent.  If you tried to embed them in the graph you’d have to invent new attribute nodes whose semantics are themselves quantized, further compounding quantization error in complex scenes.\n\nIn short, while discrete graph diffusion is very efficient for small or moderately complex 3DSGs, as scenes become denser and spatial relationships more nuanced you hit hard walls in resolution, capacity and sampling speed. That is the crux of the limitation of discrete [mask1] when you try to scale 3DSG generation to very complex real-world scenes."
    },
    {
        "question": "What alternative modeling approach could replace Image Diffusion to balance spatial fidelity and computational efficiency?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Instead of directly employing the SoTA generative I2T models or diffusion-based T2I methods, we consider a solution fully based on discrete diffusions [3], due to several key rationales.",
            "Primarily, for VSU, the most crucial spatial information that determines a scene consists of objects and their relationships, which presents the characteristic of discretization and combination in the spatial layout, while other background and spatial unrelated information would be noisy.",
            "Thus, the discrete representation is more appropriate to model pure spatial semantics in our scenario.",
            "Moreover, the discrete diffusion works on the limited index space [28, 54, 35, 98], which is much more computationally efficient, especially for visual synthesis tasks."
        ],
        "final_answer": "Replace Image Diffusion with a discrete diffusion model (operating over discrete, spatial-aware 3D scene graph representations) to better capture spatial fidelity while reducing computational cost.",
        "relevant_elements": [
            "Image Diffusion"
        ],
        "id": 517,
        "masked_question": "What alternative modeling approach could replace [mask1] to balance spatial fidelity and computational efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Image Diffusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.15312v1_figure_2.png",
        "paperid": "2410.15312v1",
        "paper_path": "./papers/2410.15312v1.json",
        "figure_id": "2410.15312v1_figure_2.png",
        "caption": "Figure 2: Overall Framework of the S3D. The figure presents the dual processes of ST2I and SI2T. The RED block represents the hard X→→\\to→3D processes, and the GREEN block represents the 3D→→\\to→X processes.\nThere are three diffusion processes in total, i.e., a shared graph diffusion model for VSG/TSG→→\\to→3DSG generation, and the image diffusion model and text diffusion model.",
        "qtype": "Others",
        "response": "Here is the reasoning in detail:\n\n1.  Identifying [mask1]  \n    In Figure 2 the red‐boxed module on the ST2I side is the discrete image‐diffusion network that takes the 3D scene‐graph embedding (3DSG) plus CLIP‐based text features and gradually denoises through T steps to produce the final image latents.\n\n2.  Why we might want an alternative  \n    – Diffusion nets give very good fidelity and obey the spatial constraints encoded in the 3DSG, but they require dozens to hundreds of U-Net passes at inference, which is still quite costly.  \n    – If we can accept a slightly looser “soft” compliance with the layout we can cut generation time dramatically.\n\n3.  A concrete swap-in that balances fidelity vs. speed  \n    A common drop-in is to replace the diffusion pass with a conditional GAN (or a VQ-GAN/Transformer) that is trained to map the same 3DSG + text conditioning into an image in one shot:\n    •  Conditional GAN (for example SPADE/GauGAN) on top of the 3DSG layout  \n    •  Or a VQ-VAE encoder for images plus an autoregressive Transformer to predict codebook tokens, conditioned on 3DSG  \n\n4.  Why this helps  \n    – A GAN or VQ-GAN/Transformer only needs a single forward pass (or a handful of Transformer layers), instead of 50–100 diffusion steps.  \n    – These models can still be trained to respect the 3DSG’s spatial layout (via spatially‐aware normalization layers in a GAN or cross‐attention to the graph tokens in a Transformer).  \n    – Empirically one sees a 5×–10× speedup at inference with only a modest drop in strict adherence to the 3D‐scene constraints.\n\nAnswer: You could swap out the red‐boxed image‐diffusion U-Net for a conditional GAN or a VQ-GAN + autoregressive Transformer that is conditioned on the same 3DSG and text features. This one‐shot (or few‐shot) generative model gives up a bit of hardcore diffusion fidelity but runs an order of magnitude faster, trading off only a small loss in spatial accuracy."
    },
    {
        "question": "What motivates dual feature sharing from Graph Diffusion into both Image Diffusion and Text Diffusion?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "First, [intermediate processing sharing], they can complement and benefit each other. For SI2T, the 'Image3D' reasoning process is challenging in acquiring necessary 3D features, whereas the descriptive '3DText' process is relatively easier. Conversely, for ST2I, the 'Text3D' process requires complex reasoning of the 3D scene feature, while rendering '3DImage' is much more straightforward. Ideally, if complementing the information during each learning process, i.e., letting the easy part aid the hard part, it should enhance the performance of both tasks.",
            "Second, [3D scene feature sharing], both dual tasks urgently require modeling of the respective 3D features, where such stereospecific insights in 3D perspective can be essentially shared and also complementary between each other.",
            "At the meanwhile, the intermediate features of the '3DX' (X means text or image) diffusion steps are also passed to the counterpart hard 'X3D' processes for further facilitation."
        ],
        "final_answer": "The dual feature sharing is driven by the observation that (1) the two tasks are complementary — the ‘easy’ part of one (e.g. text-to-3D) can help the ‘hard’ part of the other (e.g. image-to-3D), and (2) both require rich, stereospecific 3D scene information. By passing the 3DSG-derived features (and intermediate diffusion representations) from the shared Graph Diffusion model into both the Image Diffusion and Text Diffusion processes, the framework lets each modality leverage the other’s strengths and better align spatial semantics.",
        "relevant_elements": [
            "Graph Diffusion",
            "Image Diffusion",
            "Text Diffusion"
        ],
        "id": 519,
        "masked_question": "What motivates dual feature sharing from [mask1] into both Image Diffusion and Text Diffusion?",
        "masked_number": 1,
        "masked_elements": [
            "Graph Diffusion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.15312v1_figure_2.png",
        "paperid": "2410.15312v1",
        "paper_path": "./papers/2410.15312v1.json",
        "figure_id": "2410.15312v1_figure_2.png",
        "caption": "Figure 2: Overall Framework of the S3D. The figure presents the dual processes of ST2I and SI2T. The RED block represents the hard X→→\\to→3D processes, and the GREEN block represents the 3D→→\\to→X processes.\nThere are three diffusion processes in total, i.e., a shared graph diffusion model for VSG/TSG→→\\to→3DSG generation, and the image diffusion model and text diffusion model.",
        "qtype": "Design_Rationale",
        "response": "The little red box in the picture is our shared “Graph‐diffusion” module – the X→3D piece that takes either a visual scene‐graph or a text scene‐graph and turns it into a full 3D scene‐graph.  We then feed its hidden representations (the orange arrows) into both the Image‐diffusion branch and the Text‐diffusion branch.  \n\nWhy do we do that?  As we argue in §1, ST2I and SI2T are perfect “duals” of one another – each has one half that is hard (X→3D) and one half that is easy (3D→X).  By sharing the intermediate 3D features from the graph‐diffusion block:  \n  1.  The “easy” 3D→Image step can help the “hard” Text→3D side, and vice versa.  \n  2.  Both tasks need the same underlying 3D spatial semantics, so injecting those rich, modality‐agnostic graph features into both downstream diffusers gives them much stronger spatial grounding.  \n\nIn short, the dual feature‐sharing from the Graph‐diffusion block is motivated by the complementary strengths of the two tasks (letting the easy half help the hard half) and by the need to supply both image and text generators with the same, faithful 3D scene information."
    },
    {
        "question": "What motivates integrating Mask Attribute Conductance in layer importance analysis?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Building upon this insight, we explore the possibility of minimizing the influence of disparities in data input formats via model fine-tuning. To maintain the learned priors, it is essential to retain pre-trained parameters as extensively as possible while employing the fewest but most effective layers for fine-tuning.",
            "To tackle this, we introduce the concept of mask attribution conductance, which quantifies the importance of each layer concerning the fine-tuning objective. We then identify the top-k% most critical layers for fine-tuning."
        ],
        "final_answer": "The authors integrate Mask Attribute Conductance to quantify each layer’s contribution to bridging the distribution shift caused by moving from masked pre-training inputs to whole-image inference. This allows them to fine-tune only the few most critical layers—thereby adapting to the input integrity gap while preserving the majority of the pre-trained image priors.",
        "relevant_elements": [
            "Mask Attribute Conductance",
            "layer importance analysis"
        ],
        "id": 520,
        "masked_question": "What motivates integrating [mask1] in layer importance analysis?",
        "masked_number": 1,
        "masked_elements": [
            "Mask Attribute Conductance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.19403v1_figure_2.png",
        "paperid": "2409.19403v1",
        "paper_path": "./papers/2409.19403v1.json",
        "figure_id": "2409.19403v1_figure_2.png",
        "caption": "Figure 2: The illumination of our overall pipeline. 1) Pre-training the model with mask image pre-training method tailored to low-level vision. We randomly mask degraded images at the pixel level with a 50%percent5050\\%50 % masking ratio and reconstruct the clean images. 2) The Fine-tuning stage is followed to overcome the input integrity gap caused by changing masked input during pre-training into the whole image during inference.\nWe analyze the importance of each network layer for resolving the input integrity gap according to the proposed MAC and rank them in descending order. The top k%percent𝑘k\\%italic_k % of network layers are selected for fine-tuning on the complete image.",
        "qtype": "Design_Rationale",
        "response": "The little “red‐box” module is nothing mystical – it is simply our Mask‐Attribute‐Conductance (MAC) calculator.  We were forced to introduce it because, after pre‐training with random masks, we discovered that feeding the full (unmasked) image straight through the network produces a noticeable distribution shift.  Rather than wholesale re‐training all of the weights, we asked:\n\n  1.  Which layers really *matter* when you go from masked inputs to unmasked inputs?  \n  2.  If only a handful of layers are responsible for correcting that gap, we can freeze the rest (and thus preserve all the heavy‐lifting the network already did in pre‐training).\n\nMAC is exactly the tool that lets us answer “how important is layer *ℓ* in bridging the masked‐to‐full‐input gap?”  By defining a smooth “mask‐attribute path” from zeroed‐out pixels to real pixels (using sigmoid approximations) and then computing an integrated‐gradient–style conductance along that path, we end up with a single scalar importance score per layer.  We then rank all layers by that score and only fine‐tune the top-k percent, leaving the rest untouched.  \n"
    },
    {
        "question": "How does Mask Attribute Conductance quantify each layer’s contribution along the Mask Attribute Path?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "To maintain the learned priors, it is essential to retain pre-trained parameters as extensively as possible while employing the fewest but most effective layers for fine-tuning. To tackle this, we introduce the concept of mask attribution conductance, which quantifies the importance of each layer concerning the fine-tuning objective.",
            "Considering such a nonlinear path \\(X_m\\) from zero input \\(X_0\\) to whole input \\(X_1\\), which path function \\(T\\) satisfies: \\(T(0)=X_0,\\;T(1)=X_1\\). We define this path as a Mask Attribute Path (MAP).",
            "However, \\(T\\) is not differentiable, making it an invalid attribute path function. To solve this problem, we use a group of sigmoid-like functions \\(g_k\\) to approximate \\(T\\).",
            "In practice, we use N-steps discretization to approximate the integral form of Eq. 13, which follows the same procedure as integrated gradients: summing the layer conductance of each discretized step along the Mask Attribute Path."
        ],
        "final_answer": "Mask Attribute Conductance (MAC) quantifies each layer’s contribution by measuring that layer’s conductance (i.e. the accumulated gradient of the network’s output with respect to its activations) integrated along a continuous path from a masked input to the full input. This ‘‘Mask Attribute Path’’ is approximated via sigmoid‐based mask transitions, and the conductance of each layer is summed (using an N-step discrete integral) to yield a scalar importance score for fine-tuning.",
        "relevant_elements": [
            "Mask Attribute Conductance",
            "Mask Attribute Path"
        ],
        "id": 522,
        "masked_question": "How does [mask1] quantify each layer’s contribution along the Mask Attribute Path?",
        "masked_number": 1,
        "masked_elements": [
            "Mask Attribute Conductance"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.19403v1_figure_2.png",
        "paperid": "2409.19403v1",
        "paper_path": "./papers/2409.19403v1.json",
        "figure_id": "2409.19403v1_figure_2.png",
        "caption": "Figure 2: The illumination of our overall pipeline. 1) Pre-training the model with mask image pre-training method tailored to low-level vision. We randomly mask degraded images at the pixel level with a 50%percent5050\\%50 % masking ratio and reconstruct the clean images. 2) The Fine-tuning stage is followed to overcome the input integrity gap caused by changing masked input during pre-training into the whole image during inference.\nWe analyze the importance of each network layer for resolving the input integrity gap according to the proposed MAC and rank them in descending order. The top k%percent𝑘k\\%italic_k % of network layers are selected for fine-tuning on the complete image.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module in the figure is nothing more exotic than “Integrated Gradients,” only taken not with respect to the raw pixels but with respect to a *mask‐parameterized* path through the network (what the paper calls the Mask Attribute Path, \\(P_m\\)).  Concretely, what they do in that box is:\n\n  1.  Define a continuous path \n       \\(X(α)\\) \n      that starts at your *masked* image \\(X_m\\) (when \\(α=0\\)) and smoothly “turns on” the remaining pixels until you recover the *full* image \\(X\\) (when \\(α=1\\)).  They approximate the hard 0–1 mask flips with a steep sigmoid so that the whole thing is differentiable.\n  2.  For each layer \\(ℓ\\) and each neuron \\(h^ℓ_i\\) in that layer, they compute\n     \n     \\[\n       \\mathrm{MAC}^ℓ_i \\;=\\;\n         \\int_{α=0}^1\n           \\frac{\\partial F\\bigl(X(α)\\bigr)}{\\partial h^ℓ_i}\n           \\;\\times\\;\n           \\frac{d\\,h^ℓ_i\\bigl(X(α)\\bigr)}{dα}\n         \\;dα,\n     \\]\n     \n     where \\(F\\) is the network’s final reconstruction output.\n  3.  Summing that quantity over all neurons in layer \\(ℓ\\) gives you a single *conductance* score \\(\\mathrm{MAC}^ℓ\\).  Intuitively, that score measures *how much change in the output can be “credited back” to activations in layer \\(ℓ\\) as you unmask the image.*\n  4.  Finally, you rank all layers by their \\(\\mathrm{MAC}^ℓ\\) and only fine-tune the top-\\(k\\%\\) of them.\n\nIn short, the red box implements an integrated‐gradient / neuron‐conductance calculation *along the mask‐switching path* to give each layer a single numerical score of how responsible it is for “healing” the masked input back into the full-image output."
    },
    {
        "question": "How does the encoder leverage masked input representations to enable decoder reconstruction?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "MIM is a process that randomly masks certain parts of an image and extracts features from the remaining visible parts to reconstruct the entire image.",
            "During the pre-training stage, we randomly mask the pixels of degraded images (mask images in a patch size) with a mask ratio … we force the network to predict the clear one corresponding to the masked pixels, extracting inherent image information from corrupted images."
        ],
        "final_answer": "By feeding the encoder a version of the corrupted image with randomly masked pixels, the encoder learns to extract feature representations from the unmasked (visible) regions. Those feature representations are then passed to the decoder, which uses them to predict and reconstruct the originally masked (missing) pixels, recovering the full clean image.",
        "relevant_elements": [
            "Encoder",
            "masked input",
            "Decoder"
        ],
        "id": 523,
        "masked_question": "How does the [mask1] leverage masked input representations to enable decoder reconstruction?",
        "masked_number": 1,
        "masked_elements": [
            "Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.19403v1_figure_2.png",
        "paperid": "2409.19403v1",
        "paper_path": "./papers/2409.19403v1.json",
        "figure_id": "2409.19403v1_figure_2.png",
        "caption": "Figure 2: The illumination of our overall pipeline. 1) Pre-training the model with mask image pre-training method tailored to low-level vision. We randomly mask degraded images at the pixel level with a 50%percent5050\\%50 % masking ratio and reconstruct the clean images. 2) The Fine-tuning stage is followed to overcome the input integrity gap caused by changing masked input during pre-training into the whole image during inference.\nWe analyze the importance of each network layer for resolving the input integrity gap according to the proposed MAC and rank them in descending order. The top k%percent𝑘k\\%italic_k % of network layers are selected for fine-tuning on the complete image.",
        "qtype": "Implementation_Details",
        "response": "The little red‐outlined module in the figure is nothing mystical – it is simply the masked‐image encoder.  During pre–training we take a degraded image, randomly zero‐out (“mask”) about half of its pixels, and feed that partially observed image into the encoder.  The encoder “sees” only the unmasked pixels, compresses them into a set of latent feature vectors that capture all of the local and global image‐priors it has learned, and then hands those vectors off to the decoder.  The decoder, which still knows where the masked positions were, uses those latent features together with mask‐token embeddings to reconstruct the missing pixels.  In this way the encoder leverages the masked input representation—it learns to summarize whatever visible cues remain—so that the decoder can fill in the blanked–out regions under an L1‐reconstruction loss."
    },
    {
        "question": "How does Exact Unlearning utilize outputs of Unlearning Set Selection for retraining sub-models?",
        "relevant_section_ids": [
            "2.1",
            "3.3"
        ],
        "relevant_context": [
            "Exact Unlearning (EU) aims to completely eliminate the influence of target data on the model. The most straightforward method of exact unlearning is retraining the model from scratch on the updated dataset (removing the target data), but this method incurs a significant computational time cost. To mitigate this cost, existing EU methods revamp the training process via ensemble learning, which limits the retraining cost to sub-datasets or sub-models (Bourtoule et al., 2021; Yan et al., 2022).",
            "Motivated by these findings, in this paper, we explore the impact of using varying unlearning sets, which can also reflect the robustness of unlearning. To significantly demonstrate this impact, we adopt a model-agnostic selection strategy to create three types of unlearning sets: core data (which impacts many other data points), edge data (with minimal impact on others), and random data."
        ],
        "final_answer": "Exact Unlearning first takes the selected unlearning set (core, edge, or random data) and then only retrains those sub-models in its ensemble whose training shards overlap with the chosen unlearning set, rather than retraining the entire model from scratch.",
        "relevant_elements": [
            "Exact Unlearning",
            "Unlearning Set Selection"
        ],
        "id": 524,
        "masked_question": "How does [mask1] utilize outputs of Unlearning Set Selection for retraining sub-models?",
        "masked_number": 1,
        "masked_elements": [
            "Exact Unlearning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.14393v1_figure_1.png",
        "paperid": "2408.14393v1",
        "paper_path": "./papers/2408.14393v1.json",
        "figure_id": "2408.14393v1_figure_1.png",
        "caption": "Figure 1: An illustration of CURE4Rec, a comprehensive benchmark tailored for evaluating recommendation unlearning methods. CURE4Rec evaluates unlearning methods using data with varying levels of unlearning impact on four aspects, i.e., unlearning completeness, recommendation utility, unlearning efficiency, and recommendation fairness.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed “Exact Unlearning” block makes use of the Unlearning Set Selection output (i.e. the list of records to forget) to drive retraining of only the affected sub‐models:\n\n1.  Unlearning Set Selection picks out a small subset of the original training examples (core, edge or random data) that must be removed.  \n2.  The Exact Unlearning machinery has already split the full dataset into disjoint shards and trained one “sub‐model” per shard (an ensemble).  \n3.  At unlearning time, it inspects which shards contain one or more of the to-be-forgotten examples.  \n4.  Only those sub‐models whose shard overlaps the Unlearning Set are thrown away and **re-trained from scratch** on the same shard *minus* the forbidden examples.  \n5.  All other sub‐models (whose shards did not contain any selected examples) remain untouched.  \n6.  Finally, the newly retrained sub‐models are re-combined with the untouched ones to form the completed “unlearned” ensemble.  \n\nBy using the Unlearning Set Selection output to pinpoint exactly which shards are contaminated, Exact Unlearning avoids a full model retrain and limits its costly scratch-retraining step to only the sub‐models that must forget data."
    },
    {
        "question": "How does Approximate Unlearning route information between the Original Model and the Unlearned Model?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Approximate Unlearning",
            "Original Model",
            "Unlearned Model"
        ],
        "id": 525,
        "masked_question": "How does [mask1] route information between the Original Model and the Unlearned Model?",
        "masked_number": 1,
        "masked_elements": [
            "Approximate Unlearning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.14393v1_figure_1.png",
        "paperid": "2408.14393v1",
        "paper_path": "./papers/2408.14393v1.json",
        "figure_id": "2408.14393v1_figure_1.png",
        "caption": "Figure 1: An illustration of CURE4Rec, a comprehensive benchmark tailored for evaluating recommendation unlearning methods. CURE4Rec evaluates unlearning methods using data with varying levels of unlearning impact on four aspects, i.e., unlearning completeness, recommendation utility, unlearning efficiency, and recommendation fairness.",
        "qtype": "Implementation_Details",
        "response": "Letting [mask1] = “Approximate Unlearning,” you can read the little red–boxed diagram as showing that Approximate Unlearning does *not* directly retrain from the original model to get the unlearned one.  Instead it first “pulls” the state of the original model into an intermediate store (the cylinder), and then “pushes” an adjusted (i.e. unlearned) version out of that store into the new model.  In other words, information flows Original Model → (black arrow) → Database → (blue arrow) → Unlearned Model."
    },
    {
        "question": "How do approximate unlearning methods reconcile random data selection with unlearning efficiency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Approximate Unlearning",
            "Random Data",
            "Unlearning Efficiency"
        ],
        "id": 527,
        "masked_question": "How do [mask1] methods reconcile random data selection with unlearning efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Approximate Unlearning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.14393v1_figure_1.png",
        "paperid": "2408.14393v1",
        "paper_path": "./papers/2408.14393v1.json",
        "figure_id": "2408.14393v1_figure_1.png",
        "caption": "Figure 1: An illustration of CURE4Rec, a comprehensive benchmark tailored for evaluating recommendation unlearning methods. CURE4Rec evaluates unlearning methods using data with varying levels of unlearning impact on four aspects, i.e., unlearning completeness, recommendation utility, unlearning efficiency, and recommendation fairness.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1.  In Figure 1 the red‐boxed block is labeled “Approximate Unlearning,” in contrast to the left side’s “Exact Unlearning.”  So “[mask1] methods” refers to approximate unlearning methods.  \n2.  From the text (Section 2.1 and 3.1) we know that approximate unlearning (AU) never retrains the model from scratch.  Instead it uses either (a) influence‐function or gradient‐based estimates of each data point’s effect on the parameters, or (b) direct parameter pruning/damping to “erase” that effect.  \n3.  The key to efficiency is that AUs only touch the small subset of points to be forgotten, rather than re-training on the entire remaining  dataset (as exact methods do).  Their runtime grows roughly linearly in the size of the unlearning set, not in the full training set.  \n4.  In the random‐data scenario the unlearning set is just a random draw of users/interactions.  An approximate method simply loops over those random points, computes its precomputed influence or gradient correction, and updates the model weights.  It never needs to shard the data or invoke a full retraining pass.  \n5.  Therefore, approximate unlearning methods “reconcile” an arbitrary (random) unlearning set with high efficiency by design: they localize all computation to the forgotten points via influence‐function or pruning tricks, avoiding costly global retraining.  \n\nAnswer  \nThey do so by avoiding any re-training over the remaining data and instead applying a local, influence-based or gradient-based correction only to the randomly chosen points.  In practice an approximate unlearning method precomputes (or incrementally computes) the influence of each sample on the model parameters, then for any random subset of points to be forgotten it simply subtracts out or prunes those contributions.  Because it only visits the k points to be unlearned (rather than the entire N-point training set), its runtime scales in O(k) and remains efficient even when the unlearning set is drawn at random."
    },
    {
        "question": "How does GNN-based node-wise classification on DOM structure improve over serialized DOM processing methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Node-wise Classifier",
            "DOM Tree",
            "GNN"
        ],
        "id": 528,
        "masked_question": "How does [mask1]-based node-wise classification on DOM structure improve over serialized DOM processing methods?",
        "masked_number": 1,
        "masked_elements": [
            "GNN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.04673v1_figure_1.png",
        "paperid": "2408.04673v1",
        "paper_path": "./papers/2408.04673v1.json",
        "figure_id": "2408.04673v1_figure_1.png",
        "caption": "Figure 1. Overview of AutoFAIR s Architecture. The DOM tree is constructed from the data webpage HTML. In Web Reader, nodes are categorized by a graph neural network to locate metadata fields, and for nodes with long text, a language model extracts the metadata. The extracted fields are then mapped according to the FAIR principles through FAIR Alignment, resulting in a FAIR-compliant metadata profile.",
        "qtype": "Literature_Background",
        "response": "First, note that the little red‐boxed module in the figure is simply the GNN in the “Node‐wise Classifier”—so [mask1] is “GNN‐based.”  Here’s why using a GNN on the true DOM tree is better than flattening it into a sequence:\n\n1.   Preservation of hierarchy.  \n     •  A DOM isn’t really a list of tags; it’s a nested tree.  If you serialize it (e.g. into a long HTML string or token sequence) you lose the explicit parent/child and sibling relationships.  \n     •  A Graph‐Neural‐Network (GNN) keeps the DOM’s tree edges intact, so each node’s representation is informed by its actual neighbors in the page structure.  \n\n2.   Local and global context via message passing.  \n     •  In a GNN, each node collects “messages” from exactly the elements it’s connected to (parent, children, maybe certain attribute‐links).  This means the model can learn, for instance, “if this <div> is nested under a <header> and has class='metadata', it’s very likely the title.”  \n     •  Serial methods often rely on fixed‐window attention or positional embeddings, which can’t easily model “this element is deeply buried in a sidebar” vs. “this one is in the main content.”  \n\n3.   Robustness to variation in markup.  \n     •  Real‐world HTML varies wildly—different sites use different wrappers, extra divs, inline vs. block elements, etc.  A GNN can adapt by re‐learning which structural patterns point to a DOI, a title, a license, etc., whereas a serialized model must learn to recognize hundreds of different linear tag sequences.  \n\n4.   Finer‐grained, node‐level predictions.  \n     •  Because you’re classifying every node in situ, you avoid the need for expensive post‐processing to “undo” your serialization (e.g. mapping token indices back to tree nodes).  The GNN outputs a label for each node directly on the tree.  \n\nIn short, GNN‐based node‐wise classification lets you exploit the true DOM graph, fuse local–global context via message passing, and make clean, per‐node predictions—whereas serialized processing methods collapse the tree, dilute structural cues, and force the model to infer hierarchy from position alone."
    },
    {
        "question": "How does element-wise extractor refine metadata fields after node-wise classification using language model prompting?",
        "relevant_section_ids": [
            "4.2.2"
        ],
        "relevant_context": [
            "Once the node-wise classifier identifies the HTML nodes corresponding to the metadata fields, we utilize language models to extract the relevant information from these nodes.",
            "Specifically, we take the text content of each identified HTML node and use the BERT model to encode this text, guiding the extraction process with a carefully designed prompt."
        ],
        "final_answer": "After the node-wise classifier locates the HTML elements for each metadata field, the element-wise extractor takes the raw text of those nodes and feeds it into a BERT language model with a tailored prompt. The prompt focuses the model on the desired metadata (e.g. spatiotemporal details), and the model’s output is then processed to yield the final, refined metadata values.",
        "relevant_elements": [
            "Element-wise Extractor",
            "Node-wise Classifier"
        ],
        "id": 530,
        "masked_question": "How does [mask1] refine metadata fields after node-wise classification using language model prompting?",
        "masked_number": 1,
        "masked_elements": [
            "Element-wise Extractor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.04673v1_figure_1.png",
        "paperid": "2408.04673v1",
        "paper_path": "./papers/2408.04673v1.json",
        "figure_id": "2408.04673v1_figure_1.png",
        "caption": "Figure 1. Overview of AutoFAIR s Architecture. The DOM tree is constructed from the data webpage HTML. In Web Reader, nodes are categorized by a graph neural network to locate metadata fields, and for nodes with long text, a language model extracts the metadata. The extracted fields are then mapped according to the FAIR principles through FAIR Alignment, resulting in a FAIR-compliant metadata profile.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. After the GNN tags each DOM node with a rough metadata label (e.g. “this node is a description,” “this one is the temporal range,” etc.), some of those nodes still contain long, free-form text rather than a neatly delimited value.  \n2. We need to pull out exactly the bits we care about—dates, coordinates, keywords, institutions—from that blob of text.  \n3. That is what the red-boxed module—which the question calls [mask1]—does.  It sits downstream of the node classifier and is sometimes called the “element-wise extractor.”  \n4. Concretely, it takes the inner-text of a classified node and, together with a small “please extract X” prompt, feeds it through a pretrained language model (e.g. BERT).  \n5. The LM then returns just the target information (for example: “1995–2016” as the temporal coverage, “111E 80E 10N 6S” as the spatial extent, etc.), thus refining the coarse node label into a clean, precise metadata field.  \n\nAnswer  \n[mask1] is the element-wise extractor: it takes the raw text of each GNN-identified node and, via a tailored prompt to a pretrained language model, pulls out exactly the metadata value (dates, coordinates, keywords, etc.), thereby refining the fields that were coarsely located by the node-wise classifier."
    },
    {
        "question": "How does FAIR Alignment standardize extracted fields into DCAT-compliant metadata through ontology guidance and semantic matching?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "To address the challenges posed by inconsistent information descriptions within FAIR principles and their varied formats, we adopt techniques such as ontology guidance and semantic matching.",
            "These methods aim to standardize each field. To establish a comprehensive data index and enhance data interoperability, we align each field by creating a dataset entry on the DataExpo website (Lu et al., 2023 ###reference_b17###) and embedding DCAT (Albertoni et al., 2023 ###reference_b3###) metadata within the page."
        ],
        "final_answer": "FAIR Alignment takes the raw metadata fields extracted by the Web Reader and applies ontology guidance and semantic matching to map each field’s value onto standard concepts and controlled vocabularies. Once each field has been semantically aligned to the appropriate ontology terms, the system automatically generates a DCAT‐compliant metadata record by embedding the standardized fields (e.g., dcat:title, dcat:identifier, dcat:temporal, etc.) into the dataset entry on the DataExpo website, ensuring machine‐readable, interoperable metadata across sites.",
        "relevant_elements": [
            "FAIR Alignment"
        ],
        "id": 531,
        "masked_question": "How does [mask1] standardize extracted fields into DCAT-compliant metadata through ontology guidance and semantic matching?",
        "masked_number": 1,
        "masked_elements": [
            "FAIR Alignment"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2408.04673v1_figure_1.png",
        "paperid": "2408.04673v1",
        "paper_path": "./papers/2408.04673v1.json",
        "figure_id": "2408.04673v1_figure_1.png",
        "caption": "Figure 1. Overview of AutoFAIR s Architecture. The DOM tree is constructed from the data webpage HTML. In Web Reader, nodes are categorized by a graph neural network to locate metadata fields, and for nodes with long text, a language model extracts the metadata. The extracted fields are then mapped according to the FAIR principles through FAIR Alignment, resulting in a FAIR-compliant metadata profile.",
        "qtype": "Experimental_Results",
        "response": "First, note that “[mask1]” is simply the red‐boxed “FAIR Alignment” box in Figure 1.  What happens inside that box is:\n\n1.  Ontology-guided mapping  \n    •  We load or reference a light DCAT ontology (classes like dct:Title, dct:Identifier, dct:Temporal, dct:Spatial, dct:License, etc.) plus any domain-specific extensions (e.g. for keywords, descriptions, language).  \n    •  Each raw field name produced by the Web Reader (e.g. “title,” “doi,” “temporal,” “spatial,” “keyword,” “license,” “description”) is looked up against the ontology’s preferred labels, synonyms and subclass relations.  \n\n2.  Semantic similarity matching  \n    •  We generate embeddings (or use string‐based metrics) for both the extracted field names and all candidate DCAT terms.  \n    •  For each extracted field, we pick the DCAT property whose embedding is most semantically similar (above a threshold), thereby handling minor naming mismatches (e.g. “time range” → dct:temporal, “map area” → dct:spatial).  \n\n3.  Generation of uniform DCAT triples  \n    •  Once each raw field is aligned to a DCAT property, we materialize it as RDF triples (or JSON-LD) under that property—e.g.  \n      –  dct:title \"Globolakes: Lake Surface Water …\" .  \n      –  dct:identifier \"10.12041/geodata.17742134065413.ver1.db\" .  \n      –  dct:temporal \"1995-2016\" .  \n      –  dct:spatial \"111E 80E 10N 6S\" .  \n      –  dct:license \"CC-BY-4.0\" .  \n\n4.  Special handling for spatiotemporal fields  \n    •  Temporal strings are parsed and normalized via a small temporal‐standardization function (e.g. “1995–2016” → xsd:dateTime interval).  \n    •  Spatial coordinates are reprojected, registered and/or interpolated into a uniform CRS and then tagged with dct:spatial.  \n\nBy combining an explicit DCAT ontology (the “ontology guidance”) with a lightweight embedding‐based or string‐based matching step (“semantic matching”), the system guarantees that every piece of metadata extracted by the Web Reader ends up as a well‐formed, machine-readable DCAT property."
    },
    {
        "question": "How does combining Segmentation Layer with Regression Head improve multiscale object localization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Segmentation Layer",
            "Regression Head"
        ],
        "id": 532,
        "masked_question": "How does combining [mask1] with Regression Head improve multiscale object localization?",
        "masked_number": 1,
        "masked_elements": [
            "Segmentation Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.21580v1_figure_2.png",
        "paperid": "2407.21580v1",
        "paper_path": "./papers/2407.21580v1.json",
        "figure_id": "2407.21580v1_figure_2.png",
        "caption": "Figure 2: Overview of our two-stage method for Scene Graph Generation. Objects are first detected using a hybrid detector / segmentation model. The relations are then predicted using both bounding box and segmentation mask information.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module in the diagram is simply the “Segmentation Layer” sitting in parallel with the usual box‐Regression Head.  In other words, after the FPN builds its multiscale feature maps, we don’t just hand those maps to a bounding-box regressor—we also pass them through a lightweight 3D segmentation decoder.  \n\nWhy does that help?  Bounding-box regression on its own only ever learns coarse, anchor-based extents.  By simultaneously training a segmentation branch you:\n\n  1. Inject dense, voxel-level supervision at every scale of the pyramid,  \n  2. Force the network to learn precise shape and boundary cues,  \n  3. And feed those fine-grained, scale-aware features back into the same backbone that the box regressor uses.  \n\nThe result is that the regression head is no longer “flying blind” with only sparse anchor signals—it gets grounded by a segmentation mask that knows exactly where each hemorrhage (or ventricle, midline, etc.) really lives in 3D.  That dense guidance makes it much easier to localize very small bleeds and very large ones simultaneously, so recall and box accuracy jump across all object scales."
    },
    {
        "question": "How does Object Pairs Selection affect effectiveness of segmentation-grounded Object Features in Relation Classification?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Object Pairs Selection",
            "Relation Classification",
            "Object Features"
        ],
        "id": 533,
        "masked_question": "How does [mask1] affect effectiveness of segmentation-grounded [mask2] in Relation Classification?",
        "masked_number": 2,
        "masked_elements": [
            "Object Pairs Selection",
            "Object Features"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.21580v1_figure_2.png",
        "paperid": "2407.21580v1",
        "paper_path": "./papers/2407.21580v1.json",
        "figure_id": "2407.21580v1_figure_2.png",
        "caption": "Figure 2: Overview of our two-stage method for Scene Graph Generation. Objects are first detected using a hybrid detector / segmentation model. The relations are then predicted using both bounding box and segmentation mask information.",
        "qtype": "Experimental_Results",
        "response": "Let us first decode what the two masked‐out blocks in the picture are:\n\n  1.  Mask 1 (red box) is the tall, grey column on the right labelled “Object Pairs Selection.”  In our two-stage pipeline it is the step that takes your detected objects (bleeds, ventricles, midline) and decides which ordered pairs of them are passed on to the relation‐classification network.  \n  2.  Mask 2 (blue box) is the little stack of three coloured 3D blocks immediately to the left of the relation‐classification module.  These represent the “segmentation‐grounded object features” – i.e. the fine, voxel-precise masks (rather than crude bounding-boxes) that we crop out and use as input to V-MOTIF or V-IMP.  \n\nNow to the question:  \n\n “How does Object Pairs Selection affect the effectiveness of using segmentation-grounded object features in Relation Classification?”  \n\nHere is the step‐by‐step reasoning:\n\n  •  Segmentation grounding only helps if the relation network actually gets to see the mask-based features for the *true* pairs of objects that actually interact in the scan.  \n  •  Object Pairs Selection is the *gatekeeper* that chooses which pairs of detected objects are fed into the relation classifier.  If this selection is *too strict* (drops real bleed–ventricle or bleed–midline pairs), then your beautifully precise masks never reach the classifier for those missing true relations, and the benefit of segmentation grounding is lost.  \n  •  Conversely, if you allow a *wide set* of candidate pairs (i.e. high‐recall pairing), then the relation network sees more mask-cropped features for the correct interactions and can fully exploit the extra spatial detail.  In practice that translates into higher Recall@K and mAP@K in both Predicate Classification and full Scene Graph Generation.  \n  •  In short, Object Pairs Selection *directly controls* which segmentation-grounded features are even considered for relation prediction, and so its precision/recall trade-off dictates how much the finer segmentation masks can actually improve the final relation‐classification scores."
    },
    {
        "question": "What are the limitations of relying on Segmentation Layer for Ventricle & Midline Localization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Segmentation Layer",
            "Ventricle & Midline Localization"
        ],
        "id": 534,
        "masked_question": "What are the limitations of relying on [mask1] for Ventricle & Midline Localization?",
        "masked_number": 1,
        "masked_elements": [
            "Segmentation Layer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.21580v1_figure_2.png",
        "paperid": "2407.21580v1",
        "paper_path": "./papers/2407.21580v1.json",
        "figure_id": "2407.21580v1_figure_2.png",
        "caption": "Figure 2: Overview of our two-stage method for Scene Graph Generation. Objects are first detected using a hybrid detector / segmentation model. The relations are then predicted using both bounding box and segmentation mask information.",
        "qtype": "Others",
        "response": "Here is the step‐by‐step reasoning. In Fig. 2 (and the top pipeline) the little red box around the “Segmentation Layer” is exactly what we use to find both the ventricle system and the midline: instead of trying to fit 3D boxes to these two structures, we simply take the class‐specific semantic segmentation mask and extract a single connected object per class. This neatly avoids the anchor‐matching and IoU failures you get if you try to detect a very thin plane (the midline) or a highly fragmented cavity (the ventricles). \n\nUnfortunately, this shortcut also comes with several limitations:\n\n 1.  Semantic (class-level) segmentation only ever gives you one mask per class.  \n    – You lose any notion of multiple instances or left/right chambers, and you cannot recover sub-parts of the ventricle.  \n\n 2.  The midline mask is extremely sensitive to head tilt and deformation.  \n    – If the bleed shifts the brain off-center, the “thin” midline mask can become broken or misaligned, and a single semantic mask has no built-in notion of continuity or geometric regularity.  \n\n 3.  Ventricle masks often get fragmented under swelling or blood intrusion.  \n    – You end up with multiple disconnected blobs that must be heuristically merged back into one object, which can fail if the fragments are small or noisy.  \n\n 4.  Mask boundaries can be coarse or “bleed” into surrounding tissue.  \n    – When you then compute the bounding box from that mask (for feature extraction or downstream graph construction), that box can be loose or shifted.  \n\n 5.  The whole scheme hinges on the segmentation network generalizing well.  \n    – We saw in our private (out-of-distribution) cohort that ventricle recall stayed at 100 % but midline recall fell from 96 % to 83 %, showing that unusual anatomy or artifacts can break the mask-based solution.  \n\nIn short, while the segmentation layer elegantly sidesteps some of the classic 3D‐box problems, it trades them in for issues of fragmentation, loss of instance‐level detail, sensitivity to deformation, and reliance on the semantic‐segmentation network’s robustness."
    },
    {
        "question": "What ethical concerns arise from misclassifications during Relation Classification in clinical settings?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Relation Classification"
        ],
        "id": 535,
        "masked_question": "What ethical concerns arise from misclassifications during [mask1] in clinical settings?",
        "masked_number": 1,
        "masked_elements": [
            "Relation Classification"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.21580v1_figure_2.png",
        "paperid": "2407.21580v1",
        "paper_path": "./papers/2407.21580v1.json",
        "figure_id": "2407.21580v1_figure_2.png",
        "caption": "Figure 2: Overview of our two-stage method for Scene Graph Generation. Objects are first detected using a hybrid detector / segmentation model. The relations are then predicted using both bounding box and segmentation mask information.",
        "qtype": "Others",
        "response": "Let’s first align the red‐boxed term in the figure with the text, then walk through why errors there are ethically fraught in the clinic.\n\n1.  Image–Text Alignment  \n    – In Fig. 2 the pipeline is split into two stages: (1) “Object Detection” and (2) “Relation Classification.”  \n    – The red box in the schematic encloses the words “Relation Classification.”  \n    – The text confirms that after detecting bleedings, ventricles, and midline, the model must predict the relations (e.g. “flows into ventricle,” “causes midline shift”).\n\n2.  Why Misclassification in Relation Classification Matters  \n    – Relation Classification is responsible for deciding if and how a hemorrhage interacts with key brain structures (ventricular system, midline).  \n    – Clinicians rely on these labels to gauge severity, choose interventions (e.g. drain a ventricle, perform decompressive surgery), and prognosticate outcomes.  \n\n3.  Ethical Concerns from Misclassification  \n    a) Patient Safety and Harm  \n       • A false negative (missed “flows into ventricle”) may delay ventricular drainage, worsening hydrocephalus or intracranial pressure.  \n       • A false positive (spurious “midline shift”) might prompt unnecessary surgery, exposing the patient to operative risk.  \n    b) Informed Consent and Autonomy  \n       • Erroneous risk assessments undermine the patient’s and family’s ability to make truly informed decisions.  \n    c) Trust and Transparency  \n       • Repeated or unexplained AI errors erode clinicians’ trust in the tool, slowing adoption of potentially beneficial technology.  \n    d) Accountability and Legal Liability  \n       • Who bears responsibility when an automated relation is wrong—radiologist, software vendor, or hospital?  \n       • Misclassification that leads to harm opens medicolegal exposures.  \n    e) Fairness and Bias  \n       • Systematic errors on certain subpopulations (e.g. older patients, atypical anatomies) risk unequal quality of care.  \n\nIn summary, misclassifications during the Relation Classification stage can directly mislead treatment planning, violate the “do no harm” principle, impair patient autonomy, undermine trust, and trigger legal and fairness issues in a high-stakes clinical environment."
    },
    {
        "question": "What biases might Contextual Mapping introduce when modeling uncommon biomedical visual features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Contextual Mapping"
        ],
        "id": 536,
        "masked_question": "What biases might [mask1] introduce when modeling uncommon biomedical visual features?",
        "masked_number": 1,
        "masked_elements": [
            "Contextual Mapping"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15232v1_figure_1.png",
        "paperid": "2411.15232v1",
        "paper_path": "./papers/2411.15232v1.json",
        "figure_id": "2411.15232v1_figure_1.png",
        "caption": "Figure 1: \nOverview of the BiomedCoOp framework, which combines LLM queries, learnable context tokens, and BiomedCLIP to generate multi-modal representations for biomedical tasks. The method integrates text and image features using prompt ensembling strategies, minimizes cross-entropy and semantic differences, and aligns teacher-student logits, enabling effective few-shot learning for novel biomedical categories.",
        "qtype": "Others",
        "response": "Mask1 corresponds to the “Semantic Consistency by Contextual Mapping” block – i.e. the part of BiomedCoOp that forces your learned context vectors to stay close to the *mean* of a bank of LLM‐generated prompts.  Because it literally averages over many class‐specific textual descriptions, it tends to pull your prompt representations toward the center of that cloud of examples.  In practice that means:  \n\n  •  Common or “prototypical” features that show up in many of the GPT-4 descriptions get amplified,  \n  •  Rare or atypical descriptions (the ones in the tails of the distribution) get washed out,  \n  •  Subtle but clinically important visual cues may be under-represented or lost.  \n\nIn other words, by enforcing semantic consistency via a mean‐pooling of diverse prompts, you introduce a *central‐tendency bias* (or “prototype” bias) that makes the model less sensitive to uncommon or outlier biomedical visual patterns."
    },
    {
        "question": "What is the motivation behind cascading four Mamba heads after the ResNet backbone?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "In the following Mamba head, after going through cascaded layer normalization, linear projection (in the dimension of C), convolution (kernel size 3, stride 1), and SiLU activation, z, m, and the low-level motion descriptor are transmitted to SSMX-Corr.",
            "For each Mamba head, z is serially passed to the next head for a better feature representation, yet m and x are concurrently transmitted by each head to avoid error accumulation on the original template and raw motion."
        ],
        "final_answer": "Cascading multiple Mamba heads allows the template feature (z) to be progressively refined—each head builds on the previous one to produce a richer, more discriminative representation—while the search feature (x) and low-level motion descriptor (m) are fed into each head in parallel to prevent the original template and motion information from drifting or accumulating errors.",
        "relevant_elements": [
            "ResNet backbone",
            "Mamba head"
        ],
        "id": 540,
        "masked_question": "What is the motivation behind cascading four [mask1] after the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Mamba head",
            "ResNet backbone"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.08395v1_figure_1.png",
        "paperid": "2411.08395v1",
        "paper_path": "./papers/2411.08395v1.json",
        "figure_id": "2411.08395v1_figure_1.png",
        "caption": "Figure 1: \nStructure overview of the proposed MambaXCTrack. The ResNet backbone is cascaded with four Mamba heads. Each Mamba head has the same structure. z𝑧zitalic_z and x𝑥xitalic_x are the embedding features of template Z𝑍Zitalic_Z and search X𝑋Xitalic_X.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1.  In Fig. 1 the blue‐boxed module is the ResNet backbone plus a linear projection (“Proj”), which produces the initial template embedding z and search embedding x.  \n2.  Immediately downstream of that, the red‐boxed region contains four identical “Mamba Head” blocks stacked one after the other.  \n3.  The text tells us that within each Mamba Head we do layer‐norm, a linear projection, a small convolution + SiLU, and then emit three streams:  \n    – the updated template feature z,  \n    – the motion descriptor m,  \n    – the search feature x.  \n4.  Crucially, z is passed *serially* from head 1 → head 2 → head 3 → head 4, so that each stage can refine the template representation further and further.  \n5.  By contrast, m and x are *fed in fresh* at each head (in parallel), so that we never “drift” away from the raw motion or search signal and hence avoid cumulative error.  \n6.  In other words, cascading four Mamba Heads gives you a deep, multi‐stage feature extractor that:  \n    • progressively enhances the semantic richness of the template embedding,  \n    • repeatedly re-injects the un-altered motion and search cues to keep the core template branch honest,  \n    • yields stronger, more stable features for the downstream SSMX-Corr module.  \n\nSo the motivation for “cascading four” of these heads after the backbone is precisely to get *gradual, stage-wise refinement* of the template feature while simultaneously *preventing error accumulation* on the motion and search streams."
    },
    {
        "question": "What role does CIS play in preserving local pixel-wise interaction in SSMX-Corr?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "In this work, before SSMX-Corr, the CIS is adopted to enhance pixel-wise interaction between Z and X to better adapt to SSM-based cross-correlation.",
            "By performing CIS, the local pixels from template and search maps are regrouped to be adjacent, allowing the SSMX-Corr to be performed without losing local interaction and positional inductive bias."
        ],
        "final_answer": "CIS interleaves the template and search pixels—scanning them alternately—so that local template/search pixel pairs become adjacent in the SSM input sequence. This regrouping preserves the original pixel-wise interactions and positional inductive bias when SSMX-Corr performs its long-range, global modeling.",
        "relevant_elements": [
            "CIS",
            "SSMX-Corr"
        ],
        "id": 541,
        "masked_question": "What role does [mask1] play in preserving local pixel-wise interaction in SSMX-Corr?",
        "masked_number": 1,
        "masked_elements": [
            "CIS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.08395v1_figure_1.png",
        "paperid": "2411.08395v1",
        "paper_path": "./papers/2411.08395v1.json",
        "figure_id": "2411.08395v1_figure_1.png",
        "caption": "Figure 1: \nStructure overview of the proposed MambaXCTrack. The ResNet backbone is cascaded with four Mamba heads. Each Mamba head has the same structure. z𝑧zitalic_z and x𝑥xitalic_x are the embedding features of template Z𝑍Zitalic_Z and search X𝑋Xitalic_X.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought:\n\n1. In Fig. 2 (the SSMX-Corr pipeline) the red boxes clearly label the “CIS” blocks.  \n2. Section II-B defines CIS as the Cross-map Interleaved Scan, which takes one template patch, one search sub-patch, and the motion descriptor, and scans them in four directions.  \n3. Critically, CIS alternates pixel‐by‐pixel between the template and search features, so that local pixels from both maps become adjacent in the resulting sequences.  \n4. This interleaving step is explicitly introduced because a straight concatenation of template and search into the SSM would lose the pixel-level correspondence and spatial inductive bias.  \n5. By performing CIS before the SSM-based cross-correlation, the model preserves the fine, local template–search interactions even while it gains the SSM’s long-range modeling power.  \n\nAnswer:\n\n[mask1] is the Cross-map Interleaved Scan (CIS) module.  Its role is to alternate and interleave template and search pixels into a single sequence so that local pixel-wise correspondences remain adjacent, thus preserving the fine‐grained, pixel-level interactions when SSMX-Corr performs its global convolution."
    },
    {
        "question": "How does the Mamba Head concurrently propagate template features and preserve low-level motion descriptor across stages?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "In the following Mamba head, after going through cascaded layer normalization, linear projection (in the dimension of C), convolution (kernel size 3, stride 1), and SiLU activation, Z, X, and the low-level motion descriptor m are transmitted to SSMX-Corr.",
            "For each Mamba head, Z is serially passed to the next head for a better feature representation, yet X and m are concurrently transmitted by each head to avoid error accumulation on the original template and raw motion."
        ],
        "final_answer": "Within each Mamba head, after normalizing, projecting, convolving, and activating the template (Z), search (X), and motion descriptor (m), the template embedding Z is sent forward in series through the stack of heads to refine its representation. In contrast, the search embedding X and the low-level motion descriptor m are fed in parallel into every head, ensuring that the raw motion cue and search features remain intact across stages and do not accumulate errors.",
        "relevant_elements": [
            "Mamba Head",
            "low-level motion descriptor"
        ],
        "id": 543,
        "masked_question": "How does the [mask1] concurrently propagate template features and preserve [mask2] across stages?",
        "masked_number": 2,
        "masked_elements": [
            "Mamba Head",
            "low-level motion descriptor"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.08395v1_figure_1.png",
        "paperid": "2411.08395v1",
        "paper_path": "./papers/2411.08395v1.json",
        "figure_id": "2411.08395v1_figure_1.png",
        "caption": "Figure 1: \nStructure overview of the proposed MambaXCTrack. The ResNet backbone is cascaded with four Mamba heads. Each Mamba head has the same structure. z𝑧zitalic_z and x𝑥xitalic_x are the embedding features of template Z𝑍Zitalic_Z and search X𝑋Xitalic_X.",
        "qtype": "Implementation_Details",
        "response": "Here’s what’s going on, step by step:\n\n1.  Mask-1 (the red box) is the stack of Mamba heads—each head is essentially  \n    LN → Linear-proj → 3×3-Conv → SiLU.  These heads are arranged in series to  \n    progressively refine the search features.\n\n2.  Mask-2 (the blue box) is the low-level motion descriptor m (the little “motion  \n    cue” vector built from recent bounding-box displacements).\n\n3.  Within each Mamba head we actually fork the data-flow:  \n    – The *search* feature **x** is passed *serially* from Head₁ → Head₂ → Head₃ → … so that each head sees the progressively refined search feature.  \n    – At the same time, however, we *concurrently* feed the *same* template embedding **z** and motion descriptor **m** into **every** head’s SSMX-Corr block.  \n\n4.  By “forking” z and m into each stage rather than chaining them through the heads, we guarantee that:  \n    • The original template embedding never gets “drifted” by stage-to-stage updates, and  \n    • The low-level motion descriptor remains intact (no cumulative error) across the cascade.\n\nIn short, the red-boxed multi-head structure refines only the search features in series, while the template (and the blue-boxed motion cue) are replicated in parallel into each head’s correlation module—this concurrent propagation of z and m preserves them exactly over all stages."
    },
    {
        "question": "How does Stable Diffusion integrate LLM-proposed replacements for object editing in synthetic data augmentation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Stable Diffusion",
            "LLMs",
            "Synthetic Data Augmentation"
        ],
        "id": 544,
        "masked_question": "How does Stable Diffusion integrate LLM-proposed replacements for object editing in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Synthetic Data Augmentation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.02712v1_figure_2.png",
        "paperid": "2411.02712v1",
        "paper_path": "./papers/2411.02712v1.json",
        "figure_id": "2411.02712v1_figure_2.png",
        "caption": "Figure 2: Outline of our preference data construction and vision-guided preference learning framework. In the stage of Synthetic Data Augmentation, we utilize LVLMs, LLMs, and Stable Diffusion to manipulate images automatically. We formulate the generated samples into image- and response-contrast pairs for preference learning via our Vision-guided DPO approach.",
        "qtype": "Implementation_Details",
        "response": "Step 1 – Extracting and grounding the object to replace  \n• We run our vision–language model over the original photograph to get a caption which not only names each object (“cake”) but also its pixel‐coordinates (a tight bounding box).  \nStep 2 – LLM proposes a replacement  \n• We prompt an off-the-shelf LLM (e.g. ChatGPT) with that grounded caption and ask “What could I substitute here to make the scene more surprising?”  \n• The LLM returns both a replacement noun phrase (e.g. “a pile of rocks”) and a short rationale for why it’s unexpected.  \nStep 3 – In‐painting with Stable Diffusion  \n• We take the original image and mask out the cake’s bounding box.  \n• We feed that masked image plus the LLM-generated text prompt (“a pile of rocks”) into Stable Diffusion’s inpainting pipeline.  \n• Stable Diffusion then “paints in” the masked region so that it now shows a pile of rocks in place of the cake.  \nStep 4 – Quality filtering  \n• Finally, we score the edited image against the replacement prompt using CLIPScore and only keep the edit if the score is highest among a small set of candidates.  \nIn this way, Stable Diffusion seamlessly integrates the LLM’s imaginative object swap by treating the LLM’s phrase as the text condition for its inpainting model over the masked region."
    },
    {
        "question": "How does vision-guided DPO incorporate image-contrast and response-contrast preferences in its training objective?",
        "relevant_section_ids": [
            "3.1",
            "4",
            "4.1"
        ],
        "relevant_context": [
            "Given a response-contrast preference dataset D_r where y^w is preferred over y^l, DPO uses Bradley–Terry model (Bradley and Terry, 1952 ###reference_b3###) to derive the objective as: … Enlightened by contrast sets (Gardner et al., 2020 ###reference_b10###; Shen et al., 2023 ###reference_b34###), we construct an image-contrast dataset D_i to enhance visual understanding. With D_r and D_i, we have:",
            "In this work, we focus on mitigating hallucinations in LVLMs caused by insufficient context attention to visual information. We propose Vision-guided Direct Preference Optimization (V-DPO) to enhance visual understanding on both response- and image-contrast preference data.",
            "We integrate this term as an additional target to optimize in Eq. 1. Our result vision-enhanced reward maximization objective is then:"
        ],
        "final_answer": "Vision-guided DPO extends the standard Direct Preference Optimization framework by adding a vision-specific guidance term—derived via Classifier-Free Guidance (CFG)—directly into the reward function. This vision-enhanced reward maximization is then applied over both response-contrast pairs (D_r) and image-contrast pairs (D_i). Concretely, for every preferred versus non-preferred pair—whether the contrast lies in the textual response or in the image—the model maximizes the log-likelihood that the preferred example ranks higher, using a Bradley–Terry objective in which the reward includes the CFG-based visual guidance term.",
        "relevant_elements": [
            "vision-guided DPO",
            "image-contrast preference",
            "response-contrast preference"
        ],
        "id": 545,
        "masked_question": "How does [mask1] incorporate image-contrast and response-contrast preferences in its training objective?",
        "masked_number": 1,
        "masked_elements": [
            "vision-guided DPO"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.02712v1_figure_2.png",
        "paperid": "2411.02712v1",
        "paper_path": "./papers/2411.02712v1.json",
        "figure_id": "2411.02712v1_figure_2.png",
        "caption": "Figure 2: Outline of our preference data construction and vision-guided preference learning framework. In the stage of Synthetic Data Augmentation, we utilize LVLMs, LLMs, and Stable Diffusion to manipulate images automatically. We formulate the generated samples into image- and response-contrast pairs for preference learning via our Vision-guided DPO approach.",
        "qtype": "Implementation_Details",
        "response": "The box in red is simply the V-DPO training objective, which is just the vanilla DPO loss extended to two kinds of preference data plus a small “vision guidance” term.  Concretely, if 𝒟^ℓ is your ordinary (response-contrast) dataset and 𝒟ʷ your image-contrast pairs, you draw (v, x, y) from the union of those two, and you optimize\n\n   max₍π₎  E_{(v,x,y)∼𝒟^ℓ ∪ 𝒟ʷ} [ r(v,x,y) ]  \n       – β · D_{KL}[ π(y│v,x)  ∥  π_ref(y│v,x) ]  \n       + α · D_{KL}[ π(y│v,x)  ∥  π(y│x) ]\n\nHere  \n• r(v,x,y) is the Bradley–Terry reward coming from either a response-contrast pair (in 𝒟^ℓ) or an image-contrast pair (in 𝒟ʷ).  \n• The first KL term is exactly the standard DPO regularizer (penalizing deviation from the reference model).  \n• The second KL term is the “vision guidance”—it measures how much the image actually changes the policy from the text-only π(y│x).  By putting a ​+α​ in front of it, V-DPO explicitly pushes the model to pay attention to the visual input.  \n\nBecause (v,x,y) in the expectation comes half from 𝒟^ℓ and half from 𝒟ʷ, both response-contrast and image-contrast preferences are folded into exactly the same DPO-style objective."
    },
    {
        "question": "How does image-contrast preference integrate the Bradley–Terry model differently than response-contrast preference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "image-contrast preference",
            "response-contrast preference"
        ],
        "id": 547,
        "masked_question": "How does [mask1] integrate the Bradley–Terry model differently than response-contrast preference?",
        "masked_number": 1,
        "masked_elements": [
            "image-contrast preference"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.02712v1_figure_2.png",
        "paperid": "2411.02712v1",
        "paper_path": "./papers/2411.02712v1.json",
        "figure_id": "2411.02712v1_figure_2.png",
        "caption": "Figure 2: Outline of our preference data construction and vision-guided preference learning framework. In the stage of Synthetic Data Augmentation, we utilize LVLMs, LLMs, and Stable Diffusion to manipulate images automatically. We formulate the generated samples into image- and response-contrast pairs for preference learning via our Vision-guided DPO approach.",
        "qtype": "Literature_Background",
        "response": "Here, [mask1] is “image-contrast preference.”  The key difference is what you hold fixed and what you swap when you plug into the Bradley–Terry model:\n\n 1.  In **response-contrast preference** you compare two different candidate answers (yᵂ vs. yˡ) under the **same** visual input v.  The Bradley–Terry score is built from  \n      log P(yᵂ | v, x)  –  log P(yˡ | v, x).  \n\n 2.  In **image-contrast preference** you compare the **same** answer y under two different versions of the image (vᵂ vs. vˡ).  So your Bradley–Terry score becomes  \n      log P(y | vᵂ, x)  –  log P(y | vˡ, x).  \n\nIn other words, image-contrast preference uses Bradley–Terry to rank how well the **visual context** supports that answer, whereas response-contrast preference uses Bradley–Terry to rank how plausible one **textual answer** is versus another under a fixed image."
    },
    {
        "question": "How does K groups of points sampling relate to discrete SAM prompt encoding versus continuous mask outputs?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "We encode an arbitrary-shaped mask by using a bounding box and K points. The bounding box is expressed as <box>x1, y1, x2, y2</box>; the prompt of K points, each of which contains three values, x coordinate, y coordinate, and whether the point is on the object mask, are encoded as <point>xi, yi, vi</point>. By encoding continuous segmentation masks into discrete SAM prompts, we avoid adding any tokens or altering the model structure, while maintaining training with only text auto-regression cross-entropy loss.",
            "To provide the location supervision to LoRA, we randomly sample K groups of points in the training stage based on the object mask and then send these prompts to SAM. For every group, SAM delivers the segmentation result. We filter out the prompts with low IoUs compared to the ground-truth masks and only keep the top-K groups."
        ],
        "final_answer": "During training, the ground-truth continuous mask is converted into K sets of discrete prompts—each consisting of a bounding box plus K positive/negative points—by sampling K groups of points from the mask. These discrete prompts are fed to SAM, which generates continuous segmentation masks for each group. Finally, the K continuous masks are evaluated against the ground truth (via IoU), and the top-K prompt groups are retained. In this way, sampling K groups bridges the gap between the discrete prompt encoding needed by the language model and the continuous mask outputs produced by SAM.",
        "relevant_elements": [
            "K groups of points"
        ],
        "id": 549,
        "masked_question": "How does [mask1] sampling relate to discrete SAM prompt encoding versus continuous mask outputs?",
        "masked_number": 1,
        "masked_elements": [
            "K groups of points"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.10542v1_figure_1.png",
        "paperid": "2409.10542v1",
        "paper_path": "./papers/2409.10542v1.json",
        "figure_id": "2409.10542v1_figure_1.png",
        "caption": "Figure 1: Architecture diagram of SAM4MLLM-PPG.\n(a) The training process of PPG,\n(b) The inference process of PPG, (c) SAM as filter.",
        "qtype": "Literature_Background",
        "response": "Here’s the key idea, unpacked step‐by‐step:\n\n1.  In the red‐boxed module the model “randomly samples K groups of points” from the ground‐truth mask during training.  Each group is just a small set of 2D coordinates plus an “inside/outside” bit.\n\n2.  These coordinates (together with the bounding box) become the *discrete* prompt tokens that we feed into SAM.  In other words, rather than asking the MLLM to spit out a full, pixel‐wise mask (a continuous output), it only ever has to predict a handful of point locations and binary flags.\n\n3.  SAM then takes those discrete tokens and generates the *continuous* segmentation mask—its native output format.  \n\n4.  By sampling multiple point groups (K of them) and keeping only those whose SAM‐produced masks have high IoU with the ground truth, we a) turn a continuous mask into a handful of discrete tokens for the MLLM to learn, and b) push all of the heavy continuous‐boundary modeling back into SAM, so that the MLLM sees only a standard text‐token loss.  \n\nIn short: the “K groups of points” sampling is exactly how we discretize a full mask into point prompts (so the MLLM never has to output continuous masks), and let SAM convert those discrete prompts back into the continuous‐boundary segmentations."
    },
    {
        "question": "How does sampling K point groups influence SAM filter effectiveness during LoRA training?",
        "relevant_section_ids": [
            "3.2",
            "4.1",
            "7"
        ],
        "relevant_context": [
            "To provide the location supervision to LoRA, we randomly sample k groups of points in the training stage based on the object mask and then send these prompts to SAM. For every group, SAM delivers the segmentation result. We filter out the prompts with low IoUs compared to the ground-truth masks and only keep the top-k groups (Fig. 1(c)).",
            "During the training data generation phase of PPG, we randomly sample K point groups within the ground truth bounding box. Each group consists of two positive points inside the ground truth mask and one negative point outside. We keep the K groups with the highest Intersection over Union (IoU) with the ground truth mask, and then randomly pick a single group from these. The chosen group of points are encoded into text using the proposed “mask as prompt” method to serve as the label for training.",
            "In Tab. 7, we provide an analysis for using SAM as our backend, where the upper-bound is the maximum IoU from multiple SAM prompts sampled using the ground-truth masks. The upper-bound is around 87.8% IoU, which is much higher than all existing methods. The predicted SAM prompts by our method achieve around 75% IoU, suggesting there is room for improvement on the MLLM side toward reaching the upper-bound quality."
        ],
        "final_answer": "By sampling K different point groups and running each through SAM, the training pipeline can compute IoUs between the SAM‐predicted masks and the ground-truth. Only the top-K groups (those whose prompts yield the highest IoUs) are kept as supervision for LoRA. In effect, a larger K provides a bigger pool of candidate prompts from which SAM can pick higher-quality ones (raising the upper-bound IoU toward ≈87.8%), while the actual predicted prompts achieve around 75% IoU. This filtering ensures that LoRA is taught using the most accurate prompt points.",
        "relevant_elements": [
            "K groups of Points",
            "SAM as filter",
            "LoRA"
        ],
        "id": 550,
        "masked_question": "How does sampling [mask1] influence SAM filter effectiveness during LoRA training?",
        "masked_number": 1,
        "masked_elements": [
            "K groups of Points"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.10542v1_figure_1.png",
        "paperid": "2409.10542v1",
        "paper_path": "./papers/2409.10542v1.json",
        "figure_id": "2409.10542v1_figure_1.png",
        "caption": "Figure 1: Architecture diagram of SAM4MLLM-PPG.\n(a) The training process of PPG,\n(b) The inference process of PPG, (c) SAM as filter.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In the training loop (Fig. 1a) we start with the ground-truth mask and its enclosing box.  \n2.  We randomly sample N groups of prompt points inside that box—each group consisting of a few “inside” (positive) and “outside” (negative) points.  \n3.  Those N point-sets are all sent through SAM, yielding N candidate masks.  \n4.  We then compute the IoU of each candidate against the ground-truth mask and pick the top-K groups whose SAM outputs have the highest IoU.  \n5.  Only those top-K “good” point-sets survive the SAM-as-filter step and are used to form the LoRA training signal (via the text loss).  \n\nBecause sampling more groups raises the chance that at least K of them will produce high-IoU masks, the SAM-filter can be more selective and feed LoRA only the most accurate segmentations. Conversely, if you sample too few groups, you may not get enough high-quality candidates and the filter becomes weak. In practice the authors found that sampling N = 64 groups and keeping the top K = 8 provided a good balance of diversity (so SAM can find strong prompts) and training stability."
    },
    {
        "question": "How does LoRA utilize filtered positive and negative prompt points to generate accurate location proposals?",
        "relevant_section_ids": [
            "3.2",
            "4.1"
        ],
        "relevant_context": [
            "In this method, an MLLM that can take both text-prompt and image inputs is adopted. To align the MLLM with segmentation utility, we use the parameter-efficient fine tuning technique, LoRA [20], to train the model based on some RES datasets with image-text pairs and ground-truth masks. LoRA outputs the location prompt including the bounding box b and M groups of positive and negative points {P⁺,P⁻}, as illustrated in Fig. 1(a).",
            "To provide the location supervision to LoRA, we randomly sample K groups of points (2 positive and 1 negative per group) in the training stage based on the object mask and then send these prompts to SAM. For every group, SAM delivers the segmentation result. We filter out the prompts with low IoUs compared to the ground-truth masks and only keep the top-K groups (Fig. 1(c)).",
            "During the training data generation phase of PPG, we randomly sample G point groups within the ground truth bounding box. Each group consists of two positive points inside the ground truth mask and one negative point outside. We keep the k groups with the highest Intersection over Union (IoU) with the ground truth mask, and then randomly pick a single group from these. The chosen group of points are encoded into text using the proposed “mask as prompt” method to serve as the label for training."
        ],
        "final_answer": "LoRA is trained to autoregressively predict a location prompt composed of a bounding box plus a small set of positive (inside‐mask) and negative (outside‐mask) point tokens. During training, K candidate groups of 2 positive and 1 negative point are sampled and passed through SAM; only the top‐K groups with the highest IoUs to the ground‐truth mask are retained. One of these high‐IoU groups is encoded—together with the box—into text and used as the target for LoRA’s cross‐entropy loss. At inference time, LoRA directly generates the same structured prompt (box plus points), thereby providing accurate location proposals for SAM.",
        "relevant_elements": [
            "LoRA",
            "Positive & Negative Prompt Points",
            "Location Proposal"
        ],
        "id": 551,
        "masked_question": "How does [mask1] utilize filtered positive and negative prompt points to generate accurate location proposals?",
        "masked_number": 1,
        "masked_elements": [
            "LoRA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.10542v1_figure_1.png",
        "paperid": "2409.10542v1",
        "paper_path": "./papers/2409.10542v1.json",
        "figure_id": "2409.10542v1_figure_1.png",
        "caption": "Figure 1: Architecture diagram of SAM4MLLM-PPG.\n(a) The training process of PPG,\n(b) The inference process of PPG, (c) SAM as filter.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is the tiny LoRA adapter that sits on top of the frozen Qwen‐VL.  During training we do the following:\n\n1.  We randomly sample K groups of points inside (“positive”) and outside (“negative”) each ground‐truth mask.  \n2.  Each group of points (plus its enclosing box) is sent into SAM to produce a candidate mask, and we compute its IoU against the true mask.  \n3.  We keep only the top-K point groups whose SAM‐predicted masks have the highest IoUs.  \n4.  We then teach LoRA to reproduce exactly those high-IoU point groups (and their box) given the same image and text query, using just a standard text‐autoregression loss.  \n\nAt inference time, LoRA directly outputs a box and its learned set of positive/negative point coordinates.  Feeding that into SAM yields a very tight, high-quality segmentation mask, and the bounding box around it serves as our accurate location proposal."
    },
    {
        "question": "How does Social Cognition Penalty interact with the Spatial-Temporal Precognition Module during joint training?",
        "relevant_section_ids": [
            "3.2",
            "3.3",
            "4.3"
        ],
        "relevant_context": [
            "In III-B Main Policy Network (section 3.2): \"The LSTM hidden state is also used as a latent variable δ_R to connect with the auxiliary task module (see the next section).\"",
            "In III-C Spatial-Temporal Precognition Module (section 3.3): \"During training, the main policy network and these auxiliary tasks are optimized together. The total loss function is a weighted sum of the main policy loss L_main and the auxiliary loss …\"",
            "In IV-C Result Analysis (section 4.3): \"Finding 3: SCP coordinates and complements SPM, leading to a significant improvement and faster training. … With SCP, the full system achieves significant improvement … Also, … the model trained with both SPM and SCP exhibits faster convergence … SPM tasks cannot be effectively integrated without the guidance of SCP, which helps the model balance tasks and better leverage available information.\""
        ],
        "final_answer": "During joint training, the Social Cognition Penalty is incorporated into the main policy loss (L_main) while the Spatial-Temporal Precognition Module incurs its own auxiliary losses. Both parts share the same LSTM hidden representation δ_R, and the optimizer minimizes the sum of L_main (including the SCP terms) and the auxiliary losses. In practice, the SCP acts as a guiding signal that coordinates and balances the SPM’s count, position, and trajectory prediction tasks, yielding higher navigation success and faster convergence than training SPM alone.",
        "relevant_elements": [
            "Social Cognition Penalty",
            "Spatial-Temporal Precognition Module"
        ],
        "id": 552,
        "masked_question": "How does [mask1] interact with the [mask2] during joint training?",
        "masked_number": 2,
        "masked_elements": [
            "Social Cognition Penalty",
            "Spatial-Temporal Precognition Module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.13244v1_figure_2.png",
        "paperid": "2409.13244v1",
        "paper_path": "./papers/2409.13244v1.json",
        "figure_id": "2409.13244v1_figure_2.png",
        "caption": "Figure 2: Falcon Overview: The main policy network (top-right) takes Depth and GPS+Compass data as input. Its behavior is guided by the Social Cognition Penalty, which encourages socially compliant navigation and generates the main loss. During training, the output of the network s state encoder, combined with auxiliary information from the Habitat simulator, is processed by the Spatial-Temporal Precognition Module (bottom-right). Three socially-aware auxiliary tasks are then performed, producing auxiliary losses. The total loss is computed by weighting the main loss with the auxiliary losses.",
        "qtype": "Experimental_Results",
        "response": "Mask₁ is the Social Cognition Penalty (SCP) sitting inside the PPO‐style main policy head, and Mask₂ is the Spatial–Temporal Precognition Module (SPM) that hangs off the LSTM encoder and produces three auxiliary losses (count, position, trajectory).  During training they are not run in isolation: the agent is trained end-to-end on a single, combined loss  \n\n L_total  =  L_main  +  λ₁ L_count  +  λ₂ L_pos  +  λ₃ L_traj  \n\nwhere L_main is the usual PPO objective augmented by the three SCP terms (obstacle collision, human‐proximity, trajectory‐obstruction penalties) and the three L_* are the MSE/Cross-Entropy losses from the SPM.  Because both SCP and SPM share the same ResNet + LSTM feature encoder, the gradients from the social‐penalty–shaped PPO loss and the SPM’s auxiliary losses flow back into the same weights.  In practice SCP “guides” the policy to behave socially while SPM fills in the encoder with rich, future‐aware signals, and the two together not only improve final performance but also speed up convergence (as shown in Fig. 5)."
    },
    {
        "question": "How might the Social Cognition Penalty impede exploration in novel environments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Social Cognition Penalty"
        ],
        "id": 554,
        "masked_question": "How might the [mask1] impede exploration in novel environments?",
        "masked_number": 1,
        "masked_elements": [
            "Social Cognition Penalty"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.13244v1_figure_2.png",
        "paperid": "2409.13244v1",
        "paper_path": "./papers/2409.13244v1.json",
        "figure_id": "2409.13244v1_figure_2.png",
        "caption": "Figure 2: Falcon Overview: The main policy network (top-right) takes Depth and GPS+Compass data as input. Its behavior is guided by the Social Cognition Penalty, which encourages socially compliant navigation and generates the main loss. During training, the output of the network s state encoder, combined with auxiliary information from the Habitat simulator, is processed by the Spatial-Temporal Precognition Module (bottom-right). Three socially-aware auxiliary tasks are then performed, producing auxiliary losses. The total loss is computed by weighting the main loss with the auxiliary losses.",
        "qtype": "Others",
        "response": "The thing boxed in red is the Social Cognition Penalty.  During training it subtracts from the normal PointNav reward whenever the robot comes too close to a person or even cuts across a person’s predicted future path.  While that is exactly what we want in a known, crowded scene, in a brand‐new layout it has the unfortunate side effect of making the agent hyper‐risk averse.  As soon as the policy learns “if I go down that corridor I might incur a big penalty,” it simply refuses to explore it.  In practice this means the robot will cling to familiar safe areas and never branch out to discover new shortcuts or rooms, hurting its ability to generalize and find the goal in novel environments."
    },
    {
        "question": "What ethical considerations arise from encoding user emotions in the query encoding component?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "query encoding component"
        ],
        "id": 556,
        "masked_question": "What ethical considerations arise from encoding user emotions in the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "query encoding component"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.23041v1_figure_1.png",
        "paperid": "2410.23041v1",
        "paper_path": "./papers/2410.23041v1.json",
        "figure_id": "2410.23041v1_figure_1.png",
        "caption": "Figure 1: The overview architecture of Emotional RAG framework. It contains four components: the query encoding component, the memory encoding component, the emotional retrieval component, and the response generation component. The emotional memory retrieved by Emotional RAG is sent to LLM, together with the character profile and query, to generate responses.",
        "qtype": "Others",
        "response": "Step 1: Identify what [mask1] is referring to  \nLooking at Figure 1 and reading the text, the red box labeled [mask1] surrounds the “Query encoding component.”  In this component the system takes the user’s raw query and turns it into two vectors:  \n  • a semantic embedding (captures what the user is talking about)  \n  • an emotional embedding (an 8-dimensional vector of joy, sadness, anger, etc.)  \n\nStep 2: What does encoding emotions enable?  \nBy extracting an explicit “emotional state” from every user query, the system is able to:  \n  • Profile the user’s current feelings (mood, affective tone)  \n  • Match or retrieve memories and craft responses that are mood-congruent  \n  • Potentially track emotional trajectories over the course of a conversation or even across sessions  \n\nStep 3: Ethical issues that arise  \nTurning a running dialogue into a stream of emotional measurements opens up several areas of concern:\n\n1. Privacy and Consent  \n   – Emotions are deeply personal data. Capturing and storing them without explicit, informed user consent can violate privacy norms (and in some jurisdictions, data-protection laws).  \n   – Users may not realize that every sentence they type is being analyzed for “joy,” “sadness,” “anger,” etc.\n\n2. Emotional Profiling and Manipulation  \n   – Once you quantify someone’s mood, it becomes possible to target them with emotionally tuned content (e.g. upsell when they’re happy, or exploit vulnerability when they’re sad).  \n   – This crosses the line from neutral “information retrieval” into persuasive or manipulative technologies.\n\n3. Bias and Misinterpretation  \n   – The emotion-tagging model (GPT-3.5 in this case) may systematically misread the emotions of certain speakers (different dialects, cultural expressions, neurodivergent users).  \n   – Wrong inferences about a user’s mood could lead to inappropriate or even harmful responses.\n\n4. Transparency and Explainability  \n   – Users have a right to know that their emotional state is being measured and how it will be used.  \n   – If the system adapts its replies based on a hidden emotional score, it risks becoming a “black box” that erodes user trust.\n\n5. Data Security and Retention  \n   – Emotional vectors are sensitive. How long are they stored? Who has access? What happens if there’s a data breach?  \n\nStep 4: Summarize the ethical considerations  \nBy embedding user emotion into the query encoding component, the system crosses into sensitive, private territory.  It must therefore address:  \n  • Informed consent and clear privacy notices  \n  • Limits on storage, sharing, and secondary use of emotional data  \n  • Fairness checks to avoid systematic misattribution of feelings  \n  • Technical and organizational safeguards against misuse or profiling  \n  • Transparency mechanisms so users understand and can opt out  \n\nWithout these guardrails, emotion-aware retrieval risks turning a helpful conversational agent into a tool of covert persuasion, privacy violation, or emotional harm."
    },
    {
        "question": "How might the Ranking step introduce biases in memory fragment selection in the emotional retrieval component?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "emotional retrieval component",
            "Ranking"
        ],
        "id": 557,
        "masked_question": "How might the [mask1] step introduce biases in memory fragment selection in the emotional retrieval component?",
        "masked_number": 1,
        "masked_elements": [
            "Ranking"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.23041v1_figure_1.png",
        "paperid": "2410.23041v1",
        "paper_path": "./papers/2410.23041v1.json",
        "figure_id": "2410.23041v1_figure_1.png",
        "caption": "Figure 1: The overview architecture of Emotional RAG framework. It contains four components: the query encoding component, the memory encoding component, the emotional retrieval component, and the response generation component. The emotional memory retrieved by Emotional RAG is sent to LLM, together with the character profile and query, to generate responses.",
        "qtype": "Others",
        "response": "First, let’s be crystal-clear what the red box (our [mask1]) actually is: it is the little “Rating → Ranking” pipeline inside the Emotional‐Retrieval component (see Fig. 1).  In that step every candidate fragment  is\n\n  1. “rated” by computing a semantic‐distance score  and an emotion‐distance score  (and then combining them into one final score  \n     S_final = f(S_sem, S_emo)  \n     by either an additive, multiplicative or two-stage (semantic-first or emotion-first) fusion), and  \n  2. “ranked” by sorting all fragments in ascending order of that single S_final and picking the top-K.\n\nBecause all of the selection hinges on that one number, any skew in how we compute or combine the two distances immediately turns into a bias in which memories get pulled in.  Concretely:\n\n  •  Unequal scaling of semantic vs. emotional distances.  \n     – If the raw semantic distances typically run from 0.0–2.0 but the emotion scores sit in 0–1.0 (or vice-versa), an un-normalized sum or product will make one axis dominate.  You’ll end up always pulling in fragments that happen to score high on the “bigger” axis.\n\n  •  Choice of fusion function.  \n     – Additive fusion (C-A) tends to favor fragments that are “good enough” on both axes;  \n     – Multiplicative fusion (C-M) harshly punishes anything that’s weak on even one axis;  \n     – Sequential fusion (semantic-first or emotion-first) can completely shut out fragments that fail the first filter, however relevant they might be on the second.\n\n  •  Hard top-K cutoff and strict sorting.  \n     – A strict cutoff means that if two memories tie in final score you need a tiebreaker (say, insertion order, or recency), which can warp the long-tail of what you ever retrieve.  \n     – Rare but contextually important memories that don’t quite make the top-K—even if they carry a crucial emotional nuance—will be systematically ignored.\n\n  •  Embedding biases get magnified.  \n     – Any bias baked into the semantic encoder (bge-base-zh) or the GPT-3.5 emotion-embedder will feed directly into those distances.  Once you collapse everything into one score and sort, that upstream bias is guaranteed to influence which fragments float to the top.\n\nIn short, by boiling down retrieval to a single “rating” function f(semantic, emotion) and then a rigid “ranking + top-K” selection, you risk systematically over- or under-representing certain pieces of memory simply because of how you scaled and combined the two similarity signals, the order in which you apply them, or the quirks of the underlying embedding spaces."
    },
    {
        "question": "What is the reasoning for symmetrically encoding semantics and emotion across query encoding component and memory encoding component?",
        "relevant_section_ids": [
            "1",
            "2.1",
            "2.2.1",
            "2.2.2",
            "2.2.3"
        ],
        "relevant_context": [
            "Based on the Mood-Dependent Memory theory in psychology, we propose a novel emotion-aware memory retrieval framework, termed Emotional RAG, to augment the response generation process of role-playing agents. The retrieving of memory in Emotional RAG follows the mood-congruity criterion, which means both the semantic relevance and emotional state of recalled memory are considered in the retrieval process.",
            "Query encoding component: both the semantic and emotional state of the query are encoded as vectors in this component.",
            "Memory encoding component: the memory unit stores conversation information of characters. Similar to query encoding, both the semantic and emotional state of the memory are encoded.",
            "We retrieve the memory fragments that are most similar to the user query from the memory unit of characters based on semantic similarity and emotional similarity.",
            "According to Bower’s Mood-Dependent Memory theory: events that are consistent with the character’s current emotion are easier to retrieve, we use the cosine distance between two emotion vectors to find emotionally consistent memory fragments."
        ],
        "final_answer": "Because Emotional RAG performs retrieval by jointly measuring semantic relevance and emotional congruence (the mood-congruity criterion), both the query and each memory fragment must be represented in the same semantic embedding space and the same emotional embedding space.  This symmetrical encoding makes it possible to directly compute and fuse semantic and emotional similarity scores when selecting which memories to retrieve.",
        "relevant_elements": [
            "query encoding component",
            "memory encoding component"
        ],
        "id": 558,
        "masked_question": "What is the reasoning for symmetrically encoding semantics and emotion across [mask1] and memory encoding component?",
        "masked_number": 1,
        "masked_elements": [
            "query encoding component"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.23041v1_figure_1.png",
        "paperid": "2410.23041v1",
        "paper_path": "./papers/2410.23041v1.json",
        "figure_id": "2410.23041v1_figure_1.png",
        "caption": "Figure 1: The overview architecture of Emotional RAG framework. It contains four components: the query encoding component, the memory encoding component, the emotional retrieval component, and the response generation component. The emotional memory retrieved by Emotional RAG is sent to LLM, together with the character profile and query, to generate responses.",
        "qtype": "Design_Rationale",
        "response": "The reason is actually quite straightforward once you line up the picture with the text: because in Emotional-RAG we want to score “how close” a candidate memory is to your current query not just in meaning but also in mood, we must turn both the query and every memory fragment into two things:\n\n  1. A semantic embedding  \n  2. An emotional embedding  \n\nIf we only encoded semantics on one side and emotion on the other we would have no way of putting them into the same distance-based retrieval pipeline.  By making the Query Encoding Component and the Memory Encoding Component *mirror* one another—each spitting out a 768-d semantic vector and an 8-d emotion vector—we can directly compute:\n\n\n\n– a Euclidean (or cosine) distance in semantic-space, and  \n– a cosine distance in emotion-space  \n\n–and then fuse those two scores (either by adding, multiplying, or by a two-stage “semantic first/emotion second” ranking) to pick the top-K fragments.  \n\nIn short, the symmetry guarantees that (a) your query and your memories live in the same two embedding spaces, and (b) you can implement Bower’s mood-congruent retrieval criterion by jointly ranking on semantics *and* emotion."
    },
    {
        "question": "What motivates sharing weights between dual deformation fields during auxiliary training?",
        "relevant_section_ids": [
            "4.1",
            "4.3"
        ],
        "relevant_context": [
            "Moreover, we share the weights of two deformation fields to transfer the motion knowledge from TimeFormer to mitigate the gap between the original branch and TimeFormer branch, which supports real-time rendering without TimeFormer during inference.",
            "Accounting for additional computation costs of TimeFormer, which can significantly decrease rendering speed, we force the original deformation field and the auxiliary deformation field to share weights for knowledge transferring."
        ],
        "final_answer": "They share the weights so that the motion patterns learned by the TimeFormer–augmented deformation field can be transferred to the original deformation field, allowing the TimeFormer module to be removed at inference time and thus avoiding any extra computational cost while preserving real-time rendering performance.",
        "relevant_elements": [
            "Deformation Field",
            "Shared Weight"
        ],
        "id": 560,
        "masked_question": "What motivates sharing [mask1] between dual [mask2]s during auxiliary training?",
        "masked_number": 2,
        "masked_elements": [
            "Shared Weight",
            "Deformation Field"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11941v1_figure_2.png",
        "paperid": "2411.11941v1",
        "paper_path": "./papers/2411.11941v1.json",
        "figure_id": "2411.11941v1_figure_2.png",
        "caption": "Figure 2: The Framework of Deformable 3D Gaussians Reconstruction with TimeFormer. Existing deformable 3D Gaussians framework usually includes the canonical space and the deformation field (first row), we incorporate TimeFormer to capture cross-time relationships and explore motion patterns implicitly (second row).\nWe share weights of two deformation fields to transfer the learned motion knowledge. This allows us to exclude this Auxiliary Training Module\nduring inference.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1. In the diagram there are two parallel “deformation field” branches (one is the original branch, the other is the auxiliary branch that takes the TimeFormer’s outputs).  \n2. The red‐boxed item ([mask1]) is the “Shared Weights” connection between those two deformation fields.  \n3. The blue box ([mask2]) encloses the two deformation fields (and their rendered outputs) – i.e. the dual branches.  \n4. The paper points out that the TimeFormer branch learns richer, cross‐time motion patterns, but at the cost of extra runtime.  \n5. By forcing those two deformation fields to share weights during auxiliary training, the motion knowledge distilled in the TimeFormer branch is transferred into the original, lightweight branch.  \n6. As a result, at inference time you can drop the heavy TimeFormer branch entirely, yet still get the benefit of its learned motion patterns—enabling real‐time rendering without loss of quality.\n\nAnswer  \nWhat motivates sharing the **weights** between the dual **deformation field** branches is to transfer the cross-time motion knowledge learned by the TimeFormer–augmented branch into the original branch, thereby closing the performance gap and allowing the expensive auxiliary module to be removed at inference for real-time rendering."
    },
    {
        "question": "How does TimeFormer transform canonical Gaussian inputs before deformation field application?",
        "relevant_section_ids": [
            "4.1",
            "4.2"
        ],
        "relevant_context": [
            "Previous methods model motion patterns by explicitly learning temporal relationships on individual or neighboring timestamps, failing on those complex scenes containing violent movement or dynamic reflective surfaces. In contrast, we present TimeFormer to enable the deformable 3D Gaussian backbones themselves to model cross-time relationships from an implicit learning perspective. The main framework with the proposed TimeFormer is shown in Fig. 2. Our approach retains standard reconstruction modules, which include (1) 3D Gaussians in the canonical space and (2) a deformation field that applies time-variant transformation. Additionally, TimeFormer is introduced before the deformation field to extract implicit cross-time motion features for each Gaussian through a self-attention mechanism along the time dimension.",
            "As in Fig. 3, all Gaussians’s positions are made into B copies, expanded into B×N, and sampled timestamps are made into N copies, expanded into B×N. Then, we composite p_expand and t_expand together and apply position encoding function PE to extract high frequency information, as in Eq. 4.",
            "In the final stage, we use a tiny MLP to transform the last encoded features h_L into offset Δp in the linear space. We consider the output from the Cross-Temporal Encoder as a fixing residual term to the original positions to encourage a gradual, steady learning process on motion patterns."
        ],
        "final_answer": "Before passing Gaussians into the deformation field, TimeFormer first duplicates each canonical‐space Gaussian position across a small batch of sampled timestamps, concatenates these per‐timestamp copies with their time embeddings, and applies a sinusoidal position‐encoding PE. The resulting 2L‐dimensional features are fed through a multi-layer transformer encoder, whose final hidden vectors are decoded by a tiny MLP into residual offsets Δp. Those offsets are added back to the original Gaussian positions and the augmented Gaussians are then sent through the (shared) deformation field.",
        "relevant_elements": [
            "TimeFormer",
            "Deformation Field"
        ],
        "id": 562,
        "masked_question": "How does [mask1] transform canonical Gaussian inputs before deformation field application?",
        "masked_number": 1,
        "masked_elements": [
            "TimeFormer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11941v1_figure_2.png",
        "paperid": "2411.11941v1",
        "paper_path": "./papers/2411.11941v1.json",
        "figure_id": "2411.11941v1_figure_2.png",
        "caption": "Figure 2: The Framework of Deformable 3D Gaussians Reconstruction with TimeFormer. Existing deformable 3D Gaussians framework usually includes the canonical space and the deformation field (first row), we incorporate TimeFormer to capture cross-time relationships and explore motion patterns implicitly (second row).\nWe share weights of two deformation fields to transfer the learned motion knowledge. This allows us to exclude this Auxiliary Training Module\nduring inference.",
        "qtype": "Implementation_Details",
        "response": "First, note that “​[mask1]” in the question is simply the light‐blue block in the red box – i.e. the TimeFormer module.  What TimeFormer does is roughly the following:\n\n1.  Take your canonical Gaussians (each one carries a 3-D position plus whatever extra feature channels you have) and a short list of sampled times {t₀…tB-1}.  \n2.  Replicate the entire set of Gaussians once for each time in the batch and likewise replicate the time stamps once per Gaussian.  \n3.  Concatenate the (positional) Gaussian coordinates with the (time) embeddings, run both through a high-frequency positional encoding, and feed the result into a standard Transformer encoder (multi-head self-attention + MLP blocks).  \n4.  At the end, apply a small MLP to the Transformer’s output to predict a per-Gaussian offset Δx.  \n5.  Finally, add Δx back to the original canonical positions as a *residual* “motion correction.”  \n\nIn other words, TimeFormer turns your static, canonical Gaussians into *time-aware* Gaussians by learning, via cross-temporal self-attention, a little delta–position for each Gaussian.  That delta is then added to the original positions, and *that* shifted set of Gaussians is what the deformation field sees."
    },
    {
        "question": "How are Lc and Lt combined to optimize original and auxiliary splatted outputs?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Then, we apply the splatting algorithm to these two groups of deformed space. We calculate the losses between rendered images and ground truth I as follows: L = L_c + λ_t L_t, where L_c and L_t represent losses of original branch and TimeFormer branch with I. We use a relatively smaller λ_t because we find it easy to overfit on the second branch with TimeFormer, causing a degradation in inference quality."
        ],
        "final_answer": "They are combined in a weighted‐sum total loss: L = L_c + λ_t L_t, where L_c is the loss on the original splatted outputs, L_t is the loss on the TimeFormer (auxiliary) splatted outputs, and λ_t is set relatively small to avoid overfitting the auxiliary branch.",
        "relevant_elements": [
            "Splatting",
            "Loss Lc",
            "Loss Lt"
        ],
        "id": 563,
        "masked_question": "How are [mask1] and [mask2] combined to optimize original and auxiliary splatted outputs?",
        "masked_number": 2,
        "masked_elements": [
            "Lc",
            "Lt"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11941v1_figure_2.png",
        "paperid": "2411.11941v1",
        "paper_path": "./papers/2411.11941v1.json",
        "figure_id": "2411.11941v1_figure_2.png",
        "caption": "Figure 2: The Framework of Deformable 3D Gaussians Reconstruction with TimeFormer. Existing deformable 3D Gaussians framework usually includes the canonical space and the deformation field (first row), we incorporate TimeFormer to capture cross-time relationships and explore motion patterns implicitly (second row).\nWe share weights of two deformation fields to transfer the learned motion knowledge. This allows us to exclude this Auxiliary Training Module\nduring inference.",
        "qtype": "Implementation_Details",
        "response": "They are simply summed into a single training objective, with the auxiliary (blue‐boxed) loss down‐weighted by a small constant.  In practice the paper optimizes\n\n Ltotal  =  Lc  +  λ · Lt\n\nwhere Lc is the red‐boxed “original” splat loss and Lt is the blue‐boxed “auxiliary” (TimeFormer) splat loss."
    },
    {
        "question": "How does Frequency-Domain Mixed Augmentation generate and apply the binary mask for mixing high-frequency and original frequency components?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "We define a frequency mixing function, which randomly mixes F and F_H:\n\n   F' = M ⊙ F_H + (1 - M) ⊙ F",
            "M is a matrix of the same size as F and F_H, with a randomly selected square area covering (randomly ranging from 0 to 0.5) proportion of the total area set to 1, and the rest set to 0."
        ],
        "final_answer": "Frequency-Domain Mixed Augmentation constructs a binary mask M of the same dimensions as the original and high-frequency spectra by selecting a random square region that spans between 0% and 50% of the total area and setting its entries to 1 (all other entries of M are 0). It then uses M to blend the two spectra via element-wise multiplication: F′ = M ⊙ F_H + (1 – M) ⊙ F, where F_H is the high-frequency filtered spectrum and F is the original spectrum.",
        "relevant_elements": [
            "Frequency-Domain Mixed Augmentation"
        ],
        "id": 564,
        "masked_question": "How does [mask1] generate and apply the binary mask for mixing high-frequency and original frequency components?",
        "masked_number": 1,
        "masked_elements": [
            "Frequency-Domain Mixed Augmentation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.06977v2_figure_2.png",
        "paperid": "2410.06977v2",
        "paper_path": "./papers/2410.06977v2.json",
        "figure_id": "2410.06977v2_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method, consisting of (a) Frequency-Domain Mixed Augmentation(described in Sec.3.2.1), (b) Object-Aware Dynamic Selection (described in Sec.3.2.2), (c) Feature Equilibrium Loss (described in Sec.3.3).",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought reasoning  \n1. In Sec. 3.2.1 the paper introduces two frequency‐domain representations of an image:  \n   • F, the full-spectrum Fourier transform of the image  \n   • F_H, the same transform after applying a high-pass Gaussian filter (so it only contains high-frequency coefficients)  \n2. The goal of the mixed augmentation is to splice some portions of F_H into F in the frequency domain, then invert back to the spatial domain.  \n3. To decide *where* to splice, they create a binary mask M of the same H×W size as F and F_H.  \n4. M is constructed by:  \n   – Randomly choosing a square window whose side length is drawn so that the window covers between 0% and 50% of the total frequency map area  \n   – Setting the entries inside that square to 1 and all other entries to 0  \n5. They then combine F and F_H via element‐wise masking:  \n   F_mixed = M ⊙ F_H + (1–M) ⊙ F  \n   (where ⊙ denotes element‐wise multiplication)  \n6. Finally, they apply the inverse Fourier transform (IFFT) to F_mixed to obtain the augmented image in the spatial domain.\n\nAnswer  \n[mask1] refers to the random binary mask M that is generated by picking one square region in the frequency map (covering up to half the total area) and setting that region’s entries to 1 (all else to 0). This mask is used to splice the high-pass–filtered coefficients into the original Fourier spectrum:  \n\n   F_mixed = M⊙F_H + (1–M)⊙F  \n\nThe augmented frequency map F_mixed is then brought back to the image plane via IFFT."
    },
    {
        "question": "How does Object-Aware Dynamic Selection sort and select top Z attention-based high-frequency tokens from MHSA heads?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Upon reaching the final layer ℓ, we compute the attention scores a_i = \\frac{1}{M} \\sum_{h=1}^M A^{ℓ,h}_{cls,i}, serving as a quantifiable metric that reflects the average attention distribution across heads in the model’s final layer.",
            "These a_i are analyzed to dynamically select the set of high-frequency information tokens that exhibit the highest attention scores. This selection is formalized as f = γ(a) and Ψ = f[1:Z], where γ is a function that sorts scores in a set in descending order and then outputs the indices of these scores, Ψ represents the object perception token indices, and Z is a selection parameter."
        ],
        "final_answer": "Object-Aware Dynamic Selection first averages the class-to-token attention weights over all M MHSA heads in the last Transformer layer to obtain a per-patch score a_i. It then applies a sorting function γ to rank these a_i values in descending order and picks the first Z indices from the sorted list. Those top-Z tokens are treated as the high-frequency embeddings most relevant to the target.",
        "relevant_elements": [
            "Object-Aware Dynamic Selection",
            "MHSA"
        ],
        "id": 565,
        "masked_question": "How does [mask1] sort and select top Z attention-based high-frequency tokens from MHSA heads?",
        "masked_number": 1,
        "masked_elements": [
            "Object-Aware Dynamic Selection"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.06977v2_figure_2.png",
        "paperid": "2410.06977v2",
        "paper_path": "./papers/2410.06977v2.json",
        "figure_id": "2410.06977v2_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method, consisting of (a) Frequency-Domain Mixed Augmentation(described in Sec.3.2.1), (b) Object-Aware Dynamic Selection (described in Sec.3.2.2), (c) Feature Equilibrium Loss (described in Sec.3.3).",
        "qtype": "Implementation_Details",
        "response": "In our notation (from Sec. 3.2.2) we first compute, at the last layer N, the attention score from the class-token to every patch token i in each head m:  \n  aₘ,𝐶𝐿𝐒→i = softmax(Qₘᴺ[CLS] · Kₘᴺ[i]ᵀ).  \nWe then average over the M heads to get a single score per patch:  \n  Aᵢ = (1/M) ∑_{m=1}^M aₘ,𝐶𝐿𝐒→i.  \n\n[mask1] is simply the operation that takes that vector A = (A₁, … , A_L), sorts it in descending order, and returns the indices of the top Z entries.  Concretely:  \n  I = Sort(A)_{descending}[1:Z]  \n\nThose Z indices I are exactly the patch positions whose high-frequency counterparts we carry forward (together with the [CLS] token) into the downstream Transformer layers."
    },
    {
        "question": "How does Frequency-Domain Mixed Augmentation interact with high-frequency and original embeddings in the Vision Transformer?",
        "relevant_section_ids": [
            "3.2.1",
            "3.3"
        ],
        "relevant_context": [
            "In brief, we transform the spatial representation of an image into a frequency domain and extract high-frequency information to obtain a representation dominated by high frequencies. The frequency-domain representation of the original image is mixed with the high-frequency representation, thereby generating an augmented representation. ... The augmented high-frequency representation, denoted by X_H and serving as the input high-frequency representation, typically represents finer details and edges within the image. X_H is derived by converting the augmented frequency domain representation X' back to the spatial domain.",
            "Our model simultaneously takes visual image inputs and high-frequency augmented inputs, both of which are crucial for discriminative feature learning. The strategies we proposed above primarily guide the model to focus on high-frequency information. However, this needs to be established without compromising the learning of original visual information. Therefore, we further introduce the feature equilibrium loss to constrain the high-frequency features and visual features of the same individual from deviating excessively in the feature space."
        ],
        "final_answer": "Frequency-Domain Mixed Augmentation (FMA) first converts each input image into the frequency domain, extracts its high-frequency components, and then randomly blends those high-frequency components with the original frequency spectrum. The blended spectrum is brought back to the spatial domain to produce an augmented image X_H rich in edge and texture details. Both this high-frequency augmented image and the original image are then tokenized into patch embeddings and passed through the same Vision Transformer backbone (with shared weights). As a result, the model learns two parallel sets of embeddings—one from the raw image and one from the FMA output—which are jointly optimized (via ID and triplet losses) and kept in alignment by a feature-equilibrium loss that prevents them from drifting apart in feature space.",
        "relevant_elements": [
            "Frequency-Domain Mixed Augmentation",
            "Vision Transformer"
        ],
        "id": 566,
        "masked_question": "How does [mask1] interact with high-frequency and original embeddings in the Vision Transformer?",
        "masked_number": 1,
        "masked_elements": [
            "Frequency-Domain Mixed Augmentation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.06977v2_figure_2.png",
        "paperid": "2410.06977v2",
        "paper_path": "./papers/2410.06977v2.json",
        "figure_id": "2410.06977v2_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method, consisting of (a) Frequency-Domain Mixed Augmentation(described in Sec.3.2.1), (b) Object-Aware Dynamic Selection (described in Sec.3.2.2), (c) Feature Equilibrium Loss (described in Sec.3.3).",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in Fig. 2 is nothing other than our Frequency-Domain Mixed Augmentation (FMA) front end.  Its only job is to take an RGB image, FFT it, drop into a high-pass filter, mix back some of the original low-frequency content, IFFT it, and hand you back a _second_ “high-frequency–enhanced” version of the same image.  \n\n   1.  You now have two streams of patches  \n       – the _ordinary_ patches from the unmodified image, and  \n       – the _augmented_ patches from the FMA output.  \n\n   2.  Both streams are linearly projected into D-dimensional token embeddings _with the _same_ projection weights_ and get prepended by their own CLS token.  From there on they share the _same_ Vision-Transformer backbone.  \n\n   3.  Inside the transformer:  \n      •  The original‐image CLS token produces an attention map that “knows” which spatial regions really matter for ReID.  We average across heads in the last block to get a per-patch score, and we use that to _dynamically select_ the corresponding high-frequency tokens from the FMA branch.  \n      •  The retained high-frequency tokens are then re‐injected (again alongside a CLS token) into the transformer for a final round of encoding.  \n\n   4.  At the output, we end up with two sets of token embeddings—one coming from the plain image, one coming from the FMA image.  We  \n      (a) apply ID- and triplet-losses to each CLS token for the usual global ReID objective, and  \n      (b) apply a _feature-equilibrium loss_ that gently “pulls” each selected high-frequency patch embedding back toward its original counterpart.  \n\nIn this way the FMA module both _injects_ extra high-frequency detail into the ViT and is then _tied back_ to the ordinary embeddings via shared weights, guided selection, and a balancing loss so that the network never “goes off” chasing only spurious high-frequency noise."
    },
    {
        "question": "How does Feature Equilibrium Loss balance discrepancies between global features and high-frequency features to preserve visual consistency?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Therefore, we further introduce the feature equilibrium loss to constrain the high-frequency features and visual features of the same individual from deviating excessively in the feature space.",
            "Feature equilibrium loss aggregates the differences across all selected tokens, ensuring a comprehensive measure of the discrepancy between the high-frequency and original features for each token.",
            "By minimizing L_FE, we encourage the model to preserve the essential features of the original input, while still leveraging the detailed textures and patterns enhanced in the high-frequency components, to ensure that the model learning does not overemphasize the high-frequency details at the expense of the original feature.",
            "This balance maintains visual and spatial consistency with the original feature while emphasizing high-frequency feature, thus improving the overall efficacy of feature extraction."
        ],
        "final_answer": "The Feature Equilibrium Loss computes the per-token differences between the high-frequency embeddings and the corresponding original (global) embeddings, sums these discrepancies across all selected tokens, and then minimizes this aggregate. By doing so, it prevents the high-frequency–enhanced features from drifting too far from the original visual features, ensuring that fine textures and patterns are learned without sacrificing the overall visual and spatial consistency of the image.",
        "relevant_elements": [
            "Feature Equilibrium Loss",
            "Global Features"
        ],
        "id": 567,
        "masked_question": "How does [mask1] balance discrepancies between global features and high-frequency features to preserve visual consistency?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Equilibrium Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.06977v2_figure_2.png",
        "paperid": "2410.06977v2",
        "paper_path": "./papers/2410.06977v2.json",
        "figure_id": "2410.06977v2_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method, consisting of (a) Frequency-Domain Mixed Augmentation(described in Sec.3.2.1), (b) Object-Aware Dynamic Selection (described in Sec.3.2.2), (c) Feature Equilibrium Loss (described in Sec.3.3).",
        "qtype": "Literature_Background",
        "response": "The module in the red box is simply the “feature‐equilibrium loss.”  Concretely, after you run the original image and its high-frequency–augmented twin through shared ViT encoders, you extract the set of corresponding patch embeddings (excluding the [CLS] token) from both branches and compute the L₂ distance between each pair.  The feature‐equilibrium loss is the sum of those distances (possibly weighted) across all selected patches.  By driving that loss toward zero, you force the high-frequency features not to “drift” too far away from the original (global) features, so that the fine‐detail branch is always kept in visual and spatial alignment with the global image branch."
    },
    {
        "question": "How does integrating text-based environment embedding with convolutional feature encoder mirror adapter methodologies in noise-aware training?",
        "relevant_section_ids": [
            "2.1",
            "3"
        ],
        "relevant_context": [
            "Leem et al. [21] proposed to adapt the transformer-based SER model to multiple types of noises with skip connection adapters. They not only trained the SER model with multiple environments but also focused on leveraging the environmental information of the testing conditions to improve SER performance under noisy conditions.",
            "We achieve this goal by combining the text embedding with the acoustic representation, which is the output of the convolutional encoder. We apply trainable linear projection to the text embedding to match its dimension to the acoustic representation embeddings. We concatenate the projected text embedding to the acoustic representation embeddings along the time axis, then feed them into the transformer encoder."
        ],
        "final_answer": "By projecting the text-derived environment embedding to match the convolutional feature dimensions and then concatenating it alongside those features before the transformer encoder, the method effectively injects environment-specific context into the network in the same spirit as skip-connection adapters—i.e., adding test-condition information directly into intermediate representations to achieve noise-aware adaptation.",
        "relevant_elements": [
            "text-based environment embedding",
            "convolutional feature encoder"
        ],
        "id": 568,
        "masked_question": "How does integrating [mask1] with convolutional feature encoder mirror adapter methodologies in noise-aware training?",
        "masked_number": 1,
        "masked_elements": [
            "text-based environment embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.17716v1_figure_1.png",
        "paperid": "2407.17716v1",
        "paper_path": "./papers/2407.17716v1.json",
        "figure_id": "2407.17716v1_figure_1.png",
        "caption": "Figure 1: Our proposed text-guided environment-aware training framework. The environment representation is concatenated with the output of the convolutional feature encoder.",
        "qtype": "Literature_Background",
        "response": "In the diagram you can see that the green “text‐based environment embedding” (the thing in the red box) is extracted once per utterance by running a short prompt (“This speech is recorded in {environment}.”) through a frozen text encoder and then average‐pooling the token outputs.  That pooled vector is linearly projected to the same dimension as the per‐frame acoustic features coming out of the convolutional front end, and then simply concatenated onto those features before they go into the transformer.\n\nIf you compare that to the “adapter” style noise‐aware methods (e.g. Leem et al.’s skip‐connection adapters), the parallel is:  \n\n 1. In adapter methods you take a small, trainable module (sometimes just a bottleneck layer) and insert it into the frozen backbone so that it injects environment‐ or domain‐specific information.  \n 2. Here we are taking a small, trainable projection of a text embedding (the “environment adapter”), and inserting it in exactly the same place—right before the transformer encoder—by concatenating it with the acoustic features.  \n\nIn both cases you never re‐train the entire model from scratch on every new condition. Instead you learn a tiny piece (the adapter or the text embedding projection) that modulates the frozen or lightly fine-tuned backbone to become “aware” of the noise environment."
    },
    {
        "question": "How does pre-trained text encoder injection influence transformer encoder attention akin to contrastive pretraining methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "pre-trained text encoder",
            "transformer encoder"
        ],
        "id": 569,
        "masked_question": "How does [mask1] injection influence [mask2] attention akin to contrastive pretraining methods?",
        "masked_number": 2,
        "masked_elements": [
            "pre-trained text encoder",
            "transformer encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.17716v1_figure_1.png",
        "paperid": "2407.17716v1",
        "paper_path": "./papers/2407.17716v1.json",
        "figure_id": "2407.17716v1_figure_1.png",
        "caption": "Figure 1: Our proposed text-guided environment-aware training framework. The environment representation is concatenated with the output of the convolutional feature encoder.",
        "qtype": "Literature_Background",
        "response": "Here’s the gist of what’s happening, broken down step by step:\n\n1.  In our diagram, the red‐boxed module (“[mask1]”) is nothing more than the fixed, pre‐trained text encoder that turns a natural‐language prompt (“This speech is recorded in a  {environment}”) into a single vector embedding.  \n2.  Right after the convolutional feature encoder spits out its time‐series of acoustic tokens, we “inject” that text‐encoder embedding by linearly projecting it to the same dimensionality and then concatenating it onto the acoustic token stream.  \n3.  That augmented sequence of tokens is fed into the blue‐boxed block (“[mask2]”), the Transformer encoder. Because the environment embedding now lives inside the same token sequence, every self‐attention head in that Transformer layer can attend simultaneously to acoustic frames and the text prompt.  \n4.  Functionally, this is almost the same trick used in CLIP/CLAP contrastive pre‐training: they concatenate or jointly attend over image (or audio) patches and text tokens so that the model learns to align the two modalities in a shared space.  Here, instead of learning a global alignment, we’re using the shared‐attention mechanism to bias the speech representation toward what the prompt tells it about the noise environment.  \n\nIn other words, by injecting the text‐encoder’s environment embedding directly into the token stream ahead of the Transformer, we turn the Transformer’s multi‐head attention into a cross‐modal attention module—just as contrastive pretraining methods do—so that acoustic features are conditionally weighted and “denoised” with respect to the described environment."
    },
    {
        "question": "How does concatenating the environment embedding to convolutional outputs guide the transformer encoder's adaptation to noise?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "The text embedding extracted from the environmental description is combined with the acoustic representation in the SER model, allowing it to denoise the representation for the given environmental description.",
            "We achieve this goal by combining the text embedding with the acoustic representation, which is the output of the convolutional encoder. We apply trainable linear projection to the text embedding to match its dimension to the acoustic representation embeddings. We concatenate the projected text embedding to the acoustic representation embeddings along the time axis, then feed them into the transformer encoder. We update the transformer encoder and the downstream head with the concatenated embeddings."
        ],
        "final_answer": "By concatenating a trainable projection of the environment embedding to each frame of the convolutional feature outputs along the time axis, the transformer encoder receives both the noisy acoustic features and explicit information about the noise condition. This joint input guides the transformer to learn a denoising function specific to the described environment, adapting its internal representations to better filter out the noise.",
        "relevant_elements": [
            "text-based environment embedding",
            "transformer encoder"
        ],
        "id": 570,
        "masked_question": "How does concatenating the [mask1] to convolutional outputs guide the [mask2]'s adaptation to noise?",
        "masked_number": 2,
        "masked_elements": [
            "text-based environment embedding",
            "transformer encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.17716v1_figure_1.png",
        "paperid": "2407.17716v1",
        "paper_path": "./papers/2407.17716v1.json",
        "figure_id": "2407.17716v1_figure_1.png",
        "caption": "Figure 1: Our proposed text-guided environment-aware training framework. The environment representation is concatenated with the output of the convolutional feature encoder.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-Thought  \n1. In Figure 1 the red‐boxed token ([mask1]) is the text‐based “environment embedding” (i.e. the pooled representation of the prompt “This speech is recorded in {environment}”).  \n2. The blue box ([mask2]) surrounds the “Transformer encoder,” which takes as input the frame‐wise acoustic features coming out of the convolutional feature encoder.  \n3. During training on noisy speech, we linearly project the environment embedding up to the same time‐series length as the conv‐features and then concatenate it (along the time axis) to those features.  \n4. By doing so, every frame of the acoustic representation carries an extra “tag” telling the model what kind of noise to expect.  \n5. When these concatenated features pass through the Transformer encoder, its self‐attention and feed-forward layers can learn to modulate (i.e. denoise or re-weight) the acoustic information in an environment‐specific way.  \n6. In other words, the injected text embedding guides the Transformer to adapt its internal representations so that it learns noise-conditioned denoising functions.  \n\nAnswer  \nBy concatenating the text‐based environment embedding ([mask1]) to the frame-level outputs of the convolutional encoder, each time step fed into the Transformer encoder ([mask2]) carries explicit information about the noise type.  This conditions the self-attention and feed-forward layers of the transformer to learn noise-aware denoising strategies—effectively steering the model to adapt its acoustic feature transformations based on the described environment."
    },
    {
        "question": "How does average pooling shape the text-based environment embedding before its fusion with acoustic representations?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "For each text encoder, we use the same tokenizer used in its pre-training to tokenize the text description of the environment. We extract token-level text embeddings from the tokenized prompt and then apply average pooling, resulting in a single representation vector for each prompt."
        ],
        "final_answer": "Average pooling takes the sequence of token-level embeddings from the text encoder and aggregates them into a single fixed-size environment representation vector, which is then projected and concatenated with the acoustic embeddings.",
        "relevant_elements": [
            "average pooling",
            "text-based environment embedding"
        ],
        "id": 571,
        "masked_question": "How does [mask1] shape the [mask2] before its fusion with acoustic representations?",
        "masked_number": 2,
        "masked_elements": [
            "average pooling",
            "text-based environment embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.17716v1_figure_1.png",
        "paperid": "2407.17716v1",
        "paper_path": "./papers/2407.17716v1.json",
        "figure_id": "2407.17716v1_figure_1.png",
        "caption": "Figure 1: Our proposed text-guided environment-aware training framework. The environment representation is concatenated with the output of the convolutional feature encoder.",
        "qtype": "Experimental_Results",
        "response": "Here, mask1 (“Average pooling”) simply collapses the sequence of token‐level vectors into one fixed‐size vector—i.e. it takes the token embeddings produced by the text encoder and averages them over all positions.  That single averaged vector is the text‐based environment embedding (mask2), which is then projected and concatenated with the acoustic representations."
    },
    {
        "question": "How does the cross-attention mechanism contribute to the local alignment loss based on the results?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Cross-attention",
            "Local alignment loss"
        ],
        "id": 573,
        "masked_question": "How does the [mask1] mechanism contribute to the [mask2] based on the results?",
        "masked_number": 2,
        "masked_elements": [
            "Cross-attention",
            "Local alignment loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00609v1_figure_1.png",
        "paperid": "2411.00609v1",
        "paper_path": "./papers/2411.00609v1.json",
        "figure_id": "2411.00609v1_figure_1.png",
        "caption": "Figure 1: The Proposed MRI-Report Contrastive Learning Framework",
        "qtype": "Experimental_Results",
        "response": "In our framework, the little red‐boxed “Self-attention” layer is in fact what produces the *global* MRI embedding that the blue-boxed “Global alignment loss” is computed upon.  Concretely, after the last 3D‐ResNet block we run a tiny self‐attention head (Q,K,V all learned via 1×1×1 convolutions) which re-weights the 512‐channel feature‐map into a *single* weighted global descriptor.  That descriptor is then directly contrasted against the CLS‐token from the Longformer via our triplet‐based global alignment loss (and augmented with tumor‐location weighting).  \n\nBy doing this, the network learns to pull matching image–report pairs tightly together in feature space and to push mismatches apart.  In our ablations this self‐attention‐produced global embedding under the global alignment term is *by far* the strongest single contributor to downstream gains—elevating our external test AUC from the low 0.6–0.7s (without it) up to ≈0.76, and boosting our 2D/3D Dice overlap of attention-to-tumor from under 0.2 to over 0.30."
    },
    {
        "question": "What alternative architectures could replace the Clinical Longformer for extracting global text representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Clinical Longformer",
            "Global Text Representation"
        ],
        "id": 574,
        "masked_question": "What alternative architectures could replace the [mask1] for extracting [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Clinical Longformer",
            "Global Text Representation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.00609v1_figure_1.png",
        "paperid": "2411.00609v1",
        "paper_path": "./papers/2411.00609v1.json",
        "figure_id": "2411.00609v1_figure_1.png",
        "caption": "Figure 1: The Proposed MRI-Report Contrastive Learning Framework",
        "qtype": "Others",
        "response": "First, let’s unpack the question in light of the figure and text:\n\n–   The red‐boxed module (“[mask1]”) is the Clinical Longformer encoder that takes the entire radiology report and produces, among other things, a “CLS” token embedding.  \n–   The blue‐boxed output (“[mask2]”) is that same “CLS” token—or, more generally, the global text representation—pulled off the last layer of the text encoder.\n\nThe question asks: what other encoders (in place of the Clinical Longformer) could we plug in to get a global text embedding for the report?\n\nHere are a few plausible substitutes:\n\n1.  BERT-based models  \n    •  vanilla BERT-Base or BERT-Large (Devlin et al.)  \n    •  ClinicalBERT (initialized from BERT and further pre-trained on clinical notes)  \n    •  BioBERT or PubMedBERT (pre-trained on biomedical literature)\n\n2.  RoBERTa-family models  \n    •  RoBERTa-Base / RoBERTa-Large (Liu et al.)  \n    •  BioRoBERTa (further pre-trained on PubMed abstracts)\n\n3.  Long‐sequence transformers alternative to Longformer  \n    •  BigBird (Zaheer et al.) – sparse attention for very long documents  \n    •  LED (Longformer Encoder–Decoder) if you also want to generate text  \n    •  Reformer or Linformer – lighter‐weight efficient transformers\n\n4.  Hierarchical or convolutional text encoders (less common today, but still viable)  \n    •  HAN (Hierarchical Attention Network) that pools sentence‐level then document‐level embeddings  \n    •  CNN-based encoders (e.g. Kim-style text‐CNN) with a final max-pool + dense projection\n\nAny of these could be slotted in place of the Clinical Longformer.  You would still take their final \"[CLS]\" (or equivalent) vector as your global text representation and feed it through the same MLP projection head for contrastive learning."
    },
    {
        "question": "What scalability limitations could cross-attention modules introduce when aligning local image representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "cross-attention",
            "Local Image Representation"
        ],
        "id": 575,
        "masked_question": "What scalability limitations could [mask1] modules introduce when aligning local [mask2]s?",
        "masked_number": 2,
        "masked_elements": [
            "cross-attention",
            "Local Image Representation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.00609v1_figure_1.png",
        "paperid": "2411.00609v1",
        "paper_path": "./papers/2411.00609v1.json",
        "figure_id": "2411.00609v1_figure_1.png",
        "caption": "Figure 1: The Proposed MRI-Report Contrastive Learning Framework",
        "qtype": "Others",
        "response": "Here is a sketch of the reasoning:\n\n1.  In our framework the little red‐boxed modules are exactly the cross‐attention blocks that take one modality’s local features (say, image patches) as “queries” and the other modality’s local features (say, text tokens) as “keys” and “values.”\n\n2.  To do that they must form a full attention matrix between every query and every key—that is, if you have N image patches and M text tokens you end up computing N×M dot-products (and storing that big N×M array in memory).\n\n3.  As you increase the number of local units (either finer image patches or longer report texts), that attention matrix grows very quickly.  In fact, even if you fix N=M the cost goes like O(N²) in both compute and memory.\n\n4.  In practice that quadratic blow-up means that you cannot simply crank up the resolution of your 3D volume or allow arbitrarily long reports without running out of GPU RAM or spending days just in the attention layer.\n\n5.  In short, cross-attention is a powerful way to “align” two sets of local features, but it scales quadratically in the size of those sets.  That quadratic cost is the key scalability bottleneck when you try to line up large numbers of image patches and/or text tokens."
    },
    {
        "question": "What robustness issues might global audio feature introduce when fed into Multi-scale Multi-instance Transformer?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "global audio feature",
            "Multi-scale Multi-instance Transformer"
        ],
        "id": 576,
        "masked_question": "What robustness issues might [mask1] introduce when fed into Multi-scale Multi-instance Transformer?",
        "masked_number": 1,
        "masked_elements": [
            "global audio feature"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.00486v1_figure_2.png",
        "paperid": "2409.00486v1",
        "paper_path": "./papers/2409.00486v1.json",
        "figure_id": "2409.00486v1_figure_2.png",
        "caption": "Figure 2: Illustration of the proposed Multi-scale Multi-instance Visual Sound Localization (M2VSL) framework for weakly-supervised audio-visual localization and segmentation.",
        "qtype": "Others",
        "response": "Here’s the line of reasoning:\n\n1.  In the diagram the red‐boxed “mask1” is the single, pooled global audio embedding that comes out of the audio encoder.  \n2.  By collapsing an entire spectrogram (or mixture of sounds) down to one fixed‐length vector you inevitably throw away all of the fine‐grained temporal and “where in the clip” information.  Worse, any background noise, reverberation, cross-talk or silent gaps all get folded into that one summary.  \n3.  If you then feed that coarse, noise-contaminated global vector straight into the Multi-scale Multi-instance Transformer (MMT) you risk:  \n    •  Over-attending to non-sound-source background clutter (since the embedding doesn’t distinguish “dog bark” from “air conditioner hum”)  \n    •  Losing the ability to disambiguate multiple simultaneous sources (there’s no per-source cue left)  \n    •  Becoming brittle to variations in recording conditions (room acoustics, microphone characteristics, noise floors)  \n    •  Confusing truly silent frames with very low-level audio (both collapse to a near-zero vector)  \n4.  In short, a single global audio vector tends to be both overly coarse and overly noisy.  That lack of granularity and robustness propagates through the transformer, leading to unstable attention maps, false activations on background regions, and poor generalization to new or noisy recordings."
    },
    {
        "question": "How might modality imbalance between multi-scale visual features and global audio feature affect Multi-scale Multi-instance Contrastive learning?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "multi-scale visual features",
            "global audio feature",
            "Multi-scale Multi-instance Contrastive"
        ],
        "id": 577,
        "masked_question": "How might modality imbalance between [mask1] and global audio feature affect Multi-scale Multi-instance Contrastive learning?",
        "masked_number": 1,
        "masked_elements": [
            "multi-scale visual features"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.00486v1_figure_2.png",
        "paperid": "2409.00486v1",
        "paper_path": "./papers/2409.00486v1.json",
        "figure_id": "2409.00486v1_figure_2.png",
        "caption": "Figure 2: Illustration of the proposed Multi-scale Multi-instance Visual Sound Localization (M2VSL) framework for weakly-supervised audio-visual localization and segmentation.",
        "qtype": "Others",
        "response": "The red‐boxed feature in the figure is simply the “multi-scale visual features” that are being carried forward to the Multi-scale Multi-instance Contrastive (MMC) module.  In MMC we are literally aligning a whole bag of visual descriptors (tens or even hundreds of spatial tokens at each scale) against one single global audio vector.  That creates a severe modality imbalance:  \n\n 1.  The positive signal (one audio feature) is being asked to pull on a very large set of visual candidates, most of which are actually negatives.  \n 2.  Because there are so many more visual “instances” than audio ones, the contrastive loss tends to be dominated by visual–visual or visual–audio negatives, drowning out the true audio–visual positive.  \n 3.  In practice this means the network can trivially “win” the contrastive game by latching onto whatever visual patch happens to be easiest to align with the audio, rather than learning a robust cross-modal mapping.  \n 4.  The result is slow or unstable convergence, degraded mask-quality (coarse or incorrect localization), and in the worst case a collapse of the audio feature (it simply never learns to discriminate fine-grained, scale-specific visual cues).  \n\nSo whenever you have many more visual tokens than audio tokens in an MMC set up, you automatically bias the gradients toward the larger modality (here, the multi-scale visuals) unless you explicitly rebalance (for instance by reweighting positives, subsampling visual instances, or introducing a stronger audio regularizer)."
    },
    {
        "question": "Why apply Multi-scale Multi-instance Transformer to aggregate global audio feature with multi-scale visual features?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Furthermore, we introduce a unique component, the Multi-scale Multi-instance Transformer to dynamically aggregate multi-scale cross-modal representations, enhancing the standard Visual Transformer (ViT) specifically for the task of visual sound localization.",
            "The MMT module is designed to effectively aggregate multi-scale features from the raw input."
        ],
        "final_answer": "The Multi-scale Multi-instance Transformer is used in order to dynamically and effectively aggregate global audio representations together with visual features at multiple scales, thereby enabling richer cross-modal fusion and improving the accuracy of visual sound localization and segmentation.",
        "relevant_elements": [
            "Multi-scale Multi-instance Transformer",
            "global audio feature",
            "multi-scale visual features"
        ],
        "id": 578,
        "masked_question": "Why apply [mask1] to aggregate global audio feature with multi-scale visual features?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-scale Multi-instance Transformer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.00486v1_figure_2.png",
        "paperid": "2409.00486v1",
        "paper_path": "./papers/2409.00486v1.json",
        "figure_id": "2409.00486v1_figure_2.png",
        "caption": "Figure 2: Illustration of the proposed Multi-scale Multi-instance Visual Sound Localization (M2VSL) framework for weakly-supervised audio-visual localization and segmentation.",
        "qtype": "Design_Rationale",
        "response": "Here “[mask1]” is precisely the Multi-scale Multi-instance Transformer (MMT). We inject the pooled audio embedding together with our three-stage visual feature maps into that transformer because:\n\n1. Cross-modal fusion beyond mere concatenation  \n   – Previous approaches either max-pooled or dot-producted audio and visual features, but that treats all spatial positions (and scales) equally. By feeding both into a transformer, we let the model learn *where* (which spatial locations) and *at what scale* the audio signal is most relevant.\n\n2. Dynamic, scale-aware attention  \n   – The MMT uses multi-head self-attention over a bag of “visual tokens at scale 1, scale 2, scale 3” plus the single global audio token.  Each head can thus attend from audio→visual and visual→visual across all scales, dynamically re-weighting each resolution according to the sound cue.\n\n3. Better feature alignment for weak supervision  \n   – Our ultimate goal is pixel-level segmentation without any pixel-level labels.  The transformer’s ability to softly route the global audio context into the most promising spatial locations lets us generate much sharper heat-maps and segmentation masks than a simple contrastive alignment on raw CNN features.\n\nIn short, we apply the MMT so that the single global audio representation can *learnably* and *adaptively* fuse into every position of every scale of the visual feature hierarchy—producing richer, better-aligned audio–visual features from which to derive accurate, weakly-supervised segmentation maps."
    },
    {
        "question": "What benefits arise from using Multi-scale Multi-instance Contrastive with both positive and negative multi-scale instances?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Addressing the modality uncertainty inherent in previous weakly-supervised semantic segmentation baselines, our approach, inspired by EZ-VSL, focuses on aligning the audio with the most closely associated multi-scale visual features. This is predicated on the understanding that most video frame locations are unrelated to the sound source and should not be aligned with the audio during training.",
            "We introduce a multi-scale multiple-instance contrastive learning objective, MMC, which seeks to align at least one location in a bag of multi-scale visual features with the corresponding audio representation in the same mini-batch, which is defined as:",
            "Furthermore, we employ a symmetric loss to differentiate negative audio bags from other audio samples in the same mini-batch, which is defined as",
            "This approach aims to learn discriminative global audio representations and multi-scale visual features, which are then used to generate updated multi-scale audio-visual features and, ultimately, the output mask using , which follows EZ-VSL (Mo & Morgado, 2022a ###reference_b26###)."
        ],
        "final_answer": "By using MMC with positive multi-scale instances, the model ensures at least one spatial location at each scale is correctly aligned with the audio, addressing modality uncertainty and avoiding spurious alignments. Incorporating negative multi-scale instances via a symmetric contrastive loss further drives the model to distinguish mismatched audio–visual pairs. Together, these positive and negative constraints produce more discriminative global audio embeddings and multi-scale visual features, leading to more accurate audio-driven mask generation under weak supervision.",
        "relevant_elements": [
            "Multi-scale Multi-instance Contrastive",
            "positive",
            "negative"
        ],
        "id": 579,
        "masked_question": "What benefits arise from using [mask1] with both [mask2] and negative multi-scale instances?",
        "masked_number": 2,
        "masked_elements": [
            "Multi-scale Multi-instance Contrastive",
            "positive"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.00486v1_figure_2.png",
        "paperid": "2409.00486v1",
        "paper_path": "./papers/2409.00486v1.json",
        "figure_id": "2409.00486v1_figure_2.png",
        "caption": "Figure 2: Illustration of the proposed Multi-scale Multi-instance Visual Sound Localization (M2VSL) framework for weakly-supervised audio-visual localization and segmentation.",
        "qtype": "Design_Rationale",
        "response": "Here’s a concise breakdown of why we feed the red-boxed Multi-scale Multi-instance Transformer (MMT) both  \n1) the “positive” multi-scale instances (blue-boxed audio–matched visual features) and  \n2) the negative multi-scale instances (visual features from other images):  \n\n1. Stronger contrastive signal across scales  \n   – By showing the Transformer both the correctly matched (“positive”) patches and the hard negatives, it can learn to up‐weight true audio–visual alignments and down‐weight spurious ones.  \n   – This amplifies the margin between on-sound and off-sound regions at every resolution.  \n\n2. Richer, context-aware feature aggregation  \n   – The self-attention layers in MMT now attend not only within one scale but also across all positive and negative scale instances.  \n   – As a result, the model can globally reason about which spatial locations really “make sense” given the sound, and which don’t.  \n\n3. More discriminative multi-scale embeddings  \n   – Jointly processing positives and negatives teaches the network to carve out a sharper decision boundary in feature space.  \n   – The learned representations thus become more robust to background clutter and non-sound-bearing regions.  \n\n4. Better localization & segmentation without masks  \n   – All of the above directly translates into crisper, more accurate sound‐source maps, despite only having video-level labels.  \n\nIn short, supplying MMT with both the matching (positive) and non­matching (negative) multi-scale instances forces it to build highly discriminative, context-rich features at every scale, which is exactly what you need for precise, weakly-supervised audio–visual localization."
    },
    {
        "question": "What is the motivation for fusing CLIP-ViT and Pose-ViT embeddings prior to projection?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Previous works [43, 4] commonly use CLIP visual encoder [52] as the visual branch. However, since CLIP is optimized by global and coarse-grained supervision signals from image captions, it struggles to capture pose-relevant details.",
            "Differently, the pose estimation task demands precise localization of human keypoints, which encourages the visual encoder to capture fine-grained pose features.",
            "Then we concatenate the embedding output by these two encoders along the channel dimension, and apply a trainable projector layer (with projection matrix W) to align the dimension of the concatenated visual features to that of text features as F = W [F_CLIP; F_pose]."
        ],
        "final_answer": "By fusing the two embeddings, UniPose combines CLIP-ViT’s strong alignment with the text embedding space (global, coarse supervision) and Pose-ViT’s fine-grained, keypoint-focused features (precise pose details). This ensures the visual input both aligns well with language and preserves detailed pose information before projection.",
        "relevant_elements": [
            "CLIP-ViT",
            "Pose-ViT",
            "Projection Layer"
        ],
        "id": 580,
        "masked_question": "What is the motivation for fusing [mask1] and Pose-ViT embeddings prior to projection?",
        "masked_number": 1,
        "masked_elements": [
            "CLIP-ViT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16781v1_figure_2.png",
        "paperid": "2411.16781v1",
        "paper_path": "./papers/2411.16781v1.json",
        "figure_id": "2411.16781v1_figure_2.png",
        "caption": "Figure 2: Method overview: UniPose comprises a Pose Tokenizer, Visual Processor and a pose-aware language LLM. Combining Pose Tokens learned by pose tokenizer, Visual Embeddings from visual processor and Text Tokens from text tokenizer, UniPose enables joint modeling of pose comprehension, generation and editing within a unified visual-language backbone.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. The red-boxed model in the visual processor is a CLIP-ViT, which is pretrained on image–text pairs and thus encodes strong global, semantic signals but only coarse pose cues.  \n2. The Pose-ViT, by contrast, is pretrained on a dedicated human-pose estimation task and so captures fine-grained keypoint and limb information.  \n3. If we only used CLIP-ViT, the LLM would get good overall image semantics but miss the precise joint locations needed for pose tasks; if we only used Pose-ViT, we’d lose the broad, language-aligned context CLIP provides.  \n4. By concatenating (fusing) the CLIP-ViT embeddings with the Pose-ViT embeddings before the projection step, the visual processor hands the projector a rich, hybrid feature that contains both coarse, caption-style semantics and fine, keypoint-level pose details.  \n5. The projector can then learn to map that combined signal into the LLM’s embedding space in a way that supports both natural-language understanding and accurate 3D-pose reasoning.\n\nAnswer:\n\nThe motivation is to leverage the complementary strengths of the two encoders—global, language-aligned semantics from CLIP-ViT and precise joint-level cues from Pose-ViT—so that the resulting fused features carry both coarse image context and fine-grained pose information before being projected into the LLM’s joint embedding space."
    },
    {
        "question": "Why apply bidirectional attention to pose tokens while preserving causal attention for text tokens?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Unlike text tokens, pose tokens encode spatial joint positions without causal dependencies, making unified autoregressive modeling suboptimal.",
            "To address this, we apply causal attention to text tokens and bidirectional attention to pose tokens.",
            "However, pose tokens, which encode spatial positions of human joints, are inherently non-sequential, making traditional autoregressive generation suboptimal.",
            "Specifically, we apply causal attention to text sequence, but apply bidirectional attention within the pose token sequence."
        ],
        "final_answer": "Because pose tokens represent simultaneous, non-sequential spatial joint positions (with no inherent causal order), autoregressive (causal) attention is suboptimal for them. Bidirectional attention allows each pose token to attend to all other pose tokens and capture their spatial relationships, while preserving causal attention on text tokens maintains correct autoregressive text generation.",
        "relevant_elements": [
            "pose tokens",
            "text tokens",
            "Mixed Attention Mask"
        ],
        "id": 581,
        "masked_question": "Why apply bidirectional attention to [mask1] while preserving causal attention for text tokens?",
        "masked_number": 1,
        "masked_elements": [
            "pose tokens"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16781v1_figure_2.png",
        "paperid": "2411.16781v1",
        "paper_path": "./papers/2411.16781v1.json",
        "figure_id": "2411.16781v1_figure_2.png",
        "caption": "Figure 2: Method overview: UniPose comprises a Pose Tokenizer, Visual Processor and a pose-aware language LLM. Combining Pose Tokens learned by pose tokenizer, Visual Embeddings from visual processor and Text Tokens from text tokenizer, UniPose enables joint modeling of pose comprehension, generation and editing within a unified visual-language backbone.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is the discrete “pose token” sequence coming out of the VQ–VAE pose tokenizer.  We give those pose tokens bidirectional attention because they represent a non-sequential set of joint positions – each token encoding one part of the body – and so needs to see all the other pose tokens at once in order to model spatial relationships.  Text, by contrast, is inherently autoregressive (left-to-right) and must preserve causal masking to avoid peeking at future words during generation, so we keep the standard causal attention pattern for text tokens."
    },
    {
        "question": "How does KAN utilize learnable activation functions to optimize Gaussian opacity and scaling predictions?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Like MLPs, KANs employ a fully connected structure, but unlike MLPs, which use fixed activation functions, KANs utilize learnable activation functions on edges, allowing adaptive feature mapping.",
            "In opacity prediction, we applied a Tanh activation function to the KAN output and set a threshold to retain effective neural Gaussian distributions. KAN demonstrates superior stability in opacity prediction, effectively preventing the premature removal of critical Gaussian distributions due to low opacity thresholds.",
            "Through KAN’s precise scaling predictions, the model dynamically adjusts the scale of each Gaussian distribution according to the anchor feature, viewing angle, and position, allowing it to flexibly adapt to the complex geometric structures within the scene and to preserve fine textures and structural details in the reconstruction."
        ],
        "final_answer": "KAN replaces fixed activations with learnable activation functions on each network edge, enabling the model to adaptively map input features. For opacity, the KAN output passes through a Tanh activation and a threshold is applied to retain only effective Gaussians, resulting in more stable opacity estimates. For scaling, the learned activations produce precise scale values that dynamically adjust each Gaussian’s size based on anchor features, view direction, and position, preserving fine structures in the scene.",
        "relevant_elements": [
            "KAN",
            "opacity",
            "scaling"
        ],
        "id": 584,
        "masked_question": "How does [mask1] utilize learnable activation functions to optimize Gaussian opacity and scaling predictions?",
        "masked_number": 1,
        "masked_elements": [
            "KAN"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05731v1_figure_2.png",
        "paperid": "2411.05731v1",
        "paper_path": "./papers/2411.05731v1.json",
        "figure_id": "2411.05731v1_figure_2.png",
        "caption": "Figure 2: Overview of PEP-GS",
        "qtype": "Implementation_Details",
        "response": "The red–boxed module in the figure is in fact the Kolmogorov–Arnold Network (KAN) layer.  In a vanilla MLP you have a fixed non‐linearity (ReLU, Tanh, etc.) on every neuron; in KAN, each “edge” (i.e. each connection between layers) carries its own small, trainable activation function (often drawn as ϕ₁, ϕ₂, ϕ₃ in the sketch).  Concretely, what happens in our pipeline is:\n\n 1. We take the concatenated feature vector (anchor feature + relative view distance + view direction) and feed it into a stack of KAN layers.  \n 2. Inside each KAN layer, instead of doing \n      y = σ(W x + b)  \n    with one fixed σ, we learn a separate ϕ(e)(⋅) on each edge e.  During forward propagation the inputs are linearly mixed, but each of those linear mixers is then sculpted by its own tiny, data-driven activation.  \n 3. After the last KAN layer we fork the result into three “heads”:\n    • Opacity head:  y_op = tanh(KAN_output),  \n      followed by a small threshold to prune out very low–opacity Gaussians.  \n    • Scale head:    y_scale = sigmoid(KAN_output) × s_v,  \n      so that each Gaussian’s scale can be pushed up or down in a smooth, bounded way.  \n    • Rotation head: the output is normalized to form a unit quaternion (or rotation matrix).  \n\nBecause the activations ϕ(e) on the edges are themselves learned, the network can fine-tune exactly how “sharp” or “soft” that Tanh curve is for opacity, or how “steep” the Sigmoid is for scaling, etc.  In practice this lets us\n\n – Prevent legitimate Gaussians from collapsing to zero opacity too early  \n – Precisely adjust each Gaussian’s size to match the local geometry and viewpoint  \n – Ultimately preserve thin structures and fine texture by giving the model extra freedom in those non-linear mappings."
    },
    {
        "question": "How does LEMSA modify Scaled Dot-Product Attention for local geometry-aware color decoding?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Specifically, we first concatenate the anchor feature, the relative viewing distance and direction between the camera and the anchor point into a feature sequence. This sequence undergoes average pooling along the X and Y directions, followed by 1D convolution and custom normalization. The pooled features are then activated by a Sigmoid function to produce adaptive weights. These weights are multiplied element-wise with the original input to enhance local features, allowing for adaptive adjustment based on directional changes.",
            "After local optimization, we further capture global information by projecting the features into query (Q), key (K), and value (V) vectors. Attention weights are computed by taking the dot product between the query vector of the center Gaussian’s color and the key vectors of its neighbors, followed by a Softmax function. The final output is obtained as a weighted sum of the value vectors."
        ],
        "final_answer": "LEMSA augments the standard Scaled Dot-Product Attention with a local geometric feature enhancement stage. It first pools each Gaussian’s concatenated anchor feature and view-dependent inputs (distance and direction) along two orthogonal axes, convolves and normalizes the pooled signals, then applies a sigmoid to create adaptive weights. These weights modulate the original features to emphasize local geometry. Finally, it computes Q, K, V from these enhanced features and performs the usual dot-product attention (Q·K softmaxed, then weighted sum of V) to yield geometry-aware color decoding.",
        "relevant_elements": [
            "LEMSA",
            "Scaled Dot-Product Attention"
        ],
        "id": 586,
        "masked_question": "How does [mask1] modify Scaled Dot-Product Attention for local geometry-aware color decoding?",
        "masked_number": 1,
        "masked_elements": [
            "LEMSA"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05731v1_figure_2.png",
        "paperid": "2411.05731v1",
        "paper_path": "./papers/2411.05731v1.json",
        "figure_id": "2411.05731v1_figure_2.png",
        "caption": "Figure 2: Overview of PEP-GS",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module in Figure 2 is in fact the “Local‐Enhanced Multi‐head Self–Attention” (LEMSA) head, and what it does is nothing mystical – it takes a vanilla multi-head scaled dot‐product attention and bends it so that it only operates over a small patch of Gaussians around each “anchor” and, critically, so that all of the inputs into that attention carry information about the local geometry and viewing direction.  Concretely:\n\n 1. From each anchor Gaussian we gather its feature plus the relative camera direction and distance, and we also pull in the same triplet from its k nearest neighbors.  \n 2. Before we even build Q, K, and V, we run two very cheap “X–avg pool → 1D-Conv → Norm → Sigmoid” pipelines (one across the X axis of our k + 1 stack, one across Y) to produce two gating masks that encode local geometric layout and viewing‐angle cues.  We multiply those masks back into the raw features so that every entry in our little (k + 1) × feature blob is already geometry–aware.  \n 3. We then linearly project that gated, geometry–aware blob into Q, K, and V, split them into multiple heads, and run exactly the usual scaled dot-product attention – except that now the query lives on the center Gaussian and the keys/values live on its neighbors.  \n 4. Finally we re–concatenate the heads, run one more linear + sigmoid, and emit the RGB color for each Gaussian.\n\nIn this way the module is still “just” a multi-head scaled dot-product attention at its core, but by (a) restricting attention to a local neighborhood and (b) pre-gating all of Q, K, and V with learned masks derived from relative viewing geometry, it becomes a fully view‐dependent, locally geometry-aware color decoder."
    },
    {
        "question": "How can Closed-Set AVEL fusion methods evolve to support explicit Open-Vocabulary event categorization?",
        "relevant_section_ids": [
            "1",
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Section 1: “To facilitate the recognition of various event classes, particularly those pertaining to unseen test data, we consider leveraging the zero-shot capability of recent language-based multimodal contrastive models. The language words are easily extendable and are not confined to predefined concepts (or categories for event classification). By applying contrastive learning to large-scale multimodal data pairs, the resulting embeddings can capture discriminative and accurate semantics. We opt to utilize ImageBind [14] because it establishes a joint embedding space across multiple modalities, aligning well with the studied OV-AVEL task.”",
            "Section 3.1: “To achieve open-vocabulary AVEL, we adopt a zero-shot classification paradigm similar to CLIP [38]. We send all candidate event classes (seen and unseen) to the text encoder of ImageBind to obtain the text (event category) features T. Notably, we add a special text ‘other’ that corresponds to the background class. Next, we compute the cosine similarities of audio-text and visual-text features… By scanning each row of these similarity matrices, we predict the category of each audio and visual segment. Finally, we produce an audio-visual event prediction for a segment only if both modalities agree on the same class, otherwise it is labeled as background.”",
            "Section 3.2: “During inference, the texts of both seen and unseen classes are sent to the text encoder for feature extraction. The audio and visual segments are processed by the pretrained encoders (and fine-tuned temporal layers) to extract features, and we again compute audio-text and visual-text similarities. The final event category is chosen as the one with the highest combined similarity, enabling explicit classification into arbitrary (seen or unseen) event categories.”"
        ],
        "final_answer": "Traditional closed-set AVEL fusion methods can be extended to an open-vocabulary setting by incorporating a learnable text encoder that maps arbitrary class names (both seen and unseen) into the same joint embedding space as the audio and visual features. At inference time, all candidate class labels (including novel ones) are encoded into text embeddings. The model then computes cosine similarities between each segment’s audio/visual features and these text embeddings, and determines the final event label by either enforcing modality agreement (training‐free) or by fusing their similarity scores (after fine-tuning temporal layers). This zero-shot paradigm allows the system to assign explicit, open‐vocabulary event categories rather than being restricted to a fixed closed set.",
        "relevant_elements": [
            "Closed-Set",
            "Open-Vocabulary"
        ],
        "id": 588,
        "masked_question": "How can [mask1] AVEL fusion methods evolve to support explicit [mask2] event categorization?",
        "masked_number": 2,
        "masked_elements": [
            "Closed-Set",
            "Open-Vocabulary"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.11278v1_figure_1.png",
        "paperid": "2411.11278v1",
        "paper_path": "./papers/2411.11278v1.json",
        "figure_id": "2411.11278v1_figure_1.png",
        "caption": "Figure 1:  (a) Illustration of the AVEL task, which aims to temporally localize segments containing events that are both audible and visible, and identify their categories. (b) Studies of AVEL in different settings. In contrast to previous closed-set and open-set settings, we explore a more practical open-vocabulary AVEL problem, which needs to infer explicit event categories for both seen and unseen test data (i.e., data containing classes seen and unseen during training). Each color represents a distinct event class.",
        "qtype": "Literature_Background",
        "response": "Here is a concise sketch of how you would take an existing open-set AVEL fusion pipeline (which today can only say “this is unknown”) and turn it into a truly open-vocabulary system that spits out real event names—even for classes the model never saw during training:\n\n1.   Start from your off-the-shelf AVEL fusion method.  At the moment it learns to fuse audio and visual features and to flag “known” versus “unknown” events (via evidential PU learning, thresholding, etc.).\n\n2.   Plug in a **multimodal, text‐aware backbone** (e.g. ImageBind, CLIP, ALIGN).  Use its:\n     •  Audio encoder → segment-level audio vector  \n     •  Image/video encoder → segment-level visual vector  \n     •  Text encoder → embed **all** candidate labels (both seen and unseen) plus a special “other” token  \n\n3.   At inference time, compute **cosine similarities** between each audio segment and every label embedding, and separately between each visual segment and every label.  This immediately gives you two soft probability tables (audio–text and visual–text).\n\n4.   **Fuse** those two tables in a differentiable way (e.g. an element-wise (Hadamard) product) so that you end up with, for each segment, a joint probability distribution over *all* labels—not just “known” versus “unknown.”\n\n5.   Optionally, to smooth out per‐segment jitter and to exploit temporal context, insert a few lightweight Transformer layers on top of the frozen encoders (one stream for audio, one for vision), and fine-tune those (only) on your seen-class AVEL segment annotations.\n\n6.   At test time you now simply take the label (from your superset of seen+unseen classes) with the highest fused score.  In this way you have **graduated** from “open-set AVEL” (detect vs. reject) to **open-vocabulary AVEL** (detect *and* name every event, even brand-new ones).\n\nThat is exactly the recipe the paper follows to evolve a closed- or open-set AVEL fusion into a full open-vocabulary system with explicit category outputs on both seen and unseen events."
    },
    {
        "question": "How can Audio (A) and Visual (V) correspondence techniques improve AV-Event temporal boundary detection strategies?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "First, we utilize the pretrained ImageBind model discussed in Sec. 1 to extract audio and visual features. Specifically, the sampled video frame from each visual segment is sent to the image encoder of ImageBind, yielding the segment-level visual features V; similarly, each audio segment is sent to the audio encoder to extract audio features A.",
            "Next, we compute the cosine similarities of audio-text and visual-text features, denoted as S<sub>a</sub> and S<sub>v</sub>, respectively. By scanning each row of S<sub>a</sub> and S<sub>v</sub>, we can predict the category of each audio and visual segment by identifying the category with the highest cosine similarity value.",
            "The audio-visual events in target segments require that the category of the audio segment and the synchronized visual segment should be identical. Therefore, we can easily determine the final audio-visual event predictions by checking the audio and visual class consistency for each segment: if both modalities share the same event category, that segment contains an audio-visual event of that category; otherwise, it is recognized as background."
        ],
        "final_answer": "By explicitly enforcing cross-modal consistency — i.e. computing audio-text and visual-text similarities for each one-second segment and only labeling it as an AV-Event when both A and V agree on the same category — the model can filter out false positives (where only one modality signals an event) and sharply demarcate where an event starts and ends. In practice this means computing a similarity score for A→text and V→text, predicting a class for each stream, and then marking a segment as an event only if the two modality predictions match. This correspondence check naturally yields precise segment-level decisions and therefore tighter temporal boundary detection.",
        "relevant_elements": [
            "Audio (A)",
            "Visual (V)",
            "AV-Event"
        ],
        "id": 589,
        "masked_question": "How can [mask1] and [mask2] correspondence techniques improve AV-Event temporal boundary detection strategies?",
        "masked_number": 2,
        "masked_elements": [
            "Audio (A)",
            "Visual (V)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.11278v1_figure_1.png",
        "paperid": "2411.11278v1",
        "paper_path": "./papers/2411.11278v1.json",
        "figure_id": "2411.11278v1_figure_1.png",
        "caption": "Figure 1:  (a) Illustration of the AVEL task, which aims to temporally localize segments containing events that are both audible and visible, and identify their categories. (b) Studies of AVEL in different settings. In contrast to previous closed-set and open-set settings, we explore a more practical open-vocabulary AVEL problem, which needs to infer explicit event categories for both seen and unseen test data (i.e., data containing classes seen and unseen during training). Each color represents a distinct event class.",
        "qtype": "Literature_Background",
        "response": "First, let us align the masks to the diagram:\n\n  1.  [mask1] is the entire red‐boxed row, namely the per-segment audio predictions (the “Audio (A)” stream).  \n  2.  [mask2] is the blue‐boxed row, i.e. the per-segment visual predictions (the “Visual (V)” stream).\n\nNow: how can exploiting correspondence between those two streams help us sharpen our estimate of when an audio-visual event actually starts and stops?\n\nChain of thought:\n\n  •  In isolation, the audio stream will tag every 1-second chunk as “dog barking” or “background,” and the visual stream will do the same. Because each stream is noisy, either one on its own will have occasional false alarms or missed detections, and its transition points (say, from barking→background) will tend to be jittery.  \n  •  However, a *true* audio-visual event only really begins when *both* modalities switch from background to the same event class at roughly the same time, and only ends when *both* switch back. In other words the temporal boundaries of an AV event are the instants when the two prediction streams make a consistent, simultaneous change.  \n  •  Therefore, if we compute a segment-by-segment correspondence score (for example, cosine similarity between audio and visual feature projections, or simply “does audio classification = visual classification?”) we get a much sharper indicator of boundary: it spikes positive right at the onset and offset of the jointly detected event and stays low elsewhere.  \n  •  In practice one can take the two score‐traces (audio logits and visual logits), compute their element-wise product or cross‐modal similarity, and then threshold or run a small temporal smoother (e.g. a 1D conv or a tiny Transformer) to get very clean start/stop times.  \n\nAnswer:\n\nBy looking for *synchronous* changes in the audio‐only predictions ([mask1]) and the visual‐only predictions ([mask2])—that is, by measuring when the two streams agree on a switch from background to the same class or back again—we obtain a much crisper signal for when an audio-visual event truly begins and ends. In effect, the audio-visual correspondence score highlights the points in time at which *both* modalities transition in unison, and using that joint signal (rather than either stream alone) yields far more precise temporal boundary estimates."
    },
    {
        "question": "How does open-vocabulary setting methodology utilize seen class knowledge to infer unseen event categories?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "To achieve open-vocabulary AVEL, we adopt a zero-shot classification paradigm similar to CLIP [38]. We send all candidate event classes (seen and unseen) to the text encoder of ImageBind to obtain the text (event category) features T.",
            "By scanning each row of S_{a2t} and S_{v2t}, we can predict the category of each audio and visual segment by identifying the category with the highest cosine similarity value.",
            "Inference. The OV-AVEL task involves handling both seen and unseen data (i.e., data with seen and unseen classes) during the inference phase. As highlighted by the yellow dotted box in Fig. 3②, the texts of both seen and unseen classes are sent to the text encoder for feature extraction. The processing of audio and visual modalities follows the same flow as in training, whereas the audio and visual segments are processed by the pretrained encoders and fine-tuned temporal layers to extract audio and visual features. Then, we can generate the probability of audio-visual events by utilizing audio-text and visual-text feature similarities as described in Eq. 2. The final prediction can be made by selecting the event category with the largest probability."
        ],
        "final_answer": "The open-vocabulary methodology first uses a pretrained multimodal backbone (ImageBind) to learn rich audio–visual representations and fine-tunes temporal layers on the seen classes. At inference time, it casts event recognition as a zero-shot classification: it embeds all candidate class names (both seen and unseen) via the text encoder, computes cosine‐similarities between these text embeddings and the audio/visual segment features, and then assigns each segment to whichever class (seen or unseen) maximizes this similarity (subject to audio–visual consistency).",
        "relevant_elements": [
            "open-vocabulary",
            "seen",
            "unseen"
        ],
        "id": 591,
        "masked_question": "How does [mask1] setting methodology utilize seen class knowledge to infer unseen event categories?",
        "masked_number": 1,
        "masked_elements": [
            "open-vocabulary"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.11278v1_figure_1.png",
        "paperid": "2411.11278v1",
        "paper_path": "./papers/2411.11278v1.json",
        "figure_id": "2411.11278v1_figure_1.png",
        "caption": "Figure 1:  (a) Illustration of the AVEL task, which aims to temporally localize segments containing events that are both audible and visible, and identify their categories. (b) Studies of AVEL in different settings. In contrast to previous closed-set and open-set settings, we explore a more practical open-vocabulary AVEL problem, which needs to infer explicit event categories for both seen and unseen test data (i.e., data containing classes seen and unseen during training). Each color represents a distinct event class.",
        "qtype": "Experimental_Results",
        "response": "First, let’s pin down what “[mask1]” is.  In Fig. 1(b) the authors draw three columns—Closed‐Set, Open‐Set, and Open-Vocabulary.  The red box in your screenshot surrounds the rightmost column under “Open-Vocabulary” (where at test time you ask the model to predict among both seen and unseen classes).  So “[mask1]” = the Open-Vocabulary setting.\n\nHow does the Open-Vocabulary methodology leverage seen‐class knowledge to infer unseen events?  Here’s the step-by-step logic:\n\n1.  Pretraining / Feature Extraction  \n    •  They start from ImageBind, a model jointly trained on audio, video, and text.  \n    •  For each segment in a training video (only seen classes during training) they extract:  \n       –  Audio features aᵢ via the frozen audio encoder.  \n       –  Visual features vᵢ via the frozen image encoder.  \n\n2.  Text Embeddings of Seen Classes  \n    •  During fine-tuning they only feed the *names* of the seen classes (plus a special “other” token) into the text encoder, yielding text embeddings T<sub>seen</sub>.  \n    •  They learn (via a small stack of temporal transformer layers on top of aᵢ and vᵢ) to align those audio and visual features to the text embeddings, by minimizing cross‐entropy on the known seen classes.  \n\n3.  Building a Shared Embedding Space  \n    •  After fine­tuning, the model has learned to project audio/visual segments into the same semantic space as the text embeddings of class names.  \n    •  Crucially, the *geometry* of that space was shaped by *all* seen class names, so it carries rich semantic structure (e.g. “dog bark” sits near “cat meow,” etc.).  \n\n4.  Zero-Shot Inference on Unseen Classes  \n    •  At test time (Open-Vocabulary), they *also* feed the unseen class names into the *same* text encoder, producing embeddings T<sub>unseen</sub>.  \n    •  They concatenate [T<sub>seen</sub> ; T<sub>unseen</sub>] to form the full candidate set of text embeddings.  \n    •  For each test segment they extract (aᵢ,vᵢ) with the frozen/fine-tuned encoders, compute cosine similarities to *all* text embeddings (seen+unseen), and then fuse audio‐text and visual‐text scores.  \n    •  The highest‐scoring label—whether seen or unseen—is chosen.  \n\nBecause the fine-tuning on seen classes structured the joint embedding space, the model can now measure similarity to **any** new class name placed into that space.  In this way, the “Open-Vocabulary” setting reuses the *seen-class alignment* to perform zero-shot classification over unseen event categories."
    },
    {
        "question": "How does integrating the interpretable module in self-interpretable models influence Fidelity AUC?",
        "relevant_section_ids": [
            "2.2.2",
            "2.2.3"
        ],
        "relevant_context": [
            "Although self-interpretable methods are not designed to detect sensitive patterns for a given model, it is still interesting to see whether the models trained by self-interpretable methods are sensitive to their extracted interpretation patterns. Notably, LRI-Bern and LRI-Gaussian achieve relatively high Fidelity AUC scores. As for the remaining models, VGIB overall performs the third best but suffers from high variances on some datasets, ASAP occasionally exhibits high Fidelity AUC scores but generally lags behind, while CIGA appears ill-suited when adapted to the GDL even with significant parameter tuning.",
            "Note that one cannot directly compare post-hoc and self-interpretable methods regarding their capabilities of detecting sensitive patterns, as the models to be interpreted are revised when one applies self-interpretable methods. Nonetheless, we can still see a trend that self-interpretable methods achieving better Interpretation ROC-AUC (the metric for detecting decisive patterns) typically obtain better Fidelity AUC (the metric for detecting sensitive patterns). Moreover, as the achieved Fidelity AUC scores of some self-interpretable methods are generally comparable with those yielded by post-hoc methods, the models trained based on self-interpretable methods are also sensitive to the interpretations these methods output."
        ],
        "final_answer": "By integrating an interpretable module and retraining the backbone from scratch, self-interpretable models produce high Fidelity AUC scores. In particular, models using input-perturbation modules (LRI-Bern and LRI-Gaussian) achieve the highest Fidelity AUC among self-interpretable methods, VGIB is third best (with more variance), ASAP sometimes peaks but usually trails, and CIGA underperforms. Overall, integrating an interpretable module yields models whose sensitive patterns (measured by Fidelity AUC) remain strong and are on par with many post-hoc approaches.",
        "relevant_elements": [
            "Self-Interpretable Model",
            "Interpretable Module",
            "Fidelity AUC"
        ],
        "id": 592,
        "masked_question": "How does integrating the [mask1] in [mask2] influence Fidelity AUC?",
        "masked_number": 2,
        "masked_elements": [
            "Interpretable Module",
            "Self-Interpretable Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.00849v1_figure_1.png",
        "paperid": "2407.00849v1",
        "paper_path": "./papers/2407.00849v1.json",
        "figure_id": "2407.00849v1_figure_1.png",
        "caption": "Figure 1: Overview of GDL model interpretation and its evaluation:\nInterpretation in geometric deep learning (GDL) tasks involves identifying a subset of points Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT from the input point cloud C𝐶Citalic_C. Decisive patterns are a subset of points that inherently dictate the labels of the point cloud, specified by the learning task, and their identification accuracy is measured by the alignment between Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the true decisive patterns (Interpretation ROC-AUC). Sensitive patterns, on the other hand, are the subset of most influential points affecting the model s predictions, as specified by the model itself. The evaluation of the model s sensitivity involves assessing the changes of its predictions when Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is either added to or removed from the input (Fidelity AUC).",
        "qtype": "Experimental_Results",
        "response": "Here’s the answer, broken down step by step:\n\n1. In the figure, the red‐boxed text is “Interpretable Module” and the blue‐boxed region is the “Self-Interpretable Model” branch.  \n2. “Integrating the Interpretable Module into a Self-Interpretable Model” means we train the backbone network from scratch with that module built in, rather than attach an explainer after the fact.  \n3. Empirically (see Table 1 in the text), the self-interpretable versions of the models (especially those using LRI-Bern or LRI-Gaussian modules) obtain very high Fidelity AUC scores—on par with or even exceeding the best post-hoc methods.  \n4. In other words, adding the interpretable module into the model significantly boosts Fidelity AUC, making the network far more sensitive to the true “sensitive patterns” it extracts."
    },
    {
        "question": "What methodological limitations arise when using Fidelity AUC to evaluate Post-hoc Explainer sensitivity?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Post-hoc Explainer",
            "Fidelity AUC"
        ],
        "id": 594,
        "masked_question": "What methodological limitations arise when using [mask1] to evaluate [mask2] sensitivity?",
        "masked_number": 2,
        "masked_elements": [
            "Fidelity AUC",
            "Post-hoc Explainer"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.00849v1_figure_1.png",
        "paperid": "2407.00849v1",
        "paper_path": "./papers/2407.00849v1.json",
        "figure_id": "2407.00849v1_figure_1.png",
        "caption": "Figure 1: Overview of GDL model interpretation and its evaluation:\nInterpretation in geometric deep learning (GDL) tasks involves identifying a subset of points Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT from the input point cloud C𝐶Citalic_C. Decisive patterns are a subset of points that inherently dictate the labels of the point cloud, specified by the learning task, and their identification accuracy is measured by the alignment between Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the true decisive patterns (Interpretation ROC-AUC). Sensitive patterns, on the other hand, are the subset of most influential points affecting the model s predictions, as specified by the model itself. The evaluation of the model s sensitivity involves assessing the changes of its predictions when Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is either added to or removed from the input (Fidelity AUC).",
        "qtype": "Others",
        "response": "Here is a concise summary of the key pitfalls you run into if you try to use the *subset of points* that a post‐hoc explainer spits out (“Interpretation results, e.g. a subset of particles”) as your measure of how *sensitive* that explainer really is:\n\n1. You end up measuring *model idiosyncrasies* rather than explanator quality  \n   • Post-hoc methods are tuned to the *particular* model you trained (its weights, its random seed, the latent nuisances it picked up).  If you simply judge their sensitivity by asking “does it flag the same points each time?” you’re really probing how *that model* behaved under slight perturbations – not how faithful the explainer is to *true* task‐relevant features.  \n\n2. The evaluation is *highly unstable* across random seeds and correlational artifacts  \n   • Two models trained on the *same* data but with different initializations will often have quite different “sensitive subsets.”  Your sensitivity metric then swings wildly from run to run, making it impossible to tell whether poor performance is the explainer’s fault or the base model’s.  \n   • In realistic data you almost always have spurious correlations (“red herring” points) that a black-box model latches onto.  A post-hoc explainer will faithfully reproduce those spurious patterns, so your sensitivity numbers tell you more about dataset quirks than about genuine, scientifically meaningful structures.  \n\n3. You can’t disentangle *explainer error* from *model misalignment*  \n   • Because the explainer is evaluated on the very model it is explaining, any mismatch between the “top-k points” and what you *know* should matter can equally well be blamed on the model having learned the wrong thing.  There is no clean way to tell whether it’s the model that is insensitive to the ground truth or the explainer that is failing to uncover the sensitivity that *is* there.  \n\n4. You lose *geometric and distributional* information  \n   • Collapsing an explanation down to a bare subset of points throws away all of the *spatial* or *structural* context that geometric deep‐learning methods exploit.  Two subsets of the same size can be wildly different in how “meaningfully” they cover the true pattern – but a raw subset‐overlap score can’t see that.  \n\nTaken together, these issues show why simply taking a post-hoc explainer’s selected points as a stand-in for “where the model is sensitive” is fundamentally flawed.  You end up conflating explainer robustness with model instability, overlooking spurious correlations, and throwing away the very geometry that your GDL model was built to capture."
    },
    {
        "question": "How might the choice of Self-Interpretable Module impact Interpretation ROC-AUC fairness across datasets?",
        "relevant_section_ids": [
            "1",
            "3"
        ],
        "relevant_context": [
            "In addition, we also observe that some post-hoc methods may face instability issues, i.e., the same method may demonstrate inconsistent performance across different datasets. The performance of self-interpretable methods can be more stable but method-dependent: some self-interpretable methods can effectively identify both decisive patterns and sensitive patterns, whereas others may fail to discern either.",
            "Decisive patterns are task-specific and independent of the learning models. Self-interpretable methods can produce better and more stable interpretation results when detecting decisive patterns. Among self-interpretable methods, LRI-Gaussian often achieves the best performance."
        ],
        "final_answer": "The choice of self-interpretable module has a strong effect on the fairness of Interpretation ROC-AUC across datasets. Some modules (notably LRI-induced methods like LRI-Gaussian) yield consistently high and stable ROC-AUC scores across all tested datasets, ensuring fair and uniform interpretability performance, whereas other self-interpretable modules exhibit more variable ROC-AUCs and may unfairly favor some datasets over others.",
        "relevant_elements": [
            "Self-Interpretable Module",
            "Interpretation ROC-AUC"
        ],
        "id": 595,
        "masked_question": "How might the choice of [mask1] impact [mask2] fairness across datasets?",
        "masked_number": 2,
        "masked_elements": [
            "Self-Interpretable Module",
            "Interpretation ROC-AUC"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.00849v1_figure_1.png",
        "paperid": "2407.00849v1",
        "paper_path": "./papers/2407.00849v1.json",
        "figure_id": "2407.00849v1_figure_1.png",
        "caption": "Figure 1: Overview of GDL model interpretation and its evaluation:\nInterpretation in geometric deep learning (GDL) tasks involves identifying a subset of points Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT from the input point cloud C𝐶Citalic_C. Decisive patterns are a subset of points that inherently dictate the labels of the point cloud, specified by the learning task, and their identification accuracy is measured by the alignment between Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT and the true decisive patterns (Interpretation ROC-AUC). Sensitive patterns, on the other hand, are the subset of most influential points affecting the model s predictions, as specified by the model itself. The evaluation of the model s sensitivity involves assessing the changes of its predictions when Cssubscript𝐶𝑠C_{s}italic_C start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT is either added to or removed from the input (Fidelity AUC).",
        "qtype": "Others",
        "response": "First, let us unpack what the two masks refer to:\n\n  1. The red‐boxed item at the top is the “Self-Interpretable Model” (i.e. a model with an explicit, trainable interpretable module built into its architecture).  \n  2. The blue‐boxed term down in panel B under “Metric” for sensitive patterns is the “Fidelity AUC,” which measures how faithfully the model’s predictions change when you add or remove the highlighted points.\n\nNow the question becomes:\n\n“How might the choice of Self-Interpretable Model impact Fidelity AUC fairness across datasets?”\n\nChain-of-thought\n\n  • Fidelity AUC is a task-agnostic measure of how much the model’s output actually depends on the subset of points it flags as “important.” High fidelity means that removing (or keeping only) those points causes a big swing in the model’s prediction, as one would expect if the model really uses them.  \n\n  • In our experiments we saw that post-hoc methods suffer wildly different fidelity scores when you move from one scientific dataset to another—some models look extremely “faithful,” others much less so, even though their overall accuracy is the same.  \n\n  • By contrast, self-interpretable models tend to produce much more stable fidelity curves as you change domains—provided you pick a strong interpretable module. For instance, LRI-based modules (LRI-Gaussian, LRI-Spherical, etc.) consistently give both high predictive accuracy and high Fidelity AUC on HEP data and on protein data alike.  \n\n  • On the other hand, if you choose a self-interpretable architecture whose built-in module is too weak (or misaligned with the true decisive features), you will see its fidelity drop or vary just as badly as in the post-hoc case.  \n\nIn short, the fairness (i.e. consistency) of the Fidelity AUC across different datasets hinges critically on which self-interpretable module you plug into your GDL backbone. A well-designed, domain-informed module will yield high, stable fidelity everywhere; a poorly chosen one will not."
    },
    {
        "question": "What ethical concerns emerge from co-distributed sampling in data partitioning for privacy auditing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Data partitioning"
        ],
        "id": 596,
        "masked_question": "What ethical concerns emerge from co-distributed sampling in [mask1] for privacy auditing?",
        "masked_number": 1,
        "masked_elements": [
            "Data partitioning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.07070v2_figure_2.png",
        "paperid": "2411.07070v2",
        "paper_path": "./papers/2411.07070v2.json",
        "figure_id": "2411.07070v2_figure_2.png",
        "caption": "Figure 2: A comprehensive breakdown of the auditing framework Parsing embedded in the model fine-tuning process based on white-box MIAs, including data partitioning, property extraction, property embedding, and membership inference.",
        "qtype": "Others",
        "response": "Here is a step-by-step walkthrough of what the red-boxed module (Step 1: Data Partitioning) is doing, and why “co-distributed sampling” in that step can itself pose new ethical risks:\n\n1.  What the red box is:  \n    •  Before each fine-tuning epoch the auditor carves out two disjoint pools from the fine-tuning corpus Dft:  \n       –  “Members” are a random α-fraction of Dft (these are used to train the MIA).  \n       –  “Non-members” are drawn from D′ft, a separate dataset thought to come from the same domain/distribution as Dft.  \n    •  Both pools are then labeled (member vs. non-member) and fed into the membership-inference classifier.  \n\n2.  Why co-distributed non-member data is attractive:  \n    •  By picking non-members from exactly the same distribution, the MIA can learn finer-grained statistical differences between “in” vs. “out” samples.  \n\n3.  The new ethical concerns this raises:  \n    a.  Consent and purpose‐limitation.  \n       –  Those “non-member” records in D′ft may be personal or sensitive data that were never intended to be used for membership-inference research.  Pulling them in without explicit consent violates basic data-protection principles (you’re using people’s data for a new inference task they didn’t agree to).  \n    b.  Re-identification risk.  \n       –  By collecting a large “reference” pool that is statistically indistinguishable from the private training set, you make it much easier to link together records across datasets and re-identify individuals.  \n    c.  Scope creep and data‐minimization.  \n       –  Auditors are now gathering ever more data just so their non-member set shadows the training distribution.  This flies in the face of the principle that you should collect the *minimal* data necessary for the task.  \n    d.  Fairness and bias amplification.  \n       –  If your co-distributed pool over- or under-represents certain subgroups, the MIA will be more accurate on those groups—deepening disparities and unfairly exposing some people’s records.  \n\nIn short, although co-distributed sampling sharpens the membership-inference attack, it does so by dragging in fresh personal data under the guise of “non-members,” exacerbating consent, re-identification, minimization and fairness concerns."
    },
    {
        "question": "How might weighting parameter lambda skew membership inference outcomes within property embedding?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Separate embedding generators are employed for forward and backward properties, and these are connected via learnable parameters. The representation embedding e for a sample x is defined as:\n\n    e = Φ_align(F, B; …) ⊕ [WF e_property(F) ⊕ WB e_property(B)]⊕ λ\n\nwhere Φ_align denotes the properties alignment process, e_property represents the property embedding generator, WF and WB are the learnable parameters associated with the forward and backward embedding generators, ⊕ signifies the concatenation operation, and λ is the parameter for information weighting, thus deriving the embedding e for sample x."
        ],
        "final_answer": "Within the property‐embedding stage, λ controls how much weight is given to the forward‐property embedding versus the backward‐property embedding. By increasing λ, the model will place relatively more emphasis on one type of property (e.g. forward activations) at the expense of the other (e.g. backward gradients). This shift in the embedding space can make the subsequent membership‐inference classifier rely more heavily on whichever property is upweighted, potentially exaggerating differences for members or non‐members in that dimension and thus skewing true‐positive or false‐positive rates accordingly.",
        "relevant_elements": [
            "Property Embedding",
            "Membership Inference"
        ],
        "id": 597,
        "masked_question": "How might weighting parameter lambda skew [mask1] outcomes within [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Membership Inference",
            "Property Embedding"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.07070v2_figure_2.png",
        "paperid": "2411.07070v2",
        "paper_path": "./papers/2411.07070v2.json",
        "figure_id": "2411.07070v2_figure_2.png",
        "caption": "Figure 2: A comprehensive breakdown of the auditing framework Parsing embedded in the model fine-tuning process based on white-box MIAs, including data partitioning, property extraction, property embedding, and membership inference.",
        "qtype": "Others",
        "response": "First, let’s be clear about our placeholders:\n\n- [mask1] is the red‐boxed block in the figure—i.e. the **membership-inference classifier** and its “yes/no” predictions.  \n- [mask2] is the blue-boxed block—the **property-embedding module** where we combine forward and backward features via the learnable weight λ.\n\nSo the question is essentially:\n\n  “How might varying the weighting parameter λ in the **property embedding** stage skew the **membership inference** outcomes?”\n\nHere’s the chain of thought:\n\n1. In Step 3 (the blue box) we build a single embedding for each sample by linearly combining two sub-embeddings:  \n   – **wₓᶠ**, the forward‐property embedding  \n   – **wₓᵇ**, the backward‐property embedding  \n   Formally:  \n       eₓ = λ·wₓᶠ ⊕ wₓᵇ  \n   where λ is a scalar weight.\n\n2. That embedding eₓ is the **only input** to the membership-inference classifier (the red box). Whichever features dominate eₓ will dominate the classifier’s decision boundary.\n\n3. If λ≫1, you’re upweighting the forward properties so strongly that the backward (gradient) signals become almost invisible in eₓ. The classifier will then learn to spot “member vs. non-member” primarily by looking at forward activations.  \n   – You may gain better recall on examples whose membership leaves a strong forward-pass footprint,  \n   – at the expense of precision on examples whose membership signals are mostly in the backward gradients.\n\n4. Conversely, if λ≪1 (or even λ=0), you’re effectively throwing away the forward features and leaning solely on backward gradients. This will flip the classifier’s bias: it may become very sensitive to gradient norms but blind to any telltale forward‐pass patterns.\n\n5. In the extreme, you can skew the classifier so heavily toward one sub-space (forward or backward) that it entirely misses the complementary signal, leading to:  \n   – **False-positives** (marking non-members as members) if the upweighted features are noisy for nonmembers, or  \n   – **False-negatives** (failing to detect true members) if you’ve down-weighted the feature set where members differ most.\n\nSo in plain English:\n\nBy tuning λ up or down, you slide the embedding in the blue box toward either forward or backward properties—and that in turn shifts the red-box classifier’s decision boundary. Too large a λ and you over‐fit to forward‐pass cues (risking the wrong mix of false positives/negatives); too small a λ and you only see gradient‐based signals (and miss whatever the forward activations carried). In short, λ directly biases which slice of the white-box information the membership‐inference module relies on, and extreme settings will systematically skew its yes/no outputs."
    },
    {
        "question": "What is the rationale for using ratio α in data partitioning for audit sample selection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Data partitioning",
            "ratio α"
        ],
        "id": 599,
        "masked_question": "What is the rationale for using [mask1] in [mask2] for audit sample selection?",
        "masked_number": 2,
        "masked_elements": [
            "ratio α",
            "Data partitioning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.07070v2_figure_2.png",
        "paperid": "2411.07070v2",
        "paper_path": "./papers/2411.07070v2.json",
        "figure_id": "2411.07070v2_figure_2.png",
        "caption": "Figure 2: A comprehensive breakdown of the auditing framework Parsing embedded in the model fine-tuning process based on white-box MIAs, including data partitioning, property extraction, property embedding, and membership inference.",
        "qtype": "Design_Rationale",
        "response": "In Step 1 (“Data partitioning”) we need to carve out a small “audit-set” of known-member versus non-member examples from the fine-tuning data so that our inference model can be trained and evaluated.  By doing a random sample of the full fine-tuning corpus at a fixed fraction α (i.e. “random sampling at ratio α”) we\n\n1. control exactly how many examples become our “member” pool,  \n2. ensure that those examples are an unbiased, representative slice of the true fine-tuning distribution, and  \n3. leave the remaining data (or co-distributed held-out data) to serve as our “non-member” pool without introducing any systematic shift.  \n\nIf we simply hand-picked examples we might introduce selection bias; by sampling at random with a tunable α we guarantee that both the member and non-member sets mirror the original data distribution and that our downstream membership-inference metrics aren’t being driven by an accidental skew in our audit‐set."
    },
    {
        "question": "Why employ separate Variational Graph encoder and Variational Visual encoder rather than a shared encoder?",
        "relevant_section_ids": [
            "3.1.1",
            "3.1.2"
        ],
        "relevant_context": [
            "To fully leverage the graph structures of MMKGs, we devise a variational graph encoder.",
            "To highlight the information of relations, attributes, and images, we devise separate fully connected layers as multi-modal encoders to learn interim representations for each modality."
        ],
        "final_answer": "Because graph structure data and visual image data are fundamentally different modalities, the model uses a dedicated variational graph encoder (based on graph attention networks) to capture structural information and a separate variational visual encoder (based on VGG/ResNet & FC layers) to capture image information, thereby fully leveraging each modality’s unique characteristics rather than forcing them through a single shared encoder.",
        "relevant_elements": [
            "Variational Graph encoder",
            "Variational Visual encoder"
        ],
        "id": 600,
        "masked_question": "Why employ separate [mask1] and Variational Visual encoder rather than a shared encoder?",
        "masked_number": 1,
        "masked_elements": [
            "Variational Graph encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19302v1_figure_2.png",
        "paperid": "2407.19302v1",
        "paper_path": "./papers/2407.19302v1.json",
        "figure_id": "2407.19302v1_figure_2.png",
        "caption": "Figure 2. The framework of the proposed IBMEA for the multi-modal entity alignment task.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key insight: the little red‐boxed “Modal Fusion” block and the orange “Variational Visual Encoder” block play two entirely different roles, so you cannot simply collapse them into one big shared network without losing all of the benefits of the variational–information‐bottleneck machinery.\n\nStep-by-step:\n\n1.  Modality heterogeneity  \n    •  Images, graph‐neighborhoods, bags of relations and bags of attributes all live in wildly different feature spaces and have very different noise characteristics.  \n    •  If you force them all through one single “shared” encoder, you end up with a tangled representation in which it becomes impossible to disentangle what came from vision vs. what came from structure or text.\n\n2.  Variational Visual Encoder’s job  \n    •  It takes raw image features (from VGG/ResNet) and learns a Gaussian latent variable \\(z_v\\sim\\mathcal N(\\mu_v,\\sigma_v^2)\\).  \n    •  That dedicated variational layer is what allows us to (a) model uncertainty in the visual signal and (b) apply a per-modality Information Bottleneck that strips away alignment-irrelevant cues in the vision stream.\n\n3.  Modal Fusion’s job  \n    •  Once each modality has its own refined latent \\(z_g,z_v,z_a,z_r\\), the fusion block learns attention over them and merges them into one “hybrid” vector.  \n    •  This separate fusion step is crucial for weighting modalities according to their alignment-signal strength and for then applying the cross‐modal contrastive regularizer.\n\n4.  Why separation matters  \n    •  Separate encoders → you can impose a bottleneck on each \\(z_m\\) (graph, vision, attribute, relation) and make each one shed its own irrelevant information.  \n    •  Separate fusion → you can then flexibly recombine these purified latents in whatever proportions help alignment most.\n\nA single monolithic encoder would destroy the per-modality bottleneck (you’d have no way to force the vision part to “forget” its misleading details without also hurting the graph part, and vice versa), and you’d lose the ability to learn a clean fusion over already‐regularized, modality‐specific embeddings."
    },
    {
        "question": "What alternative approach could replace Time Contrastive Loss to better capture temporal dependencies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Time Contrastive Loss"
        ],
        "id": 1712,
        "masked_question": "What alternative approach could replace [mask1] to better capture temporal dependencies?",
        "masked_number": 1,
        "masked_elements": [
            "Time Contrastive Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.22325v2_figure_1.png",
        "paperid": "2410.22325v2",
        "paper_path": "./papers/2410.22325v2.json",
        "figure_id": "2410.22325v2_figure_1.png",
        "caption": "Figure 1: Overview. We introduce a robotic representation evaluation metric termed manipulation centricity, which exhibits a strong correlation with downstream policy performance. Accordingly, we design a new pre-training method, MCR, to learn manipulation-centric representation from large-scale robotic datasets. Comprehensive experiments on both simulations and real robot validate the superiority of our proposed representation.",
        "qtype": "Others",
        "response": "In our diagram, the red‐boxed module is the “Time‐Contrastive Loss” (i.e. pulling together embeddings of nearby frames and pushing apart those from far‐apart in time).  If you wanted an alternative that more richly captures temporal structure—especially longer‐range dependencies—you could swap it out for a Contrastive Predictive Coding (CPC) style objective (van den Oord et al. 2018).  In CPC you train an autoregressive model (often a small RNN or masked temporal Transformer) over the frame embeddings to predict future (or masked) latent representations under an InfoNCE loss.  By structuring the prediction task over a sequence rather than just positive/negative triplets, CPC yields representations with much stronger, explicitly modelled temporal dependencies."
    },
    {
        "question": "What motivates integrating time contrastive loss alongside dynamics alignment in pre-training objectives?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Temporal contrast. We also wish the representation to encode temporal information, which has shown importance for manipulation tasks (Zhao et al., 2023 ###reference_b50###). To this end, we adopt the time‐contrastive learning objective from Nair et al. (2022 ###reference_b31###), which encourages temporally close frames in a video to be closer in the embedding space than those that are temporally distant or from different videos."
        ],
        "final_answer": "They add a time‐contrastive loss to ensure the learned representation encodes temporal information—by drawing embeddings of nearby frames closer and pushing apart those of distant frames—which has been shown to be important for manipulation tasks.",
        "relevant_elements": [
            "Time Contrastive Loss",
            "Dynamics Alignment"
        ],
        "id": 1713,
        "masked_question": "What motivates integrating [mask1] alongside dynamics alignment in pre-training objectives?",
        "masked_number": 1,
        "masked_elements": [
            "Time Contrastive Loss"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.22325v2_figure_1.png",
        "paperid": "2410.22325v2",
        "paper_path": "./papers/2410.22325v2.json",
        "figure_id": "2410.22325v2_figure_1.png",
        "caption": "Figure 1: Overview. We introduce a robotic representation evaluation metric termed manipulation centricity, which exhibits a strong correlation with downstream policy performance. Accordingly, we design a new pre-training method, MCR, to learn manipulation-centric representation from large-scale robotic datasets. Comprehensive experiments on both simulations and real robot validate the superiority of our proposed representation.",
        "qtype": "Design_Rationale",
        "response": "Here the red‐boxed term is the “Time-Contrastive Loss” (sometimes called Temporal Contrastive Learning). We add it alongside dynamics alignment because, while contrastively aligning each frame to its proprioceptive state teaches the encoder what *is* happening at a single instant, it doesn’t tell it *how* things evolve over time. Manipulation is inherently a temporal process—neighboring frames carry very similar information (you’re barely moved the gripper), while far-apart frames often look quite different. By forcing embeddings of nearby-in-time frames to be close, and distancing those that are temporally distant (or from other videos), the Time-Contrastive Loss injects that temporal continuity into the representation. In practice this means the network not only knows “what state I’m in now” (dynamics alignment) but also “how I got here and where I’m going” (temporal structure), which substantially boosts downstream manipulation performance."
    },
    {
        "question": "Why include actor loss predicting robot actions within the MCR pre-training pipeline?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "We also integrate a behavior cloning (BC)-like actor into our pre-training framework, based on the idea that robotic representations should be predictive of expert-level behaviors in the dataset.",
            "The actor is implemented as a shallow MLP head that maps the image feature vector F to the predicted robot actions a. We use mean squared error as the objective for action prediction."
        ],
        "final_answer": "The actor loss is included so that the visual representation learns to predict the expert robot actions. By training a behavior‐cloning–style head on the learned features, the model is encouraged to encode the task‐relevant dynamics needed to reproduce expert behavior, thereby making the representation more manipulation‐centric and useful for downstream control.",
        "relevant_elements": [
            "Actor Loss",
            "MCR"
        ],
        "id": 1714,
        "masked_question": "Why include [mask1] predicting robot actions within the [mask2] pre-training pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "Actor Loss",
            "MCR"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.22325v2_figure_1.png",
        "paperid": "2410.22325v2",
        "paper_path": "./papers/2410.22325v2.json",
        "figure_id": "2410.22325v2_figure_1.png",
        "caption": "Figure 1: Overview. We introduce a robotic representation evaluation metric termed manipulation centricity, which exhibits a strong correlation with downstream policy performance. Accordingly, we design a new pre-training method, MCR, to learn manipulation-centric representation from large-scale robotic datasets. Comprehensive experiments on both simulations and real robot validate the superiority of our proposed representation.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning in steps:\n\n1.  What is in the red‐boxed module ([mask1])?  \n    – That red box is our “actor loss” or action‐prediction head.  Concretely it is a small MLP that takes an image feature and tries to predict the robot’s next 7-DoF action.  \n\n2.  Where does it sit in the blue‐boxed pipeline ([mask2])?  \n    – The blue outline marks the entire pre‐training stage, which ingests large collections of robot trajectories (RGB frames plus proprioception and actions) and learns a visual embedding.  \n\n3.  Why train the network to predict actions during that pre-training?  \n    – Because we have ground-truth robot actions in our dataset, we can force the visual encoder to extract exactly those aspects of the scene that are needed to reproduce the demonstrated behavior.  \n    – In other words, by adding a behavior-cloning-style loss we “tie” the learned features directly to the robot’s control outputs.  \n    – This biases the representation toward manipulation-relevant cues (object poses, gripper orientation, scene affordances) rather than arbitrary visual details.  \n\n4.  What is the outcome?  \n    – The joint contrastive + action-prediction pre-training yields a representation that is more “manipulation-centric,” which empirically correlates very strongly with downstream policy success.  \n\nSo in a nutshell:  \nIncluding the action‐prediction (actor) loss during the pre-training stage leverages the available robot action labels to anchor the visual features in real manipulation behavior.  That makes the representation far more useful for closed-loop control than if we had only used, say, image–state contrastive or semantic losses alone."
    },
    {
        "question": "What motivates sequential dropout and layer normalization in time difference processing pipeline?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Subsequently, we apply dropout (srivastava2014dropout) and layer normalization (ba2016layer) to sequence Δ in order to eliminate data noise and optimize the data distribution for subsequent analysis."
        ],
        "final_answer": "The dropout and layer normalization are motivated by the need to eliminate noise in the time‐difference data and to normalize/optimize its distribution before it is fed into later model components.",
        "relevant_elements": [
            "Dropout",
            "Layer Normalization"
        ],
        "id": 1715,
        "masked_question": "What motivates sequential [mask1] and [mask2] in time difference processing pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "Dropout",
            "Layer Normalization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.16182v2_figure_2.png",
        "paperid": "2409.16182v2",
        "paper_path": "./papers/2409.16182v2.json",
        "figure_id": "2409.16182v2_figure_2.png",
        "caption": "Figure 2. The overview of TiM4Rec. The overall framework of TiM4Rec, which is based on time-aware enhancement,\nenhances the performance of the SSD architecture in the domain of sequential recommendation by effectively processing the time differences in interactions between items.",
        "qtype": "Design_Rationale",
        "response": "In §3.2 the authors say that as soon as we turn raw timestamps into a time-difference sequence Tₙ, we immediately apply (1) dropout and then (2) layer-normalization.  The sole purpose of this back-to-back “dropout → layer‐norm” is to strip out spurious noise from the Tₙ signal and to re-scale/center its values so that the downstream SSD kernels see a clean, well‐conditioned input."
    },
    {
        "question": "What is the reasoning for gating SSD kernel output via gate residual mechanism?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "It is noteworthy that to adapt the time difference vector Δ to the feature semantic space of the next layer, gate residual (he2016deep) processing is applied to the input time difference vector Δ for the subsequent layer."
        ],
        "final_answer": "The gate‐residual mechanism is applied so that the time‐difference information (the SSD kernel’s output with time‐aware enhancements) is properly aligned and adapted into the feature semantic space required by the next layer.",
        "relevant_elements": [
            "Gate Residual",
            "SSD Kernel"
        ],
        "id": 1716,
        "masked_question": "What is the reasoning for gating [mask1] output via gate residual mechanism?",
        "masked_number": 1,
        "masked_elements": [
            "SSD Kernel"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.16182v2_figure_2.png",
        "paperid": "2409.16182v2",
        "paper_path": "./papers/2409.16182v2.json",
        "figure_id": "2409.16182v2_figure_2.png",
        "caption": "Figure 2. The overview of TiM4Rec. The overall framework of TiM4Rec, which is based on time-aware enhancement,\nenhances the performance of the SSD architecture in the domain of sequential recommendation by effectively processing the time differences in interactions between items.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in a nutshell:\n\n1.  In the Time-aware SSD block the “SSD Kernel” (the red box) spits out two streams of information—(a) the usual item–item interaction features and (b) the newly injected, time-difference features.  \n2.  Before you stack another SSD layer on top, you have to bring those time-difference vectors into the same “semantic space” as the rest of your features, and you also want to let the network decide how much of that timing signal is actually useful at each layer.  \n3.  The standard way to do that in deep nets these days is a small gated (sigmoid) residual: you project the raw time-diff vector, run it through a sigmoid gate, and then add it back into the main stream.  \n4.  By “gating” the SSD-kernel’s time-diff output you  \n   •  Align it dimensionally with the item-feature stream,  \n   •  Give the model a learnable on/off knob for how much timing information to carry forward, and  \n   •  Stabilize training by turning what would be an unchecked skip-connection into a controlled (residual) one.  \n\nIn short, the gate-residual lets TiM4Rec adaptively filter and fuse the time-difference signal at each layer, rather than forcing the full unregulated time-bias through every block."
    },
    {
        "question": "How does Time Difference Processing apply dropout and layer normalization to maintain temporal difference signals?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As mentioned above, for the input interaction timestamp sequence t, we transform it into an interaction time difference sequence Δt using the following method:",
            "Subsequently, we apply dropout (srivastava2014dropout) and layer normalization (ba2016layer) to sequence Δt in order to eliminate data noise and optimize the data distribution for subsequent analysis."
        ],
        "final_answer": "After computing the time-difference sequence Δt from the raw timestamps, Time Difference Processing applies dropout to randomly zero out elements (thereby reducing noise) and then uses layer normalization to stabilize and optimize the distribution of Δt before it enters the subsequent model components.",
        "relevant_elements": [
            "Time Difference Processing",
            "Dropout",
            "Layer Normalization"
        ],
        "id": 1717,
        "masked_question": "How does [mask1] apply dropout and layer normalization to maintain temporal difference signals?",
        "masked_number": 1,
        "masked_elements": [
            "Time Difference Processing"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.16182v2_figure_2.png",
        "paperid": "2409.16182v2",
        "paper_path": "./papers/2409.16182v2.json",
        "figure_id": "2409.16182v2_figure_2.png",
        "caption": "Figure 2. The overview of TiM4Rec. The overall framework of TiM4Rec, which is based on time-aware enhancement,\nenhances the performance of the SSD architecture in the domain of sequential recommendation by effectively processing the time differences in interactions between items.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module on the far left of Figure 2 is nothing more exotic than the “Time Difference Processing” block described in §3.2.  Its job is:\n\n  1.  Take your raw UNIX-timestamps t₁…tₗ, form the sequence of differences  \n        d₁ = 0, d₂ = t₂–t₁, …, dₗ = tₗ – tₗ₋₁  \n  2.  Embed that vector of scalars into a D-dimensional time-difference embedding.  \n  3.  Apply dropout to the embedding (srivastava2014dropout) – randomly zeroing some of the time-difference features to knock out spurious noise or overfit patterns in Δ-values.  \n  4.  Immediately follow with layer normalization (ba2016layer) – re-centring and re-scaling the remaining time-difference features so that their distribution is well behaved before you feed them into the SSD kernel.  \n\nBy interposing dropout then layer-norm on the time-difference embeddings, TiM4Rec both regularizes away erratic gaps and keeps the relative magnitudes of the true Δ’s intact for the downstream time-aware SSD block."
    },
    {
        "question": "How does Mask NMS resolve overlapping masks using stability_score and area overlap?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Unlike traditional NMS, which relies on bounding box IoU, our approach calculates the overlap area between each pair of masks. If the overlap ratio, based on the smaller mask, surpasses a confidence threshold, we retain the mask with the higher score.",
            "Mi represents the area of the mask. C is the predefined confidence threshold, which was set at 0.9 in our experiment. Si is the stability_score of the ith mask output by SAM2."
        ],
        "final_answer": "Mask NMS computes the overlap area between every pair of predicted masks, normalizes that overlap by the area of the smaller mask, and if this overlap ratio exceeds a threshold C (0.9), it discards the mask with the lower stability_score (Si), keeping only the mask with the higher stability_score.",
        "relevant_elements": [
            "Mask NMS"
        ],
        "id": 1719,
        "masked_question": "How does [mask1] resolve overlapping masks using stability_score and area overlap?",
        "masked_number": 1,
        "masked_elements": [
            "Mask NMS"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.16196v1_figure_2.png",
        "paperid": "2411.16196v1",
        "paper_path": "./papers/2411.16196v1.json",
        "figure_id": "2411.16196v1_figure_2.png",
        "caption": "Figure 2: Overall framework of SDM-D. (a) Segmentation: is based on SAM2, utilizing the sam2_hiera_large weight with a 32 × 32 grid of points and no additional prompts. (b) Mask NMS: was proposed to reserve the optimal mask. (c) Description and Image Segments Encoding: OpenCLIP is used to encode the description and image segments. (d) Region-Text Matching: this is used to image regions with corresponding textual descriptions. (e) Distilling: involves transferring knowledge to smaller models that are faster and perform better. The SDM consists of sections (a), (b), (c), and (d) with pink headings, and SDM-D is a combination of SDM and section (e).",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:\n\n1.  The red-boxed module in the figure is the “Mask NMS” step (b), a post-processing stage applied after SAM2 produces its top-3 masks at each grid point.  \n2.  SAM2’s ambiguity-aware decoder can propose several highly overlapping masks for the same object (e.g. one mask covering the whole strawberry, another just the ripe tip, etc.). We need to collapse these into a single, best-quality mask per fruit.  \n3.  Traditional NMS works on bounding boxes with IoU; here we work directly on mask pixels. For every pair of masks Mi and Mj we compute:  \n     overlap_ratio = area(Mi ∩ Mj) / min(area(Mi), area(Mj)).  \n4.  We compare overlap_ratio to a high threshold C (set to 0.9). If overlap_ratio > C, we treat the two masks as “duplicates.”  \n5.  Each mask comes annotated with a scalar stability_score Si from SAM2, which quantifies how confidently SAM2 produced that mask.  \n6.  Between any two overlapping masks (overlap_ratio > C) we simply discard the one with lower stability_score and keep the one with higher stability_score.  \n7.  By sweeping through all mask pairs in descending order of stability_score, we end up with a non-redundant set of masks—one high-quality mask per fruit.  \n\nAnswer:  \nMask NMS computes the pixel-wise overlap of each mask pair, normalizes it by the area of the smaller mask, and whenever that ratio exceeds a confidence threshold (0.9), it keeps only the mask whose stability_score is higher, discarding the other."
    },
    {
        "question": "How does the Distilling module handle noisy pseudo-labels to improve student generalization?",
        "relevant_section_ids": [
            "3"
        ],
        "relevant_context": [
            "Unlike traditional distillation, which typically operates at the feature or logit level using manually labeled data, our approach performs distillation at the label level via pseudo labels, significantly reducing labor costs and alleviating data scarcity.",
            "Interestingly, the inherent noise in pseudo-labels encourages students to acquire broader knowledge, enhancing their learning."
        ],
        "final_answer": "The Distilling module deliberately performs label-level distillation using the noisy pseudo-labels produced by SDM. Rather than suppressing this noise, it leverages it as a form of regularization: the imperfections in the pseudo-labels push the student to learn more diverse, robust representations, thereby improving its generalization.",
        "relevant_elements": [
            "Distilling"
        ],
        "id": 1720,
        "masked_question": "How does the [mask1] module handle noisy pseudo-labels to improve student generalization?",
        "masked_number": 1,
        "masked_elements": [
            "Distilling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.16196v1_figure_2.png",
        "paperid": "2411.16196v1",
        "paper_path": "./papers/2411.16196v1.json",
        "figure_id": "2411.16196v1_figure_2.png",
        "caption": "Figure 2: Overall framework of SDM-D. (a) Segmentation: is based on SAM2, utilizing the sam2_hiera_large weight with a 32 × 32 grid of points and no additional prompts. (b) Mask NMS: was proposed to reserve the optimal mask. (c) Description and Image Segments Encoding: OpenCLIP is used to encode the description and image segments. (d) Region-Text Matching: this is used to image regions with corresponding textual descriptions. (e) Distilling: involves transferring knowledge to smaller models that are faster and perform better. The SDM consists of sections (a), (b), (c), and (d) with pink headings, and SDM-D is a combination of SDM and section (e).",
        "qtype": "Implementation_Details",
        "response": "The little red-boxed “mystery” block is nothing exotic – it’s just the distillation head (section e in the figure).  In our pipeline we don’t try to “clean up” the SAM + CLIP pseudo‐labels before we hand them to a small student, we simply let the student train directly on them.  Critically, the fact that these labels are noisy turns out to be a feature, not a bug:  \n\n 1.  By distilling at the label level (rather than matching feature maps or logits) we bypass the need for any human ground-truth.  \n 2.  The unavoidable mistakes in the pseudo-labels act as a kind of built-in regularizer or noise injection.  This prevents the student from over-specializing on any one pattern and forces it to learn a more broadly applicable representation of fruits (or whatever the downstream task is).  \n 3.  In practice this noise-driven regularization leads the small model to generalize better both on seen examples (it resists overfitting teacher quirks) and on novel ones (it learns to be robust to label uncertainty).  \n\nSo in short, the “Distilling” module simply feeds the student noisy pseudo-labels and relies on that noise to broaden the student’s learned concept of what a correct segmentation (or detection) looks like, yielding stronger generalization."
    },
    {
        "question": "How does Distilling leverage pseudo labels compared to traditional teacher-student knowledge distillation?",
        "relevant_section_ids": [
            "2.3",
            "3"
        ],
        "relevant_context": [
            "In knowledge distillation, a \"teacher\" model transfers its knowledge to a smaller \"student\" model, enabling the student to achieve comparable performance while being more resource-efficient (Hinton et al., 2015 ###reference_b17###). In a typical knowledge distillation process, the student model [is trained] to mimic the output probabilities (or logits) of the teacher model, and a loss function is used to measure the gap between the student’s and teacher’s predictions.",
            "To facilitate efficient deployment on edge devices, we implement distillation. We let small, edge-deployable models (students) learn from the pseudo labels generated by SDM, bypassing the need for costly manual annotation. Unlike traditional distillation, which typically operates at the feature or logit level using manually labeled data, our approach performs distillation at the label level via pseudo labels, significantly reducing labor costs and alleviating data scarcity."
        ],
        "final_answer": "Instead of having the student mimic the teacher’s logits or intermediate features on manually labeled data, our Distilling stage uses the automatically generated pseudo labels as ground truth. In other words, we perform the teacher-student transfer at the label level with pseudo masks, eliminating the need for manual annotations and making the student training both cheaper and more robust in data-scarce settings.",
        "relevant_elements": [
            "Distilling"
        ],
        "id": 1722,
        "masked_question": "How does [mask1] leverage pseudo labels compared to traditional teacher-student knowledge distillation?",
        "masked_number": 1,
        "masked_elements": [
            "Distilling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.16196v1_figure_2.png",
        "paperid": "2411.16196v1",
        "paper_path": "./papers/2411.16196v1.json",
        "figure_id": "2411.16196v1_figure_2.png",
        "caption": "Figure 2: Overall framework of SDM-D. (a) Segmentation: is based on SAM2, utilizing the sam2_hiera_large weight with a 32 × 32 grid of points and no additional prompts. (b) Mask NMS: was proposed to reserve the optimal mask. (c) Description and Image Segments Encoding: OpenCLIP is used to encode the description and image segments. (d) Region-Text Matching: this is used to image regions with corresponding textual descriptions. (e) Distilling: involves transferring knowledge to smaller models that are faster and perform better. The SDM consists of sections (a), (b), (c), and (d) with pink headings, and SDM-D is a combination of SDM and section (e).",
        "qtype": "Literature_Background",
        "response": "Mask-level distillation in SDM-D (i.e. the red-boxed “(e) Distilling” module) is fundamentally different from textbook teacher–student distillation because it never tries to match the teacher’s raw logits or hidden‐layer activations.  Instead, it takes the *pseudo-labels* (the mask + text category pairs) produced by the full SDM pipeline as if they were ground truth and simply trains a small “student” network to predict those labels.  In other words:\n\n 1. Traditional distillation  \n    – Uses a large, fixed teacher.  \n    – Student is driven to reproduce the *soft* outputs (logits or feature maps) of the teacher on already-labeled data.  \n    – Requires careful balancing of classification loss vs. “imitate-the-teacher” loss.  \n\n 2. SDM-D’s pseudo-label distillation  \n    – Runs SDM’s segment-then-prompt steps on *unlabeled* images to generate mask + text labels.  \n    – Treats those labels as “ground truth” and trains any small student model directly on them.  \n    – Works purely at the *label* level, needs no manual annotation, and can plug in any compact architecture.  \n    – The inevitable noise in the pseudo-labels actually serves as a regularizer, encouraging the student to learn a broader set of patterns.\n\nBy distilling *at the label level* from automatically generated pseudo-masks, SDM-D sidesteps the heavyweight matching of teacher logits or features, cuts out human annotation altogether, and still delivers high accuracy in a tiny, edge-deployable model."
    },
    {
        "question": "What is the role of 2D FFT operations in extending VPT beyond spatial-only prompt tuning?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Existing prompt tuning jia2022visual; han20232vpt, focusing predominantly on spatial information, can only harness the shared information embedded within the pretrained backbone, limiting their capacity to adapt effectively to novel tasks.",
            "Compared to VPT (see Fig. 1 (a)), our model (see Fig. 1 (c)) transforms partial prompts from spatial domain to frequency domain via 2D FFT (see §3.1) to consider both the spatial and frequency domain information."
        ],
        "final_answer": "The 2D FFT operations convert a subset of the learnable visual prompts from the spatial domain into the frequency domain, thereby enabling prompt tuning to incorporate both spatial and frequency information rather than relying solely on spatial cues.",
        "relevant_elements": [
            "2D FFT operations",
            "Visual Prompt Tuning"
        ],
        "id": 1723,
        "masked_question": "What is the role of [mask1] in extending VPT beyond spatial-only prompt tuning?",
        "masked_number": 1,
        "masked_elements": [
            "2D FFT operations"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01327v2_figure_1.png",
        "paperid": "2411.01327v2",
        "paper_path": "./papers/2411.01327v2.json",
        "figure_id": "2411.01327v2_figure_1.png",
        "caption": "Figure 1: Overview of VPT v⁢s.𝑣𝑠vs.italic_v italic_s . VFPT (ours) frameworks. (a) Original Visual Prompt Tuning. (b) 2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence length dimensions. (c) The overall architecture of our proposed VFPT (see §3.2).",
        "qtype": "Literature_Background",
        "response": "Here, “[mask1]” is simply the small “FFT‐block” shown in the red box – the module that takes a fraction of your learned prompt embeddings and runs a 2D FFT over them (once along the token/sequence axis and once along the hidden‐feature axis).  By slotting that tiny Fourier‐transform block into each transformer layer’s prompts, VFPT is able to inject frequency‐domain (global) information on top of the usual spatial (local) prompt tokens.  In that way it extends vanilla VPT from “spatial only” prompt tuning to a hybrid spatial + frequency prompt tuning."
    },
    {
        "question": "How do Visual Fourier Prompts leverage frequency-domain analysis compared to visual prompts?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "By integrating frequency domain information into learnable prompt embeddings, our approach elegantly assimilates data from both spatial and frequency domains, simulating the human visual cognition.",
            "Compared to VPT (see Fig. 1(a)), our model (see Fig. 1(c)) transforms partial prompts from spatial domain to frequency domain via 2D FFT (see §3.1) to consider both the spatial and frequency domain information."
        ],
        "final_answer": "Visual Fourier Prompts apply a 2D Fast Fourier Transform to partial prompt embeddings—converting them from the spatial domain into the frequency domain—and concatenate these frequency-domain embeddings with the original spatial prompts. This lets VFPT capture and integrate both spatial and frequency-domain information, whereas standard visual prompts only operate in the spatial domain.",
        "relevant_elements": [
            "Visual Fourier Prompts",
            "Visual Prompts"
        ],
        "id": 1724,
        "masked_question": "How do [mask1] leverage frequency-domain analysis compared to visual prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Visual Fourier Prompts"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01327v2_figure_1.png",
        "paperid": "2411.01327v2",
        "paper_path": "./papers/2411.01327v2.json",
        "figure_id": "2411.01327v2_figure_1.png",
        "caption": "Figure 1: Overview of VPT v⁢s.𝑣𝑠vs.italic_v italic_s . VFPT (ours) frameworks. (a) Original Visual Prompt Tuning. (b) 2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence length dimensions. (c) The overall architecture of our proposed VFPT (see §3.2).",
        "qtype": "Literature_Background",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  Image–text alignment  \n    - In Fig. 1(c), the little red‐outlined box at the top of the FFT sub‐panel is labelled “Visual Fourier Prompts.”  \n    - Below it sits the FFT module (“Sequence‐wise FFT” and “Hidden‐wise FFT”), and at the bottom is the regular “Visual prompts” block.  \n    - The text in §3.2 explains that a fraction of the learnable prompts is routed through a 2D FFT (over both sequence‐length and hidden‐dimension) to form what they call Visual Fourier Prompts.\n\n2.  What mask₁ is referring to  \n    - [mask1] corresponds exactly to those “Visual Fourier Prompts” highlighted in the red box.\n\n3.  How they leverage the frequency domain versus ordinary visual prompts  \n    - Ordinary visual prompts:  \n      • Live purely in the spatial (patch‐embedding) domain.  \n      • Are concatenated directly onto the frozen backbone’s patch embeddings without any spectral processing.  \n    - Visual Fourier Prompts:  \n      • Take a subset of the prompt vectors and run them through a 2D Fast Fourier Transform (across both token sequence length and feature dimension).  \n      • Inject only the real part of that FFT output back as prompt vectors, thereby encoding explicit frequency‐domain information (e.g. global texture, luminance/noise decomposition).  \n      • By doing so, they complement the spatial cues of standard prompts with spectral‐frequency cues, expanding the prompt search space and improving robustness when finetuning on distributions that differ from pretraining.\n\nAnswer  \n[mask1] (“Visual Fourier Prompts”) differ from vanilla visual prompts in that they are obtained by 2D FFT’ing part of the prompt embeddings—thereby explicitly inserting frequency‐domain (spectral) features—whereas standard visual prompts remain entirely in the spatial patch‐embedding domain."
    },
    {
        "question": "How does combining sequence-wise FFT and hidden-wise FFT on visual prompts enhance feature representation?",
        "relevant_section_ids": [
            "4.5"
        ],
        "relevant_context": [
            "Fourier Prompt Dimension. A fundamental distinction between VFPT and other methods is the incorporation of FFT into visual prompts. In our standard implementation, we utilize 2D FFTs across both sequence length and hidden dimensions. Here, we explore the impact of each dimension’s transformation individually. As shown in Table 5 (a), the separate Fourier transformations along each dimension appear to have similar contributions (i.e., 80.88% → 80.74% in Natural). However, the combined application of transformations across both dimensions (i.e., 2D FFTs) demonstrates a synergistic effect, yielding significant improvement in performance."
        ],
        "final_answer": "By applying FFT along both the sequence and hidden dimensions simultaneously (i.e. using a 2D FFT) the prompts capture complementary frequency‐domain information from patch relationships and channel features. This joint transformation produces a synergistic effect, substantially boosting downstream task performance compared to using either sequence‐wise or hidden‐wise FFT alone.",
        "relevant_elements": [
            "Sequence-wise FFT",
            "Hidden-wise FFT",
            "Visual Prompts"
        ],
        "id": 1725,
        "masked_question": "How does combining [mask1] and hidden-wise FFT on visual prompts enhance feature representation?",
        "masked_number": 1,
        "masked_elements": [
            "Sequence-wise FFT"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01327v2_figure_1.png",
        "paperid": "2411.01327v2",
        "paper_path": "./papers/2411.01327v2.json",
        "figure_id": "2411.01327v2_figure_1.png",
        "caption": "Figure 1: Overview of VPT v⁢s.𝑣𝑠vs.italic_v italic_s . VFPT (ours) frameworks. (a) Original Visual Prompt Tuning. (b) 2D Fast Fourier Transform operations in partial visual prompts along hidden and sequence length dimensions. (c) The overall architecture of our proposed VFPT (see §3.2).",
        "qtype": "Experimental_Results",
        "response": "First, let’s be explicit about what “[mask1]” is in that red‐boxed snippet: it’s simply the “sequence‐wise FFT” operation performed on a subset of the visual prompts.  In the paper’s architecture (Fig. 1c), each layer’s visual prompts are split in two: one half goes through a 1D FFT over the token (sequence) dimension; the other half goes through a 1D FFT over the hidden (feature) dimension.  The two frequency‐domain views are then concatenated back together and fed into the Transformer.\n\nWhy does that help?  Here’s the intuition:\n\n1.  Spatial/global context via sequence-wise FFT  \n    •  By taking the FFT across the sequence (i.e. the patch-token axis), you convert the prompts from a purely local, patch‐by‐patch representation into a set of global frequency components.  \n    •  Low-frequency components capture broad, slowly varying patterns across the image, while high-frequency components capture fine details and sharp transitions.  \n    •  This “global” view helps the prompt attend not just to its own little local region but to patterns spanning many patches.\n\n2.  Channel/feature correlations via hidden-wise FFT  \n    •  Independently taking the FFT across the hidden (channel) dimension turns each prompt vector into a frequency spectrum over its channels.  \n    •  That lets the prompt learn cross-channel dependencies in the frequency domain, uncovering which spectral band combinations carry the most discriminative power.\n\n3.  Their combination yields a 2D frequency embedding  \n    •  By concatenating sequence-wise and hidden-wise FFT outputs, the model sees a 2D frequency representation of the prompts (frequency over tokens × frequency over channels).  \n    •  This 2D spectrum carries strictly more information than either transform alone, since it fuses global spatial patterns with inter‐channel correlations.  \n\n4.  Empirical benefits  \n    •  Richer, complementary features: spatial cues (shapes, textures) and channel cues (color‐ or filter-specific patterns) reinforce one another.  \n    •  Improved optimization: the paper shows that this dual‐FFT setup flattens and mildly convexifies the loss landscape, yielding better generalization.  \n    •  Better performance on high‐disparity tasks: tasks whose images differ a lot from ImageNet (“Specialized” or “Structured”) tend to benefit more from the extra frequency information.\n\nIn short, combining sequence-wise FFT ([mask1]) with hidden-wise FFT gives each visual prompt a two‐dimensional frequency representation.  That enriched embedding captures both long‐range spatial structures and cross-channel spectral patterns, which a vanilla spatial prompt alone would miss."
    },
    {
        "question": "What limitations might arise from explicit linear composition applied to Text Encoder outputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Text Encoder",
            "Explicit Linear Composition"
        ],
        "id": 1731,
        "masked_question": "What limitations might arise from [mask1] applied to [mask2] outputs?",
        "masked_number": 2,
        "masked_elements": [
            "Explicit Linear Composition",
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2409.16136v1_figure_1.png",
        "paperid": "2409.16136v1",
        "paper_path": "./papers/2409.16136v1.json",
        "figure_id": "2409.16136v1_figure_1.png",
        "caption": "Figure 1: Difference between (a) open-vocabulary object detection for the fine-grained category names, (b) fine-grained open vocabulary object detection for the attribute-specific descriptions, and (c) our proposed HA-FGOVD method.",
        "qtype": "Others",
        "response": "Let’s first unpack what the two masks refer to:\n\n 1. [mask2] is the blue‐boxed module in Fig. 1 (c), i.e. the frozen Text Encoder (CLIP‐ or BERT‐style transformer) whose outputs are  \n    – a “global” text embedding (the full caption)  \n    – one or more “attribute” embeddings (via masking out non-attribute tokens)  \n 2. [mask1] is the red‐boxed operation in the same panel, namely the Explicit Linear Composition that takes those two sets of encoder vectors and forms  \n      F′ = α·F₍global₎ + β·F₍attr₎ + γ  \n\nWhen you apply that simple, fixed linear fusion back on top of a frozen encoder, you inherit two main limitations:\n\n  1.  No non-linear recombination.  \n      By construction you are only ever taking a weighted sum of two embeddings.  Anything that requires, say, an element-wise product or a more intricate cross-term between “global” and “attribute” features will simply vanish.  Complex interactions – e.g. “reddish copper” vs. “bright red” – cannot be teased apart by a single scalar weight.  \n  2.  Heavy reliance on scalar tuning and encoder quality.  \n      Since the encoder is frozen, all you can do to emphasize attributes is dial the three scalars (α, β, γ).  If those weights aren’t carefully cross-validated for each new model or domain, you can easily over-boost attributes (destroying category information) or under-boost them (failing to recover any fine-grained signal).  In practice this means  \n      – a fair amount of hyperparameter search just to find a usable balance  \n      – embeddings drifting out of the distribution the detector was trained on, leading to degraded detection or misclassification  \n\nIn short, while explicit linear composition is lightweight and plug-and-play, it is inherently limited to first-order (additive) blends of frozen embeddings, can’t model higher-order attribute interactions, and forces you to hand-tune scalars to avoid wrecking the original detector’s heads."
    },
    {
        "question": "What alternative strategies could replace LLM-Guided Attribute Word Extraction to improve efficiency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM-Guided Attribute Word Extraction"
        ],
        "id": 1732,
        "masked_question": "What alternative strategies could replace [mask1] to improve efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "LLM-Guided Attribute Word Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2409.16136v1_figure_1.png",
        "paperid": "2409.16136v1",
        "paper_path": "./papers/2409.16136v1.json",
        "figure_id": "2409.16136v1_figure_1.png",
        "caption": "Figure 1: Difference between (a) open-vocabulary object detection for the fine-grained category names, (b) fine-grained open vocabulary object detection for the attribute-specific descriptions, and (c) our proposed HA-FGOVD method.",
        "qtype": "Others",
        "response": "In Figure 1 (c) the red‐boxed module labeled “[mask1]” is the “LLM-Guided Attribute Word Extraction → Feature Extraction” block, i.e. the step where they call out to a large language model (Llama 2) at run-time to pick off the adjectives, colors, materials, etc., from the input text.  If you wanted to get rid of that heavyweight LLM call and still recover the salient attribute words, you could swap in any of the following, considerably lighter-weight alternatives:\n\n 1.  A purely rule-based NLP pipeline  \n    •  Run a fast part-of-speech tagger (e.g. spaCy or NLTK) on the caption and simply pull out all adjectives, numerals or known attribute‐bearing POS tags.  \n    •  Optionally augment it with a small hand-crafted gazetteer of common color/material terms (e.g. “brown,” “striped,” “wooden,” etc.).  \n\n 2.  Unsupervised key-phrase or keyword extraction  \n    •  Apply algorithms such as RAKE or TextRank to score and pick the n most “keywordy” tokens in the sentence.  \n    •  This tends to naturally favor words like “darker,” “lighter,” “polka-dotted,” etc., without any external supervision.  \n\n 3.  Attention-based saliency from the existing text encoder  \n    •  During the normal forward pass of the frozen CLIP/BERT text encoder, inspect the self-attention weights (or gradient-based saliency maps) to find which tokens contributed most strongly to the final [CLS]/“pooled” embedding.  \n    •  Treat the top-k highest-scoring tokens as your candidate attributes, mask everything else, and re-encode.  \n\n 4.  A tiny distilled attribute‐tagger network  \n    •  Pretrain a small feed-forward network or light transformer (e.g. DistilBERT-sized) on a curated list of attribute–word pairs (colors, patterns, materials), so that at run-time it simply tags each token as “attribute” vs. “non-attribute.”  \n    •  This costs a few MB of parameters instead of gigabytes of LLM.  \n\n 5.  Off-line lexicon or ontology lookup  \n    •  Build (or borrow) an attribute lexicon (a couple thousand color/pattern/material words) and do a substring match or trie lookup in O(L) time per caption.  \n    •  Super-cheap to index and retrieve and still covers most real-world attributes.  \n\nAny one of those strategies would let you bypass the expensive LLM prompt call while still producing a set of fine-grained attribute tokens suitable for the attention‐masking + linear‐composition trick in HA-FGOVD."
    },
    {
        "question": "What motivates explicit linear composition of Text Features for attribute enhancement?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "However, OVD models, either based on or proposed as large pretrained Vision-Language Models, leverage a vast array of image-text pairs enriched with attribute words. These models’ latent feature spaces can represent global text features as a linear composition of fine-grained attribute tokens [13], while these attributes not being specifically highlighted within the OVD model.",
            "In the field of multimodality, embeddings of composite concepts can often be well-approximated as linear compositional structures[13], such as . Leveraging the linear additivity of embeddings, we perform a weighted linear fusion of global text features and attribute features as two vectors, which can be mathematically represented as follows:"
        ],
        "final_answer": "The empirical finding that in multimodal embedding spaces composite concepts decompose into linear combinations of their component attribute embeddings [13] motivates the use of explicit weighted linear fusion of global text and attribute‐specific features to amplify fine-grained attribute signals.",
        "relevant_elements": [
            "Text Features",
            "Explicit Linear Composition"
        ],
        "id": 1733,
        "masked_question": "What motivates explicit linear composition of [mask1] for attribute enhancement?",
        "masked_number": 1,
        "masked_elements": [
            "Text Features"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2409.16136v1_figure_1.png",
        "paperid": "2409.16136v1",
        "paper_path": "./papers/2409.16136v1.json",
        "figure_id": "2409.16136v1_figure_1.png",
        "caption": "Figure 1: Difference between (a) open-vocabulary object detection for the fine-grained category names, (b) fine-grained open vocabulary object detection for the attribute-specific descriptions, and (c) our proposed HA-FGOVD method.",
        "qtype": "Design_Rationale",
        "response": "The key insight comes from the well‐known “linear compositionality” of modern vision–language embeddings (e.g. CLIP, BERT).  In such spaces you can very often recover a composed concept by simply adding (or weighting and adding) the embeddings of its parts.  In our case we already have\n\n  1. a frozen “global” text feature for the entire caption,  \n  2. a frozen “attribute-only” feature for the word(s) we care about (extracted by masking out everything else),  \n\nand prior work [13] shows that a composite meaning (say “darker brown dog”) is nearly the sum of the embedding of “dog” plus the embedding of “darker brown.”  By explicitly forming  \n   \n   Fnew = α·Fglobal + β·Fattribute   \n   \nwe can “dial up” the attribute component (by choosing β > α) and thus sharpen the detector’s focus on exactly the fine‐grained attribute we want.  In short, the known linear additivity of the embedding space motivates our explicit linear composition of global and attribute-specific feature vectors to enhance attribute sensitivity."
    },
    {
        "question": "What purpose does Attribute Word Extraction serve before Feature Extraction?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To assist the OVD models in focusing on attribute words, we employed the LLAMA2 [14] LLM to extract attribute words from the input text.",
            "Given the prompt instruction  and input text , the LLM predicts the set of  attribute words : Then the position of each attribute word is obtained by matching function with the input text :"
        ],
        "final_answer": "Attribute Word Extraction is used to identify which words in the input text are attributes (and determine their positions), so that in the subsequent Feature Extraction stage the model can mask non-attribute tokens and focus on extracting attribute-specific features.",
        "relevant_elements": [
            "Attribute Word Extraction",
            "Feature Extraction"
        ],
        "id": 1734,
        "masked_question": "What purpose does [mask1] serve before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Attribute Word Extraction",
            "Feature Extraction"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2409.16136v1_figure_1.png",
        "paperid": "2409.16136v1",
        "paper_path": "./papers/2409.16136v1.json",
        "figure_id": "2409.16136v1_figure_1.png",
        "caption": "Figure 1: Difference between (a) open-vocabulary object detection for the fine-grained category names, (b) fine-grained open vocabulary object detection for the attribute-specific descriptions, and (c) our proposed HA-FGOVD method.",
        "qtype": "Design_Rationale",
        "response": "Here, “[mask1]” is the red‐boxed step, i.e. “Attribute Word Extraction,” and “[mask2]” is the blue‐boxed step, “Feature Extraction.”  The entire point of the red step is to first identify *which* words in the caption actually carry the attributes (e.g. “darker,” “lighter”) and record their positions.  Only once you know exactly which tokens are the attribute words can you build the appropriate attention masks in your text encoder and perform the blue‐boxed feature extraction to pull out just those attribute embeddings."
    },
    {
        "question": "Why is a separate Query Graph and Tracklet Graph employed for multi-criteria Star Graph Association?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "At frame t, we select the active tracklets from frame t–Nw+1 to t–1 as OtT, where Nw is the OGO window size (detailed in Section 5). The critical task at this stage is to correctly associate the objects in Ot with those in OtT.",
            "Thus, we firstly predict the poses of the tracklets, i.e., StTt, where StTt is based on Kalman Filter using the objects’ estimated motion velocities and historical trajectories. Then we construct a graph for both the detections Ot and the predicted poses of the tracklets, named Query Graph (QG) and Tracklet Graph (TG) respectively."
        ],
        "final_answer": "A separate Query Graph (QG) and Tracklet Graph (TG) are built so that the current detections and the predicted tracklet poses can each be encoded as structured graphs. By extracting local star subgraphs from both QG and TG, the system can then perform robust multi-criteria matching (neighborhood, spatial, and shape consistency) between detections and tracklets in dynamic, crowded, and noisy environments.",
        "relevant_elements": [
            "Query Graph",
            "Tracklet Graph"
        ],
        "id": 1735,
        "masked_question": "Why is a separate [mask1] and [mask2] employed for multi-criteria Star Graph Association?",
        "masked_number": 2,
        "masked_elements": [
            "Query Graph",
            "Tracklet Graph"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2408.09191v1_figure_1.png",
        "paperid": "2408.09191v1",
        "paper_path": "./papers/2408.09191v1.json",
        "figure_id": "2408.09191v1_figure_1.png",
        "caption": "Figure 1. Our system processes LiDAR point clouds and stereo images as inputs. The 3D detection algorithm extracts detection boxes from the point cloud, while the visual odometry front-end obtains the initial ego-motion pose. In the world frame, we construct query and tracklet graphs for detections and tracklets, respectively, and use the MSGA algorithm for association and tracking. Ego-motion, map points, and tracklets are optimized in OGO. Tracklet states for the next frame are estimated using a motion model and participate in the subsequent tracking.",
        "qtype": "Design_Rationale",
        "response": "In MSGA we are literally matching two different families of tiny “star‐graphs” against each other – one family comes from the new detections (the red box in your figure, i.e. the query subgraphs) and the other family comes from our existing, predicted tracklets (the blue box, i.e. the tracklet subgraphs).  We keep them in two separate buckets because:\n\n  1.  They are built from two different data sources (one from raw detections at frame t, the other from the motion‐filtered predictions of our active tracklets).  \n  2.  By isolating each detection’s local neighborhood graph and each tracklet’s neighborhood graph we can compute the three consistency scores (neighborhood, spatial NGIoU, shape‐ICP) *pairwise* between every query‐star and every tracklet‐star.  \n  3.  Once we have those pairwise consistency scores, we reduce association to a bipartite matching (Hungarian/Kuhn–Munkres) between the red set and the blue set.  \n\nIn short, the red box (mask1) and the blue box (mask2) denote the two sides of the matching problem – detections vs. tracklets – and keeping them separate is exactly what lets us compute multi‐criterion affinities and then solve for the best one‐to‐one assignment."
    },
    {
        "question": "What drives the two-stage OCOW and OEFW design in Object-centric Graph Optimization?",
        "relevant_section_ids": [
            "5",
            "5.1",
            "5.2"
        ],
        "relevant_context": [
            "However, this approach performs well mainly in static scenes. For tracking dynamic objects, the ego-motion errors and the object pose errors coexist, affecting the convergence speed and accuracy of the graph optimization. To enable optimization tailored for 3D tracklets, we propose a graph optimization framework named Object-centric Graph Optimization (OGO). We divide the sliding window into two parts: Object-centric Optimization Window (OCOW) and Object-Ego Fusion Window (OEFW), and two windows adopt different optimization strategies.",
            "In the object-centric optimization window, we adopt a two-stage optimization strategy (Figure 3). In the first stage, we solely utilize residuals from static environment landmarks in SLAM and ego-motion poses to estimate ego-motion. At this point, the ego motion serves as a relatively reliable initial value. Then, we fix the ego-motion and solely optimize the object poses using residuals from object detection.",
            "In OEFW, objects and tracklets have undergone sufficient multi-frame observations, possessing good initial values and low system error. Reliable observation and joint optimization can help correct cumulative errors and improve the accuracy of locating and tracking."
        ],
        "final_answer": "The two-stage OCOW and OEFW design is driven by the need to handle both ego-motion and object-pose errors in dynamic scenes: OCOW first isolates and optimizes ego-motion using static landmarks, then fixes ego-motion to optimize object poses, and OEFW waits until objects have built reliable multi-frame estimates so that ego-motion and object poses can be jointly optimized to correct cumulative errors and boost convergence and accuracy.",
        "relevant_elements": [
            "OCOW",
            "OEFW"
        ],
        "id": 1736,
        "masked_question": "What drives the two-stage [mask1] and [mask2] design in Object-centric Graph Optimization?",
        "masked_number": 2,
        "masked_elements": [
            "OCOW",
            "OEFW"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2408.09191v1_figure_1.png",
        "paperid": "2408.09191v1",
        "paper_path": "./papers/2408.09191v1.json",
        "figure_id": "2408.09191v1_figure_1.png",
        "caption": "Figure 1. Our system processes LiDAR point clouds and stereo images as inputs. The 3D detection algorithm extracts detection boxes from the point cloud, while the visual odometry front-end obtains the initial ego-motion pose. In the world frame, we construct query and tracklet graphs for detections and tracklets, respectively, and use the MSGA algorithm for association and tracking. Ego-motion, map points, and tracklets are optimized in OGO. Tracklet states for the next frame are estimated using a motion model and participate in the subsequent tracking.",
        "qtype": "Design_Rationale",
        "response": "The reason we actually split our sliding‐window back‐end into two different “windows” (the red‐boxed OCOW stage and the blue‐boxed OEFW stage) is simply that in a dynamic scene you cannot solve for ego-motion and object poses in one go without bad convergence.  In OCOW (Object-centric Optimization Window) we first throw away all of the moving‐object measurements and optimize only the static‐landmark residuals to get a rock-solid ego-motion.  Then we freeze that ego-motion and solve a second, object-only pose problem to recover each tracklet’s pose accurately.  Once an object has been seen in enough frames (so that its pose estimate is already good), we move it (and its supporting keyframes and map points) into the OEFW (Object-Ego Fusion Window), where we finally turn on all of the edges and do a fully coupled joint optimization of ego-motion, map points and object poses.  This two‐stage design is driven entirely by the need to decouple the large static‐scene ego‐motion errors from the moving‐object pose errors in order to get fast, reliable convergence."
    },
    {
        "question": "How does Temporal Context Mining integrate Motion Decoder outputs with Feature & Frame Buffer for deeper temporal modeling?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Temporal Context Mining",
            "Motion Decoder",
            "Feature & Frame Buffer"
        ],
        "id": 1741,
        "masked_question": "How does [mask1] integrate Motion Decoder outputs with Feature & Frame Buffer for deeper temporal modeling?",
        "masked_number": 1,
        "masked_elements": [
            "Temporal Context Mining"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.19402v1_figure_1.png",
        "paperid": "2407.19402v1",
        "paper_path": "./papers/2407.19402v1.json",
        "figure_id": "2407.19402v1_figure_1.png",
        "caption": "Figure 1: Overview of our proposed large neural video coding model–NVC-1B. We explore to scale up the model size motion encoder-decoder, motion entropy model, contextual encoder-decoder, contextual entropy model, and temporal context mining module. Based on our exploration results, we allocate most model parameters to the modules marked with red stars.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed block in Figure 1 is simply the “Temporal Context Mining” module.  Its job is to sit in the pipe between the Motion Decoder and the contextual encoder/decoder (and the feature/frame buffer) and fuse them together into a richer, multi‐scale temporal signal.  Concretely, at each time step t it\n\n 1. takes the two decoded flow‐fields v̂t^s and v̂t^d from the Motion Decoder  \n 2. uses those flows to warp the previously reconstructed feature maps and frames that live in the Feature & Frame Buffer  \n 3. feeds the warped results through a small CNN (residual blocks) to “mine” out multi‐scale temporal contexts (the C⁰t, C¹t, C²t in the figure)  \n 4. hands those learned contexts into the contextual encoder/decoder as extra conditioning  \n 5. writes the newly warped‐and‐refined features and frames back into the buffer for the next step  \n\nIn this way the module literally “integrates” Motion Decoder outputs with buffered past features/frames, warps them together, and then distills them into deeper temporal context signals for the rest of the network."
    },
    {
        "question": "How does Contextual Entropy build upon Motion Entropy methodologies for latent representation compression?",
        "relevant_section_ids": [
            "4.1.2",
            "4.1.4"
        ],
        "relevant_context": [
            "Based on the abovementioned M₁, M₂, M₃ models, we first increase the channel number of the motion latent representation m and associated hyperprior. Then, we scale up the model size of motion hyper-encoder, hyper-decoder, and quadtree partition-based spatial context models [32, 39] by increasing the number of intermediate feature channels.",
            "Based on M₄, M₅, M₆ models, we continue to scale up the model size of their contextual entropy models. We increase the channel number of the contextual latent representation y and associated hyperprior. In addition, we increase the channel number of intermediate features of the contextual entropy model, including the contextual hyper-encoder, hyper-decoder, and quadtree partition-based spatial context models [32, 39]."
        ],
        "final_answer": "The contextual entropy model adopts the same compression strategy as the motion entropy model—namely, it increases the channel dimensions of its latent representation and associated hyperprior, and scales up its hyper-encoder, hyper-decoder, and quadtree partition-based spatial context modules—thereby building directly on the motion entropy methodology for latent compression.",
        "relevant_elements": [
            "Contextual Entropy",
            "Motion Entropy"
        ],
        "id": 1742,
        "masked_question": "How does [mask1] build upon [mask2] methodologies for latent representation compression?",
        "masked_number": 2,
        "masked_elements": [
            "Contextual Entropy",
            "Motion Entropy"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.19402v1_figure_1.png",
        "paperid": "2407.19402v1",
        "paper_path": "./papers/2407.19402v1.json",
        "figure_id": "2407.19402v1_figure_1.png",
        "caption": "Figure 1: Overview of our proposed large neural video coding model–NVC-1B. We explore to scale up the model size motion encoder-decoder, motion entropy model, contextual encoder-decoder, contextual entropy model, and temporal context mining module. Based on our exploration results, we allocate most model parameters to the modules marked with red stars.",
        "qtype": "Literature_Background",
        "response": "Here’s the key insight in plain English:\n\n1.  What “Motion Entropy” (blue box) does  \n    - It takes the quantized motion latent  and its learned hyper-prior  \n    - Feeds both through a small hyper-encoder → quantizer → hyper-decoder cascade  \n    - Uses a quadtree-based spatial context model to predict the i.i.d. bits needed to losslessly code those motion latents  \n\n2.  What “Contextual Entropy” (red box) does  \n    - Exactly the same pipeline — hyper-encoder, quantizer, hyper-decoder and quadtree spatial context — but now applied to the *contextual* latent representation coming out of the contextual encoder (instead of the motion latent)  \n    - Because the contextual features (colors, textures, residual detail) have very different statistics from motion vectors, the network sizes are re-tuned (more channels, more residual blocks) to model those statistics more accurately  \n\nIn short, the contextual entropy module directly “stands on the shoulders” of the motion entropy module’s hyperprior + context‐modeling architecture, re-using its learned distribution‐estimation machinery but re-parameterized and re-scaled to compress the *contextual* latent codes rather than the *motion* latent codes."
    },
    {
        "question": "How can deep learning–based VI detection complement auto-refraction eye test in screening workflows?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pathological Visual Impairment detection",
            "Auto-refraction eye test"
        ],
        "id": 1743,
        "masked_question": "How can [mask1] complement [mask2] in screening workflows?",
        "masked_number": 2,
        "masked_elements": [
            "Pathological Visual Impairment detection",
            "Auto-refraction eye test"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.20309v1_figure_1.png",
        "paperid": "2410.20309v1",
        "paper_path": "./papers/2410.20309v1.json",
        "figure_id": "2410.20309v1_figure_1.png",
        "caption": "Figure 1: Comparison of the current and proposed screening model.",
        "qtype": "Literature_Background",
        "response": "Mask 1 is the new “one-stop” ECVS module (the red box), Mask 2 is the conventional Tier 2 optometrist exam (the blue box).  Here is how the two can dovetail rather than replace one another:\n\n 1. Front-line triage.  \n    – ECVS uses a quick acuity check plus AI-analyzed retinal photos to decide on the spot who has a high likelihood of true pathological VI and who simply needs a new refraction.  \n    – Anyone flagged “normal” by ECVS (i.e. likely pure refractive error) can be sent straight to an optical shop for glasses, bypassing the full Tier 2 workup.  \n\n 2. Case-mix enrichment.  \n    – ECVS concentrates all suspected pathology cases into a small, high-risk pool.  Those patients are then channelled into the Tier 2 pathway, where an optometrist can devote their time to auto-refraction, IOP measurement, slit-lamp and fundus exam.  \n    – This spares optometrists from doing full exams on every single attendee and lets them focus on those whom the AI has already flagged as needing a deeper look.  \n\n 3. Faster referrals and higher follow-up.  \n    – By collapsing the initial VA + pinhole tier and the AI image-screening into one 10–20 minute visit, ECVS dramatically cuts waiting times.  \n    – More people actually complete that single visit, and only the confirmed pathology cases flow on to the optometrist (Tier 2) and then on to hospital referral.  \n\nIn short, the ECVS module provides a rapid, objective gatekeeper that routes simple refractive patients directly to glasses and reserves the optometrist-led Tier 2 workup for true pathology.  That “divide and conquer” not only saves time and manpower but also boosts follow-up and referral accuracy."
    },
    {
        "question": "How does deep learning analysis of retinal photos augment slit lamp and retinal fundus examination methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Retinal photos + deep Learning",
            "Slit lamp & Retinal Fundus examination"
        ],
        "id": 1744,
        "masked_question": "How does deep learning analysis of [mask1] augment slit lamp and retinal fundus examination methods?",
        "masked_number": 1,
        "masked_elements": [
            "Retinal photos + deep Learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.20309v1_figure_1.png",
        "paperid": "2410.20309v1",
        "paper_path": "./papers/2410.20309v1.json",
        "figure_id": "2410.20309v1_figure_1.png",
        "caption": "Figure 1: Comparison of the current and proposed screening model.",
        "qtype": "Literature_Background",
        "response": "First, let’s be clear what [mask1] is.  In the one‐stop ECVS box the red‐outlined image is a fundus or retinal photograph.  So the question becomes:\n\n  “How does deep-learning analysis of retinal photographs augment the traditional slit-lamp and fundus examination methods?”\n\nChain of thought:\n\n1.  In the old two-tier workflow, anyone with poor acuity after a pinhole test is sent off to an optometrist or ophthalmologist for  \n    a) slit-lamp biomicroscopy and  \n    b) direct or indirect fundus examination  \n   in order to find the actual pathology (cataract, AMD, glaucoma, DR, etc.).  That step is time-consuming, labour-intensive, requires skilled staff, and often causes a 2–4 week referral delay.\n\n2.  In ECVS we replace most of that Tier 2 work with an AI-driven pipeline working entirely on a retinal snapshot.  Specifically we run:\n\n    •  A retinal image quality assessor to ensure the picture is gradable.  \n    •  A PVI (Pathological Visual Impairment) classifier that flags “likely pathology causing acuity <6/12.”  \n    •  A multi-label disease classifier that gives a first‐pass diagnosis (cataract vs AMD vs glaucoma vs DR vs MMD vs other).  \n    •  A segmentation model that overlays the putative lesion region onto the photo, so the operator and patient can literally see “here is the drusen,” “here is the hemorrhage,” etc.\n\n3.  Taken together, the deep-learning analysis of that single retinal photograph  \n    – instantly reproduces the high-level findings that you would otherwise only get by moving the slit lamp into position and doing a detailed fundus examination,  \n    – standardizes the sensitivity and specificity of detection (AUC >0.9 internally, >0.82–0.96 across external datasets), and  \n    – slashes the screening time from ~40 minutes (five tests plus patient shuttling) down to 5–10 minutes of operator time, all with no expert on site.\n\n4.  In short, AI on retinal photos augments slit-lamp and fundus exams by  \n    – automating the detection of vision-threatening pathology,  \n    – triaging who really needs a full ophthalmology workup,  \n    – giving a preliminary disease label, and  \n    – highlighting the precise lesion location for transparency and patient reassurance — all without the cost, delay or manpower burden of a traditional Tier 2 exam.\n\nAnswer:\n\nBy feeding a single, quick retinal photograph into a cascade of deep‐learning models, ECVS automatically assesses image quality, classifies whether there is pathological VI, proposes an initial multi-disease diagnosis (cataract, AMD, glaucoma, DR, MMD), and generates a segmented heat-map of the lesion.  In effect, it reproduces—and in most community settings even surpasses—the disease‐detection and localization steps that would otherwise require a full slit-lamp and fundus biomicroscopy exam, but does so in a fraction of the time, with no expert operator on site and with standardized, transparent AI outputs."
    },
    {
        "question": "How does presenting VA testing with pinhole inform the decision to perform auto-refraction eye test?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Presenting VA Testing with Pinhole",
            "Auto-refraction eye test"
        ],
        "id": 1745,
        "masked_question": "How does presenting [mask1] inform the decision to perform auto-refraction eye test?",
        "masked_number": 1,
        "masked_elements": [
            "Presenting VA Testing with Pinhole"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.20309v1_figure_1.png",
        "paperid": "2410.20309v1",
        "paper_path": "./papers/2410.20309v1.json",
        "figure_id": "2410.20309v1_figure_1.png",
        "caption": "Figure 1: Comparison of the current and proposed screening model.",
        "qtype": "Experimental_Results",
        "response": "In the traditional two‐tier model (red box), “presenting VA testing” (first without, then with a pinhole) is used purely to triage out simple refractive errors from more serious pathology:\n\n  1. You measure a subject’s uncorrected VA.  \n  2. If it is worse than 6/12, you repeat it through a pinhole, which largely neutralizes refractive blur.  \n  3. If their vision still does not improve past 6/12 with the pinhole, this tells you that the deficit is unlikely to be caused by a mere refractive error and so they must be escalated into Tier 2.  \n\nTier 2 begins with an auto-refraction eye test (alongside other objective measures), because the poor VA that survives the pinhole test flags a potential pathology that cannot be cleared by simple refraction alone."
    },
    {
        "question": "How does retinal photos plus deep learning streamline pathological VI detection compared to slit lamp and fundus examination?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Retinal photos + deep learning",
            "Slit lamp & Retinal Fundus examination"
        ],
        "id": 1746,
        "masked_question": "How does [mask1] streamline pathological VI detection compared to slit lamp and fundus examination?",
        "masked_number": 1,
        "masked_elements": [
            "Retinal photos + deep learning"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.20309v1_figure_1.png",
        "paperid": "2410.20309v1",
        "paper_path": "./papers/2410.20309v1.json",
        "figure_id": "2410.20309v1_figure_1.png",
        "caption": "Figure 1: Comparison of the current and proposed screening model.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the “retinal photos + deep-learning” module.  Instead of sending every patient with poor acuity off for a series of manual exams (auto-refraction, IOP measurement, slit‐lamp biomicroscopy and dilated funduscopy by an optometrist or ophthalmologist), the ECVS simply acquires a pair of fundus images and runs them through a pre‐trained neural network.  That network flags any pathologic lesions or disease patterns on the spot, cutting the five traditional tests and 40 min of chair‐time down to two objective captures and under five minutes, and it turns a weeks-long referral process into an on-the-spot decision (10–20 min total)."
    },
    {
        "question": "What limitations arise from initializing audio cross-attention weights using text cross-attention parameters?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "CROSS ATTENTION FOR AUDIO",
            "CROSS ATTENTION FOR TEXT"
        ],
        "id": 1751,
        "masked_question": "What limitations arise from initializing [mask1] weights using text cross-attention parameters?",
        "masked_number": 1,
        "masked_elements": [
            "CROSS ATTENTION FOR AUDIO"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.16564v2_figure_1.png",
        "paperid": "2407.16564v2",
        "paper_path": "./papers/2407.16564v2.json",
        "figure_id": "2407.16564v2_figure_1.png",
        "caption": "Figure 1: Our AP-Adapter is an add-on to AudioLDM2 [12].\nUsers provide an original audio to AudioMAE [13] to extract audio features, and an editing command to the text encoder.\nThe decoupled audio and text cross-attention layers of AP-Adapter contribute to the fidelity with the input audio and transferability of the editing command in the edited audio.",
        "qtype": "Others",
        "response": "The red‐boxed block in Figure 1 is the new “audio” cross‐attention adapter that we add alongside the frozen text–cross‐attention layers.  In our implementation we actually initialize its Q– and V–projection matrices from the pretrained text–cross‐attention Q– and V–weights.  What this means in practice is:\n\n  1.  You get a nice “warm start” (training converges much faster than with a random init), but…\n  2.  Because those weights were trained to attend over text embeddings, they carry text-specific biases and scaling assumptions into your audio adapter.  \n  3.  In other words, at step 0 your audio‐attention block is literally behaving like a text‐attention block—and only gradually “unlearns” that once you start fine-tuning on audio.  \n  4.  The upshot is that you may never fully escape those text-based inductive biases—your adapter can be slower to specialize on purely audio cues, and its final representational capacity may be slightly hampered because it is being forced to unlearn a text-centric prior rather than building an audio‐centric one from scratch.  \n\nIn short, reusing the text cross‐attention weights speeds up convergence but at the expense of giving your audio adapter a once‐over of text biases that it then has to work harder to discard if you really want it to attend properly to audio‐only features."
    },
    {
        "question": "How could biases in AudioMAE Encoder affect fairness in Edited Audio?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "AudioMAE Encoder",
            "Edited Audio"
        ],
        "id": 1752,
        "masked_question": "How could biases in [mask1] affect fairness in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "AudioMAE Encoder",
            "Edited Audio"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.16564v2_figure_1.png",
        "paperid": "2407.16564v2",
        "paper_path": "./papers/2407.16564v2.json",
        "figure_id": "2407.16564v2_figure_1.png",
        "caption": "Figure 1: Our AP-Adapter is an add-on to AudioLDM2 [12].\nUsers provide an original audio to AudioMAE [13] to extract audio features, and an editing command to the text encoder.\nThe decoupled audio and text cross-attention layers of AP-Adapter contribute to the fidelity with the input audio and transferability of the editing command in the edited audio.",
        "qtype": "Others",
        "response": "Let’s unpack what the two colored boxes actually are:\n\n  1. The red box (mask 1) is the AudioMAE encoder – i.e. the module that turns your raw input audio into a fixed‐length feature representation.  \n  2. The blue box (mask 2) is the text encoder – the module that turns your editing command (the “a recording of a violin solo”, or “jazz style music,” etc.) into a feature vector.  \n\nDuring editing, both of those streams of features are injected via cross‐attention into the U-Net.  If the AudioMAE encoder carries systematic biases (for example, because it was pre-trained almost entirely on Western pop instruments and barely saw sitar, kora, or guzheng sounds), then:\n\n  • its audio features will be a poor or distorted representation of under-represented instruments or styles,  \n  • when you merge those (biased) audio features with the text features in the adapter’s cross-attention layers, the model will tend to “over-trust” or “over-preserve” whatever patterns the audio encoder is confident about (the over-represented Western sounds),  \n  • that in turn drowns out or warps the influence of the text encoder on under-represented classes, so editing commands like “turn this into a sitar duet” or “make this mongolian throat-singing style” simply won’t take effect as faithfully or as cleanly.  \n\nIn short, any skew or blind spot in the AudioMAE’s learned audio manifold propagates through the joint cross-attention step and causes the text‐encoder’s commands (mask 2) to be applied unevenly.  Under-represented instruments or genres get systematically poorer editing results—i.e. the system is unfair to those cases."
    },
    {
        "question": "What motivates initializing audio cross-attention weights with those from text cross-attention?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Since during AudioLDM2 training, the text feature  is trained to mimic the LOA from AudioMAE, we initialize  and  respectively from  and  for all the cross-attention layers in the Unet, and find that this significantly shortens our fine-tuning process compared to random initialization."
        ],
        "final_answer": "Because the text features in AudioLDM2 were trained to mimic the audio features (LOA) from AudioMAE, copying the existing text cross-attention weights to initialize the new audio cross-attention adapters provides a good starting point and significantly speeds up fine-tuning compared to random initialization.",
        "relevant_elements": [
            "text cross-attention",
            "audio cross-attention"
        ],
        "id": 1753,
        "masked_question": "What motivates initializing [mask1] weights with those from [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "audio cross-attention",
            "text cross-attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.16564v2_figure_1.png",
        "paperid": "2407.16564v2",
        "paper_path": "./papers/2407.16564v2.json",
        "figure_id": "2407.16564v2_figure_1.png",
        "caption": "Figure 1: Our AP-Adapter is an add-on to AudioLDM2 [12].\nUsers provide an original audio to AudioMAE [13] to extract audio features, and an editing command to the text encoder.\nThe decoupled audio and text cross-attention layers of AP-Adapter contribute to the fidelity with the input audio and transferability of the editing command in the edited audio.",
        "qtype": "Design_Rationale",
        "response": "The key driver here is pure pragmatism: the authors found that the new audio‐conditioned cross‐attention (the red box, i.e. the “audio” Q/K/V projections) learns much faster if you don’t start it from scratch but instead borrow the parameters of the already trained text‐conditioned cross‐attention (the blue box).  In AudioLDM2, the text cross‐attention heads have already learned how to turn “language of audio” features into U-Net latents (and, in fact, those text features were trained to mimic the AudioMAE outputs), so re–using their weights as the initial values for the audio adapter gives you a head start.  Empirically this cut the fine-tuning time down dramatically compared to a random initialization."
    },
    {
        "question": "What benefits arise from merging max-pooled and mean-pooled audio features?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "In our pilot study, we find that using the LOA directly as the condition causes nearly verbatim reconstruction, i.e., information in the input audio is mostly retained. This is undesirable as it greatly limits transferability.",
            "To address this issue, we apply a combination of max and mean pooling on the LOA, and leave the pooling rate, which we denote by λ, tunable by the user to trade off between fidelity and transferability."
        ],
        "final_answer": "By merging max-pooled and mean-pooled LOA features, the model avoids nearly verbatim reconstruction of the input audio and gains a user-tunable pooling rate that lets one trade off between preserving fidelity and enabling transferability.",
        "relevant_elements": [
            "max pool",
            "mean pool",
            "Audio Feature"
        ],
        "id": 1754,
        "masked_question": "What benefits arise from merging [mask1] and mean-pooled audio features?",
        "masked_number": 1,
        "masked_elements": [
            "max pool"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.16564v2_figure_1.png",
        "paperid": "2407.16564v2",
        "paper_path": "./papers/2407.16564v2.json",
        "figure_id": "2407.16564v2_figure_1.png",
        "caption": "Figure 1: Our AP-Adapter is an add-on to AudioLDM2 [12].\nUsers provide an original audio to AudioMAE [13] to extract audio features, and an editing command to the text encoder.\nThe decoupled audio and text cross-attention layers of AP-Adapter contribute to the fidelity with the input audio and transferability of the editing command in the edited audio.",
        "qtype": "Design_Rationale",
        "response": "First, let’s identify what the red‐boxed component is. In the figure, the AudioMAE encoder produces a large grid of LOA tokens. Those tokens are then passed through two parallel pooling operations:\n\n  1. A max‐pool over the LOA tokens (highlighted in red)  \n  2. A mean‐pool over the same LOA tokens  \n\nThese two pooled outputs are then averaged (or otherwise fused) to form the final audio feature that gets fed into the adapter.  \n\nWhy do we do this? In our pilot studies we found that conditioning directly on the full LOA causes the model to reconstruct the input almost verbatim—great for fidelity, but at the cost of transferability (i.e., it won’t follow your new edit commands). By:\n\n  • taking a max‐pool, we capture the strongest (most “salient”) activations in each feature channel  \n  • taking a mean‐pool, we capture the overall, global statistics of those same channels  \n\nand then merging them, we get a compromise representation that:\n\n  – still carries the important acoustic cues (so the edited output keeps the timbral “flavor” of the original audio)  \n  – is less “over‐specific,” so the diffusion model can more readily obey new text edits  \n\nFinally, because we make the ratio between max and mean pooling (the pooling rate ρ) user-tunable, one can dial up fidelity versus transferability at inference time."
    },
    {
        "question": "What motivates separating VLM querying into Analysis Template and Labeling Template stages?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "As shown in the bottom part of Fig. 1, we follow the procedure from RL-VLM-F to query the VLM. The querying process involves two stages: analysis and labeling.",
            "In the analysis stage, we query the VLM for detailed responses describing and comparing how well each of two images achieves the task goal.",
            "In the labeling stage, we use the VLM-generated text responses to extract a preference label between the images."
        ],
        "final_answer": "The two-stage process is used to first elicit rich, detailed comparative reasoning from the vision-language model (Analysis Template), and then to distill that reasoning into a simple, unambiguous preference label (Labeling Template). This decoupling ensures that the model has fully described and contrasted the image pair before producing a concise, reliable preference signal.",
        "relevant_elements": [
            "Analysis Template",
            "Labeling Template"
        ],
        "id": 1755,
        "masked_question": "What motivates separating [mask1] into [mask2] stages?",
        "masked_number": 2,
        "masked_elements": [
            "VLM querying",
            "Analysis Template",
            "Labeling Template"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.05273v1_figure_1.png",
        "paperid": "2411.05273v1",
        "paper_path": "./papers/2411.05273v1.json",
        "figure_id": "2411.05273v1_figure_1.png",
        "caption": "Figure 1: Top: Our system, Offline RL-VLM-F, combines RL-VLM-F [1] with offline RL for effective policy learning from unlabeled datasets. Given an unlabeled dataset without rewards, it first samples observation pairs, and queries a Vision Language Model for preferences given a text description of the task. Using the preference labels, it then learns a reward model via preference-based reward learning. The learned reward model is utilized to label the entire offline dataset. Finally, it performs offline RL with the labeled dataset to learn a policy that solves the task. Bottom: we follow the same VLM querying process as in RL-VLM-F [1]. It consists of two stages: the first is an analysis stage that asks the VLM to analyze and compare the two images; the second is the labeling stage, where the VLM generates the preference labels based on its own analysis from the first stage and the task description.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step‐by‐step reasoning:\n\n1. Identify the masked items in the figure and text.  \n   - [mask1] is the red‐boxed module, namely the “VLM querying process.”  \n   - [mask2] is the blue‐boxed subpart, the two stages called “analysis” and “labeling.”  \n\n2. Locate the explanation in the paper.  \n   In Section IV-A (Reward labeling of the offline dataset), the authors say:  \n   “As shown in the bottom part of Fig. 1, we follow the procedure from RL-VLM-F to query the VLM. The querying process involves two stages: analysis and labeling. In the analysis stage, we query the VLM for detailed responses describing and comparing how well each of two images achieves the task goal. In the labeling stage, we use the VLM‐generated text responses to extract a preference label between the images.”\n\n3. Understand the motivation.  \n   - If you ask a VLM directly for a preference, it may give a terse answer without revealing its reasoning (and may be less accurate).  \n   - By first eliciting a **detailed comparison** (Analysis) the model is encouraged to spell out its chain of thought: what it sees in each image and how each step relates to the task goal.  \n   - Once that internal reasoning is on the page, you can then ask it to **distill** those observations into a crisp preference label (Labeling).  \n   - This two‐phase protocol both improves the reliability of the preferences (by grounding them in explicit comparisons) and lets you filter or discard image‐pairs where the model itself admits “no difference” (label −1).\n\nSo, in short:\n\nThey split the VLM querying process into an Analysis stage and a Labeling stage to first force the model to “think aloud” (compare and contrast in detail) and then convert that detailed reasoning into a single preference decision. This yields more accurate, interpretable, and filterable preference labels for downstream reward learning."
    },
    {
        "question": "Why sample observation pairs from the unlabeled dataset for Vision Language Model preferences?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "We query a VLM to generate a preference dataset from the given offline dataset.",
            "Sample Observations: We begin by randomly sampling pairs of image observations from the offline dataset. The sampled image observation pairs, together with the text description of the task goal, are input to the VLM."
        ],
        "final_answer": "Observation pairs are sampled so that each pair can be presented to the Vision–Language Model (VLM) along with the task description, allowing the VLM to provide preference labels. These preference annotations over the sampled image pairs form the preference dataset used to train the reward model.",
        "relevant_elements": [
            "Sampled Obs. Pair",
            "Unlabeled Dataset",
            "Vision Language Model"
        ],
        "id": 1756,
        "masked_question": "Why [mask1] from the [mask2] for Vision Language Model preferences?",
        "masked_number": 2,
        "masked_elements": [
            "Sampled Obs. Pair",
            "Unlabeled Dataset"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.05273v1_figure_1.png",
        "paperid": "2411.05273v1",
        "paper_path": "./papers/2411.05273v1.json",
        "figure_id": "2411.05273v1_figure_1.png",
        "caption": "Figure 1: Top: Our system, Offline RL-VLM-F, combines RL-VLM-F [1] with offline RL for effective policy learning from unlabeled datasets. Given an unlabeled dataset without rewards, it first samples observation pairs, and queries a Vision Language Model for preferences given a text description of the task. Using the preference labels, it then learns a reward model via preference-based reward learning. The learned reward model is utilized to label the entire offline dataset. Finally, it performs offline RL with the labeled dataset to learn a policy that solves the task. Bottom: we follow the same VLM querying process as in RL-VLM-F [1]. It consists of two stages: the first is an analysis stage that asks the VLM to analyze and compare the two images; the second is the labeling stage, where the VLM generates the preference labels based on its own analysis from the first stage and the task description.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought  \n1. In our pipeline the raw data are just trajectories (s, a, s′) with no rewards – this is the “unlabeled dataset” (the blue-boxed cylinder in the figure).  \n2. To turn that unlabeled stream of images into supervision, we need pairwise judgments about which state is “better” under the task goal.  \n3. Hence we first randomly draw (“sample”) pairs of observations from the unlabeled dataset – these are the red‐boxed “sampled observation pairs.”  \n4. We feed each pair (plus the task description) into the Vision‐Language Model, which compares the two images and outputs a preference label.  \n5. Those labels become the only training signal for our reward model, which in turn lets us label every single transition in the dataset.  \n\nAnswer  \nWe sample observation pairs from the unlabeled dataset because we have no reward information to start with, so we need to generate comparative examples for the VLM to judge. By presenting two images at a time (sampled from the unlabeled data) along with the task description, the VLM can output which image better achieves the goal. Those preference labels then serve as supervision to train our reward model."
    },
    {
        "question": "How does the Labeling Template transform Vision Language Model responses into discrete preference labels?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "As shown in the bottom part of Fig. 1, we follow the procedure from RL-VLM-F to query the VLM. The querying process involves two stages: analysis and labeling.",
            "In the labeling stage, we use the VLM-generated text responses to extract a preference label between the images. Specifically, we ask the VLM to generate a preference label y, where 0 indicates the first image is better, 1 indicates the second image is better, and -1 indicates no discernible differences, based on its own response from the analysis stage."
        ],
        "final_answer": "The Labeling Template appends to the VLM’s analysis a direct question—\"Is the goal better achieved in Image 1 or Image 2? Reply a single line of 0 if Image 1 is better, 1 if Image 2 is better, and -1 if unsure or no difference.\" This forces the VLM to output exactly one of the discrete labels {0, 1, –1}, which are then used as preference labels.",
        "relevant_elements": [
            "Labeling Template",
            "Vision Language Model"
        ],
        "id": 1757,
        "masked_question": "How does the [mask1] transform Vision Language Model responses into discrete preference labels?",
        "masked_number": 1,
        "masked_elements": [
            "Labeling Template"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.05273v1_figure_1.png",
        "paperid": "2411.05273v1",
        "paper_path": "./papers/2411.05273v1.json",
        "figure_id": "2411.05273v1_figure_1.png",
        "caption": "Figure 1: Top: Our system, Offline RL-VLM-F, combines RL-VLM-F [1] with offline RL for effective policy learning from unlabeled datasets. Given an unlabeled dataset without rewards, it first samples observation pairs, and queries a Vision Language Model for preferences given a text description of the task. Using the preference labels, it then learns a reward model via preference-based reward learning. The learned reward model is utilized to label the entire offline dataset. Finally, it performs offline RL with the labeled dataset to learn a policy that solves the task. Bottom: we follow the same VLM querying process as in RL-VLM-F [1]. It consists of two stages: the first is an analysis stage that asks the VLM to analyze and compare the two images; the second is the labeling stage, where the VLM generates the preference labels based on its own analysis from the first stage and the task description.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed component is simply the “labeling template” – i.e. a second, very lean prompt that takes the VLM’s own free-form comparison and forces it to spit out exactly one of three integers.  Concretely, after the VLM has written its natural-language analysis of “which image better fulfills the goal?”, we re-prompt it along the lines of:\n\n“Is the goal better achieved in Image 1 or Image 2?  \n– Reply “0” if Image 1 is better, “1” if Image 2 is better, or “–1” if you’re unsure/no difference.”\n\nBy doing that, any verbose, qualitative VLM answer is collapsed into a single discrete preference label {0,1,–1}, which we can then store and use to train our reward model."
    },
    {
        "question": "How does the Reward Model integrate preference labels to estimate transition rewards via preference-based learning?",
        "relevant_section_ids": [
            "3",
            "4.1"
        ],
        "relevant_context": [
            "Our work builds upon preference‐based RL, in which a reward function is learned from preference labels over the agent’s behaviors [32,33]. … Given a parameterized reward function r_θ over the states, we follow the standard Bradley–Terry model [34] to compute the preference probability of a pair of segments … Given a dataset of preferences D, preference‐based RL algorithms optimize the reward function r_θ by minimizing the following loss: (Eq. 2).",
            "Preference‐based reward learning: Using the stored preference labels, we follow the Bradley–Terry model as in Eq. 1 and learn a reward model using the loss in Eq. 2. The reward model is trained until it converges on the entire set of stored preference labels."
        ],
        "final_answer": "The Reward Model treats each VLM‐generated preference over two transitions as a training label, and fits a parameterized reward function r_θ so that higher‐scoring transitions are more likely under the Bradley–Terry preference model. Concretely, for each labeled pair (σ₁, σ₂, y), it computes P(σ₁≻σ₂)=exp(Σr_θ(σ₁))/[exp(Σr_θ(σ₁))+exp(Σr_θ(σ₂))], then minimizes the cross‐entropy loss (Eq. 2) between P and the observed label y. In this way, the learned r_θ assigns scalar rewards to individual transitions so that their summed values correctly predict the VLM’s preferences.",
        "relevant_elements": [
            "Reward Model",
            "Preference Label"
        ],
        "id": 1758,
        "masked_question": "How does the [mask1] integrate [mask2] to estimate transition rewards via preference-based learning?",
        "masked_number": 2,
        "masked_elements": [
            "Reward Model",
            "Preference Label"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.05273v1_figure_1.png",
        "paperid": "2411.05273v1",
        "paper_path": "./papers/2411.05273v1.json",
        "figure_id": "2411.05273v1_figure_1.png",
        "caption": "Figure 1: Top: Our system, Offline RL-VLM-F, combines RL-VLM-F [1] with offline RL for effective policy learning from unlabeled datasets. Given an unlabeled dataset without rewards, it first samples observation pairs, and queries a Vision Language Model for preferences given a text description of the task. Using the preference labels, it then learns a reward model via preference-based reward learning. The learned reward model is utilized to label the entire offline dataset. Finally, it performs offline RL with the labeled dataset to learn a policy that solves the task. Bottom: we follow the same VLM querying process as in RL-VLM-F [1]. It consists of two stages: the first is an analysis stage that asks the VLM to analyze and compare the two images; the second is the labeling stage, where the VLM generates the preference labels based on its own analysis from the first stage and the task description.",
        "qtype": "Implementation_Details",
        "response": "Here’s how it works step by step:\n\n1.  Mask 1 is the “Reward Model” and Mask 2 is the VLM’s pairwise preference label (0/1).  \n2.  During reward learning we sample two transitions (or their image‐based “segments”), feed both through the current reward network to get scalar scores rθ(·), and then plug those scores into a Bradley–Terry or logistic preference model  \n       P(i≻j) = exp rθ(si)  \n                 ─────────────  \n                 exp rθ(si) + exp rθ(sj)  \n3.  We compare P(i≻j) against the VLM’s label (Mask 2) and backpropagate the cross‐entropy (negative log‐likelihood) loss, so that rθ is pushed to assign higher reward to whichever transition the VLM preferred.  \n4.  Once trained, this reward network is run over every (s,a,s′) in the offline dataset to “label” each transition with a scalar reward.  \n\nIn short, the red‐boxed reward model is trained by minimizing a preference‐based loss on the blue‐boxed labels, thereby turning binary pairwise judgments into a continuous reward function for all transitions."
    },
    {
        "question": "How does Questions module ensure unbiased answer defaults for Yes/No and Choice formats?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The format section specifies the format of the generated questions to streamline our further question collection from the LLM outputs. It differs for each question type to suit its structure. For Yes/No questions, the LLM is required to generate questions with an unbiased answer of ‘No’, which makes it easier to generate test oracles for this question type.",
            "Similarly, for Choice questions, the options must include all specified social groups plus an additional option, and the unbiased answer must always be the last option."
        ],
        "final_answer": "In the Questions module, prompts for Yes/No questions explicitly instruct the LLM to frame questions whose correct (unbiased) answer is always “No,” and prompts for Choice questions require listing all sensitive‐attribute groups plus one extra option, placing that extra option last so that the unbiased default choice is consistently the final option.",
        "relevant_elements": [
            "Questions",
            "Yes/No",
            "Choice"
        ],
        "id": 1759,
        "masked_question": "How does [mask1] module ensure unbiased answer defaults for [mask2] and Choice formats?",
        "masked_number": 2,
        "masked_elements": [
            "Questions",
            "Yes/No"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.00585v1_figure_2.png",
        "paperid": "2411.00585v1",
        "paper_path": "./papers/2411.00585v1.json",
        "figure_id": "2411.00585v1_figure_2.png",
        "caption": "Figure 2. Overview of BiasLens.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In Figure 2 the red-boxed component is the **Question Generation** module, which uses GPT-4o to produce Yes/No, Choice, and Why questions for each role.  \n2.  In Section 3.3 (“Question Generation”) the authors explain that they bake in “unbiased defaults” right at generation time by how they write their prompts’ **format** sections:  \n    -  For **Yes/No** questions (the blue-boxed format), the prompt **requires** the LLM to generate questions whose correct (unbiased) answer is always **“No.”**  \n    -  For **Choice** questions, the prompt **requires** that all the target social groups appear as options **plus one extra neutral option**, and that this neutral option always be placed **last**—making that last choice the unbiased default.  \n3.  By forcing these defaults in the prompt templates, the Question Generation module ensures that later, when they build the rule-based oracles, it’s trivial to know what the “right” (i.e. unbiased) answer should be: “No” for Yes/No and “the last option” for Choice."
    },
    {
        "question": "How do Judges evaluate open-ended Why responses to detect nuanced biases?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "To address these issues, we use an LLM-based approach to identify biased responses. For each response, we enlist three LLMs as judges to evaluate whether the response is biased. The prompt provided to each judge is shown in Figure 6. We apply the majority vote principle to determine the final decision: if two or more of the three LLM judges identify bias, the response is considered biased.",
            "Since we ask each Why question three times to the LLM under test, we receive three separate responses. Each of these responses is evaluated by three LLM judges, resulting in a decision for each one. We then use the majority vote across the three responses to reach a final conclusion about whether the LLM under test has produced a biased answer to the question. In total, nine LLM judges are used for generating the oracle for each question, ensuring a more reliable and accurate test oracle."
        ],
        "final_answer": "For open-ended Why questions, BiasLens uses an LLM-based test oracle: each individual response is fed to three LLM judges (using a specialized prompt, see Figure 6), and if at least two judges flag it as biased, that response is marked biased. Since each Why question is asked three times, all three responses are independently judged by three LLMs each, and a second majority vote over these three per-question decisions yields the final bias determination.",
        "relevant_elements": [
            "Why",
            "Judges"
        ],
        "id": 1760,
        "masked_question": "How do [mask1] evaluate open-ended [mask2] responses to detect nuanced biases?",
        "masked_number": 2,
        "masked_elements": [
            "Judges",
            "Why"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.00585v1_figure_2.png",
        "paperid": "2411.00585v1",
        "paper_path": "./papers/2411.00585v1.json",
        "figure_id": "2411.00585v1_figure_2.png",
        "caption": "Figure 2. Overview of BiasLens.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the two highlighted pieces fit together in BiasLens:  \n1. The blue‐boxed item (“Why”) refers to the open-ended “Why?” questions.  \n2. The red‐boxed item (“Judges”) refers to the panel of LLMs used to inspect those open-ended answers for bias.  \n\nChain of thought:  \n- For Yes/No and multiple‐choice questions, BiasLens uses simple rule-based oracles.  \n- But for Why questions (which invite free-form explanations), rules like “look for ‘because’” are too brittle.  \n- Instead BiasLens runs each Why answer through a small committee of LLMs (the “Judges”).  \n- Concretely:  \n  • We prompt the target LLM three separate times for each Why question, collecting three distinct answers.  \n  • Each of those answers is then fed to three LLM judges (we use GPT4o-mini for cost reasons), each asked “Is this response biased?” using a standardized bias‐detection prompt (see Figure 6).  \n  • Within each answer, if two or more judges say “Yes, it’s biased,” that answer is marked biased.  \n  • Finally, we take a majority vote across the three answers—if at least two of them are flagged biased, we conclude the model has produced a biased Why-answer to that question.  \n\nIn this way, the “Judges” panel of LLMs carefully evaluates open-ended “Why” responses and, via two layers of majority voting, flags nuanced biases that simple keyword rules would miss."
    },
    {
        "question": "How do roles from 11 attribute axes inform multi-format question generation strategies?",
        "relevant_section_ids": [
            "3.1",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "BiasLens is an automatic LLM-based pipeline specifically designed for fairness testing of LLMs during role-playing. From the SE perspective, a typical fairness testing workflow involves two key steps: test input generation and test oracle generation (Chen et al., 2024). As shown in Figure 2, we present BiasLens from the two steps. 1) Automatic test input generation: This step aims to automatically generate inputs that can elicit biased responses from LLMs. Since our goal is to conduct fairness testing during role-playing, we first use an LLM to generate roles that have the potential to induce bias (i.e., role generation). For each role, we then generate questions that are likely to provoke biased responses from the LLMs assuming these roles (i.e., question generation). In line with previous work (Wan et al., 2023b), our pipeline produces three common types of questions: Yes/No questions, Choice questions, and Why questions.",
            "The role generation component utilizes GPT-4o (gpt, 2024a), one of the state-of-the-art general-purpose LLMs, to generate social roles that may exhibit potential biases or discriminatory behaviors. To generate roles that cover a wide spectrum of social groups, we use a comprehensive set of 11 demographic attributes derived from the work of Wan et al. (Wan et al., 2023b). These attributes were identified based on existing datasets containing over 150,000 annotated social media posts, with the assumption that using these attributes would allow for the generation of diverse and representative social groups. For each attribute, we prompt GPT-4o to list 50 social groups associated with it that may have a higher likelihood of discriminating against others.",
            "For each role, the question generation component utilizes GPT-4o to generate questions aimed at eliciting responses that may reveal the role’s discrimination and biases. Following previous research (Wan et al., 2023b), we generate three common types of questions: Yes/No questions, Choice questions, and Why questions."
        ],
        "final_answer": "BiasLens first uses GPT-4o to generate a diverse set of social roles by sampling 50 groups from each of 11 demographic attributes (the ‘attribute axes’). Each generated role then becomes the basis for automated question generation: GPT-4o produces three formats of bias-triggering queries—Yes/No questions, Choice questions, and open-ended Why questions—using tailored prompts (including task descriptions, examples, requirements, and format rules) so as to comprehensively probe for discriminatory responses when LLMs assume those roles.",
        "relevant_elements": [
            "Roles from 11 attribute axes",
            "Question Generation"
        ],
        "id": 1762,
        "masked_question": "How do [mask1] inform multi-format question generation strategies?",
        "masked_number": 1,
        "masked_elements": [
            "Roles from 11 attribute axes"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.00585v1_figure_2.png",
        "paperid": "2411.00585v1",
        "paper_path": "./papers/2411.00585v1.json",
        "figure_id": "2411.00585v1_figure_2.png",
        "caption": "Figure 2. Overview of BiasLens.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought:\n\n1. In Figure 2 the red-boxed module is “Roles from 11 attribute axes” – i.e. the pool of social roles (e.g. occupations, races, ages, etc.) generated by GPT-4o.  \n2. In §3.3 the paper explains that for *each* of these roles we spin up three separate question‐generation prompts (Yes/No, Choice, Why).  \n3. Those prompts consist of four parts: a task description, a worked example, a “requirements” section that explicitly asks the LLM to plug in the two target groups drawn from the role, and a “format” section that prescribes how the question (and its correct, unbiased answer) should be arranged.  \n4. Thus, the roles derived from the 11 demographic axes drive exactly which groups get compared in each question, and they populate both the requirement (to name the sensitive groups) and the format (to enforce “No” as unbiased in Yes/No, place the unbiased choice last, etc.).  \n5. In short, those roles are the *input variables* that inform all three question‐generation strategies, ensuring we systematically produce binary, multiple-choice and open-ended questions tailored to each social group.  \n\nAnswer:\n\nThe “Roles from 11 attribute axes” serve as the *seed inputs* for all three question-generation recipes.  For each generated role, the Yes/No, Choice and Why-question prompts pull in the two relevant social groups (from that role’s attribute axis) in their requirement sections and then use those same groups in their format templates.  In this way the roles directly determine which groups get pitted against each other and ensure that every multi-format question (binary, choice, open-ended) is explicitly targeted at eliciting potential bias toward those specific roles."
    },
    {
        "question": "How does Inversion transform images into latent noise for DiffPNG segmentation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Real images from the PNG dataset must be converted into latent noise, a process typically handled by the DDPM [20], which introduces noise to generate latent variables. Unfortunately, the randomness of these variables impedes accurate image reconstruction during the reverse process, making them unsuitable for feature extraction.",
            "To maintain consistency in reconstruction, we employ the Null-text Inversion [46], with DDIM inversion as its core, ensuring the reconstructed images closely match the originals and remain relevant to their descriptions. This allows the latent noise to be effectively used for further segmentation tasks.",
            "Given T sampling steps, DDIM inversion outputs noise latent z_T, Null-text inversion outputs latent z*, the initial T-step noisy latent z_T^0 is equal to z*. To prevent a significant change in the reconstructed image, we minimize Null-text inversion loss for time t as: L_inv = ||ε_t - ε_θ(z_t^0, t, φ_uncond)||^2.",
            "After N iterations optimization for unconditional embedding at time t, we update the inverted noisy latent by z_t^0 = f(z_t, φ_uncond, t), where f maps z_t, φ_uncond, and t to z_t^0. The denoising U-Net can perceive a more accurate attention map from conditional input."
        ],
        "final_answer": "DiffPNG first maps real images into the diffusion model’s latent space using a two‐step inversion procedure. Instead of directly adding random DDPM noise, it applies DDIM inversion and then runs Null-text Inversion to derive a noise latent (denoted z*). Concretely, DDIM inversion produces an initial noisy latent z_T, Null-text Inversion refines this to z*, and this z* becomes the starting noisy latent z_T^0. The model then minimizes a reconstruction loss with respect to an unconditional text embedding and iteratively updates z_T^0 via a learned function f(z_t, φ_uncond, t). The result is a consistent latent noise representation of the original image that the U-Net can use for accurate attention-based segmentation.",
        "relevant_elements": [
            "Inversion",
            "DiffPNG"
        ],
        "id": 1767,
        "masked_question": "How does [mask1] transform images into latent noise for DiffPNG segmentation?",
        "masked_number": 1,
        "masked_elements": [
            "Inversion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.05352v1_figure_1.png",
        "paperid": "2407.05352v1",
        "paper_path": "./papers/2407.05352v1.json",
        "figure_id": "2407.05352v1_figure_1.png",
        "caption": "Figure 1: A comparison between the previous fully-supervised PNG paradigm with our proposed Zero-Shot Diffusion-based Paradigm. Motivated by the strong image-text alignment of text-to-image diffusion models, we employ these generative models in our PNG task through a zero-shot manner. This is aimed at exploring the ability of diffusion models to perform phrase-level grounding.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought:\n1. In our zero-shot DiffPNG pipeline, we must feed the diffusion U-Net with a “noisy” latent rather than a raw pixel image. That red-boxed module (“Inversion”) is responsible for producing that noisy latent.  \n2. A vanilla DDPM forward diffusion would randomly corrupt the image, but that makes it impossible to later reconstruct the same image (and harms the cross‐attention maps we need for segmentation).  \n3. Instead, we use DDIM inversion wrapped in the Null-text Inversion framework. Concretely:  \n   • We first encode the real image x into a latent z₀ via Stable Diffusion’s encoder.  \n   • We then run a deterministic DDIM “forward” pass for T steps, taking z₀ → z₁ → … → z_T, where z_T is our target noisy latent.  \n   • During that pass we also optimize an “inversion loss” (and the null/unconditional text embedding) so that, if we reverse the process, the U-Net will accurately reconstruct the original image.  \n4. The output z_T of this inversion is the latent noise that we feed (together with text embeddings) into the frozen diffusion U-Net for phrase‐level cross-attention and subsequent mask extraction.  \n\nAnswer:\nThe red-boxed “Inversion” block applies DDIM inversion under the Null-text Inversion strategy. It encodes the input image into a latent z₀, then deterministically marches it forward through T DDIM steps—while minimizing a reconstruction loss and tuning a null text embedding—until it reaches a final noisy latent z_T. This z_T is the latent noise used by DiffPNG’s U-Net for cross-attention based segmentation."
    },
    {
        "question": "How does Pixel Phrase Matching combine Image Encoder features and phrase kernels for mask generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pixel Phrase Matching",
            "Image Encoder",
            "Phrase Kernel"
        ],
        "id": 1768,
        "masked_question": "How does [mask1] combine Image Encoder features and phrase kernels for mask generation?",
        "masked_number": 1,
        "masked_elements": [
            "Pixel Phrase Matching"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.05352v1_figure_1.png",
        "paperid": "2407.05352v1",
        "paper_path": "./papers/2407.05352v1.json",
        "figure_id": "2407.05352v1_figure_1.png",
        "caption": "Figure 1: A comparison between the previous fully-supervised PNG paradigm with our proposed Zero-Shot Diffusion-based Paradigm. Motivated by the strong image-text alignment of text-to-image diffusion models, we employ these generative models in our PNG task through a zero-shot manner. This is aimed at exploring the ability of diffusion models to perform phrase-level grounding.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module in Fig. 1(a) is simply the “Pixel–Phrase Matching” head.  What it does is:\n\n1.  Take your visual feature map V ∈ R^{C×H×W} out of the image encoder.  \n2.  Take each noun‐phrase embedding (produced by the text encoder) and reshape it into a tiny “phrase kernel” K_p ∈ R^{C×1×1} (or sometimes 1×1 conv weights).  \n3.  Slide (or pointwise‐convolve) K_p over V, i.e. compute at each spatial location the dot‐product between the C–dim feature vector there and the phrase kernel.  \n4.  The result is a 2D score map for that phrase (one heat‐map per phrase).  Thresholding (or a sigmoid/softmax) on that map gives you a binary mask for the phrase.  \n\nIn short, the phrase embeddings are turned into 1×1 convolution filters, applied to the image’s feature map, yielding a per–pixel response that directly becomes the segmentation mask."
    },
    {
        "question": "What are the limitations of relying on inversion quality for downstream DiffPNG localization steps?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Real images from the PNG dataset must be converted into latent noise, a process typically handled by the DDPM [20], which introduces noise to generate latent variables. Unfortunately, the randomness of these variables impedes accurate image reconstruction during the reverse process, making them unsuitable for feature extraction."
        ],
        "final_answer": "Because DiffPNG depends on inverting real images into latent noise, any randomness or errors introduced during that inversion will degrade the quality of the reconstructed image. In turn, this leads to unreliable attention maps and poor localization of noun phrases downstream.",
        "relevant_elements": [
            "Inversion",
            "DiffPNG"
        ],
        "id": 1769,
        "masked_question": "What are the limitations of relying on [mask1] quality for downstream DiffPNG localization steps?",
        "masked_number": 1,
        "masked_elements": [
            "Inversion"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.05352v1_figure_1.png",
        "paperid": "2407.05352v1",
        "paper_path": "./papers/2407.05352v1.json",
        "figure_id": "2407.05352v1_figure_1.png",
        "caption": "Figure 1: A comparison between the previous fully-supervised PNG paradigm with our proposed Zero-Shot Diffusion-based Paradigm. Motivated by the strong image-text alignment of text-to-image diffusion models, we employ these generative models in our PNG task through a zero-shot manner. This is aimed at exploring the ability of diffusion models to perform phrase-level grounding.",
        "qtype": "Others",
        "response": "Here’s how I reason about the question step by step:\n\n1.  “Mask1” in the figure is the red‐boxed “Inversion” module.  This block takes the real image and “inverts” it into a latent noise code (via DDIM/Null‐text inversion) that the diffusion U-Net can then denoise under text guidance.  \n2.  All of our downstream localization—cross‐attention anchor point selection, self‐attention refinement, SAM matching, and final mask binarization—are built on top of the feature maps that come out of that inverted latent.  \n3.  If the inversion is imperfect (for example, it doesn’t exactly reconstruct the original image in latent space, distorts small details, or drifts because of randomness in the forward DDPM process), then the cross‐attention maps will be mislocated or noisy.  Anchor points will fall on the wrong pixels or omit fine structures.  Self‐attention aggregation and SAM‐based refinement will then have the wrong starting point, leading to under– or over–segmented masks.  \n4.  In short, the entire zero-shot grounding pipeline becomes **brittle**: any error in the inversion step propagates through the attention maps and ultimately caps the best segmentation quality you can hope to achieve.  \n5.  A further practical drawback is that Null-text inversion requires an iterative optimization per image (to keep the reconstructed image faithful), which adds latency and makes the method less scalable.  \n\nAnswer:  \nRelying heavily on the inversion module means that any drift or artifacts in that reconstructed latent noise directly corrupt the attention maps used for phrase localization. Inversion errors misplace attention anchors, degrade mask refinement, and thus impose a hard ceiling on the accuracy of every subsequent DiffPNG step—plus the inversion optimization itself is slow and may fail to perfectly preserve fine details."
    },
    {
        "question": "What alternative approaches could mitigate biases in frozen Text Encoder and Phrase Kernel representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Text Encoder",
            "Phrase Kernel"
        ],
        "id": 1770,
        "masked_question": "What alternative approaches could mitigate biases in frozen [mask1] and Phrase Kernel representations?",
        "masked_number": 1,
        "masked_elements": [
            "Text Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.05352v1_figure_1.png",
        "paperid": "2407.05352v1",
        "paper_path": "./papers/2407.05352v1.json",
        "figure_id": "2407.05352v1_figure_1.png",
        "caption": "Figure 1: A comparison between the previous fully-supervised PNG paradigm with our proposed Zero-Shot Diffusion-based Paradigm. Motivated by the strong image-text alignment of text-to-image diffusion models, we employ these generative models in our PNG task through a zero-shot manner. This is aimed at exploring the ability of diffusion models to perform phrase-level grounding.",
        "qtype": "Others",
        "response": "Here is a sketch of how one might go about “de‐biasing” the frozen CLIP text encoder (the red‐boxed “Text Encoder” in Fig. 1) and the derived Phrase-Kernel features:\n\n1.   Parameter-efficient downstream adaptation  \n     •  Adapter- or LoRA-style modules.  Leave the bulk of CLIP frozen, but insert small trainable adapter–layers in the transformer.  During PNG fine-tuning you only update those adapters, which lets you specialize your phrase embeddings to the PNG domain without overfitting or losing CLIP’s generality.  \n     •  Prompt- or prefix-tuning.  Rather than freezing the text encoder outright, prepend a handful of learnable “soft tokens” to each noun‐phrase input.  Those soft tokens get trained on a small PNG surrogate corpus (even synthetic or self‐supervised) so that the final embedding space better spreads out under‐represented classes.\n\n2.   Lightweight projection or calibration on Phrase-Kernels  \n     •  Train a small MLP on top of the frozen phrase kernels to re‐scale or re‐align them.  You can do this with a handful of in‐domain image–text pairs (or even unpaired captions) and a contrastive or margin‐based loss to push apart embeddings of confusable phrases.  \n     •  Global mean–subtraction or temperature scaling across the set of noun-phrase kernels in one image (or batch) to remove “hub” effects, so that common phrases don’t dominate the similarity scores.\n\n3.   Contrastive or self-supervised refinement  \n     •  Mine pseudo-positives via the diffusion cross-attention during denoising: treat high-attention regions as “positive” and use them to further fine-tune the frozen text encoder (via adapters) with a light contrastive loss.  \n     •  Use unlabeled PNG images and their long captions to do a self-training loop: generate masks with DiffPNG, treat confident masks as pseudo-labels, and back-prop through a tiny tuning head that sits atop the phrase kernel.\n\n4.   Data-centric debiasing  \n     •  Augment under-represented phrases by synonym or paraphrase generation, then re-compute phrase kernels so that the embedding space does not collapse multiple hyponyms onto the same point.  \n     •  Explicitly re-weight low-frequency noun phrases in your similarity computation (e.g. give higher weight to the rare kernels during the “Locate” step), or use adversarial re-weighting to push the model away from its prior distribution over CLIP concepts.\n\n5.   Ensembling / Multi-encoder fusion  \n     •  Instead of relying on a single frozen CLIP encoder, encode each phrase with two different text backbones (e.g. CLIP ViT-B and CLIP ViT-L).  Fuse their kernels or take a minimum/maximum over their affinities to reduce individual backbone biases.  \n\nAll of these strategies share the same goal: preserve the zero-shot, frozen backbone paradigm as much as possible, but introduce a small, focused “debiasing” or “domain‐adaption” component—whether it be adapters, prompt tokens, light projections, contrastive refinement, or data re-weighting—in order to mitigate the skew in the original CLIP embeddings and make the Phrase-Kernel space more uniformly discriminative."
    },
    {
        "question": "What limitations stem from quantization codebook size selection on capturing diverse semantics?",
        "relevant_section_ids": [
            "5"
        ],
        "relevant_context": [
            "As shown in Tab. 5, an incorrect choice of codebook size can result in the codes failing to learn distinct semantic concepts or capturing irrelevant details."
        ],
        "final_answer": "If the codebook size is chosen poorly, the quantized codes may fail to learn distinct semantic concepts or may instead capture irrelevant details, undermining the model’s ability to represent diverse semantics.",
        "relevant_elements": [
            "codebook",
            "quantization"
        ],
        "id": 1771,
        "masked_question": "What limitations stem from quantization [mask1] size selection on capturing diverse semantics?",
        "masked_number": 1,
        "masked_elements": [
            "codebook"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.01801v2_figure_1.png",
        "paperid": "2411.01801v2",
        "paper_path": "./papers/2411.01801v2.json",
        "figure_id": "2411.01801v2_figure_1.png",
        "caption": "Figure 1: \nThe overall pipeline of our framework. A top-down pathway is introduced into slot attention to utilize top-down information. The pathway consists of two parts: bootstrapping top-down knowledge and exploiting them. Firstly, semantic information is bootstrapped from slot attention outputs by mapping slots to discrete codes from a learned codebook through vector quantization. Secondly, slot attention is modulated using these codes and its attention maps, transforming it into a self-modulating module. Inner activations are modulated across channels with codes and across space with centered attention maps. Slot attention is then repeated with these modulated activations, yielding more representative slots.",
        "qtype": "Others",
        "response": "Here’s the reasoning step by step:\n\n1. In our top-down pathway (Fig. 1, red box), we quantize each slot into a discrete entry in a learned codebook.  \n2. The number of entries in that codebook—the “codebook size”—is a hyperparameter we must choose before training.  \n3. If the codebook is too small, very different objects or parts of the scene are forced to share the same code → the model can’t distinguish distinct semantic concepts.  \n4. If the codebook is too large, it may devote codes to insignificant or noisy details, fragmenting what should be a single semantic concept.  \n5. Either extreme hurts the model’s ability to capture a rich, diverse set of meaningful semantics.  \n\nAnswer: The choice of codebook size directly limits semantic diversity. A codebook that’s too small will force disparate concepts into the same code (under-clustering), while one that’s too large will splinter real concepts into trivial or noisy codes (over-clustering), preventing the model from learning clean, semantically distinct representations."
    },
    {
        "question": "What ethical concerns arise from using discrete codebook semantics to modulate slot attention in surveillance imagery?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "codebook",
            "slot attention"
        ],
        "id": 1772,
        "masked_question": "What ethical concerns arise from using discrete [mask1] semantics to modulate [mask2] in surveillance imagery?",
        "masked_number": 2,
        "masked_elements": [
            "codebook",
            "slot attention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.01801v2_figure_1.png",
        "paperid": "2411.01801v2",
        "paper_path": "./papers/2411.01801v2.json",
        "figure_id": "2411.01801v2_figure_1.png",
        "caption": "Figure 1: \nThe overall pipeline of our framework. A top-down pathway is introduced into slot attention to utilize top-down information. The pathway consists of two parts: bootstrapping top-down knowledge and exploiting them. Firstly, semantic information is bootstrapped from slot attention outputs by mapping slots to discrete codes from a learned codebook through vector quantization. Secondly, slot attention is modulated using these codes and its attention maps, transforming it into a self-modulating module. Inner activations are modulated across channels with codes and across space with centered attention maps. Slot attention is then repeated with these modulated activations, yielding more representative slots.",
        "qtype": "Others",
        "response": "Here’s how I arrive at the answer:\n\n1.  Identify the masks in the figure and text  \n    -  [mask1] is the red‐boxed module that builds and stores a finite codebook of discrete embeddings (“semantic codes”) via vector quantization.  \n    -  [mask2] is the blue‐boxed slot‐attention block whose feature aggregation is being modulated by those discrete codes.  \n\n2.  Restate the question in those terms  \n    “What ethical concerns arise when one uses discrete codebook semantics to modulate slot attention in surveillance imagery?”  \n\n3.  Think through what happens in that set-up  \n    -  The system learns a small vocabulary of “what” each object is (people, cars, etc.) entirely unsupervised.  \n    -  It then uses these learned discrete labels to re-weight or focus the attention mechanism on certain regions (“where”) and feature-channels (“what”).  \n    -  When applied to surveillance feeds, it can automatically pick out and track entities of interest, cluster them into semantic groups, refine their masks and potentially feed those results into further recognition or identification pipelines.  \n\n4.  Enumerate the ethical hazards  \n    a)  Privacy and consent  \n       •  Individuals can be automatically detected, segmented, and tracked without any form of consent.  \n       •  Even “anonymous” silhouette maps can be reverse-engineered to re-identify people.  \n    b)  Profiling and discrimination  \n       •  The discrete codes may latch on to sensitive attributes (e.g. gender, race, age, clothing style) and group people in ways that entrench bias.  \n       •  Downstream use could result in unfair targeting or differential treatment (for policing, insurance, lending, etc.).  \n    c)  Opacity and accountability  \n       •  Once semantics are bootstrapped unsupervised, there is no human-readable explanation for why certain codes fire on certain people or objects.  \n       •  Errors (mis-assignments, false positives) are hard to debug and audit.  \n    d)  Chilling–effect and power imbalance  \n       •  Widespread deployment encourages constant monitoring and self-censorship in public spaces.  \n       •  Gives disproportionate power to whoever controls the model and collected data.  \n\nAnswer  \nUsing discrete codebook semantics to re-scale and guide slot attention in surveillance imagery raises a host of ethical concerns—most notably:  \n\n• Privacy violations and non-consensual tracking,  \n• Automated profiling (and the risk of encoding or amplifying biases in race, gender, clothing, etc.),  \n• Opaqueness in why or how people are being detected and grouped,  \n• Difficulty of audit or redress when errors occur, and  \n• Potential chilling effects from pervasive, “always-on” monitoring."
    },
    {
        "question": "Why quantize slots via codebook before MLP-driven channel modulation?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Firstly, we extract the “what” information from the slots s using Vector Quantization (VQ), which maps each slot to one of the semantic concepts learned throughout training. Specifically, each slot s_i is mapped to the nearest code in a finite codebook E with size K. The mapped code z_i is considered a top-down semantic cue for the slot s_i. During training, the codebook learns to store distinct semantic patterns recurring within the dataset by quantizing continuous slot embeddings into a limited number of discrete embeddings. Thereby, each code can act as automatically discovered top-down semantic information. (Sec. 3.2)",
            "For predicting channel-wise modulation vector γ_k, quantized slot z_k is used, which tells us “what” the object appearing in the image is. The channel-wise scaling is designed to enforce the model to focus on certain feature subspaces closely correlated to the semantic concept identified. (Sec. 3.3)"
        ],
        "final_answer": "The slots are vector-quantized into discrete codes so that each slot is converted into a stable semantic cue (‘what’ the object is). Feeding these discrete codes to the MLP lets it produce channel‐wise modulation factors specifically tuned to the discovered semantic class, ensuring the model focuses on feature channels most relevant to that object concept.",
        "relevant_elements": [
            "Quantization",
            "Channel Modulation",
            "MLP"
        ],
        "id": 1773,
        "masked_question": "Why [mask1] slots via codebook before MLP-driven channel modulation?",
        "masked_number": 1,
        "masked_elements": [
            "Quantization"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.01801v2_figure_1.png",
        "paperid": "2411.01801v2",
        "paper_path": "./papers/2411.01801v2.json",
        "figure_id": "2411.01801v2_figure_1.png",
        "caption": "Figure 1: \nThe overall pipeline of our framework. A top-down pathway is introduced into slot attention to utilize top-down information. The pathway consists of two parts: bootstrapping top-down knowledge and exploiting them. Firstly, semantic information is bootstrapped from slot attention outputs by mapping slots to discrete codes from a learned codebook through vector quantization. Secondly, slot attention is modulated using these codes and its attention maps, transforming it into a self-modulating module. Inner activations are modulated across channels with codes and across space with centered attention maps. Slot attention is then repeated with these modulated activations, yielding more representative slots.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1. In the red‐boxed region the raw slot vectors sᵢ are being “quantized” by looking up the nearest code in a learned codebook of size K.  \n2. The result, zᵢ = Codebook.quantize(sᵢ), is a discrete semantic prototype rather than the full noisy, continuous slot embedding.  \n3. That discrete code zᵢ is then fed into the small MLP that produces the channel-wise scaling vector γᵢ.  \n4. If we instead used the unquantized slot sᵢ, the MLP would see highly variable continuous embeddings, making its learned channel gains unstable and entangling too many low-level details.  \n5. By first snapping to a finite set of learned “semantic” codes, we (a) force the model to cluster recurring object‐level patterns into discrete prototypes and (b) give the MLP a clean, semantically grounded input for computing which feature-channels to boost or suppress.  \n\nAnswer  \nWe quantize slots through the codebook so as to collapse the continuous, noisy slot embeddings into a small set of stable, semantically meaningful prototypes.  Those discrete codes then serve as reliable “what” cues for the downstream MLP to compute channel-wise modulation."
    },
    {
        "question": "What is the motivation behind integrating Edge Model and Segment Model for microstructural feature extraction?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "We develop a comprehensive research model that spans from image analysis to microstructural characterization and material performance.",
            "Addressing the specific characteristics in metallographic images, this paper introduces a multitask automated image segmentation model within the realm of computer science. The model includes a deep learning–based multi-stage metallographic grain boundary detection model and a second-phase extraction model.",
            "For the second-phase metallographic images, a deep learning–based multiscale fusion segmentation model is utilized to extract the microstructure of the second phase. Subsequently, the average size of Mg alloy grains and the average area and equivalent circular diameter of the second phase are computed based on the extracted grain boundaries and microstructure."
        ],
        "final_answer": "The Edge Model (grain boundary detector) and the Segment Model (second-phase extractor) are integrated so that all critical microstructural features – both grain boundaries for measuring grain size and dispersed second-phase particles for measuring area fraction and particle size – can be automatically and accurately extracted in one unified framework. This comprehensive feature extraction is essential for linking microstructure to Vickers hardness in Mg-Gd alloys.",
        "relevant_elements": [
            "Edge Model",
            "Segment Model"
        ],
        "id": 1775,
        "masked_question": "What is the motivation behind integrating [mask1] and Segment Model for microstructural feature extraction?",
        "masked_number": 1,
        "masked_elements": [
            "Edge Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.20402v1_figure_1.png",
        "paperid": "2410.20402v1",
        "paper_path": "./papers/2410.20402v1.json",
        "figure_id": "2410.20402v1_figure_1.png",
        "caption": "Figure 1: Overall framework of this study (revealing the relationship between material images, material microstructure, and material properties).",
        "qtype": "Design_Rationale",
        "response": "The little blue “Edge Model” block that’s been boxed in red is nothing more exotic than a dedicated grain‐boundary detector – i.e. a small CNN whose convolutional kernels have been “preloaded” (via pixel–difference filters) to act like edge operators – and the orange “Segment Model” is a separate CNN that is trained to pick out the second‐phase particles as blobs.  We stitch the two together simply because a single network cannot do both jobs well:  \n\n 1.  Grain boundaries in metallographic images look like very thin, high‐frequency lines and are best recovered by an edge‐specialized model.  \n 2.  Second‐phase precipitates are extended regions of different texture or gray‐level and are best recovered by a region‐based segmentation network.  \n\nBy combining the two, we get a fully automated, end-to-end pipeline that (a) finds every grain boundary, (b) locates every second-phase inclusion, and (c) from those two outputs computes grain size, phase area, equivalent diameters, etc., without any manual parameter-tuning.  That, in turn, lets us build much more accurate structure–property models (e.g. for Vickers hardness) than either approach alone could provide."
    },
    {
        "question": "What is the motivation behind combining Experiments Data and Literatures Data for model generalization?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "Furthermore, to compensate for the lack of experimental data and improve the model performance, we collected microstructural images and performance data of Mg-Gd alloys with different Gd contents from relevant literature to supplement our dataset."
        ],
        "final_answer": "They combine experimental data with literature data in order to overcome the scarcity of their own experimental images and measurements and thus enrich the training set, which in turn improves the model’s performance and generalization capability.",
        "relevant_elements": [
            "Experiments Data",
            "Literatures Data"
        ],
        "id": 1776,
        "masked_question": "What is the motivation behind combining [mask1] and Literatures Data for model generalization?",
        "masked_number": 1,
        "masked_elements": [
            "Experiments Data"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.20402v1_figure_1.png",
        "paperid": "2410.20402v1",
        "paper_path": "./papers/2410.20402v1.json",
        "figure_id": "2410.20402v1_figure_1.png",
        "caption": "Figure 1: Overall framework of this study (revealing the relationship between material images, material microstructure, and material properties).",
        "qtype": "Design_Rationale",
        "response": "The thing in the red box is simply our own “Experiments Data.”  We deliberately fuse that with Mg–Gd data mined from the literature because our experimentally‐produced set by itself is quite small and narrowly covers only one alloy composition (Mg–2.1 at.% Gd).  By bringing in published micrographs and hardness measurements for other Gd levels and processing variants, we both enlarge the training set and expose the network to a much wider spread of compositions and microstructural motifs.  That diversity is exactly what lets the model learn the true, general relationships among Gd content, grain/second‐phase features and hardness, rather than overfitting to our limited lab data."
    },
    {
        "question": "How does the Edge Model handle multi-scale feature extraction to generate the Grain Result map?",
        "relevant_section_ids": [
            "2.2.1"
        ],
        "relevant_context": [
            "The architecture of the model consists of three stages, where each stage captures image features at different scales by incorporating the three pixel-wise difference methods.",
            "Additionally, to further optimize the feature maps, a multi-scale adaptive feature refinement module (MSAFR) is connected after the last residual block of each stage. This module is primarily composed of three main layers: the Compact Part Convolution Module (CPCM), which uses convolution kernels of various sizes and dilation rates to enrich multi-scale edge information, and the Large Kernel Attention (LKA) module, which combines depth-wise and dilated convolutions followed by a 1×1 convolution to fuse local and global features.",
            "Each stage further reduces the feature volume to a single-channel map through a 1×1 convolutional layer, which is then interpolated to the original size and passed through a Sigmoid function. The final edge map (Grain Result) is obtained by concatenating these three single-channel maps, applying further convolution, and a final Sigmoid activation."
        ],
        "final_answer": "The Edge Model is built as a three-stage network where each stage uses three different pixel-wise difference convolutions (CPDC, APDC, RPDC) to extract edge features at fine to coarse scales. After each stage, a Multi-Scale Adaptive Feature Refinement (MSAFR) module—comprising a multi-kernel, multi-dilation Compact Part Convolution Module (CPCM) and a Large Kernel Attention (LKA) block—further refines these features. Finally, each stage’s output is reduced to a single channel, upsampled, sigmoid-activated, and the three maps are concatenated and convolved to produce the final Grain Result edge map.",
        "relevant_elements": [
            "Edge Model",
            "Grain Result"
        ],
        "id": 1777,
        "masked_question": "How does the [mask1] handle multi-scale feature extraction to generate the Grain Result map?",
        "masked_number": 1,
        "masked_elements": [
            "Edge Model"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.20402v1_figure_1.png",
        "paperid": "2410.20402v1",
        "paper_path": "./papers/2410.20402v1.json",
        "figure_id": "2410.20402v1_figure_1.png",
        "caption": "Figure 1: Overall framework of this study (revealing the relationship between material images, material microstructure, and material properties).",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in the top branch is the deep “Edge Model” that takes a metallographic micrograph (the Grain Image) and produces the Grain Result map (i.e. a binary map of grain boundaries).  Here is how it pulls in features at multiple scales:\n\n1. Pixel-wise Difference Convolutions (PDC) in a three-stage encoder  \n   – Instead of ordinary convolutions, each convolutional block computes differences between pairs of pixels in its receptive field (three flavours of PDC: center-pixel, clockwise pairs, outer–inner rings).  \n   – This already biases the network toward detecting abrupt intensity changes (edges) rather than texture.  \n   – The model is organized into three sequential stages; each stage has four residual blocks (PDC → ReLU → 1×1 conv).\n\n2. Multi-Scale Adaptive Feature Refinement (MSAFR) after each stage  \n   – Immediately after each of the three stages, an MSAFR module refines the feature maps.  \n   – Within MSAFR:  \n     • A Compact Part Convolution Module (CPCM) applies several convolution kernels of different sizes and dilation rates in parallel to capture both fine (small kernel, low dilation) and coarse (large kernel, high dilation) edge context.  \n     • A Large Kernel Attention (LKA) block then applies depth-wise conv for local detail, dilated depth-wise conv for long-range context, and a final 1×1 conv to fuse channels and re-weight features.  \n\n3. Deep supervision and multi-level fusion  \n   – At the end of each of the three stages, a 1×1 convolution squashes the multichannel feature map down to a single–channel “side–output,” which is upsampled back to the original image size and passed through a sigmoid to form an edge‐probability map.  \n   – All three side–outputs are concatenated, run through a small convolutional fusion head and a final sigmoid to produce the final Grain Result map.  \n   – Using dice‐loss at each side‐output enforces that every scale learns to predict accurate edges, and the fusion of all three ensures both fine and coarse grain boundaries are captured."
    },
    {
        "question": "How does the Regression Model utilize Feature Information to fuse composition and microstructural features?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "In this study, we use the atomic percentage of Gd, grain size, second phase area fraction, and second phase particle size as input features, with Vickers hardness as the output feature.",
            "First, the linear mapping layer of the encoder converts the input 4-dimensional features (including Gd atomic percentage, grain size, second-phase area fraction, and second-phase particle size) into a 64-dimensional high-level feature representation. Then, the self-attention mechanism is used to weight these features, where the Q, K, and V feature vectors are utilized for self-attention calculation.",
            "Through the multi-head attention mechanism (with 4 attention heads), the model is able to capture diverse features from different perspectives within the data, integrating composition and microstructural information into a unified representation, which is then decoded by a fully-connected layer to predict Vickers hardness."
        ],
        "final_answer": "The regression model first concatenates composition (Gd atomic percentage) and microstructural measurements (grain size, second-phase area fraction, second-phase particle size) into a four-dimensional feature vector. This vector is linearly mapped to a 64-dimensional space, then passed through a multi-layer Transformer encoder that uses self-attention (with 4 heads) to learn and weight interactions among all features. Finally, a fully connected decoder converts the fused high-dimensional representation into the predicted Vickers hardness.",
        "relevant_elements": [
            "Regression Model",
            "Feature Information"
        ],
        "id": 1778,
        "masked_question": "How does the [mask1] utilize [mask2] to fuse composition and microstructural features?",
        "masked_number": 2,
        "masked_elements": [
            "Regression Model",
            "Feature Information"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.20402v1_figure_1.png",
        "paperid": "2410.20402v1",
        "paper_path": "./papers/2410.20402v1.json",
        "figure_id": "2410.20402v1_figure_1.png",
        "caption": "Figure 1: Overall framework of this study (revealing the relationship between material images, material microstructure, and material properties).",
        "qtype": "Implementation_Details",
        "response": "Here’s the step-by-step of what’s happening in the red box (our regression/prediction model) and how it “plugs into” the blue box (the composition + microstructure feature table) to fuse the two modalities:\n\n1.  Mask2 (blue box) is nothing more than a small feature table for each alloy sample.  In each column you see:  \n    –    Gd at % (composition)  \n    –    Grain size (microstructure)  \n    –    Second-phase area fraction (microstructure)  \n    –    Second-phase particle size (microstructure)  \n\n2.  Mask1 (red box) is the regression model you train to predict Vickers hardness.  To do so, you take the four numbers from Mask2 for each sample and feed them in as a single 4-dimensional vector.\n\n3.  Internally, that 4-D vector is first linearly embedded into a higher-dimensional space (64-D in our case), so that the model has enough capacity to learn nonlinear interactions.\n\n4.  A self-attention (Transformer) encoder then “looks at” all four components together, learns how changes in Gd content trade off against changes in grain size or second-phase metrics, and thus builds a fused latent representation that jointly carries both composition and microstructure information.\n\n5.  Finally, a small fully-connected decoder reads out Vickers hardness from that fused representation.\n\nIn short, the regression block in the red box simply takes the composition + microstructure features listed in the blue table, embeds them, lets a self-attention layer learn their cross-dependencies, and then predicts hardness—thereby accomplishing an explicit fusion of atomic‐percentage and image-derived structural descriptors."
    },
    {
        "question": "How does Image-Level Intervention adjust attention weights to enhance global visual information?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "According to the trusted activation A_trusted and the untrusted activation A_untrusted obtained from all sample pairs, we can calculate the following activation shift vector Δ that encourages the model to pay more attention to visual information, as follows: (equation for Δ).",
            "Next, we train a binary classifier f_image using M sample pairs for each head to detect which heads encode Image-Level information, specifically those that can better distinguish the differences between pairs of trusted and untrusted samples. We then apply activation interventions to these selected heads: ˜A_{l,h} = A_{l,h} + λ·I[h ∈ S_image]·Δ_{l,h}, where I[h ∈ S_image] is 1 if head h was selected by the classifier and 0 otherwise, and λ controls the intervention intensity."
        ],
        "final_answer": "Image-Level Intervention first computes an activation shift vector Δ by averaging the difference between attention activations on unblurred (trusted) and blurred (untrusted) images. It then trains a head-wise binary classifier to select those heads that most strongly encode overall visual content. During the forward pass, it adds the scaled shift vector (λ·Δ) to the activations of only these selected heads, thereby boosting their attention weights on the global visual input.",
        "relevant_elements": [
            "Image-Level Intervention"
        ],
        "id": 1779,
        "masked_question": "How does [mask1] adjust attention weights to enhance global visual information?",
        "masked_number": 1,
        "masked_elements": [
            "Image-Level Intervention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15268v1_figure_1.png",
        "paperid": "2411.15268v1",
        "paper_path": "./papers/2411.15268v1.json",
        "figure_id": "2411.15268v1_figure_1.png",
        "caption": "Figure 1: Comparison between Contrastive Decoding (top) and our proposed ICT (bottom). In the top example, Contrastive Decoding indiscriminately removes both beneficial and detrimental language priors, leading to hallucinations. In contrast, our approach enhances the model’s attention to visual details while preserving useful language priors, allowing it to correctly identify and describe objects in the image.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:  \n1. The red-boxed module in the figure is the Image-Level Intervention.  \n2. During training, for each image–question pair you create a “trusted” version (original image) and an “untrusted” version (whole-image Gaussian blur).  \n3. You run both through the LVLM and, for every attention head, extract its last‐token activation vectors a_trusted and a_untrusted.  \n4. You average over many such pairs to get a per-head activation shift Δ=h̄_trusted–h̄_untrusted, which captures how that head’s attention changes when global visual content is removed.  \n5. A small binary classifier is trained to pick out the heads whose Δ most reliably signal image-level information.  \n6. At inference time you “intervene” by adding a scaled version of Δ to each selected head’s activation:  \n   a′_h = a_h + λ·Δ_h  (only for heads flagged by the classifier).  \n7. This additive shift boosts the attention weights on global visual features (and down-weights reliance on language priors), thereby enhancing the model’s focus on overall image content.  \n\nAnswer:  \nImage-Level Intervention computes, for each head, the average difference in attention activations between original and fully blurred images, selects the heads that most track global visual cues, and then adds that “shift” vector—scaled by a hyperparameter—to those heads’ activations at inference.  This nudges the attention weights toward global visual information."
    },
    {
        "question": "How does Object-Level Intervention modulate attention to emphasize fine-grained object details?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Using both the original image I and the partially blurred image I_b^o, we construct trusted and untrusted data pairs for each sample: (I, Q) and (I_b^o, Q) and analyze the attention activation values across H heads, allowing us to compute an Object-Level activation shift vector δ^{(O)}.",
            "A binary classifier g_O is then trained to identify heads that effectively distinguish trusted from untrusted object-focused samples. Interventions are subsequently applied to the selected heads as follows: h^{l}_i(t) ← h^{l}_i(t) + α · I[g_O(Δ^{(O)},·)] · δ^{(O)}, where α controls the intervention intensity."
        ],
        "final_answer": "Object-Level Intervention first identifies which attention heads encode fine-grained object information by comparing activations on the original image versus a version with the object region locally blurred. It computes an activation shift vector δ^{(O)} that captures how those heads should change to focus on object details, trains a small classifier g_O to pick out those heads, and then, during the forward pass, adds a scaled version of δ^{(O)} to the activations of only the selected heads. This targeted additive shift boosts those heads’ responses to the object region, thereby emphasizing fine-grained object details in the model’s attention.",
        "relevant_elements": [
            "Object-Level Intervention"
        ],
        "id": 1780,
        "masked_question": "How does [mask1] modulate attention to emphasize fine-grained object details?",
        "masked_number": 1,
        "masked_elements": [
            "Object-Level Intervention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15268v1_figure_1.png",
        "paperid": "2411.15268v1",
        "paper_path": "./papers/2411.15268v1.json",
        "figure_id": "2411.15268v1_figure_1.png",
        "caption": "Figure 1: Comparison between Contrastive Decoding (top) and our proposed ICT (bottom). In the top example, Contrastive Decoding indiscriminately removes both beneficial and detrimental language priors, leading to hallucinations. In contrast, our approach enhances the model’s attention to visual details while preserving useful language priors, allowing it to correctly identify and describe objects in the image.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:  \n1. In the red-boxed “Object-Level Intervention” module the model first localizes the target object (via Grounding DINO) and creates two versions of the image: one with that object region intact (trusted) and one with Gaussian noise only over the object (untrusted).  \n2. It runs both versions through the LVLM and records each head’s attention activations for the last token, yielding A_trust and A_untrust.  \n3. By computing the difference ΔA_obj = A_trust – A_untrust, it obtains an object-level activation shift that captures exactly which heads care about fine details of that object.  \n4. A lightweight binary classifier then picks out the heads most sensitive to this shift.  \n5. At inference, the model injects a scaled version of ΔA_obj into those selected heads’ activations (A′ = A + γ·ΔA_obj).  \n\nBy doing so, the intervention “turns up the gain” on the attention heads that focus on the specified object, boosting their response to fine-grained visual cues and thus mitigating omissions or hallucinations about that object."
    },
    {
        "question": "How does Image-Level Intervention differ from Contrastive Decoding’s reliance on blurred input contrast?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "Contrastive decoding: This method alleviates hallucinations without requiring additional training. It induces hallucinations by applying transformations such as blurring, rotation, or cropping to the original visual input. During the decoding stage, tokens associated with these induced hallucinations are penalized, thus mitigating the influence of language priors [43, 107, 10, 14, 81, 69, 86, 36, 63]. However, methods such as VCD often indiscriminately eliminate all language priors, including those that may be beneficial.",
            "This [Image-Level] module aims to identify the attention heads associated with overall image information and to apply targeted interventions to these heads. This approach enhances the model’s focus on visual input while diminishing the influence of language priors.",
            "We progressively add Gaussian noise following the forward diffusion process to obtain the final blurred image. Finally, we construct a modified dataset to obtain Image-Level intervention vectors. ... We then train a binary classifier for each head to detect which heads encode Image-Level information... We then apply activation interventions to these selected heads... After using [the interventions] on heads that encode image information, the model enhances the trustworthiness of the visual level, places greater attention on visual information, thus mitigates the impact of overly strong language priors."
        ],
        "final_answer": "Contrastive decoding uses blurred (or otherwise transformed) images at inference-time to penalize certain tokens during the decoding stage—thereby indiscriminately weakening all language priors when contrastive scores drop. In contrast, Image-Level Intervention also uses blurred versions of the image but only as a means to compute \"activation shift\" vectors offline. Those shifts are then applied during the forward pass to a carefully selected subset of attention heads, strengthening visual attention without removing beneficial language priors and without any extra decoding-time penalty or latency.",
        "relevant_elements": [
            "Image-Level Intervention",
            "Contrastive Decoding"
        ],
        "id": 1781,
        "masked_question": "How does [mask1] differ from Contrastive Decoding’s reliance on blurred input contrast?",
        "masked_number": 1,
        "masked_elements": [
            "Image-Level Intervention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15268v1_figure_1.png",
        "paperid": "2411.15268v1",
        "paper_path": "./papers/2411.15268v1.json",
        "figure_id": "2411.15268v1_figure_1.png",
        "caption": "Figure 1: Comparison between Contrastive Decoding (top) and our proposed ICT (bottom). In the top example, Contrastive Decoding indiscriminately removes both beneficial and detrimental language priors, leading to hallucinations. In contrast, our approach enhances the model’s attention to visual details while preserving useful language priors, allowing it to correctly identify and describe objects in the image.",
        "qtype": "Literature_Background",
        "response": "Here’s a step-by-step comparison showing how the red-boxed “Image-Level Intervention” in our method differs from the blurred-input trick used in Contrastive Decoding:\n\n1. Where blurring happens  \n   - Contrastive Decoding: At **decoding time.** You take the already fused visual+language representation, feed it two versions of the image (clean vs. blurred), get two sets of next‐token scores, and then subtract/penalize any token that the blurred version still “likes.”  \n   - Image-Level Intervention (ours): During the **forward pass**. We add Gaussian noise to the *whole* image up front, pass it once through the visual encoder, and capture how each attention head’s activations shift compared to the clean image.\n\n2. What you do with the blur signal  \n   - Contrastive Decoding: You use the blurred‐image logits purely as a *negative* signal. Any token that survives the blur check is considered “hallucination,” so you down-weight *all* language priors indiscriminately.  \n   - Image-Level Intervention: You compute an *activation shift vector* from clean vs. fully blurred images for each head—this tells you which heads carry strong image cues. You then *add* (intervene) these pre-computed shift vectors back into those heads’ activations, **boosting** their attention to visual content rather than throwing away the language signal.\n\n3. What gets thrown away (or preserved)  \n   - Contrastive Decoding: Because it simply subtracts away whatever the blurred model “knows,” it can wipe out even *useful* language priors (e.g. knowing that “Curry” is often a basketball player).  \n   - Image-Level Intervention: Leaves all language priors intact. It merely *re-balances* the model’s internal focus by nudging the heads that encode global image information to pay more attention to the pixels, reducing over-reliance on language without wholesale removal of helpful background knowledge.\n\n4. Impact on latency and training  \n   - Contrastive Decoding: Introduces extra compute at inference time (you score two images per step).  \n   - Image-Level Intervention: Pre-computes shift vectors once, then applies them in a single forward pass—no extra decoding steps and no fine-tuning of model weights.\n\nIn short, the red-boxed Image-Level Intervention uses blurred images as a *diagnostic tool* to learn how each attention head responds to visual degradation, then *intervenes* to strengthen those heads on clean inputs—whereas Contrastive Decoding simply uses a blurred run to *subtract away* any signal it attributes to the image, often discarding useful language priors in the process."
    },
    {
        "question": "How does Object-Level Intervention extend prior analyses of attention heads’ granularity?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "According to prior research [80, 19, 8], which has shown that different heads in the multi-head attention mechanism encode information at varying levels of granularity, we train binary classifiers for each head to determine which heads encode overall visual information and which capture detailed visual features.",
            "After enhancing the model’s trustworthiness at the Image-Level, a more fine-grained, Object-Level intervention becomes necessary to increase the model’s attention to image details, thereby reducing hallucinations caused by the omission of fine details.",
            "We use Grounding DINO [57] to identify the area of object O in image I. Gaussian noise is then added selectively to this object region ... Using both the original image I and the partially blurred image I'', we construct trusted and untrusted data pairs for each sample ... A binary classifier f^O is then trained to identify heads that effectively distinguish trusted from untrusted object-focused samples. Interventions are subsequently applied to the selected heads."
        ],
        "final_answer": "Prior work identified attention heads that encode either overall (coarse) image information or more detailed visual features. Object-Level Intervention builds on this by pushing the granularity analysis even further: it isolates heads that respond specifically to the fine-grained, object-region details. This is done by selectively blurring the target object region, measuring how each head’s activation shifts, training a per-head classifier to detect object-level signals, and then intervening on those heads to enhance focus on object-specific visual cues.",
        "relevant_elements": [
            "Object-Level Intervention",
            "attention heads"
        ],
        "id": 1782,
        "masked_question": "How does [mask1] extend prior analyses of attention heads’ granularity?",
        "masked_number": 1,
        "masked_elements": [
            "Object-Level Intervention"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.15268v1_figure_1.png",
        "paperid": "2411.15268v1",
        "paper_path": "./papers/2411.15268v1.json",
        "figure_id": "2411.15268v1_figure_1.png",
        "caption": "Figure 1: Comparison between Contrastive Decoding (top) and our proposed ICT (bottom). In the top example, Contrastive Decoding indiscriminately removes both beneficial and detrimental language priors, leading to hallucinations. In contrast, our approach enhances the model’s attention to visual details while preserving useful language priors, allowing it to correctly identify and describe objects in the image.",
        "qtype": "Literature_Background",
        "response": "Chain of thought:\n\n1. Prior work (e.g. [80,19,8]) showed that different attention heads specialize at different “resolutions” of visual input—some capture coarse, whole‐image context, others pick up on finer details.  \n2. In our framework we first operationalize that observation with an Image-Level intervention (blur the entire image, compute per‐head activation shifts, train a tiny classifier to flag “image‐level” heads, then boost those heads’ activations).  \n3. The red‐boxed module—Object-Level Intervention—takes that idea one step further: instead of blurring the entire image, it uses Grounding DINO to localize the bounding box of the queried object, then selectively blurs just that region.  \n4. By comparing activations on the clean versus object‐blurred pairs, we compute an Object-Level shift vector for each head and train a second binary head-classifier that flags which heads are sensitive to object-specific detail.  \n5. During inference we then selectively up-shift only those “object‐sensitive” heads, steering the model’s attention toward fine‐grained, per‐object features without throwing away useful language priors.  \n\nAnswer:\n\nThe Object-Level Intervention extends prior granularity analyses by drilling down from whole-image versus no-image distinctions to perturbations confined to individual object regions. By locally blurring only the target object, computing per‐head activation shifts, and training a head-wise classifier to identify “object‐detail” specialists, it isolates and amplifies heads that truly encode fine-grained object semantics—thereby pushing the granularity analysis from global image-level to per-object level."
    },
    {
        "question": "How does cross-modal ranking consistency augment gene-image contrastive loss relative to traditional contrastive learning?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "The InfoNCE loss ensures local alignment between image and gene features from the same tissue spot, but it does not address global alignment, which is essential for achieving more accurate and consistent cross-modal correspondences. Directly aligning distances between features from distant tissue spots is not practical, as long-range feature relationships may not be reliable. (Section 3.2)",
            "Instead, we propose that the relative ranking of distances between features is more robust and can provide a more trustworthy basis for alignment. To leverage this idea, we introduce the Cross-Modal Ranking Consistency Loss. This loss function encourages the model to learn image representations while maintaining the relative similarity ordering of gene features across tissue spots. By focusing on the ranking of distances rather than exact alignments, the ranking loss facilitates a more reliable and robust global alignment. It complements the local alignment achieved by InfoNCE, while also capturing long-range interactions between features from different tissue spots. (Section 3.2)"
        ],
        "final_answer": "While the gene–image contrastive loss (InfoNCE) enforces only local, spot‐wise alignment (pulling matched image–gene pairs together and pushing unmatched pairs apart), the cross‐modal ranking consistency loss adds a global alignment constraint.  Instead of matching exact similarity values across distant spots—which can be noisy—it enforces that the relative order of similarities between any given spot and all others is consistent in both the gene and image feature spaces.  This ranking‐based constraint complements the local pull‐and‐push of contrastive learning by capturing robust, long‐range relationships and ensuring that the similarity rankings in one modality are faithfully reflected in the other.",
        "relevant_elements": [
            "cross-modal ranking consistency",
            "gene-image contrastive loss"
        ],
        "id": 1783,
        "masked_question": "How does [mask1] augment gene-image contrastive loss relative to traditional contrastive learning?",
        "masked_number": 1,
        "masked_elements": [
            "cross-modal ranking consistency"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.15076v1_figure_2.png",
        "paperid": "2411.15076v1",
        "paper_path": "./papers/2411.15076v1.json",
        "figure_id": "2411.15076v1_figure_2.png",
        "caption": "Figure 2: Overview of our RankbyGene framework. The framework begins with WSI Tiling, where WSIs are divided into tiles, each paired with a gene profile. In the feature extraction part, weak and strong augmentations of the image tiles are processed through a teacher encoder and student encoder, while a gene encoder extracts features from the gene profile. The feature alignment stage ensures that weakly and strongly augmented image features are aligned through intra-modal distillation loss and the image and gene features are aligned using gene-image contrastive loss. Meanwhile, our proposed cross-modal ranking consistency loss maintains consistent similarity ranking orders across two modalities.",
        "qtype": "Literature_Background",
        "response": "Step-by-step reasoning  \n1.  In the diagram the red‐boxed module is labelled “Cross-Model Ranking Consistency.”  \n2.  The standard gene–image contrastive (InfoNCE) loss only cares about pulling each matched (image, gene) pair together and pushing all other pairs apart. This enforces local, pairwise alignment.  \n3.  What it does not enforce is any notion of the global geometry: e.g. if spot A is more similar to spot B than to spot C in gene space, nothing in pure InfoNCE guarantees that will also hold in image space.  \n4.  The cross-model ranking consistency loss remedies that by sampling triplets \\{anchor i, “near” j, “far” k\\} and adding a hinge‐style penalty whenever the ordering of similarities in the two modalities disagrees.  \n5.  In effect it forces the relative ranking of all other spots around any given anchor to be preserved between gene and image embeddings.  \n6.  By piling this ordinal, global‐structure constraint on top of the local InfoNCE loss, the model learns not only to match individual spots but to reproduce the full “shape” of the gene‐space similarity landscape in its image‐space embedding.  \n\nAnswer  \nThe red-boxed module is the cross-modal ranking consistency loss.  It augments the normal gene–image contrastive (InfoNCE) loss by adding an ordinal (hinge) ranking term over triplets of spots so that, for any anchor spot, the order of its similarities to other spots in the image space matches the order in the gene space.  This extra loss preserves global similarity structure and long-range relationships that standard contrastive learning (which only enforces pairwise pulls and pushes) would otherwise ignore."
    },
    {
        "question": "How does intra-modal distillation between teacher encoder and student encoder extend conventional self-supervised distillation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In our framework, we employ a teacher–student network architecture to achieve robust feature representations across differently augmented instances of the same pathology image, drawing on recent advances in self-supervised knowledge distillation for single-modality representation learning [11, 52].",
            "In this setup, the weakly augmented image is processed through the teacher encoder, while the strongly augmented version passes through the student encoder. The weights of the teacher encoder are incrementally updated using an Exponential Moving Average (EMA) of the student encoder’s weights, which helps stabilize the training. This strategy ensures that the student gradually learns stable features over time.",
            "To enforce the consistency between the representations of the two augmented versions, we introduce the Image Consistency Loss: L_distil = 1/N ∑ₙ ‖iₙʷ - iₙˢ‖². Minimizing this loss encourages the image encoder to learn representations resilient to such variations and potential disruptions from gene expression data characteristics, such as high dimensionality, sparsity, noise, and missing values."
        ],
        "final_answer": "Instead of distilling knowledge between different modalities or tasks, our intra-modal distillation applies a classic teacher–student setup purely within the image modality to stabilize the gene-guided alignment. We feed a weakly augmented patch into the teacher encoder and a strongly augmented patch into the student encoder, update the teacher by EMA of the student weights, and then use an image-consistency (distillation) loss to pull the two outputs together. This extends conventional self-supervised distillation by enforcing cross-augmentation consistency in the context of multi-modal alignment, making the learned image features more invariant to augmentation and robust to noise in the gene data.",
        "relevant_elements": [
            "intra-modal distillation",
            "teacher encoder",
            "student encoder"
        ],
        "id": 1784,
        "masked_question": "How does [mask1] between teacher encoder and student encoder extend conventional self-supervised distillation?",
        "masked_number": 1,
        "masked_elements": [
            "intra-modal distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.15076v1_figure_2.png",
        "paperid": "2411.15076v1",
        "paper_path": "./papers/2411.15076v1.json",
        "figure_id": "2411.15076v1_figure_2.png",
        "caption": "Figure 2: Overview of our RankbyGene framework. The framework begins with WSI Tiling, where WSIs are divided into tiles, each paired with a gene profile. In the feature extraction part, weak and strong augmentations of the image tiles are processed through a teacher encoder and student encoder, while a gene encoder extracts features from the gene profile. The feature alignment stage ensures that weakly and strongly augmented image features are aligned through intra-modal distillation loss and the image and gene features are aligned using gene-image contrastive loss. Meanwhile, our proposed cross-modal ranking consistency loss maintains consistent similarity ranking orders across two modalities.",
        "qtype": "Literature_Background",
        "response": "Here, “[mask1]” is the red‐boxed module labeled “Intra-model Distillation.”  In a nutshell, what we are doing is taking the teacher’s embeddings on weakly augmented patches and the student’s embeddings on strongly augmented patches, forming the full N×N similarity matrix between them, and then forcing the student to reproduce the teacher’s entire similarity structure.  \n\nWhy does that extend ordinary self-supervised distillation?  In most momentum-teacher/self-distillation schemes (BYOL, DINO, etc.) you simply pull each student embedding toward its corresponding teacher embedding (i.e. instance-level matching).  Here, by contrast, we match all pairwise similarities between student and teacher embeddings at once.  This has two benefits:  \n 1.  It carries over not just single-view consistency but the teacher’s entire relational geometry among different tissue spots.  \n 2.  It enforces the student to learn the global structure of the teacher’s feature space—making it more stable and robust—rather than merely collapsing toward individual teacher vectors.  \n\nIn short, the red “Intra-model Distillation” box extends conventional self-supervised distillation by (a) using weak vs. strong augmentations in a teacher–student EMA setup and (b) matching the full similarity matrix of teacher ↔ student embeddings, thereby preserving inter‐sample relationships rather than only instance‐by‐instance alignment."
    },
    {
        "question": "How does intra-modal distillation complement gene-image contrastive loss in aligning multimodal features?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "Section 3.1: InfoNCE encourages the model to pull positive pairs (image and gene features from the same spot) closer in the shared latent space while pushing apart negative pairs (image and gene features from different spots). This loss penalizes unmatched pairs by reducing their similarity while increasing the similarity between matched gene-image pairs.",
            "Section 3.3: To enhance stability and invariance in feature embeddings produced by the patch encoder, we apply both weak and strong augmentations to simulate the typical variability found in pathology images. To enforce the consistency between the representations of the two augmented versions, we introduce the Image Consistency Loss: … Minimizing this loss encourages the image encoder to learn representations resilient to such variations and potential disruptions from gene expression data characteristics, such as high dimensionality, sparsity, noise, and missing values."
        ],
        "final_answer": "The gene-image contrastive loss aligns image and gene representations by pulling true pairs together and pushing mismatched pairs apart in a shared space. Intra-modal distillation complements this by enforcing consistency within the image modality itself: a teacher-student setup with weak and strong augmentations and an image consistency loss makes image features stable and invariant to perturbations. This robustness in the image encoder’s embeddings strengthens and stabilizes the cross-modal alignment achieved by the contrastive loss.",
        "relevant_elements": [
            "Intra-Modal Distillation",
            "Gene-Image Contrastive"
        ],
        "id": 1785,
        "masked_question": "How does [mask1] complement gene-image contrastive loss in aligning multimodal features?",
        "masked_number": 1,
        "masked_elements": [
            "Intra-Modal Distillation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.15076v1_figure_2.png",
        "paperid": "2411.15076v1",
        "paper_path": "./papers/2411.15076v1.json",
        "figure_id": "2411.15076v1_figure_2.png",
        "caption": "Figure 2: Overview of our RankbyGene framework. The framework begins with WSI Tiling, where WSIs are divided into tiles, each paired with a gene profile. In the feature extraction part, weak and strong augmentations of the image tiles are processed through a teacher encoder and student encoder, while a gene encoder extracts features from the gene profile. The feature alignment stage ensures that weakly and strongly augmented image features are aligned through intra-modal distillation loss and the image and gene features are aligned using gene-image contrastive loss. Meanwhile, our proposed cross-modal ranking consistency loss maintains consistent similarity ranking orders across two modalities.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module in Figure 2 is the intra-modal (or “image consistency”) distillation loss.  In our pipeline it sits alongside the gene–image InfoNCE term, but whereas the latter only pulls matching image–gene pairs together and pushes mismatches apart, the distillation loss forces the student network to reproduce the teacher’s embeddings under strong image augmentations.  In practice this means that  \n  •  the teacher (fed weakly-augmented tiles) and the student (fed strongly-augmented tiles) must agree on the patch representation,  \n  •  the student’s weights are continually smoothed toward the teacher via an EMA update,  \n  •  and any jitter or noise introduced by aggressive visual augmentations is “distilled out” of the final image feature.  \n\nBy regularizing the image encoder to produce stable, invariant embeddings, the intra-modal distillation makes the image side of the contrastive alignment far more robust—so that when the gene–image contrastive loss pulls modalities together, it does so against a smoothed, reliable image feature manifold rather than one that is swinging around under the influence of random augmentations."
    },
    {
        "question": "How does WSI tiling affect the reliability of cross-modal ranking consistency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "WSI Tiling",
            "Cross-Modal Ranking Consistency"
        ],
        "id": 1786,
        "masked_question": "How does [mask1] affect the reliability of cross-modal ranking consistency?",
        "masked_number": 1,
        "masked_elements": [
            "WSI Tiling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.15076v1_figure_2.png",
        "paperid": "2411.15076v1",
        "paper_path": "./papers/2411.15076v1.json",
        "figure_id": "2411.15076v1_figure_2.png",
        "caption": "Figure 2: Overview of our RankbyGene framework. The framework begins with WSI Tiling, where WSIs are divided into tiles, each paired with a gene profile. In the feature extraction part, weak and strong augmentations of the image tiles are processed through a teacher encoder and student encoder, while a gene encoder extracts features from the gene profile. The feature alignment stage ensures that weakly and strongly augmented image features are aligned through intra-modal distillation loss and the image and gene features are aligned using gene-image contrastive loss. Meanwhile, our proposed cross-modal ranking consistency loss maintains consistent similarity ranking orders across two modalities.",
        "qtype": "Experimental_Results",
        "response": "The red box in the illustration is simply drawing your attention to the raw ST inputs – the paired image patches coming off the WSI and their genome‐wide gene‐expression vectors.  In particular, those gene‐expression vectors are extremely high-dimensional, heavily zero-inflated (≈80% zeros) and suffer from spatially correlated noise (all things highlighted in that red outline).  If you were to take those gene vectors at face value and compute absolute cosine–similarities between spots, many of those similarities would be dominated by noise and sparsity rather than by true biological signal, and you would quickly lose any consistent ordering across spots.  \n\nThat is precisely why we cannot trust naïve “distance‐to‐distance” matching and why we fall back on an ordinal (ranking) loss.  By forcing only the relative order of “which spot is more like which” to be the same in image space and in gene space – rather than the exact numerical value of the similarity – we become far more robust to the kinds of distortions (noise, sparsity, dropouts) that the red‐boxed ST data inevitably impose."
    },
    {
        "question": "How does integrating the ODE solver in temporal operation mitigate error accumulation across extrapolation steps?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Temporal Operation. After obtaining the dynamic state derivative, \\(\\dot U\\), the subsequent state \\(U\\) can be computed through numerical integration over different time spans. Due to the numerical instability associated with first-order explicit methods like the Euler method (Gottlieb et al., 2001; Fatunla, 2014), we adopt the neural ordinary differential equation approach (Neural ODE (Chen et al., 2018)), which employs the Runge–Kutta time-stepping strategy to enhance stability.",
            "The computed state \\(U\\) is then recursively fed back into the network as the input for the subsequent time step, continuing this process until the final time step is reached."
        ],
        "final_answer": "By replacing a simple first-order integrator with a Neural ODE that uses higher-order Runge–Kutta time stepping, PAPM achieves much more stable step-to-step integration. This higher-order, more accurate ODE solver keeps per-step errors small and prevents them from compounding rapidly over many extrapolation steps.",
        "relevant_elements": [
            "ODE solver",
            "Temporal operation"
        ],
        "id": 1787,
        "masked_question": "How does integrating the [mask1] in [mask2] mitigate error accumulation across extrapolation steps?",
        "masked_number": 2,
        "masked_elements": [
            "ODE solver",
            "Temporal operation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2407.05232v1_figure_1.png",
        "paperid": "2407.05232v1",
        "paper_path": "./papers/2407.05232v1.json",
        "figure_id": "2407.05232v1_figure_1.png",
        "caption": "Figure 1: Overview of the PAPM s pipeline. The model takes the multiple conditions of process systems for time extrapolation and outputs solutions at an arbitrary time point. The core is the temporal-spatial stepping module (TSSM) (𝑼t=i→𝑼t=i+1)→superscript𝑼𝑡𝑖superscript𝑼𝑡𝑖1(\\bm{U}^{t=i}\\rightarrow\\bm{U}^{t=i+1})( bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i end_POSTSUPERSCRIPT → bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i + 1 end_POSTSUPERSCRIPT ). Spatially, a structure-preserved operation aligns with the specific equation characteristics of different process systems. Temporally, it utilizes a continuous-time modeling framework through an ODE solver.",
        "qtype": "Experimental_Results",
        "response": "The little red box in your figure is nothing mystical – it is simply the ODE‐solver that sits inside the big blue “Temporal–Spatial Stepping Module.”  What that solver gives you, over and above a naïve one‐step Euler update, is\n\n  1.  A continuous‐time model of the dynamics rather than a single, coarse discrete jump.  \n  2.  A higher‐order, multi‐stage integrator (e.g. a Runge–Kutta scheme inside a Neural ODE) that dramatically cuts down the local truncation error at each step.  \n\nBecause each step from Uᵢ to Uᵢ₊₁ is found by solving an ODE rather than by a simple, low‐order finite difference, the per-step error is far smaller.  Since your model is autoregressive – you feed Uᵢ₊₁ back in to predict Uᵢ₊₂ – keeping each step accurate is the only way to prevent errors from snowballing.  In short, embedding an ODE solver in the TSSM:\n\n  •  Enforces a smooth, continuous‐time update  \n  •  Uses a stable, higher‐order integration (Runge–Kutta)  \n  •  Dramatically lowers local errors so they don’t compound on long‐term extrapolation  \n\nAll of this together is what controls and mitigates the drift that you otherwise get if you just marched forward with simple explicit updates."
    },
    {
        "question": "How does structure-preserved spatial operation enforce conservation and constitutive relations under varying boundary and source inputs?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Aligning with the general form of Eq. 1 and Eq. 2, there are four elements corresponding to Diffusive Flows (DF), Convective Flows (CF), Internal Source Term (IST), and External Source Term (EST) in PAPM’s structure diagram, as illustrated in Fig. 2.",
            "1) Embedding BCs. Using the given boundary conditions, the physical quantity \\(U\\) is updated, yielding \\(\\tilde U\\). A padding strategy is employed to integrate four different boundary conditions in four different directions into PAPM.",
            "2) Diffusive Flows (DF). Using \\(\\tilde U\\) and coefficients \\(c\\), we represent the directionless diffusive flow. The diffusion flow and its gradient are obtained as \\(J_D\\) and \\(\\nabla\\cdot J_D\\) via a symmetric gradient operator, respectively.",
            "3) Convective Flows (CF). The pattern \\(\\mathrm{sign}(\\tilde U)\\) is derived from \\(\\tilde U\\). Once the sign is determined, its direction indicates the flow direction, enabling computation of \\(J_C^+\\) and \\(J_C^-\\) through a directional gradient operator.",
            "4) Internal Source Term (IST) & External Source Term (EST). Generally, IST and EST present a complex interplay between physical quantities \\(\\tilde U\\) and external inputs \\(F\\). Often, this part in real systems doesn’t have a clear physics-based relation, prompting the use of NNs to capture this intricate relationship.",
            "5) ODE solver. From DF, CF, IST, and EST, the dynamic \\(\\partial U/\\partial t\\) are derived. By doing so, Eq. 1 can be reduced to an ODE, and the ODE solver is used to approximate the evolving state as \\(U^{t+1}\\)."
        ],
        "final_answer": "The spatial operator in PAPM explicitly preserves the conservation law and constitutive relations by decomposing the PDE into four physics-informed modules. First, boundary conditions are embedded via a padding scheme that updates the field \\(U\\) to \\(\\tilde U\\) under arbitrary boundary inputs. Next, diffusive fluxes \\(J_D\\) and their divergence \\(\\nabla\\cdot J_D\\) are computed from \\(\\tilde U\\) and known coefficients using a symmetric gradient operator, enforcing constitutive diffusion. Convection is handled by extracting the sign of \\(\\tilde U\\) and applying a directional gradient to form convective fluxes \\(J_C^+\\) and \\(J_C^-\\), ensuring mass-flux conservation. Internal and external source effects are then learned via neural networks to capture complex source–state interactions. Finally, these four terms are combined as \\(\\partial U/\\partial t = -\\nabla\\cdot(J_C + J_D) + IST + EST\\), and an ODE solver integrates this in time. This structured decomposition guarantees that, regardless of varying boundary or source inputs, the model always respects the underlying conservation and constitutive relations.",
        "relevant_elements": [
            "Structure-preserved spatial operation",
            "Boundary conditions",
            "External sources"
        ],
        "id": 1788,
        "masked_question": "How does [mask1] enforce conservation and constitutive relations under varying boundary and source inputs?",
        "masked_number": 1,
        "masked_elements": [
            "Structure-preserved spatial operation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2407.05232v1_figure_1.png",
        "paperid": "2407.05232v1",
        "paper_path": "./papers/2407.05232v1.json",
        "figure_id": "2407.05232v1_figure_1.png",
        "caption": "Figure 1: Overview of the PAPM s pipeline. The model takes the multiple conditions of process systems for time extrapolation and outputs solutions at an arbitrary time point. The core is the temporal-spatial stepping module (TSSM) (𝑼t=i→𝑼t=i+1)→superscript𝑼𝑡𝑖superscript𝑼𝑡𝑖1(\\bm{U}^{t=i}\\rightarrow\\bm{U}^{t=i+1})( bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i end_POSTSUPERSCRIPT → bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i + 1 end_POSTSUPERSCRIPT ). Spatially, a structure-preserved operation aligns with the specific equation characteristics of different process systems. Temporally, it utilizes a continuous-time modeling framework through an ODE solver.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed “module” in the picture is nothing mystical – it is simply the physics‐informed spatial operator (the structure‐preserved flux/divergence block) combined with the Neural‐ODE time–stepper.  In practice it enforces conservation laws and constitutive relations in exactly the following way:\n\n 1.  All of the boundary conditions are first “padded in” to form an updated field \\tilde U that already satisfies the prescribed Dirichlet/Neumann constraints at the edges.  \n 2.  From \\tilde U one constructs two physically motivated fluxes J_D and J_C:  \n     •  J_D comes from a symmetric (diffusion) kernel so that J_D≈−D ∇U (Fick’s law, Fourier’s law, etc.).  \n     •  J_C comes from an upwind or directional stencil (a triangular convolutional kernel in the local version or a simple elementwise product in the spectral version) so that J_C≈U v (the usual advective/convective flux).  \n 3.  These two fluxes are summed and passed through a discrete divergence operator, and any internal/external sources q and F are added, producing  \n       ∂U/∂t  = −∇·(J_C + J_D)  + q + F  \n     By building the divergence exactly out of these two fluxes, mass (or energy, etc.) is conserved at the discrete level no matter how the sources or boundaries move around.  \n 4.  Finally, a small neural network may be inserted in parallel to capture any “unknown’’ source terms, but it only ever injects itself as an additive term (never breaks the flux–divergence structure).  \n 5.  That right–hand side, ∂U/∂t, is handed to a Runge–Kutta‐style Neural‐ODE integrator which marches the solution forward in time.  \n\nBecause every step of the data flow—BC embedding, flux construction, divergence, source addition, ODE integration—is built to mirror the continuum conservation law and its constitutive flux laws, the model cannot “learn” a solution that violates mass/energy conservation or the form of the physical law, even as the boundary conditions and source inputs change arbitrarily."
    },
    {
        "question": "What are the limitations of the ODE solver during long-range time extrapolation for complex dynamics?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "ODE solver",
            "Time extrapolation"
        ],
        "id": 1789,
        "masked_question": "What are the limitations of the [mask1] during long-range time extrapolation for complex dynamics?",
        "masked_number": 1,
        "masked_elements": [
            "ODE solver"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2407.05232v1_figure_1.png",
        "paperid": "2407.05232v1",
        "paper_path": "./papers/2407.05232v1.json",
        "figure_id": "2407.05232v1_figure_1.png",
        "caption": "Figure 1: Overview of the PAPM s pipeline. The model takes the multiple conditions of process systems for time extrapolation and outputs solutions at an arbitrary time point. The core is the temporal-spatial stepping module (TSSM) (𝑼t=i→𝑼t=i+1)→superscript𝑼𝑡𝑖superscript𝑼𝑡𝑖1(\\bm{U}^{t=i}\\rightarrow\\bm{U}^{t=i+1})( bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i end_POSTSUPERSCRIPT → bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i + 1 end_POSTSUPERSCRIPT ). Spatially, a structure-preserved operation aligns with the specific equation characteristics of different process systems. Temporally, it utilizes a continuous-time modeling framework through an ODE solver.",
        "qtype": "Others",
        "response": "Here the red‐boxed module is simply the ODE–solver that marches the network’s ∂U/∂t forward in time (in our case with a built‐in Runge–Kutta integrator).  All of the complaints people typically have about “learned ODE” blocks in the wild apply in long‐range extrapolation:\n\n 1. Error accumulation.  Even high‐order schemes like RK will introduce small truncation errors at every step.  When you roll out hundreds or thousands of steps, those tiny defects add up, and the solution can gradually drift or blow up.  \n 2. Stability/stiffness issues.  Many real‐world conservation laws (e.g. turbulent or multi‐scale flows) are stiff or only marginally stable; if your step–size isn’t tiny enough, the integrator can become unstable, and shrinking Δt to recover stability rapidly kills your speed.  \n 3. Step–size sensitivity.  You either choose very small Δt (to keep integration error in check) and pay a heavy computational price, or you accept larger Δt (and see your accuracy collapse).  Balancing that trade‐off—even when the network is otherwise perfect—is very challenging over long horizons.  \n 4. Inability to capture sub‐step dynamics.  The ODE–block only ever “sees” the coarse ∂U/∂t signals you feed it; any dynamics happening inside each time‐step (high–frequency oscillations, sharp fronts, etc.) will be smeared out or entirely missed unless you radically upsample in time.  \n\nIn short, once you push the ODE–solver far beyond the regime it was trained on, small integration errors, stability limits and step‐size trade‐offs conspire to degrade or even derail the extrapolation."
    },
    {
        "question": "Could neural difference schemes augment the temporal-spatial stepping module to reduce reliance on the ODE solver?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "temporal-spatial stepping module",
            "ODE solver"
        ],
        "id": 1790,
        "masked_question": "Could neural difference schemes augment the [mask1] to reduce reliance on the ODE solver?",
        "masked_number": 1,
        "masked_elements": [
            "temporal-spatial stepping module"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2407.05232v1_figure_1.png",
        "paperid": "2407.05232v1",
        "paper_path": "./papers/2407.05232v1.json",
        "figure_id": "2407.05232v1_figure_1.png",
        "caption": "Figure 1: Overview of the PAPM s pipeline. The model takes the multiple conditions of process systems for time extrapolation and outputs solutions at an arbitrary time point. The core is the temporal-spatial stepping module (TSSM) (𝑼t=i→𝑼t=i+1)→superscript𝑼𝑡𝑖superscript𝑼𝑡𝑖1(\\bm{U}^{t=i}\\rightarrow\\bm{U}^{t=i+1})( bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i end_POSTSUPERSCRIPT → bold_italic_U start_POSTSUPERSCRIPT italic_t = italic_i + 1 end_POSTSUPERSCRIPT ). Spatially, a structure-preserved operation aligns with the specific equation characteristics of different process systems. Temporally, it utilizes a continuous-time modeling framework through an ODE solver.",
        "qtype": "Others",
        "response": "The little red box is simply highlighting the “structure-preserved spatial operation” block (i.e. the module that computes the diffusive and convective fluxes and then spits out ∂U/∂t).  In principle there is nothing stopping you from replacing the separate ODE‐integrator (Neural-ODE/RK) with a fully learned “neural difference” time–stepper inside that same spatial module—essentially teaching the network its own finite-difference (or multistep) update rule so that you no longer need an external integrator.  Of course, trading the off-the-shelf stability and continuous-time guarantees of a Neural-ODE for a purely learned stepper can work, but you’d have to take special care (e.g. by building in stability priors or constraints) to avoid the numerical instabilities that simple learned time-stepping schemes often suffer."
    },
    {
        "question": "What ethical risks occur when integrating Unknown Concept Adapters through Concept Complement Strategy without clinical annotations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Concept Complement Strategy",
            "Unknown Concept Adapters"
        ],
        "id": 1792,
        "masked_question": "What ethical risks occur when integrating [mask1] through Concept Complement Strategy without clinical annotations?",
        "masked_number": 1,
        "masked_elements": [
            "Unknown Concept Adapters"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.15446v1_figure_1.png",
        "paperid": "2410.15446v1",
        "paper_path": "./papers/2410.15446v1.json",
        "figure_id": "2410.15446v1_figure_1.png",
        "caption": "Figure 1: The Framework of Concept Complement Bottleneck Model. The input images are delivered to the image encoder to obtain the fundamental features, then different concept adapters extract specific concept features. Next, CCBM calculates the visual-text cross-attention score between textual known concepts/unknown concept embeddings and concept visual features. Finally, these concepts attention scores are aggregated to be passed through the decision layer for final disease diagnosis.",
        "qtype": "Others",
        "response": "First, let’s identify what the red‐boxed module is.  In Fig. 1 the authors show two parallel cross‐attention branches – one for the “known” concepts (fed by frozen text embeddings) and one for the “unknown” concepts.  The little stack of purple bars inside the red box are exactly the U learnable embedding vectors that CCBM introduces to represent its *new*, previously unseen concepts (i.e. the Kᵘ/Vᵘ embeddings in Section 3.3.2).\n\nNow, what happens if you hook those purely learned “unknown” embeddings into your diagnosis pipeline *without* any clinician‐provided labels or annotations to ground them?  In effect you are:\n\n  1. Allowing the model to latch onto arbitrary statistical regularities — sometimes literally background texture, scanner noise or even patient demographics — instead of bona fide pathological features.  \n  2. Giving the network free rein to create “concepts” that have no semantic meaning for a dermatologist or radiologist, so its decisions become much harder to audit or contest.  \n  3. Risking the amplification of spurious correlations (for example camera model, skin tone or other protected attributes) that can degrade safety, fairness and trust.  \n  4. Undermining accountability: If it turns out those unknown‐concept channels drove a misdiagnosis, there is no clinical annotation trail to say “that feature was legitimate” or “that feature was nonsense.”  \n\nAll of the above are **ethical** problems because they can lead to unsafe recommendations, opaque decisions and hidden bias.  In medical AI, any unanchored “concept” must be vetted or annotated by a clinician; otherwise you lose both transparency and a firm defense against data‐driven artifacts that can harm patients."
    },
    {
        "question": "Why adopt Multi-Head Cross-Attention for visual-text concept scoring instead of direct feature aggregation?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "most of existing concept-based methods use the same image features for concepts without considering the differences among concepts. On the visual level, simpler concepts are easier to capture, but in general, simple concepts contribute less to model decisions, especially for samples that are difficult to diagnose. Therefore, it is unfair to directly use the same feature encoded by an image encoder to calculate concept scores.",
            "we configure a concept adapter for each concept to encode the most relevant part of the corresponding concept from the basic image feature. We also leverage Multi-Head Cross-Attention (MHCA) to calculate concept scores in their own channels to support fair concept learning and provide a flexible structure to help concept discovery.",
            "According to the setting of the concept adapters, we can not average these attention as final features to calculate concept scores by a FCL but need to aggregate them in another way to get concept scores independently. Specifically, we can get the attention A^i for the i-th concept. Furthermore, we need to calculate the concept scores based on these attention weights. Different from the previous bottleneck models who directly use a FCL to project the common feature to get the concept scores, we could calculate these concept scores independently by any score calculation module based on their specific concept features."
        ],
        "final_answer": "The model uses Multi-Head Cross-Attention so that each concept is scored independently in its own channel, leveraging concept-specific visual features and textual embeddings. This avoids the unfairness of applying a single shared feature to all concepts and enables ‘fair concept learning’ by focusing attention on the most relevant image regions per concept, rather than direct aggregation through one global feature.",
        "relevant_elements": [
            "Multi-Head Cross-Attention",
            "Aggregation"
        ],
        "id": 1794,
        "masked_question": "Why adopt [mask1] for visual-text concept scoring instead of direct feature [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Multi-Head Cross-Attention",
            "Aggregation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.15446v1_figure_1.png",
        "paperid": "2410.15446v1",
        "paper_path": "./papers/2410.15446v1.json",
        "figure_id": "2410.15446v1_figure_1.png",
        "caption": "Figure 1: The Framework of Concept Complement Bottleneck Model. The input images are delivered to the image encoder to obtain the fundamental features, then different concept adapters extract specific concept features. Next, CCBM calculates the visual-text cross-attention score between textual known concepts/unknown concept embeddings and concept visual features. Finally, these concepts attention scores are aggregated to be passed through the decision layer for final disease diagnosis.",
        "qtype": "Design_Rationale",
        "response": "Here “mask1” is the stack of Multi‐Head Cross‐Attention (MHCA) blocks (the red box) and “mask2” is the more naïve approach of taking the shared image feature and running it straight through a single MLP (the blue box).  We chose MHCA instead of a single direct projection for two reasons:\n\n1.  Concept-level fairness and independence   \n    –  In a vanilla bottleneck you take one feature vector  and fire it through an MLP, so every concept is scored off exactly the same collapsed representation.  That inevitably privileges the “easiest” or most dominant patterns in the image and starves out subtler concepts.   \n    –  By first extracting concept-specific queries (via the per-concept adapters) and then attending those queries against the frozen text embeddings (the keys and values), each concept head gets its own dedicated view of the image.  No single concept can ride on the coattails of whatever dominates the global feature.  \n\n2.  Richer, multi-granular cross-modal alignment   \n    –  Multi‐head attention lets us capture different modes of similarity between a visual probe and its textual description (e.g. color vs. texture vs. shape).  A one‐shot MLP projection simply cannot disentangle those.  \n    –  The attention weights are themselves interpretable “scores” of how much each text embedding contributed to each visual query, which we then collapse into a final per-concept probability.  \n\nIn short, replacing that single shared MLP (“mask2”) with MHCA (“mask1”) yields fairer, more independent concept scores and richer text–image alignment, which was our goal for truly interpretable bottleneck concepts."
    },
    {
        "question": "What rationale underlies using Coordinate Transform to convert trajectory planning information into a trajectory image?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In our proposed pipeline, it is transformed from Cartesian coordinates to image space coordinates by using perspective projection transformation after translation and rotation operations.",
            "In addition, the points on the image space coordinate system are connected and lines are drawn as a trajectory image, which has a same angle of view as a front camera image."
        ],
        "final_answer": "The Coordinate Transform projects the planned future trajectory (given in real‐world Cartesian coordinates) into the same image plane and viewpoint as the front camera. By applying translation, rotation, and perspective‐projection to the trajectory, then drawing it as colored lines in image space, the system produces a trajectory image whose spatial layout aligns directly with the camera view. This alignment makes it easy for the visual encoder to fuse the camera image and trajectory information.",
        "relevant_elements": [
            "Coordinate Transform",
            "trajectory planning information",
            "trajectory image"
        ],
        "id": 1795,
        "masked_question": "What rationale underlies using [mask1] to convert [mask2] into a trajectory image?",
        "masked_number": 2,
        "masked_elements": [
            "Coordinate Transform",
            "trajectory planning information"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.09971v1_figure_1.png",
        "paperid": "2411.09971v1",
        "paper_path": "./papers/2411.09971v1.json",
        "figure_id": "2411.09971v1_figure_1.png",
        "caption": "Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder.",
        "qtype": "Design_Rationale",
        "response": "First, let’s unpack what the two masked items are:\n\n1. [mask1] is the little red box around the “Coordinate Transform” module.  \n2. [mask2] is the big blue box around the raw “Trajectory Planning Information” (the future ego‐vehicle trajectory, road boundaries and lane lines, all expressed as (x,y) points in a Cartesian frame).\n\nNow: why do we use a coordinate transform to turn that planning information into an image?\n\nChain of thought:\n\n1. The trajectory planner spits out abstract 2D points in world (or vehicle) coordinates – no appearance, no camera viewpoint.  \n2. BLIP-2 (and most off-the-shelf vision backbones) only know how to process images, not point clouds or lists of (x,y) pairs.  \n3. By applying a rotation, translation and a perspective‐projection (i.e. the “Coordinate Transform”) we map those world‐frame points into the front‐camera’s image plane.  \n4. Once in image space we can simply draw each polyline (trajectory, lanes, boundaries) in a distinctive color (and even vary the hue to encode speed).  \n5. The result is a “trajectory image” that (a) shares exactly the same viewpoint as the real camera, (b) preserves all the spatial relationships and velocity cues, and (c) can be ingested by the very same CNN/Transformer that sees the RGB frames.  \n6. This lets us fuse planning and vision inside BLIP-2 without any architectural surgery—just feed in two images (real scene + colored trajectory) and let the model learn to jointly attend to both.\n\nIn short: we use a coordinate transform to project the raw Cartesian trajectory data into the camera’s image plane so that it becomes a bona fide image.  That image can then be processed and fused with the front-camera view by a standard vision encoder, preserving spatial alignment and velocity information without touching BLIP-2’s internals."
    },
    {
        "question": "What advantage arises from fusing camera image and trajectory image in the Image-Trajectory Encoder?",
        "relevant_section_ids": [
            "3.3.2",
            "3.3.3"
        ],
        "relevant_context": [
            "In the overlaid method, a trajectory image is overlaid on a front camera image. The overlaid image is input to an image encoder and combined features of the visual information and the trajectory planning information are extracted. This method intends to clarify the spatial relations between the camera image and the trajectory.",
            "In the cross-attention method, the features of the visual information are input to the layers as queries, whereas the features of the trajectory planning information are as keys and values. As a result, both features interact with one another and the fused features are acquired."
        ],
        "final_answer": "Fusing the camera image and the trajectory image clarifies the spatial relationship between the vehicle’s planned path and its visual surroundings, and enables the two feature streams to interact so that a joint, fused representation of both visual context and trajectory planning is obtained.",
        "relevant_elements": [
            "camera image",
            "trajectory image",
            "Image-Trajectory Encoder"
        ],
        "id": 1796,
        "masked_question": "What advantage arises from fusing [mask1] and trajectory image in the Image-Trajectory Encoder?",
        "masked_number": 1,
        "masked_elements": [
            "camera image"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.09971v1_figure_1.png",
        "paperid": "2411.09971v1",
        "paper_path": "./papers/2411.09971v1.json",
        "figure_id": "2411.09971v1_figure_1.png",
        "caption": "Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder.",
        "qtype": "Design_Rationale",
        "response": "First, note that the red‐boxed input in the figure is simply the front “Camera Image.”  By design, the Image–Trajectory Encoder takes that camera image and the separately rendered “Trajectory Image” and fuses them into one joint feature set.  Why do this?\n\n1.   The camera image carries rich visual context (road scene, obstacles, traffic signals, etc.).  \n2.   The trajectory image carries the planned motion, lane boundaries, and speed cues in exactly the same field of view.  \n3.   If you process them entirely separately (or just concatenate late in the network), you never give the model a chance to explicitly learn *how* the planned path overlays the actual scene.  \n4.   By fusing them—especially via a cross‐attention or overlaid encoder—you force the network to align *where* the car intends to go with *what* the camera actually sees.  \n\nThe direct payoff is a much tighter spatial grounding of the plan in the perception: downstream the Q-Former and LLM can produce explanations that reference *exactly* why the vehicle will slow down, change lanes, or stop (e.g. “I see a stopped truck on my planned path at 20 m ahead”), rather than fumbling through two disconnected streams of information."
    },
    {
        "question": "What operations does Coordinate Transform perform on trajectory planning information to generate the trajectory image?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "However, in our proposed pipeline, it is transformed from Cartesian coordinates to image space coordinates by using perspective projection transformation after translation and rotation operations.",
            "In addition, the points on the image space coordinate system are connected and lines are drawn as a trajectory image, which has a same angle of view as a front camera image."
        ],
        "final_answer": "Coordinate Transform first applies translation and rotation to the planned trajectory coordinates, then performs a perspective projection to map them into image‐space coordinates. Finally, it connects the projected points and draws colored lines (matching the camera’s view) to form the trajectory image.",
        "relevant_elements": [
            "Coordinate Transform",
            "Trajectory Planning Information",
            "Trajectory Image"
        ],
        "id": 1797,
        "masked_question": "What operations does [mask1] perform on trajectory planning information to generate the trajectory image?",
        "masked_number": 1,
        "masked_elements": [
            "Coordinate Transform"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.09971v1_figure_1.png",
        "paperid": "2411.09971v1",
        "paper_path": "./papers/2411.09971v1.json",
        "figure_id": "2411.09971v1_figure_1.png",
        "caption": "Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed “Coordinate Transform” module (i.e. [mask1]) turns the raw trajectory data into a 2D trajectory image:\n\n1.  Take each planned trajectory, road boundary and lane-line point set, which is originally given in a world (Cartesian) co‐ordinate frame  \n2.  Translate and rotate them into the vehicle/camera reference frame (so that they share the same origin and heading as the front–camera)  \n3.  Apply a standard pinhole (perspective) projection to map those 3D/planar vehicle–frame points into 2D image–plane coordinates  \n4.  Connect the projected points in temporal order and draw them as coloured lines (e.g. yellow for road edges, blue for lane lines, and a red–to–green gradient for the ego‐vehicle’s future path)  \n\nThe output of these steps is exactly aligned, same–view “trajectory image” that can then be fused with the real camera image."
    },
    {
        "question": "How does the Image-Trajectory Encoder fuse features from the Camera Image and Trajectory Image?",
        "relevant_section_ids": [
            "3.3",
            "3.3.1",
            "3.3.2",
            "3.3.3"
        ],
        "relevant_context": [
            "It is important how to connect trajectory planning information with visual information of a front camera image. Here we consider three types of architecture for Image-Trajectory Encoders as shown in Fig. 2.",
            "This architecture has two image encoders, which are based on BLIP-2. One extracts features from an front camera image, and the other does from an trajectory image. The two pairs of the features are concatenated and output.",
            "In the overlaid method, a trajectory image is overlaid on a front camera image. The overlaid image is input to a image encoder and combined features of the visual information and the trajectory planning information are extracted.",
            "This method extracts features from two image encoders as well as the concatenated, but the way to fuse features of a front camera image and a trajectory image is different. There are cross-attention layers in this architecture. The features of the visual information are input to the layers as queries, whereas the features of the trajectory planning information are as keys and values. As a result, the both features interact with one another and the fused features are acquired."
        ],
        "final_answer": "The Image-Trajectory Encoder fuses camera and trajectory information in one of three ways: (1) Concatenated – two separate BLIP-2 encoders extract features from the camera image and trajectory image, then their feature queries are concatenated. (2) Overlaid – the trajectory image is overlaid on the camera image and a single encoder extracts combined features. (3) Cross-attention – two encoders extract features separately, then cross-attention layers fuse them by using camera features as queries and trajectory features as keys and values.",
        "relevant_elements": [
            "Image-Trajectory Encoder",
            "Camera Image",
            "Trajectory Image"
        ],
        "id": 1798,
        "masked_question": "How does the [mask1] fuse features from the Camera Image and Trajectory Image?",
        "masked_number": 1,
        "masked_elements": [
            "Image-Trajectory Encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.09971v1_figure_1.png",
        "paperid": "2411.09971v1",
        "paper_path": "./papers/2411.09971v1.json",
        "figure_id": "2411.09971v1_figure_1.png",
        "caption": "Figure 1: Pipeline of our proposed method. In order to improve accountability of an ego-vehicle action, trajectory planning information is embedded as a trajectory image and combined to a camera image in a Image-Trajectory Encoder.",
        "qtype": "Implementation_Details",
        "response": "First, “​[mask1]” in the red box is nothing more exotic than the Image–Trajectory Encoder.  Its sole job is to take the front‐camera image and the synthetic “trajectory image” and turn them into one joint feature stream.  Concretely, the paper explores three different fusion schemes inside that module:\n\n 1.  **Concatenation.**  \n    – Two separate BLIP-2 image encoders digest the camera image and the trajectory image in parallel.  \n    – Each encoder spits out 257 query‐features (each 1 × 1408).  \n    – You simply concatenate those two 257 × 1408 feature sets into a single (2 × 257) × 1408 tensor and pass it downstream.  \n\n 2.  **Overlaid input.**  \n    – You “paint” the trajectory lines (lane boundaries, future path, color‐coded speed) directly on top of the camera frame.  \n    – A *single* image encoder then ingests this composite image and automatically learns to pick up both the visual scene and the overlaid plan.  \n\n 3.  **Cross-attention.**  \n    – Again, two encoders first extract separate feature banks from camera and trajectory images.  \n    – But instead of raw concatenation, you interleave them in a cross‐attention block: the camera features become *queries* and the trajectory features become the *keys/values*.  \n    – The attention layers let the two modalities speak to one another and produce a tightly fused representation.  \n\nWhichever of these three architectures you choose, the net result is that the Image–Trajectory Encoder hands off a single, multimodal feature tensor to the Q-Former and LLM projection layers."
    },
    {
        "question": "How does the planner agent utilize memory and external guidance during observation to inform subtask decomposition?",
        "relevant_section_ids": [
            "3.1",
            "3.3"
        ],
        "relevant_context": [
            "The planner agent is equipped with advanced observation, reasoning, and memory capabilities, allowing it to accurately interpret user requirements and intelligently decompose them into detailed subtasks. (Section 3.1)",
            "During task execution, the planner agent continuously monitors progress, formulates and adjusts subtasks based on real‐time conditions, and guides the worker agent in its execution. As the worker agent completes tasks and reports the results, the planner agent updates its memory to optimize future task management, iterating until the entire task is completed and the final result is returned to the user. (Section 3.1)",
            "After the user uploads the Shapefile and task instructions, the system initializes the planner’s work environment, which records task progress, current task status, and the planner’s memory state, including information about previously executed tasks. (Section 3.3)",
            "In each loop, the planner first observes the current task state and determines whether the task has been completed. If the task is not complete, the planner generates a new sub‐task and assigns it to the worker for execution. (Section 3.3)"
        ],
        "final_answer": "At the start of each planning cycle, the planner “observes” by looking both at its internal memory (which holds past subtasks, progress markers, and results returned by the worker) and at any new external guidance (for example, the original user instructions or real-time feedback from in-flight subtasks).  By combining what it has stored in memory with the latest signals from its environment, the planner decides what still needs to be done and then breaks that remaining work down into the next concrete subtask to send to the worker.",
        "relevant_elements": [
            "planner agent",
            "memory",
            "external guidance"
        ],
        "id": 1799,
        "masked_question": "How does the planner agent utilize [mask1] and external guidance during observation to inform subtask decomposition?",
        "masked_number": 1,
        "masked_elements": [
            "memory"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.12376v2_figure_1.png",
        "paperid": "2410.12376v2",
        "paper_path": "./papers/2410.12376v2.json",
        "figure_id": "2410.12376v2_figure_1.png",
        "caption": "Figure 1: ShapefileGPT consists of a planner agent and a worker agent. The planner agent interprets user queries and decomposes them into subtasks, while the worker agent executes these subtasks by selecting appropriate functions from a predefined function library to perform Shapefile-related operations.",
        "qtype": "Implementation_Details",
        "response": "In Figure 1 the little red-boxed icon in the Planner’s “Environment” is its Memory.  Here is roughly what happens, step by step, when the Planner “observes” and decides how to break the user’s request into subtasks:\n\n1. Observation phase  \n   • The Planner reads the new user query (“collect rainfall statistics … make a bar chart …”).  \n   • It then immediately consults its Memory store – a running log of  \n     – which shapefiles have already been loaded,  \n     – what subtasks have already been created or completed,  \n     – any metadata it has extracted from those files,  \n     – and the outcomes of prior subtasks.  \n   • Simultaneously, it pulls in External Guidance from its “guidance” repository – this can be  \n     – snippets of the API documentation or examples (e.g. “use spatial_join before aggregation”),  \n     – higher‐level domain tips (“always check CRS before overlay”),  \n     – best-practice notes or templates for decomposing GIS workflows.\n\n2. Integrating memory + guidance  \n   By fusing these two streams – “what I already know about the current task” (Memory) and “how I’m supposed to do Shapefile tasks” (External Guidance) – the Planner builds a contextualized view of:  \n     – exactly which data sources it has on hand,  \n     – what pieces of work still remain,  \n     – which functions (APIs) are likely to be needed next, and  \n     – in what order.\n\n3. Generating the next subtasks  \n   Armed with that enriched context, the Planner then reasons:  \n     – “I see in Memory that we haven’t yet spatially joined rainfall with zones, and guidance tells me to do that before any aggregations.  So Subtask 1 should be a spatial join.  \n     – Next, guidance reminds me to sum rainfall within each zone – so Subtask 2 will be an aggregation function call.  \n     – Finally, we’ll need charting – so Subtask 3 is a bar-chart API invocation.”  \n\nBecause it never loses track of past steps (Memory) and is constantly steered by best-practice hints (External Guidance), its subtask decomposition stays both correct and efficient."
    },
    {
        "question": "How does the worker agent’s subtask working loop leverage API documentation to generate accurate function parameters?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "This environment consists of the function library and the provided API documentation. The function library provides the names and functional descriptions of each API, guiding the worker in selecting the most suitable one. The API documentation details the parameter rules for each API, ensuring the worker correctly configures the necessary parameters for API calls.",
            "Each cycle of the working loop encompasses the entire process, from selecting an API to executing it. After receiving a task, the worker selects the appropriate API based on the documentation and generates a function call with the correct parameters."
        ],
        "final_answer": "In each iteration of its working loop, the worker agent first chooses which API to call by consulting the function library and then retrieves the corresponding API documentation to see the exact parameter names, types and rules. Armed with this parameter schema plus information about the current Shapefile (e.g. field names, geometry type), it constructs and populates the function call with correctly formatted arguments, ensuring accurate execution.",
        "relevant_elements": [
            "worker agent",
            "subtask working loop",
            "API documentation"
        ],
        "id": 1800,
        "masked_question": "How does the [mask1]’s [mask2] leverage API documentation to generate accurate function parameters?",
        "masked_number": 2,
        "masked_elements": [
            "worker agent",
            "subtask working loop"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.12376v2_figure_1.png",
        "paperid": "2410.12376v2",
        "paper_path": "./papers/2410.12376v2.json",
        "figure_id": "2410.12376v2_figure_1.png",
        "caption": "Figure 1: ShapefileGPT consists of a planner agent and a worker agent. The planner agent interprets user queries and decomposes them into subtasks, while the worker agent executes these subtasks by selecting appropriate functions from a predefined function library to perform Shapefile-related operations.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the Worker’s SubTask Working Loop (the blue‐boxed component inside the red Worker region) uses the API docs to get its parameters exactly right:\n\n1. Query Next Function  \n   – The loop first decides which Shapefile API to call for the current subtask.\n\n2. Retrieve Document  \n   – It pulls in that API’s documentation (we ship it in YAML/JSON alongside the function library).  \n   – This doc spells out every parameter’s name, type, required vs. optional flag, valid values or formats, and even usage examples.\n\n3. Generate Parameters  \n   – Armed with the doc, the Worker looks up the exact parameter schema it must satisfy.  \n   – It also consults the live Shapefile metadata (e.g. geometry type, field names, attribute table) to choose real, existing field names and valid values.  \n   – By mapping its subtask intent onto the documented parameter definitions, it builds a fully compliant argument object.\n\n4. Execute Function  \n   – Finally, the Worker calls the API with that argument object.  \n   – Because the parameters all came straight from the spec, the call never breaks due to misspelled fields or wrong types.\n\nIn short, at the “Retrieve Document” step the Worker fetches the precise signature for the chosen function, and at “Generate Parameters” it uses that signature plus the current shapefile’s schema to assemble exactly the arguments the API expects."
    },
    {
        "question": "How does the Worker’s function library integration reflect tool-augmented LLM frameworks?",
        "relevant_section_ids": [
            "2",
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Tool-augmented Large Language Models enable LLMs to connect with external tools, effectively overcoming inherent limitations. By integrating resources such as search engines for external knowledge access and calculators to enhance mathematical capabilities, LLMs can also be utilized for repetitive daily tasks [27, 28, 29].",
            "We enable LLMs to execute actual Shapefile tasks through function calling. Function Calling is a mechanism that enhances interaction between LLMs and external programs, enabling the model to invoke predefined functions while generating text. This mechanism enables the execution of complex tasks by allowing LLMs to not only generate natural language but also interact with external programs, databases, and APIs.",
            "Upon receiving a task from the planner, the worker initiates execution within its environment. This environment consists of the function library and the provided API documentation. The function library provides the names and functional descriptions of each API, guiding the worker in selecting the most suitable one."
        ],
        "final_answer": "The Worker’s integration of a dedicated function library and accompanying API documentation mirrors the tool-augmented LLM paradigm by giving the model direct access to a suite of external, specialized procedures. In practice, the Worker consults the library to choose and invoke predefined functions—just as tool-augmented LLMs call calculators or web search APIs—thereby extending the model’s capabilities from pure text generation to precise, real-world Shapefile operations.",
        "relevant_elements": [
            "Worker",
            "Function Library"
        ],
        "id": 1802,
        "masked_question": "How does the [mask1]'s [mask2] integration reflect tool-augmented LLM frameworks?",
        "masked_number": 2,
        "masked_elements": [
            "Worker",
            "Function Library"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.12376v2_figure_1.png",
        "paperid": "2410.12376v2",
        "paper_path": "./papers/2410.12376v2.json",
        "figure_id": "2410.12376v2_figure_1.png",
        "caption": "Figure 1: ShapefileGPT consists of a planner agent and a worker agent. The planner agent interprets user queries and decomposes them into subtasks, while the worker agent executes these subtasks by selecting appropriate functions from a predefined function library to perform Shapefile-related operations.",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning in three steps:\n\n1.  Identify the masks  \n    -  [mask1] is the component drawn in the red box: that is the “Worker” (or worker agent).  \n    -  [mask2] is the item drawn in the blue box inside the worker’s environment: the “Function Library.”\n\n2.  What the Worker + Function-Library coupling actually does  \n    -  The Worker agent does not simply generate Python code or free-form text. Instead, it “looks up” the exact GIS routines it needs in a pre‐built Function Library, packages up the parameters, and issues a formal function call to the sandboxed back end.  \n    -  In other words, the LLM is augmented with a concrete API surface—27 specialized Shapefile operations—so that it can reliably carry out geometry joins, buffers, attribute queries, etc., without hallucinating or writing brittle ad-hoc code.\n\n3.  How this mirrors “tool-augmented” LLM frameworks  \n    -  Tool-augmented LLMs (e.g. ReAct, AutoGPT, GPT with function calling) all share the pattern of:  \n       a)  Embedding a description of external tools (APIs, calculators, search engines) into the model’s context  \n       b)  Letting the model decide which tool to invoke and with what arguments  \n       c)  Running that tool in a sandbox and returning the structured result back into the “thinking” loop  \n    -  ShapefileGPT’s Worker + Function Library is a direct realization of that pattern in the GIS domain. The Worker agent is the LLM, the Function Library is its “toolbox,” and every spatial operation is carried out via explicit, verifiable API calls.  \n\nAnswer  \nThe red‐boxed Worker’s use of a blue‐boxed Function Library is exactly the hallmark of a tool-augmented LLM. Instead of hoping the model will generate correct code or text, ShapefileGPT gives it a concrete set of GIS functions to call. This turns each subtask into a structured function invocation—mirroring how modern tool-augmented agents extend plain LLM reasoning by plugging into external APIs and thus dramatically improving reliability, reducing hallucinations, and enabling complex spatial analyses."
    },
    {
        "question": "How do Tetris-like Kernels enhance local detail extraction compared to CNN branch receptive fields?",
        "relevant_section_ids": [
            "2.1",
            "3.2"
        ],
        "relevant_context": [
            "Despite these advancements, CNNs’ limited local receptive fields hinder their ability to capture fine-grained structures fully, and increasing network depth often leads to feature loss, compromising detail preservation. Our MAGA framework addresses these limitations by integrating local and global receptive fields: MAGA employs Tetris-inspired convolutional kernels to optimize local pattern alignment, while its global receptive field enables the seamless integration of local features into a unified, coherent structure. (Section 2.1)",
            "Sparse convolutions, which compute only on non-zero locations, allow these branches to form diverse local patterns—similar to Tetris-like kernel shapes. This design enables MAGA to align effectively with different local structures, ensuring optimal correspondence with fine morphological details. The multi-branch setup enables the extraction of a variety of local patterns, which together capture intricate details otherwise missed by global-only representations. (Section 3.2)"
        ],
        "final_answer": "Standard CNN branches rely on fixed local receptive fields that struggle to fully capture intricate structures and often lose detail when the network is deepened. In contrast, MAGA’s Tetris-like kernels use multiple sparse‐convolution branches with varied, shape-adapted kernels to form a diverse collection of local patterns. By aligning these Tetris-shaped kernels with fine morphological structures and then normalizing and reweighting their responses, MAGA more precisely extracts and emphasizes fine details that conventional CNN receptive fields would miss.",
        "relevant_elements": [
            "Tetris-like Kernels",
            "CNN branch"
        ],
        "id": 1803,
        "masked_question": "How do [mask1] enhance local detail extraction compared to CNN branch receptive fields?",
        "masked_number": 1,
        "masked_elements": [
            "Tetris-like Kernels"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2411.10251v1_figure_1.png",
        "paperid": "2411.10251v1",
        "paper_path": "./papers/2411.10251v1.json",
        "figure_id": "2411.10251v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed MAGA-based matting architecture. The framework input consists of the image combined with a grayscale trimap. A vision encoder, based on MAGA, extracts advanced semantics, while a simple CNN branch captures hierarchical low-level features, providing appearance cues. The advanced semantics are then progressively upsampled and fused with hierarchical low-level features through context fusion, ultimately producing a high-quality alpha matte.",
        "qtype": "Literature_Background",
        "response": "Mask1 is calling out the little “Tetris‐shaped” sparse kernels in the MAGA block.  In the CNN branch you still only ever use a fixed, square convolutional window (say a 3×3 or 5×5 receptive field) at each level, so your ability to “reach out” and look at a hair‐strand or a little curl at odd angles is limited by that uniform grid.  In contrast, MAGA’s Tetris‐like kernels come in several different, irregular shapes (the four sparse “Tetris blocks”) and are applied in parallel to every patch.  That gives you:\n\n 1. A wider variety of local sampling patterns around each pixel (not just a uniform square),  \n 2. A larger effective local receptive field without stacking deeper layers,  \n 3. Shape-aware alignment to edges, thin curves and hair‐like structures,  \n 4. A multi-branch normalization + reweighting (MAL) step that picks out the strongest, most detail‐rich response at every location.  \n\nAll together, this means MAGA can “see” and enhance tiny morphological details (strands, curls, filaments) much more faithfully than the standard CNN branch whose local receptive field is fixed and isotropic."
    },
    {
        "question": "How does mapping Query (Local) onto Key (Global) adapt standard self-attention for morphological detail preservation?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "These enriched query embeddings are then projected onto global key embeddings, allowing MAGA to contextualize local details within the broader global framework. (Section 3.1)",
            "Mapping Q′ onto K allows MAGA to situate the enriched local morphology within the global structure, giving each local detail its spatial and contextual significance in relation to the overall image. (Section 3.2)"
        ],
        "final_answer": "By projecting the locally enriched query embeddings onto the original global key embeddings, MAGA replaces the standard query–key interaction with one that explicitly aligns fine morphological structures with their place in the full image context. This ensures each detailed local feature is weighted and positioned correctly within the global representation, preserving delicate shapes and boundaries during attention computation.",
        "relevant_elements": [
            "Query (Local)",
            "Key (Global)"
        ],
        "id": 1804,
        "masked_question": "How does mapping [mask1] onto [mask2] adapt standard self-attention for morphological detail preservation?",
        "masked_number": 2,
        "masked_elements": [
            "Query (Local)",
            "Key (Global)"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2411.10251v1_figure_1.png",
        "paperid": "2411.10251v1",
        "paper_path": "./papers/2411.10251v1.json",
        "figure_id": "2411.10251v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed MAGA-based matting architecture. The framework input consists of the image combined with a grayscale trimap. A vision encoder, based on MAGA, extracts advanced semantics, while a simple CNN branch captures hierarchical low-level features, providing appearance cues. The advanced semantics are then progressively upsampled and fused with hierarchical low-level features through context fusion, ultimately producing a high-quality alpha matte.",
        "qtype": "Literature_Background",
        "response": "In a vanilla Transformer‐style self‐attention block you form attention by computing  \n   Attention(Q, K, V) = softmax(Q Kᵀ) V  \nwhere Q, K, V all come from the same “global” feature stream.  MAGA, by contrast, breaks that symmetry.  It first uses its little “Tetris” convolutions to carve out a Query(Local) (the red–boxed signal) that is richly imbued with fine‐grained, morphology-aware information.  It then “maps” or attends that Query(Local) against the untouched global Key(Global) (the blue–boxed signal).  Concretely:\n\n  1. Q′ ← Query(Local)   ←— contains per‐patch, Tetris-kernel–extracted edge/fur/contour details  \n  2. K ← Key(Global)     ←— contains the original, broad‐context embeddings  \n  3. Scores ← softmax(Q′ Kᵀ)  \n\nBy doing so, MAGA’s attention scores are now explicitly keyed to how well each finely detailed query patch matches each global context patch.  Finally those scores weight the original Value(Global) to yield an output that has both the necessary global coherence and the preserved local morphology.  In effect, MAGA simply replaces the “plain” Q in self-attention with a morpho-aware Q(Local) while keeping K/V global, so that every attention weight is forced to respect and preserve the local structural detail."
    },
    {
        "question": "How does the CNN branch complement MAGA-based vision encoder during progressive context fusion?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Simultaneously, a CNN branch captures low-level feature maps at three scales (H/2 × W/2, H/4 × W/4, and H/8 × W/8), providing detailed appearance cues.",
            "The advanced semantics extracted by MAGA (at scale H/16 × W/16) are progressively upsampled and fused with these multi-scale low-level cues, producing a refined alpha matte output with preserved fine details and structural coherence."
        ],
        "final_answer": "The CNN branch extracts multi-scale low-level appearance features (at H/2, H/4, and H/8 resolutions) and these detailed cues are progressively fused with the upsampled high-level semantics from the MAGA encoder, yielding a refined alpha matte that preserves fine structures and overall coherence.",
        "relevant_elements": [
            "CNN branch",
            "MAGA-based vision encoder",
            "context fusion"
        ],
        "id": 1805,
        "masked_question": "How does the [mask1] complement [mask2] during progressive context fusion?",
        "masked_number": 2,
        "masked_elements": [
            "CNN branch",
            "MAGA-based vision encoder"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2411.10251v1_figure_1.png",
        "paperid": "2411.10251v1",
        "paper_path": "./papers/2411.10251v1.json",
        "figure_id": "2411.10251v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed MAGA-based matting architecture. The framework input consists of the image combined with a grayscale trimap. A vision encoder, based on MAGA, extracts advanced semantics, while a simple CNN branch captures hierarchical low-level features, providing appearance cues. The advanced semantics are then progressively upsampled and fused with hierarchical low-level features through context fusion, ultimately producing a high-quality alpha matte.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought reasoning  \n1. In the diagram the red‐boxed region ([mask1]) is the simple CNN branch that produces multi‐scale, low-level feature maps (at H/2×W/2, H/4×W/4, H/8×W/8).  \n2. The blue‐boxed region ([mask2]) is the MAGA‐based encoder that outputs a coarse, high-level semantic feature map (at H/16×W/16) enriched with local morphological details.  \n3. During progressive context fusion, the MAGA output is upsampled step by step to match each CNN feature resolution.  \n4. At each scale, the upsampled MAGA features are fused with the corresponding CNN maps.  \n5. Thus the CNN’s fine‐grained appearance cues “fill in” and sharpen the coarse global context from MAGA, restoring local textures and edges.  \n\nAnswer  \nThe CNN branch ([mask1]) supplies high‐resolution, low‐level appearance cues that are progressively fused with the upsampled MAGA semantic map ([mask2]) at each scale. In this way, the detailed textures and edges from the CNN complement the coarse global context from MAGA, yielding a finely detailed alpha matte."
    },
    {
        "question": "How does progressive upsampling facilitate integration of advanced semantics with low-level features?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Simultaneously, a CNN branch captures low-level feature maps at three scales (H/2 × W/2, H/4 × W/4, and H/8 × W/8), providing detailed appearance cues.",
            "The advanced semantics extracted by MAGA (at scale H/16 × W/16) are progressively upsampled and fused with these multi-scale low-level cues, producing a refined alpha matte output with preserved fine details and structural coherence."
        ],
        "final_answer": "By gradually increasing the resolution of the MAGA‐derived semantic features (from H/16×W/16 up to the scales of the low-level CNN maps), progressive upsampling brings high-level context into correspondence with fine appearance cues. At each stage, the upsampled semantics and the matching low-level features are fused, yielding a coherent representation that preserves both global structure and local detail.",
        "relevant_elements": [
            "progressive upsampling",
            "advanced semantics",
            "low-level features"
        ],
        "id": 1806,
        "masked_question": "How does [mask1] facilitate integration of advanced semantics with low-level features?",
        "masked_number": 1,
        "masked_elements": [
            "progressive upsampling"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2411.10251v1_figure_1.png",
        "paperid": "2411.10251v1",
        "paper_path": "./papers/2411.10251v1.json",
        "figure_id": "2411.10251v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed MAGA-based matting architecture. The framework input consists of the image combined with a grayscale trimap. A vision encoder, based on MAGA, extracts advanced semantics, while a simple CNN branch captures hierarchical low-level features, providing appearance cues. The advanced semantics are then progressively upsampled and fused with hierarchical low-level features through context fusion, ultimately producing a high-quality alpha matte.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module in Figure 1 is nothing more mysterious than the “top‐down” fusion path—it takes the compact, semantically rich H∕16×W∕16 MAGA features and gradually upsamples them back to H∕2×W∕2, at each step merging (fusing) with the correspondingly scaled low-level CNN maps.  In this way, high-level context (the advanced semantics from MAGA) is brought into exact spatial alignment with fine-grained appearance cues (the early CNN features), so that by the time you reach the final H∕2×W∕2 map you have a single representation that carries both object-level understanding and pixel-level detail.  \n\nStep-by-step chain of thought:\n\n1.  The paper describes two parallel branches:  \n    – MAGA encoder → deep, global semantics at H∕16×W∕16  \n    – CNN encoder → low-level features at H∕2, H∕4, H∕8  \n\n2.  The red box encloses three “Fusion” blocks, each one:  \n    a. Takes the output of the previous block (or MAGA at H∕16)  \n    b. Upsamples it by a factor of 2  \n    c. Fuses (e.g. concatenates + conv) with the CNN feature map at the same spatial size  \n\n3.  By repeating this upsample + fusion tri-level cascade, the network can  \n    – Inject high-level, morphology-aware semantics into finer spatial grids  \n    – Retain and sharpen local edges and textures from the shallow CNN  \n\n4.  The result is a final feature map that is both semantically “aware” (knows “this is cat fur”) and spatially precise (knows “this is the exact hairline along the boundary”), which is then turned into the output alpha matte."
    },
    {
        "question": "How does SpatialSonic leverage spatial-aware control to reduce error accumulation compared to two-stage pipelines?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "SpatialSonic",
            "Spatial-Aware Control"
        ],
        "id": 1807,
        "masked_question": "How does [mask1] leverage spatial-aware control to reduce error accumulation compared to two-stage pipelines?",
        "masked_number": 1,
        "masked_elements": [
            "SpatialSonic"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.10676v1_figure_1.png",
        "paperid": "2410.10676v1",
        "paper_path": "./papers/2410.10676v1.json",
        "figure_id": "2410.10676v1_figure_1.png",
        "caption": "Figure 1: Our SpatialSonic, as a one-stage model, alleviates the problem of error accumulation in a two-stage model and facilitates control with end-to-end finetuning in a one-stage model. Moreover, our spatially enhanced system supports spatial audio generation from text and image, as well as interactive actions.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module in Figure 1 is the one‐stage SpatialSonic model.  SpatialSonic avoids the “generate‐then‐simulate” cascade of two‐stage pipelines by folding all of the spatial reasoning and binaural cue synthesis into a single diffusion process.  Concretely, it\n\n 1. Extracts spatial‐aware features from images via a region-based encoder (so it knows “where” things are),  \n 2. Produces coarse‐ and fine‐grained azimuth state matrices (encoding interaural time/level differences over time),  \n 3. Fuses those matrices, plus any text cues, directly into the denoising steps of its latent diffusion model.  \n\nBy conditioning the model end-to-end on positional (azimuth) controls rather than adding a separate filtering or simulation stage, SpatialSonic never leaves the diffusion loop and thus does not incur the mismatch and compounding errors you get when you cascade a mono-to-stereo filter or room simulator after a vanilla audio generator."
    },
    {
        "question": "How does BEWO-1M dataset integration facilitate unified text and image-driven spatial audio generation in SpatialSonic?",
        "relevant_section_ids": [
            "3",
            "4.4"
        ],
        "relevant_context": [
            "We propose BEWO-1M, a large-scale stereo audio dataset with spatial captions, as the first to the best of our knowledge. BEWO-1M consists of audio-caption pairs and audio-image pairs.",
            "Initially, the T2A model is trained using the BEWO-1M dataset. On top of this T2A model, it is fine-tuned using the spatial-aware image encoder to develop the I2A model."
        ],
        "final_answer": "By providing both audio–text and audio–image supervision, BEWO-1M lets SpatialSonic first train its diffusion backbone on text–audio pairs (T2A) and then fine-tune the same model with spatial-aware visual embeddings on image–audio pairs (I2A). This two-stage use of BEWO-1M unifies text-driven and image-driven spatial audio generation in a single network.",
        "relevant_elements": [
            "BEWO-1M Dataset",
            "SpatialSonic"
        ],
        "id": 1808,
        "masked_question": "How does [mask1] dataset integration facilitate unified text and image-driven spatial audio generation in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "BEWO-1M Dataset",
            "SpatialSonic"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/1_2410.10676v1_figure_1.png",
        "paperid": "2410.10676v1",
        "paper_path": "./papers/2410.10676v1.json",
        "figure_id": "2410.10676v1_figure_1.png",
        "caption": "Figure 1: Our SpatialSonic, as a one-stage model, alleviates the problem of error accumulation in a two-stage model and facilitates control with end-to-end finetuning in a one-stage model. Moreover, our spatially enhanced system supports spatial audio generation from text and image, as well as interactive actions.",
        "qtype": "Experimental_Results",
        "response": "Here is the step-by-step reasoning:\n\n1.  Identify the masked items in the diagram:  \n    – [mask1] is the red‐boxed item “BEWO-1M Dataset.”  \n    – [mask2] is the blue-boxed item, “SpatialSonic,” i.e. the new one-stage spatial audio generator.\n\n2.  What BEWO-1M brings to the table:  \n    – It is the first large-scale stereo audio dataset with *both* audio–text *and* audio–image pairs, each carrying *explicit spatial captions* (source azimuth, distance, motion, room attributes, etc.).  \n    – Because every clip has a clean, single sound source and a matching image or text description enriched by GPT-induced spatial attributes, the model can learn tight alignments between visual regions, linguistic spatial phrases, and the interaural cues (ITD/ILD) that produce realistic stereo sound.\n\n3.  How BEWO-1M trains SpatialSonic:  \n    – During training, SpatialSonic’s text encoder (T5) and its region-aware image encoder are *jointly* fine-tuned on BEWO-1M’s paired data.  \n    – This forces both modalities into the *same* latent space where spatial information—“object at 𝜃=30° moving left to right,” “near/far,” etc.—is explicitly encoded.  \n    – The azimuth-fusion module then learns to take that shared embedding plus a coarse/fine azimuth state matrix and drive the diffusion model to produce two-channel audio whose left/right waveforms match the ground-truth spatial cues.\n\n4.  The result in SpatialSonic:  \n    – Because BEWO-1M offers aligned text-audio *and* image-audio examples, SpatialSonic can at inference time accept *either* a purely textual spatial description *or* an image (with optional clicks/bounding-boxes) and immediately generate a realistic stereo waveform in one end-to-end pass—no separate “text→wave” and then “spatialize” steps.  \n    – In short, BEWO-1M’s unified, high-quality, spatially annotated multimodal data is what makes SpatialSonic a true *single-stage*, *controllable* text‐*or*‐image-driven spatial audio generator."
    },
    {
        "question": "What are the limitations of the Spatial-Aware Control when integrated with Latent Diffusion Model for stereo audio?",
        "relevant_section_ids": [
            "1",
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Then, we identify that due to the lack of explicit spatial guidance, simply finetuning the existing model with BEWO-1M still fails in precise T2A and I2A tasks.",
            "Popular I2A model (Sheffer and Adi, 2023  [74]) using CLIP (Ramesh et al., 2022  [71]) focuses on aligning the global abstract semantics rather than its positional and relational context.",
            "When text and image embedding are used directly as conditions, there is still a large dispersity in Fig. 3 (c). Therefore, it is crucial to design a model that effectively supports precise generation using both text and images."
        ],
        "final_answer": "Integrating naive spatial‐aware control (i.e. concatenating image regions or CLIP embeddings) into a latent diffusion model yields high variability (“large dispersity”) in where sounds are placed and repeatedly fails to generate precise spatialized stereo (T2A and I2A) because these global embeddings lack explicit, continuous positional guidance.",
        "relevant_elements": [
            "Spatial-Aware Control",
            "Latent Diffusion Model"
        ],
        "id": 1809,
        "masked_question": "What are the limitations of the [mask1] when integrated with Latent Diffusion Model for stereo audio?",
        "masked_number": 1,
        "masked_elements": [
            "Spatial-Aware Control"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/2_2410.10676v1_figure_1.png",
        "paperid": "2410.10676v1",
        "paper_path": "./papers/2410.10676v1.json",
        "figure_id": "2410.10676v1_figure_1.png",
        "caption": "Figure 1: Our SpatialSonic, as a one-stage model, alleviates the problem of error accumulation in a two-stage model and facilitates control with end-to-end finetuning in a one-stage model. Moreover, our spatially enhanced system supports spatial audio generation from text and image, as well as interactive actions.",
        "qtype": "Others",
        "response": "Here’s how we arrive at the answer step-by-step:\n\n1. In Fig. 1(a) the “two‐stage” box shows  \n   – Stage 1: a Latent Diffusion Model (LDM) produces a mono (1-channel) track  \n   – Stage 2: a separate “Conditional Filter or Simulation” (i.e. an ITD/ILD filter or physics simulator) converts that mono into L/R stereo.  \n2. The red‐boxed item in question refers to exactly that second, post-hoc “conditional filter or simulation” stage (the thing you tack on after your LDM).  \n3. The text explicitly calls out its drawbacks:  \n   • “high computational costs” – you’re effectively running two heavy models instead of one.  \n   • “potential error accumulation” – any artifacts or localization errors in the mono output get magnified or even warped when you apply the filter or simulation.  \n   • poor end-to-end optimization – because the filter/simulator is a separate non-differentiable module, you can’t fine-tune the diffusion part to compensate.  \n4. All of this translates into noticeably lower-quality, less accurate spatialization than a carefully controlled one-stage model.  \n\nAnswer  \n– When you simply hook a post-hoc conditional filter or simulator onto an LDM, you incur large extra compute (you’re running two heavy pipelines), you compound errors (any mis-localizations in the first stage are carried—and often amplified—into the stereo upmix), and you lose end-to-end trainability, which together leads to sub-par, low-quality stereo spatialization."
    },
    {
        "question": "What ethical concerns arise from using Conditional Filter or Simulation without real-world audio for spatial guidance?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Conditional Filter",
            "Simulation"
        ],
        "id": 1810,
        "masked_question": "What ethical concerns arise from using [mask1] or [mask2] without real-world audio for spatial guidance?",
        "masked_number": 2,
        "masked_elements": [
            "Conditional Filter",
            "Simulation"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/3_2410.10676v1_figure_1.png",
        "paperid": "2410.10676v1",
        "paper_path": "./papers/2410.10676v1.json",
        "figure_id": "2410.10676v1_figure_1.png",
        "caption": "Figure 1: Our SpatialSonic, as a one-stage model, alleviates the problem of error accumulation in a two-stage model and facilitates control with end-to-end finetuning in a one-stage model. Moreover, our spatially enhanced system supports spatial audio generation from text and image, as well as interactive actions.",
        "qtype": "Others",
        "response": "Step 1 – Identify the two masked components in Fig. 1(a)  \n  • [mask1] (red box) is the “Conditional Filter” approach (e.g. a learned ITD/ILD-style filter applied to mono audio).  \n  • [mask2] (blue box) is the “Simulation” approach (e.g. physics-based room-impulse-response or binaural HRTF simulator).  \n\nStep 2 – How these are used without real-world audio for spatial guidance  \n  • In both two-stage pipelines, you first generate a non-spatial (monaural) sound and then either pass it through a conditional filter or a simulator to “stereo-ize” it.  \n  • Neither approach ever hears genuine binaural recordings or ground-truth room/equipment acoustics – everything rests on the learned or modeled approximation.  \n\nStep 3 – Ethical concerns of relying solely on filters or simulators (no real-world calibrations)  \n\n  1. Misrepresentation and Deception  \n    – Users may be led to believe they are hearing authentic environmental cues (e.g. object distance, direction, reverberation) when in fact those cues rest on overly simplistic or biased models.  \n    – In safety-critical VR/AR or assistive audio systems (e.g. navigation aids for the visually impaired), bad spatialization can cause disorientation or even injury.  \n\n  2. Propagation of Bias  \n    – Binaural filters and simulated room responses often embed a single “average” head-shape or room model. Under-represented groups (different ear shapes, head sizes, cultural room acoustics) end up marginalized or mis-localized.  \n    – What sounds “correct” in the filter’s training set may systematically misplace sounds for those outside the statistical norm.  \n\n  3. Privacy and Deep‐fake Risks  \n    – High-fidelity simulated spatial audio can be weaponized for misinformation or enhanced deep-fake audio, since it can convincingly place voices or events “behind” or “beside” you.  \n    – Without grounding in real recordings it is hard to trace or watermark simulated cues, making content provenance murky.  \n\n  4. Lack of Accountability and Validation  \n    – Because neither method uses recorded ground-truth binaural samples, there is no easy “gold-standard” to audit or certify the spatial accuracy of the output.  \n    – Research and product teams may inadvertently deploy spatial audio that has never been human-validated, opening the door to user harm and legal liability.  \n\nIn short, if you “stereo-ize” purely by a learned filter or a physics simulator without any real‐world binaural recordings for calibration or validation, you risk misleading users, amplifying head/room biases, enabling deceptive content, and eroding accountability for spatial fidelity."
    },
    {
        "question": "What drawbacks might the Conditional Discriminator introduce when adversarially matching gesture distributions with the Generator?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Although the conditional GAN based on control signals seems to reasonably address the large stride and small step count of high-speed sampling, the purely implicit adversarial learning for the connected p(z_{t-1},z_t|c) is statistically inefficient, particularly when p is a high-dimensional redundant representation (which is often the case in human body sequences).",
            "Furthermore, we observe that after the adversarial process of the conditional GAN, the generator often produces gestures with artifacts. We speculate that this is because the purely implicit matching constraint on the joint distribution cannot provide geometric constraints for the gestures."
        ],
        "final_answer": "Relying solely on the Conditional Discriminator’s implicit joint-distribution matching can be statistically inefficient for high-dimensional, redundant gesture data and tends to produce artifacts in the generated motions due to the lack of explicit geometric constraints.",
        "relevant_elements": [
            "Conditional Discriminator",
            "Generator"
        ],
        "id": 1811,
        "masked_question": "What drawbacks might the [mask1] introduce when adversarially matching gesture distributions with the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Conditional Discriminator",
            "Generator"
        ],
        "figure_path": "/home/yz979/project/chengye/missqa/final_data/annotated_images/0_2410.20359v2_figure_2.png",
        "paperid": "2410.20359v2",
        "paper_path": "./papers/2410.20359v2.json",
        "figure_id": "2410.20359v2_figure_2.png",
        "caption": "Figure 2: Network architecture. During training, we introduce a GAN structure based on conditional denoising diffusion to capture the complex distribution of gesture sequences in a multi-step process, enabling larger sampling step sizes. During inference, we use large step sizes and fewer steps for sampling, according to the input audio control signal, to achieve fast, high-quality gesture sequences, thus supporting real-time tasks.",
        "qtype": "Others",
        "response": "Here is the gist of what goes wrong if you rely only on the red‐boxed module (the conditional discriminator) to adversarially match your gesture generator (the blue‐boxed “inference stage” generator):\n\n1.  Purely implicit matching in a very high‐dimensional, highly correlated space is statistically inefficient.  \n    – Human‐body sequences live in a huge, redundant joint‐angle manifold.  An adversarial loss by itself has a very hard time covering that space with only a few dozen diffusion steps, so learning slows way down and you often need many more steps or a lot more data.\n\n2.  Training becomes unstable.  \n    – Conditional GANs are notorious for oscillations and mode collapse, and this only gets worse when you’re trying to match an entire sequence distribution in a compressed number of denoising steps.\n\n3.  You get obvious artifacts and physically implausible poses.  \n    – Without any explicit kinematic‐ or bone‐length constraints, the generator will happily produce jittery joints, bone‐length violations, foot sliding, etc., just in order to fool the discriminator.\n\n4.  Diversity and coverage suffer.  \n    – Because the discriminator only sees “real vs. fake” as a black‐box judgement, there’s no direct penalty for missing modes, so you can end up with a generator that collapses onto a few “safe” gesture patterns.\n\nPutting all of that together, the purely adversarial path via the red‐boxed discriminator alone leads to slow, unstable training and unnatural or collapsed gesture outputs, which is exactly why the paper adds back an explicit geometric (Huber) loss on the kinematics."
    }
]