[
    {
        "question": "What advantage does coupling the Conditional Discriminator with Transformer Encoders confer on denoising quality?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Conditional Discriminator",
            "Transformer Encoders"
        ],
        "id": 1813,
        "masked_question": "What advantage does coupling the [mask1] with [mask2] confer on denoising quality?",
        "masked_number": 2,
        "masked_elements": [
            "Conditional Discriminator",
            "Transformer Encoders"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.20359v2_figure_2.png",
        "paperid": "2410.20359v2",
        "paper_path": "./MISS-QA/papers/2410.20359v2.json",
        "figure_id": "2410.20359v2_figure_2.png",
        "caption": "Figure 2: Network architecture. During training, we introduce a GAN structure based on conditional denoising diffusion to capture the complex distribution of gesture sequences in a multi-step process, enabling larger sampling step sizes. During inference, we use large step sizes and fewer steps for sampling, according to the input audio control signal, to achieve fast, high-quality gesture sequences, thus supporting real-time tasks.",
        "qtype": "Design_Rationale",
        "response": "By wiring the adversarial discriminator into the same network that does the time‐step conditioning and denoising (the transformer encoder “denoiser”), you get much stronger supervision than a vanilla L₂ loss on Gaussian residuals.  In practice the discriminator forces the transformer to match the true, often multi-modal distribution of “real” denoised poses (conditioned on audio and step‐index), rather than just regressing toward the single‐mode MMSE solution.  The result is sharper, more lifelike motion, fewer artifacts and less jitter when you take big steps with only a handful of sampling iterations."
    },
    {
        "question": "What motivates placing Posterior Sampling after Transformer Encoders in the generator pipeline?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Posterior Sampling",
            "Transformer Encoders"
        ],
        "id": 1814,
        "masked_question": "What motivates placing [mask1] after Transformer Encoders in the generator pipeline?",
        "masked_number": 1,
        "masked_elements": [
            "Posterior Sampling"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.20359v2_figure_2.png",
        "paperid": "2410.20359v2",
        "paper_path": "./MISS-QA/papers/2410.20359v2.json",
        "figure_id": "2410.20359v2_figure_2.png",
        "caption": "Figure 2: Network architecture. During training, we introduce a GAN structure based on conditional denoising diffusion to capture the complex distribution of gesture sequences in a multi-step process, enabling larger sampling step sizes. During inference, we use large step sizes and fewer steps for sampling, according to the input audio control signal, to achieve fast, high-quality gesture sequences, thus supporting real-time tasks.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning step–by–step:\n\n1.  In our generator (the teal‐shaded box in the left half of Fig. 2) we first feed the noisy gesture \\(x_t\\), the audio/style/seed-gesture conditioning and a Gaussian noise vector \\(z\\) through a stack of Transformer encoders.  The output of those encoders is our *estimate* \\(\\hat x_0\\) of the clean gesture at time zero.\n\n2.  In a standard DDPM reverse pass you don’t stop at predicting \\(x_0\\) – you then use the analytic Gaussian *posterior*  \n   \\[\n     q\\bigl(x_{t-1}\\,\\big|\\,x_t,x_0\\bigr)\n   \\]\n   to actually sample your next state \\(x_{t-1}\\).  \n\n3.  In our adversarial (GAN) formulation we need to compare *real* samples from  \n   \\[\n     q\\bigl(x_{t-1}\\,\\big|\\,x_t,x_0\\bigr)\n   \\]\n   against *fake* samples from our generator’s learned distribution  \n   \\[\n     p_G\\bigl(x_{t-1}\\,\\big|\\,x_t, c\\bigr)\\,,\n   \\]\n   so that the discriminator can push the generator to match the true diffusion posterior.\n\n4.  Consequently we place a **posterior‐sampling** block *after* the Transformer so that:  \n   a) we first predict \\(\\hat x_0\\),  \n   b) then we compute (and sample from) the Gaussian posterior \\(q(x_{t-1}\\mid x_t,\\hat x_0)\\),  \n   c) and feed that \\(\\hat x_{t-1}\\) into the discriminator as the “fake” posterior sample.\n\n5.  In short, the red‐boxed “[mask1]” is simply that posterior sampling step.  We position it after the Transformer so that our network learns to (i) predict the clean gesture, (ii) convert that prediction back into the diffusion domain via the exact DDPM posterior, and (iii) let the conditional GAN adversarially match the generator’s multi-step sampling distribution to the true one."
    },
    {
        "question": "What motivates using pre-trained LLM for design principle learning instead of manual rule extraction?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Current methods often require specialized tools to map architectures into a shared latent space, followed by expert analysis to extract underlying design rules (Yuan et al. 2022), which reduces the level of automation.",
            "With the emergence of pre-trained Large Language Models (LLMs) (Wu et al. 2024; Liu et al. 2024), LLMs offer a promising solution to address the aforementioned challenges. By representing architectures in programming languages, the task of learning design principles can be framed as a language-inductive learning problem, a domain where LLMs have demonstrated proficiency (Imani, Du, and Shrivastava 2023). Therefore, leveraging LLMs as reasoning agents for automatic design principle learning is a logical step. Given their pre-training on vast knowledge, in-context learning can be employed to tackle this task, thereby mitigating the constraints posed by the number of architecture samples. Furthermore, owing to their contextual awareness, LLMs can automatically translate design principles into actionable architectural knowledge for NAS methods."
        ],
        "final_answer": "Manual rule extraction is cumbersome because it relies on specialized tools to embed architectures into a shared latent space and demands expert analysis to distill design rules, leading to low automation and high resource costs. In contrast, pre-trained LLMs can ingest architecture implementations as code, perform language-based inductive reasoning to learn general design principles with few examples, and directly translate those principles into actionable constraints—thereby boosting automation, reducing sample requirements, and improving efficiency.",
        "relevant_elements": [
            "pre-trained LLM",
            "design principle learning"
        ],
        "id": 1815,
        "masked_question": "What motivates using [mask1] for design principle learning instead of manual rule extraction?",
        "masked_number": 1,
        "masked_elements": [
            "pre-trained LLM"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.11330v1_figure_1.png",
        "paperid": "2408.11330v1",
        "paper_path": "./MISS-QA/papers/2408.11330v1.json",
        "figure_id": "2408.11330v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed LAPT. This framework consists of two stages. In the learning stage of design principles, LLM is driven by specific prompts to learn general design principles from a set of architectures. In the architecture search stage, the learned principles are applied to initialize the search space for each new task. Then, architectures found in the refined search space are used to update these principles, aiming to build the task-specific search space.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in two parts.  \n\nStep 1: Image–text alignment  \n- If you look at the red‐boxed region in the left half of Figure 1, it surrounds exactly the module labelled “Pre-trained LLM.”  \n- In the text you see that the paper proposes to “employ a pre-trained LLM to learn design principles from well-established neural architectures represented in programming codes.”  \n\nSo the masked term [mask1] is simply “pre-trained LLM.”  \n\nStep 2: Why use a pre-trained LLM instead of manual rule extraction?  \n1. Manual extraction of design rules  \n   - Requires specialized tools to map every architecture into some common latent space  \n   - Relies on experts poring over hundreds of architectures to write down “if-then” rules  \n   - Is extremely time- and resource-intensive (e.g. Radosavovic et al. had to label 500+ nets)  \n   - Doesn’t scale well to new tasks or architectures  \n\n2. Pre-trained LLMs as reasoning agents  \n   - Are already trained on massive code and language corpora, so they “understand” architectural code snippets out of the box  \n   - Can do in-context learning: a handful of example architectures is enough to induce high-level design principles  \n   - Automatically translate those principles into natural-language constraints you can plug back into NAS  \n   - Greatly reduce the human effort and computation needed compared to hand-crafting extraction pipelines  \n\nPutting it together: by leveraging a pre-trained LLM, the authors avoid the brittle, expert-driven, and expensive process of hand-writing extraction rules, instead using the LLM’s built-in code understanding and few-shot learning ability to automatically mine and express design principles from existing architectures."
    },
    {
        "question": "How does principle adaptation refine the initial search space to better guide architecture search?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "However, due to domain shift where the architecture performs differently on different tasks, S_r may not be optimal for all the tasks.",
            "To alleviate the potential negative effects of domain shift, this work adapts P to the target task based on the newly found architectures.",
            "Specifically, the NAS method is applied to find architectures from S_r in an iterative way. In each iteration, if the better-performing architectures are found, LLM is prompted to update P based on these architectures; otherwise, LLM is required to describe effects of other available candidate operators and information sources that are not in C_r, promoting the exploration for other promising regions in S_r."
        ],
        "final_answer": "Principle adaptation takes the initial design principles and—through an iterative loop of searching and evaluation—updates them based on architectures found on the target task. Whenever higher-performing architectures emerge, the LLM refines the principles to emphasize the operators and connections that proved beneficial. If performance stalls, the LLM suggests other candidate operators or sources to explore. This dynamic updating yields a progressively customized subspace that better aligns with the task’s characteristics, thereby guiding the architecture search more effectively.",
        "relevant_elements": [
            "principle adaptation",
            "initial search space"
        ],
        "id": 1816,
        "masked_question": "How does [mask1] refine the initial search space to better guide architecture search?",
        "masked_number": 1,
        "masked_elements": [
            "principle adaptation"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.11330v1_figure_1.png",
        "paperid": "2408.11330v1",
        "paper_path": "./MISS-QA/papers/2408.11330v1.json",
        "figure_id": "2408.11330v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed LAPT. This framework consists of two stages. In the learning stage of design principles, LLM is driven by specific prompts to learn general design principles from a set of architectures. In the architecture search stage, the learned principles are applied to initialize the search space for each new task. Then, architectures found in the refined search space are used to update these principles, aiming to build the task-specific search space.",
        "qtype": "Design_Rationale",
        "response": "First, let’s align the red‐boxed module in the figure with the prose.  In the diagram, the red box surrounds the “Principle Adaptation” loop that sits between the NAS inner‐loop and the LLM.  In the text this is described under “Principle transfer and adaptation” (Alg. 2): once you have your initial subspace S′ (built by translating the transferred principles Pₜ into {Cᵢ,Sᵢ}), you run NAS on S′; if you discover better architectures, you feed them back to the LLM to *update* Pₜ; if you don’t, you ask the LLM to propose new operators or connections that it hasn’t yet included.  \n\nChain of thought:  \n1. We start with an initial refined search space S′ = {Cᵢ,Sᵢ}, carved out of the full space by the transferred principles Pₜ.  \n2. We apply our NAS method inside S′ to find high‐performing candidates.  \n3. The red-boxed “Principle Adaptation” block now takes those newly found architectures and re-prompts the LLM to *revise* the design principles.  \n4. The updated principles then yield a *new* set of layer‐wise operator pools Cᵢ and connection sets Sᵢ.  \n5. This gives us an even more sharply focused subspace for the next NAS iteration—pruning away operators and links that didn’t show up in the winners, and adding or boosting those that did.  \n\nBy closing this loop, Principle Adaptation dynamically tightens (or, when necessary, broadens) the candidate operator and connection sets so that each subsequent NAS run concentrates on ever more promising regions of the search space."
    },
    {
        "question": "How does Prompt Architecture Implementation engage LLM chain-of-thought reasoning to extract general design principles?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Firstly, the pre-trained LLM benefits from exposure to a wide array of programming languages, allowing it to gain awareness of the neural architecture from source codes (Zheng et al. 2023  ###reference_b39###). Nevertheless, due to the token limitation, it becomes infeasible to feed all architecture source codes directly into the LLM. To tackle this issue, Python classes that can instantiate an architecture based on its architectural parameters, i.e., θ, are set as prompts.",
            "Secondly, instructing LLMs to reason the general design principles from such architectures is not trivial, given the complex and diverse DNN architectures. To address this issue, drawing inspiration from the effective utilization of the “chain of thought” method in LLMs, we steer the LLM towards a step-by-step reasoning process as follows: Step1: input architectural parameters of the given architectures into the LLM; Step2: prompt LLM identifying common patterns within these architectures; Step3: summarize the design principle behind these common patterns."
        ],
        "final_answer": "The Prompt Architecture Implementation feeds the LLM concise Python class templates that encode each architecture’s parameters and then explicitly instructs it to apply a chain-of-thought: first ingest the architectural parameters, next identify recurring structural patterns across the examples, and finally distill those patterns into high-level design principles.",
        "relevant_elements": [
            "Prompt Architecture Implementation",
            "LLM"
        ],
        "id": 1817,
        "masked_question": "How does [mask1] engage LLM chain-of-thought reasoning to extract general design principles?",
        "masked_number": 1,
        "masked_elements": [
            "Prompt Architecture Implementation"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.11330v1_figure_1.png",
        "paperid": "2408.11330v1",
        "paper_path": "./MISS-QA/papers/2408.11330v1.json",
        "figure_id": "2408.11330v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed LAPT. This framework consists of two stages. In the learning stage of design principles, LLM is driven by specific prompts to learn general design principles from a set of architectures. In the architecture search stage, the learned principles are applied to initialize the search space for each new task. Then, architectures found in the refined search space are used to update these principles, aiming to build the task-specific search space.",
        "qtype": "Implementation_Details",
        "response": "Here is a concise, step-by-step breakdown of how the red-boxed prompt (“[mask1]”) actually *drives* the LLM to use chain-of-thought and pull out general design principles:\n\n1.  **Code Stub to Fix the Representation**  \n    The prompt first injects a tiny Python class—  \n    ```python\n    class Model(nn.Module):\n        layer1 = operators(1)\n        def forward(self, x): …\n    ```  \n    This forces the LLM to “load” each architecture in exactly the same, highly compressed form (just its layer parameters and connections), rather than wading through dozens of full source files.  \n\n2.  **Explicit Step-by-Step Instructions**  \n    Immediately after that code snippet, the prompt tells the model:  \n    - Step 1: Read in the architectural parameters.  \n    - Step 2: Identify common patterns across all given architectures.  \n    - Step 3: Summarize the *design principle* that explains those patterns.  \n\n3.  **Unfolding a Chain of Thought**  \n    By (a) giving it a *minimal*, uniform representation to parse, and then (b) *explicitly* asking it to proceed in clearly enumerated steps, the prompt coaxes the LLM into a chain-of-thought style:  \n    – first “internalize” what each operator and connection means,  \n    – then “scan” for the recurring motifs (e.g. “every high-accuracy model had an identity skip at the end”),  \n    – finally “abstract” those observations into a pithy rule (“always include an identity link to preserve gradient flow,” etc.).  \n\n4.  **Emergence of General Rules**  \n    That three-stage scaffold is exactly what turns a black-box LLM into something resembling a rule extractor.  Instead of simply parroting back snippets of code, the model is guided—one micro-step at a time—toward *distilling* the underlying heuristics that actually drive good performance.  \n\nIn short, “[mask1]” is not just a blob of code: it’s a *guided, multi-step prompt* that (1) normalizes the inputs, (2) lays out a clear chain of reasoning, and (3) harvests the LLM’s internal “thought process” to output crisp, general‐purpose design principles."
    },
    {
        "question": "How does Principle Adaptation leverage LLM interactions to refine Updated Principles after architecture search iterations?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "However, due to domain shift where the architecture performs differently on different tasks, P may not be optimal for all the tasks. To alleviate the potential negative effects of domain shift, this work adapts P to the target task based on the newly found architectures. As a result, a task-specific subspace can be built progressively.",
            "Specifically, the NAS method is applied to find architecture from S_r in an iterative way. In each iteration, if the better-performing architectures are found, LLM is prompted to update P based on these architectures; otherwise, LLM is required to describe effects of other available candidate operators and information sources that are not in S_r, promoting the exploration for other promising regions in S_r."
        ],
        "final_answer": "Principle Adaptation uses iterative prompts to an LLM based on the outcomes of each search iteration. Whenever better architectures are discovered, those architectures are fed back to the LLM, which is asked to revise and enrich the design principles accordingly. If no improved architectures appear, the LLM is instead prompted to analyze the remaining unused operators and connections and describe their potential impact, thereby expanding exploration into other promising regions of the search space.",
        "relevant_elements": [
            "Principle Adaptation",
            "LLM"
        ],
        "id": 1818,
        "masked_question": "How does [mask1] leverage LLM interactions to refine Updated Principles after architecture search iterations?",
        "masked_number": 1,
        "masked_elements": [
            "Principle Adaptation"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.11330v1_figure_1.png",
        "paperid": "2408.11330v1",
        "paper_path": "./MISS-QA/papers/2408.11330v1.json",
        "figure_id": "2408.11330v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed LAPT. This framework consists of two stages. In the learning stage of design principles, LLM is driven by specific prompts to learn general design principles from a set of architectures. In the architecture search stage, the learned principles are applied to initialize the search space for each new task. Then, architectures found in the refined search space are used to update these principles, aiming to build the task-specific search space.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed module (the “Principle Adaptation” block at the bottom) uses LLM calls to refine the design principles after each NAS iteration:\n\n1.  Alignment – what the red box is  \n    •  In the diagram, that red rectangle encloses the “Principle Adaptation” step for task T.  It sits immediately downstream of the NAS loop: once you’ve searched the current subspace  and obtained new candidate architectures, you hand those results to the LLM.\n\n2.  Inputs to the LLM  \n    •  The newly found architectures (both the top performers and, if no improvements appear, examples of operators or connections that were tried but failed).  \n    •  The current set of design principles  that were used to carve out the last subspace.\n\n3.  The LLM prompt  \n    •  If the NAS run discovered better architectures, you prompt the LLM:  \n       “Here are the architectures that outperformed our previous best.  Please analyze which operators or connection patterns led to the gains, and revise our high-level design rules accordingly.”  \n    •  If the NAS run failed to beat the previous best, you instead prompt:  \n       “We got no improvement in this region.  Please suggest other operators or connections that we should consider, and update our principles to explore those areas.”\n\n4.  LLM reasoning → Updated Principles  \n    •  The LLM does a chain-of-thought style analysis on the concrete architectures, extracting which structural motifs or layer choices are consistently helpful (or unhelpful).  \n    •  It edits the textual “Principles P_T” – for example, adding a rule like “use 5×5 convolutions only in early layers” or dropping an outdated heuristic – to produce an improved, task‐specific principle set.\n\n5.  Feedback loop  \n    •  These updated principles are fed back into the next NAS iteration to re-refine the candidate operator set  and information‐source set  for each layer.  \n    •  Over successive cycles, the search space homes in on the subregion best suited to the task.\n\nIn short, that red‐boxed “Principle Adaptation” module is a tight LLM–NAS feedback loop: you run a NAS pass, summarize its outcomes to the LLM, let the LLM revise the abstract design principles, and then use those refined rules in the very next NAS search."
    },
    {
        "question": "How does ground-aware depth assumption integrate with virtual camera projection to compute pixel depths?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Ground-aware Assumption. For each pixel at coordinates (u, v) in the virtual view, its 3D coordinates in the virtual camera frame X_v are calculated based on the pixel’s position in the image and the depth assumptions. Let the camera height be h, the focal lengths of the camera be f_x and f_y, and the principal point (image center) be (c_x, c_y).",
            "We first project all pixels to the ground plane to compute the initial assumption of 3D coordinates in virtual camera frame as, [formula].",
            "The Euclidean distance to optical center is computed as d. Then we compare the distance d with threshold d_max, if d <= d_max, the points connected to corresponding pixels in the images are assumed on the ground, X_ground.",
            "If d > d_max, we assume that the points lie on a cylindrical-like surface at a fixed distance R from the camera’s optical center. In this case, the 3D coordinates are computed as: [formula] where R is the fixed radius."
        ],
        "final_answer": "Within the virtual camera projection, each pixel’s depth is first estimated by a ground-aware rule: project the virtual‐view pixel onto the ground plane to get a 3D coordinate and compute its Euclidean distance d from the camera’s optical center. If d is below a threshold (d_max), the pixel is assumed to lie on the ground and its 3D position (and hence depth) is taken directly from that ground projection. If d exceeds d_max, the pixel is instead assumed to lie on a cylindrical surface at fixed radius R around the camera; its 3D depth is then computed according to that cylinder assumption. These per‐pixel 3D coordinates are then used in the subsequent virtual‐to‐original camera warping steps.",
        "relevant_elements": [
            "Virtual Camera Projection",
            "Ground-aware Depth Assumption"
        ],
        "id": 1819,
        "masked_question": "How does [mask1] integrate with [mask2] to compute pixel depths?",
        "masked_number": 2,
        "masked_elements": [
            "Ground-aware Depth Assumption",
            "Virtual Camera Projection"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.13864v1_figure_2.png",
        "paperid": "2410.13864v1",
        "paper_path": "./MISS-QA/papers/2410.13864v1.json",
        "figure_id": "2410.13864v1_figure_2.png",
        "caption": "Figure 2: Overview of UniDrive framework. We transform the input images into a unified virtual camera space to achieve universal driving perception. To estimate the depth of pixels in the virtual view for projection, we propose a ground-aware depth assumption strategy. To obtain the most effective virtual camera space for multiple real camera configurations, we propose a data-driven CMA-ES (Hansen, 2016) based optimization strategy. To evaluate the efficacy of our framework, we propose an automatic data generation platform in CARLA (Dosovitskiy et al., 2017).",
        "qtype": "Implementation_Details",
        "response": "Let’s unpack it step by step:\n\n1.  Plug-and-Play Virtual Camera Projection (the blue box) needs a depth value for every pixel in its “virtual” view before it can carry out the usual 3D→2D reprojection into the real cameras.\n\n2.  The Ground-aware Depth Assumption (the red box) is exactly the sub‐module inside that virtual‐projection block that generates those per-pixel depths.  Concretely, for each pixel (u,v) in the virtual camera we do\n\n   a.  Use the intrinsics (focal lengths fx,fy and principal point) to turn (u,v) into a ray in the virtual‐camera frame.  \n   b.  Compute its Euclidean distance D to the camera center if the point lay on the ground plane.  \n   c.  Compare D to a threshold τ.  \n        –  If D<τ we assume the ray hits the flat ground: we solve for the depth z so that the 3D point lies on z=−h (camera height).  \n        –  Otherwise we pretend the ray hits a cylindrical shell of radius r around the camera and solve for that intersection depth.  \n\n3.  Those two simple closed‐form cases give you a scalar depth z(u,v) for every virtual‐view pixel.  You now have a full set of 3D points in the virtual frame.\n\n4.  The rest of the plug-and-play projection simply (i) transforms those 3D points into world coordinates, (ii) transforms into each real camera’s frame, and (iii) reprojects with the real camera intrinsics to find which pixel in the real images corresponds to (u,v).\n\nIn short, Ground-aware Depth Assumption lives inside the Virtual Camera Projection module, providing it with the per‐pixel depth hypotheses it needs to do the point‐wise warping between the virtual and the real cameras."
    },
    {
        "question": "How does iterative optimization adjust virtual camera configurations to minimize projection error across multi-camera setups?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "To achieve this, we adopt the heuristic optimization based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) (Hansen, 2016 ###reference_b9###) to find an optimized set of virtual camera configurations.",
            "Objective Function. Given multiple driving perception systems with varying multi-camera confgirations indexed by , the total error across all systems is expressed as , where  includes both the intrinsic and extrinsic camera parameters of virtual multi-camera framework,  is the total quantity of virtual cameras and  is the total quantity of multi-camera driving systems that share the same perception model. We aim to minimize this error by sampling and updating the virtual camera parameters iteratively through a CMA-ES based optimization method.",
            "Optimization Method. Our Optimization strategy begins by defining a multivariate normal distribution , where  represents the mean vector,  denotes the step size, and  is the covariance matrix at iteration . The configuration space  is discretized with a density , and  candidate configurations  are sampled at each iteration .",
            "Initialization begins with the initial mean , step size , and covariance matrix . The updated mean vector  is calculated in the subsequent iteration to serve as the new center for the search distribution concerning the virtual camera configuration. The process can be mathematically expressed as:\n\nwhere  is the number of top solutions selected to update , and  are weights determined by solution performance.",
            "The evolution path , which tracks the direction of successful optimization steps, is updated as:\n\nwhere  is the learning rate for updating the covariance matrix.",
            "The covariance matrix , which defines the distribution’s shape for camera configurations, is adjusted at each iteration as follows:\n\nSimilarly, the evolution path for the step size, , is updated, and the global step size  is then adjusted to balance exploration and exploitation:\n\nwhere  is the learning rate for updating , and  is a normalization factor controlling the adjustment rate of the global step size."
        ],
        "final_answer": "The virtual camera parameters (intrinsics and extrinsics) are optimized by treating them as a multivariate search distribution whose mean, covariance, and step‐size are iteratively updated via CMA‐ES.  At each iteration, a population of candidate virtual configurations is sampled from the current Gaussian (mean mᵗ, covariance Cᵗ, step‐size σᵗ), their projection errors are evaluated, and the best-performing candidates are used to update the mean mᵗ⁺¹.  An evolution path tracks the successful search direction and is used to update the covariance matrix Cᵗ⁺¹, shaping the search distribution, while a separate path adjusts the global step‐size σᵗ⁺¹.  This cycle of sampling, evaluation, and updating continues until the total reprojection error across all multi‐camera setups is minimized.",
        "relevant_elements": [
            "Iterative Optimization",
            "Virtual Camera Configurations"
        ],
        "id": 1820,
        "masked_question": "How does [mask1] adjust virtual camera configurations to minimize projection error across multi-camera setups?",
        "masked_number": 1,
        "masked_elements": [
            "Iterative Optimization"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.13864v1_figure_2.png",
        "paperid": "2410.13864v1",
        "paper_path": "./MISS-QA/papers/2410.13864v1.json",
        "figure_id": "2410.13864v1_figure_2.png",
        "caption": "Figure 2: Overview of UniDrive framework. We transform the input images into a unified virtual camera space to achieve universal driving perception. To estimate the depth of pixels in the virtual view for projection, we propose a ground-aware depth assumption strategy. To obtain the most effective virtual camera space for multiple real camera configurations, we propose a data-driven CMA-ES (Hansen, 2016) based optimization strategy. To evaluate the efficacy of our framework, we propose an automatic data generation platform in CARLA (Dosovitskiy et al., 2017).",
        "qtype": "Implementation_Details",
        "response": "Here, [mask1] is the little red‐boxed “Iterative Optimization” module on the right side of the diagram – in other words, our CMA-ES loop.  Concretely, it works as follows:\n\n1.  Initialize a multivariate normal search distribution over the full set of virtual‐camera parameters (both intrinsics and extrinsics) with some mean μ₀, step-size σ₀ and covariance Σ₀.  \n2.  At each iteration t, draw a batch of candidate configurations {Θₜ⁽i⁾} from 𝒩(μₜ, σₜ²Σₜ).  \n3.  For each candidate Θₜ⁽i⁾, perform the virtual‐camera reprojection of all 3D boxes in all real‐camera setups and compute the total weighted projection–angle error (our objective E(Θₜ⁽i⁾)).  \n4.  Rank the candidates by their error, pick the top λ* performers, and recompute the new mean μₜ₊₁ and update the evolution paths.  \n5.  Update the covariance Σ and step size σ according to the standard CMA-ES rules so that future samples concentrate in parameter regions that produced low reprojection error.  \n6.  Repeat sampling, evaluation, selection and parameter-distribution updates until convergence.\n\nBy continuously re-centering and reshaping the search distribution around the best‐performing virtual‐camera configurations, the algorithm “zeroes in” on the set of intrinsics/extrinsics that minimize the projection error across all multi-camera driving rigs."
    },
    {
        "question": "How does Ground-aware Depth Assumption interplay with Minimizing Projection Error strategies to enhance virtual view transformations?",
        "relevant_section_ids": [
            "3.2",
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Ground-aware Assumption. For each pixel at coordinates uᵢⱼ in the virtual view, its 3D coordinates in the virtual camera frame Xᵢⱼ are calculated based on the pixel’s position in the image and the depth assumptions. We first project all pixels to the ground plane to compute the initial assumption of 3D coordinates in virtual camera frame as … Then we compare the distance dᵢⱼ with threshold D_thres; if dᵢⱼ ≤ D_thres, the points connected to corresponding pixels in the images are assumed on the ground. If dᵢⱼ > D_thres, we assume that the points lie on a cylindrical-like surface at a fixed distance R from the camera’s optical center.",
            "To evaluate the accuracy of the Virtual Camera Projection method in the context of a 3D object detection task, we propose a weighted projection error metric based on angular discrepancies between the virtual and original camera views. … The weighted error for each corner point is then calculated as Êₚ = dₚ · (|Δθₚ| + |Δφₚ|).",
            "Given a set of multi-camera systems, we aim to design a unified virtual camera configuration that minimizes the reprojection error across all original camera configurations. To achieve this, we adopt the heuristic optimization based on the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) … The objective is to minimize the total projection error E(Θ) by iteratively sampling and updating the virtual camera parameters."
        ],
        "final_answer": "The ground-aware depth assumption provides an initial per-pixel 3D hypothesis—pixels closer than a threshold are placed on the ground plane, while farther pixels are lifted onto a cylindrical surface—thus yielding a first set of virtual camera projections. The minimizing-projection-error strategy then measures the angular and distance discrepancies between these ground-informed projections and the true virtual view via a weighted error metric. Finally, it uses an optimizer (CMA-ES) to adjust virtual camera parameters so as to minimize that projection error. In combination, the ground-aware assumption shapes the projection candidates and the error-minimization loop refines the virtual camera configuration, resulting in more accurate and robust virtual view transformations.",
        "relevant_elements": [
            "Ground-aware Depth Assumption",
            "Minimizing Projection Error"
        ],
        "id": 1821,
        "masked_question": "How does [mask1] interplay with [mask2] strategies to enhance virtual view transformations?",
        "masked_number": 2,
        "masked_elements": [
            "Ground-aware Depth Assumption",
            "Minimizing Projection Error"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.13864v1_figure_2.png",
        "paperid": "2410.13864v1",
        "paper_path": "./MISS-QA/papers/2410.13864v1.json",
        "figure_id": "2410.13864v1_figure_2.png",
        "caption": "Figure 2: Overview of UniDrive framework. We transform the input images into a unified virtual camera space to achieve universal driving perception. To estimate the depth of pixels in the virtual view for projection, we propose a ground-aware depth assumption strategy. To obtain the most effective virtual camera space for multiple real camera configurations, we propose a data-driven CMA-ES (Hansen, 2016) based optimization strategy. To evaluate the efficacy of our framework, we propose an automatic data generation platform in CARLA (Dosovitskiy et al., 2017).",
        "qtype": "Literature_Background",
        "response": "Here is the high‐level “story” of how the red‐boxed Ground-aware Depth Assumption feeds into the blue-boxed Minimizing Projection‐Error loop to give you crisp, stable virtual views:\n\n1. Ground-aware Depth Assumption (red box)  \n   –  We know that most of the scene around a car is either “flat” (road) or roughly “cylindrical” (other cars, poles, people).  \n   –  So for each pixel in our virtual view we pick one of two depth priors:  \n     •  If its distance to the virtual optical center is below some threshold  it lies on the ground plane (use a planar formula)  \n     •  Otherwise we assume it lies on a fixed–radius cylinder around the car.  \n   –  That gives us a cheap, closed-form 3D estimate for every virtual‐pixel → world‐point.\n\n2. Point-wise / Image-level Projection  \n   –  Using those depth‐tagged 3D points we project them back into each real camera via its extrinsic + intrinsic matrices.  \n   –  We warp and blend all the real‐camera images into a single synthetic “virtual” image.\n\n3. Computing a Projection-Error (blue box)  \n   –  To judge how well our depth‐+-camera assumptions line up, we re-project known 3D box corners through our entire pipeline and measure angular errors in pitch/yaw.  \n   –  We weight each corner by its distance and sum up into a single scalar loss.\n\n4. Iterative Minimization with CMA-ES  \n   –  Now that we have a differentiable “projection‐error” loss, we wrap it inside a CMA-ES loop that tweaks:  \n     •  the virtual-camera extrinsics/intrinsics  \n     •  (optionally) the ground/cylinder threshold   \n   –  After each sample we recompute the ground-aware projections, re-warp the views, re-evaluate the angular loss, and guide CMA-ES to lower that error.\n\nIn short: the ground-aware depth assumption gives you a fast, physics-inspired way to lift pixels into 3D; the minimizing-projection-error machinery then closes the loop, automatically adjusting your virtual‐camera setup (and even your depth thresholds) until those lifted points line up as precisely as possible with all real cameras. The result is a virtual view transformation that is both geometrically sound (thanks to the depth prior) and empirically tight (thanks to the reprojection‐error minimization)."
    },
    {
        "question": "How does AFR integrate self-attention and feed-forward layers to optimize features for Riemann geometry projections?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The textual AFR and point cloud AFR are identical, with each consisting of a stack of six Self-Attention Encoders (Vaswani et al. 2017). These AFR modules fine-tune the features of their respective modalities and map them into a common feature space, enabling the subsequent computation of Riemann Attention. Internally, each AFR layer consists of multi-head self-attention (MSA) sub-layers and feed-forward neural network (FFN) sub-layers. Each of these sub-components (MSA and FFN) is encapsulated within residual connections and layer normalization operations.",
            "The AFR receives text or point cloud inputs, using a scaled dot-product attention mechanism to describe both visual and textual features. The output of the self-attention operator is defined as: ...",
            "We utilize a compact feed-forward network (FFN) to extract features, which are already integrated into more extensive representations. The FFN is composed of two nonlinear layers: where a and b are hyperparameters, v represents the input vector, W₁ and W₂ are learnable weight matrices, and b₁ and b₂ are bias terms.",
            "A complete encoding layer (Enc) can be described as follows: where Add & Norm includes a residual connection and layer normalization. The multi-layer encoder (AFR) is constructed by stacking these encoding layers sequentially, with the input of each layer being derived from the output of the preceding layer. In the AFR, stacking multiple encoder layers enables the automatic adjustment of weights between features, ensuring that crucial ones receive greater attention."
        ],
        "final_answer": "The AFR applies a stack of six transformer‐style encoder layers—each combining a multi‐head self‐attention sublayer and a two‐layer feed‐forward network—wrapped with residual connections and layer normalization. By first using scaled dot‐product self‐attention to capture contextual relationships and then refining these attended features through the feed‐forward network, AFR adaptively weights and enhances the initial text or point‐cloud representations. This produces optimized features in a common space that are well‐suited for subsequent Riemannian geometry projections.",
        "relevant_elements": [
            "Adaptive Feature Refiner",
            "Riemann Geometry Projection"
        ],
        "id": 1823,
        "masked_question": "How does [mask1] integrate self-attention and feed-forward layers to optimize features for Riemann geometry projections?",
        "masked_number": 1,
        "masked_elements": [
            "Adaptive Feature Refiner"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.13712v1_figure_2.png",
        "paperid": "2408.13712v1",
        "paper_path": "./MISS-QA/papers/2408.13712v1.json",
        "figure_id": "2408.13712v1_figure_2.png",
        "caption": "Figure 2: The overall architecture of RMARN proposed in this article. The Global Pooling Similarity module directly calculates the cosine distance between the text feature sequence and the point cloud feature sequence after pooling, while the Riemann Local Similarity module considers the point cloud sequence and the text sequence as two fields on a manifold and calculates the similarity between any two token pairs. Among them, Tμ(Pi)superscriptsubscript𝑇𝜇subscript𝑃𝑖T_{\\mu}^{(P_{i})}italic_T start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT and Pμ(Qi)superscriptsubscript𝑃𝜇subscript𝑄𝑖P_{\\mu}^{(Q_{i})}italic_P start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT represent the i-th token of the text and point cloud feature sequence, respectively.",
        "qtype": "Literature_Background",
        "response": "The module boxed in red in Figure 2 is nothing more exotic than a small Transformer encoder stack – what the paper calls the “Adaptive Feature Refiner” (AFR) – sitting in front of the Riemann‐geometry projection blocks.  In practice it works exactly like a standard Transformer encoder layer:\n\n  1.  It takes the raw CLIP/text or PointNet/cloud embedding and first runs it through a multi-head self-attention (MSA) sub-layer.  This lets every token (word or 3D point descriptor) “look at” all the others and re-weight itself in light of the full context.  \n  2.  The output of that MSA goes through a residual‐connection + layer‐norm (“Add & Norm”).  \n  3.  Next, it passes through a small two‐layer feed-forward network (FFN) with a non‐linear activation (GELU).  \n  4.  Finally you again apply a residual‐connection + layer‐norm around the FFN output.  \n\nThey stack six of these identical encoder blocks back-to-back.  By interleaving self-attention (to capture global, token-wise relationships) with per-token non-linear transformations (the FFN) and wrapping each in residual connections and layer‐norm, the AFR “sharpens” and re-weights the raw features so that when they enter the subsequent Riemann Local Similarity projector, they already live in a smooth, common embedding space that is ideally suited for parallel transport and manifold‐based dot-products."
    },
    {
        "question": "How does LRF module build on low-rank matrix decomposition theories to enhance Riemann Local Similarity outputs?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Given the inherent constraints of compressing data within the model, redundant information inevitably persists within both point cloud feature sequences and text feature sequences, hindering the model’s generalization capabilities and exacerbating computational intricacies. Consequently, it becomes imperative to leverage low-rank priors (Hu et al. 2021  ###reference_b8###) as a means of eliminating this redundant information.",
            "When given the original feature map  containing redundant information, we can use the following equation to extract the low rank component  from it: where  is the regularization coefficient that balances sparse loss and data restoration loss. Assuming  is orthogonal, then the minimization problem has a closed solution , where  is the soft interval function:",
            "This article uses neural networks to approximate the mapping of . Since the total similarity  is a function of , it is: Therefore, a complete neural network can be used to simultaneously approximate without explicitly approximating  and  separately."
        ],
        "final_answer": "The LRF module leverages low-rank matrix decomposition by casting the Riemann Local Similarity feature map as a sum of a low-rank component and noise, minimizing ||X – X_low||²_F + λ||D X||₁ to recover X_low via soft-thresholding under an orthogonality assumption. It then uses a neural network to learn this low-rank mapping and directly compute the refined similarity S_total = gₚ(X_low), thereby removing redundancy, focusing on sparse yet informative correspondences, and enhancing the robustness of the local similarity outputs.",
        "relevant_elements": [
            "Low Rank Filter",
            "Riemann Local Similarity"
        ],
        "id": 1824,
        "masked_question": "How does LRF module build on low-rank matrix decomposition theories to enhance [mask1] outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Riemann Local Similarity"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.13712v1_figure_2.png",
        "paperid": "2408.13712v1",
        "paper_path": "./MISS-QA/papers/2408.13712v1.json",
        "figure_id": "2408.13712v1_figure_2.png",
        "caption": "Figure 2: The overall architecture of RMARN proposed in this article. The Global Pooling Similarity module directly calculates the cosine distance between the text feature sequence and the point cloud feature sequence after pooling, while the Riemann Local Similarity module considers the point cloud sequence and the text sequence as two fields on a manifold and calculates the similarity between any two token pairs. Among them, Tμ(Pi)superscriptsubscript𝑇𝜇subscript𝑃𝑖T_{\\mu}^{(P_{i})}italic_T start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT and Pμ(Qi)superscriptsubscript𝑃𝜇subscript𝑄𝑖P_{\\mu}^{(Q_{i})}italic_P start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT represent the i-th token of the text and point cloud feature sequence, respectively.",
        "qtype": "Literature_Background",
        "response": "First, let’s be crystal about what lives inside that red box.  It is precisely the Riemann-Local-Similarity pipeline – i.e. the **token-level similarity maps** (sometimes called the Riemann Attention Map) that come out of the AFR → Geometry-Projection block.  Those raw similarity maps still carry a lot of “junk’’ or redundancy, so the authors graft on the Low–Rank-Filter (LRF) module.  \n\nHere is how the LRF borrows from classical low-rank matrix decomposition to **clean up** those similarity maps:\n\n 1.  **Start with the raw similarity tensor** S (shape: #text-tokens × #point-tokens).  Because S is computed pairwise on every token, it contains many weak or spurious correlations.\n\n 2.  **Pose a decomposition**  \n      X* = argmin_X  ∥X–S∥_F²  +  λ‖D X‖₁  \n    In words, we seek X as close as possible to S in Frobenius norm, but also sparse (or low-rank) when viewed through some difference operator D.  \n\n 3.  **Exploit the closed-form “soft‐thresholding”** solution.  Under orthonormality assumptions (or by SVD), the minimizer X* is given by applying a soft‐threshold to the singular values of S.  This is exactly the classic matrix “denoise by keeping only the big singular values’’ trick.\n\n 4.  **Amortize it with a tiny neural net**.  Rather than literally SVD every time, they train a small feed-forward subnetwork to mimic the S→X mapping.  At runtime you feed in the raw map S, and the net spits out a low-rank, sparse version X̂.  \n\n 5.  **Plug the cleaner map X̂ back in**.  This “denoised” similarity map now has far fewer spurious entries and far stronger peaks on the truly important token‐to‐token correspondences.  Downstream computations (e.g. the final similarity convolution or the contrastive loss) then operate on these compressed, high-signal maps rather than the full, noisy originals.\n\nBy doing that, the LRF module turns a very high-dimensional, overcomplete local-similarity tensor into a much more compact and robust set of token-level affinities, giving you all of the fine‐grained alignment you wanted but at a fraction of the noise and computational cost."
    },
    {
        "question": "How does the Low Rank Filter reduce redundant features prior to computing Riemann Local Similarity?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Given the inherent constraints of compressing data within the model, redundant information inevitably persists within both point cloud feature sequences and text feature sequences, hindering the model’s generalization capabilities and exacerbating computational intricacies.",
            "When given the original feature map X containing redundant information, we can use the following equation to extract the low rank component M from it:",
            "Assuming D is orthogonal, then the minimization problem has a closed solution M = S_λ(X), where S_λ is the soft interval function:",
            "This article uses neural networks to approximate the mapping of M."
        ],
        "final_answer": "The Low Rank Filter formulates a minimization problem that decomposes the input feature map X into a low-rank component M by balancing a reconstruction term (||X–M||_F^2) and an ℓ₁ sparsity penalty (λ||D M||₁). Under an orthogonality assumption on D, this has a closed-form solution given by a soft-thresholding (soft interval) operator S_λ(X). In practice, the network learns to approximate this soft-thresholding mapping, thereby pruning redundant features before the Riemann Local Similarity computation.",
        "relevant_elements": [
            "Low Rank Filter",
            "Riemann Local Similarity"
        ],
        "id": 1825,
        "masked_question": "How does the [mask1] reduce redundant features prior to computing Riemann Local Similarity?",
        "masked_number": 1,
        "masked_elements": [
            "Low Rank Filter"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.13712v1_figure_2.png",
        "paperid": "2408.13712v1",
        "paper_path": "./MISS-QA/papers/2408.13712v1.json",
        "figure_id": "2408.13712v1_figure_2.png",
        "caption": "Figure 2: The overall architecture of RMARN proposed in this article. The Global Pooling Similarity module directly calculates the cosine distance between the text feature sequence and the point cloud feature sequence after pooling, while the Riemann Local Similarity module considers the point cloud sequence and the text sequence as two fields on a manifold and calculates the similarity between any two token pairs. Among them, Tμ(Pi)superscriptsubscript𝑇𝜇subscript𝑃𝑖T_{\\mu}^{(P_{i})}italic_T start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT and Pμ(Qi)superscriptsubscript𝑃𝜇subscript𝑄𝑖P_{\\mu}^{(Q_{i})}italic_P start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT represent the i-th token of the text and point cloud feature sequence, respectively.",
        "qtype": "Experimental_Results",
        "response": "The little “red‐boxed” block sitting immediately upstream of the Riemann Local Similarity operator in Fig. 2 is nothing more exotic than a pair of Low-Rank Filters (one for the point cloud branch, one for the text branch).  Its sole job is to strip away as much of the “flat,” redundant information in each token sequence as it can before we go and compute our costly manifold-based attention.  Concretely:\n\n 1. We start from the full AFR-refined feature tensor X (either X=T for text or X=P for point clouds) that still contains both signal and a lot of highly correlated, low-information components.  \n 2.  We pose a small convex sub‐problem  \n       Z⋆ = argmin_Z  ||X – Z||_F²  + λ·||D X||₁,  \n    where D is chosen orthogonal so that this decomposes X into a “sparse” plus “low-rank” part.  \n 3.  Thanks to the orthogonality of D, the above has a closed‐form solution in one line:  \n       Z⋆ = Dᵀ·SoftThreshold(D X, λ).  \n 4.  In practice we do not run an explicit singular‐value decomposition; we merely train a tiny neural module to mimic exactly that soft‐thresholding step.  \n\nThe net effect is that Z⋆ carries all of the “interesting” token–token variations that we really want to compare on the manifold, while jettisoning the bulk of the trivial, redundant modes.  By feeding Z⋆ (instead of X) into the Riemann Local Similarity block, we reduce both parameter counts and computation while at the same time sharpening the model’s attention on the handful of correspondences that actually matter."
    },
    {
        "question": "What role does the Adaptive Feature Refiner serve before Riemannian Geometry Projection in preparing modality features?",
        "relevant_section_ids": [
            "3",
            "3.1"
        ],
        "relevant_context": [
            "After initial feature extraction, features from both modalities undergo further refinement through their respective Adaptive Feature Refiners (AFRs). These refiners are specialized modules designed to enhance the quality of extracted features by adapting them to the specific characteristics of the task at hand. This refinement process results in highly detailed representations, denoted as  for text and  for point clouds, where  and  represent the sequence lengths, and  and  represent the dimensionality of the features in their respective domains.",
            "The textual AFR and point cloud AFR are identical, with each consisting of a stack of six Self-Attention Encoders (Vaswani et al. 2017  ###reference_b23###). These AFR modules fine-tune the features of their respective modalities and map them into a common feature space, enabling the subsequent computation of Riemann Attention."
        ],
        "final_answer": "Before the Riemannian Geometry Projection, the Adaptive Feature Refiner (AFR) fine-tunes and adaptively enhances the raw modality features—using stacks of self-attention and feed-forward layers—and maps both text and point-cloud features into a shared, high-quality feature space suitable for subsequent Riemannian projections and similarity computations.",
        "relevant_elements": [
            "Adaptive Feature Refiner",
            "Riemannian Geometry Projection"
        ],
        "id": 1826,
        "masked_question": "What role does the [mask1] serve before Riemannian Geometry Projection in preparing modality features?",
        "masked_number": 1,
        "masked_elements": [
            "Adaptive Feature Refiner"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.13712v1_figure_2.png",
        "paperid": "2408.13712v1",
        "paper_path": "./MISS-QA/papers/2408.13712v1.json",
        "figure_id": "2408.13712v1_figure_2.png",
        "caption": "Figure 2: The overall architecture of RMARN proposed in this article. The Global Pooling Similarity module directly calculates the cosine distance between the text feature sequence and the point cloud feature sequence after pooling, while the Riemann Local Similarity module considers the point cloud sequence and the text sequence as two fields on a manifold and calculates the similarity between any two token pairs. Among them, Tμ(Pi)superscriptsubscript𝑇𝜇subscript𝑃𝑖T_{\\mu}^{(P_{i})}italic_T start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT and Pμ(Qi)superscriptsubscript𝑃𝜇subscript𝑄𝑖P_{\\mu}^{(Q_{i})}italic_P start_POSTSUBSCRIPT italic_μ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_POSTSUPERSCRIPT represent the i-th token of the text and point cloud feature sequence, respectively.",
        "qtype": "Experimental_Results",
        "response": "The mystery block in the red box is nothing exotic – it is simply the “Adaptive Feature Refiner” (AFR) sitting in front of the Riemannian Geometry Projection.  In other words, before we ever try to treat our text‐tokens and point‐cloud tokens as points on a manifold, we first run them through a stack of self‐attention + feed-forward layers that\n\n  1.  Contextualize each token (so that each one “knows” about its neighbours),  \n  2.  Re-weight and denoise the raw CLIP or PointNet output,  \n  3.  Project both modalities into a single, shared latent space  \n\nOnly once both streams have been “adaptively refined” in this way do we hand them off to the Riemannian projection module."
    },
    {
        "question": "How does Execution-based Representative Sample Selection influence seed inputs for genetic operators in GO-COT prompting?",
        "relevant_section_ids": [
            "2.2",
            "2.4"
        ],
        "relevant_context": [
            "As shown in Algorithm 1, SBLLM abstracts the correct code based on the ASTs (Abstract Syntax Trees), and ensures that only one sample with identical abstractions can be chosen. The top k samples are retained as the selected representative samples S, while the remaining samples are discarded.",
            "The first two steps involve combining the advantages observed in the selected representative samples S and referring to the retrieved patterns to identify unexploited optimization methods, which correspond to the crossover and mutation operators in the evolutionary algorithm, respectively.",
            "The Input Placeholder includes the code that LLMs need to optimize along with the representative samples S and the retrieved patterns P."
        ],
        "final_answer": "By evaluating each optimized code snippet for correctness and speedup, SBLLM’s execution‐based sample selection picks a small, diverse set of high‐fitness code variants (the representative samples S). These selected samples then serve as the seed inputs for GO-COT’s genetic operators: the crossover step combines their optimization traits, and the mutation step explores new methods, guided by the provided patterns.",
        "relevant_elements": [
            "Execution-based Representative Sample Selection",
            "Genetic Operator-inspired Chain-of-thought Prompting"
        ],
        "id": 1827,
        "masked_question": "How does [mask1] influence seed inputs for genetic operators in GO-COT prompting?",
        "masked_number": 1,
        "masked_elements": [
            "Execution-based Representative Sample Selection"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.12159v1_figure_1.png",
        "paperid": "2408.12159v1",
        "paper_path": "./MISS-QA/papers/2408.12159v1.json",
        "figure_id": "2408.12159v1_figure_1.png",
        "caption": "Figure 1: The overview of SBLLM.",
        "qtype": "Experimental_Results",
        "response": "Let me walk through the red‐boxed region and see how it feeds into the GO-COT prompt:\n\n1.  Image‐Text Alignment  \n    – The red box in Fig. 1 denotes “Execution-based Representative Sample Selection.”  \n    – Text (Sec II-B) tells us this module takes all of the current optimized candidates, runs them against test cases, computes accuracy and speedup, then:  \n      • splits them into correct/incorrect groups  \n      • abstracts each via its AST to detect duplicates  \n      • re-ranks by speedup, selects one example per distinct AST, and tacks on the most informative incorrect variants  \n    – The output is a small set of high-fitness, diverse “representative samples.”\n\n2.  How those samples become seed inputs for genetic operators in GO-COT  \n    – In Sec II-D the GO-COT prompt has an “Input Placeholder” that is populated by exactly these representative samples (denoted Ŝ).  \n    – The genetic‐operator instructions then explicitly tell the LLM to perform:  \n       a) a “crossover” step – combining advantageous traits from the Ŝ samples, and  \n       b) a “mutation” step – drawing in fresh optimizations from the retrieved patterns.  \n    – Thus, the samples selected in step 1 act as the parent‐code “seeds” that the crossover and mutation operators in the GO-COT chain-of-thought use to spawn a new, hopefully better, optimized program.\n\nAnswer  \n[mask1] is the “Execution-based Representative Sample Selection” module.  By filtering, abstracting, and re-ranking candidate snippets according to correctness, speedup, and AST diversity, it outputs a small set of representative code samples that directly fill the seed‐input slot of the GO-COT prompt.  Those samples serve as the parent codes for the crossover and mutation operators."
    },
    {
        "question": "What limitations might execution-based representative sample selection encounter with non-deterministic code outputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Execution-based Representative Sample Selection"
        ],
        "id": 1829,
        "masked_question": "What limitations might [mask1] encounter with non-deterministic code outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Execution-based Representative Sample Selection"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.12159v1_figure_1.png",
        "paperid": "2408.12159v1",
        "paper_path": "./MISS-QA/papers/2408.12159v1.json",
        "figure_id": "2408.12159v1_figure_1.png",
        "caption": "Figure 1: The overview of SBLLM.",
        "qtype": "Others",
        "response": "Here’s a step‐by‐step reasoning leading to the answer:\n\n1.  In the figure, the red‐boxed block is the **“Execution‐based Representative Sample Selection”** module.  \n2.  That module judges each candidate patch by actually running it on test cases and measuring  \n    -  correctness (does it still produce the right outputs?) and  \n    -  performance (how much speed-up does it deliver?).  \n3.  All of its downstream decisions—AST–based abstraction, de-duplication and the eventual re-ranking of samples—hinge on those measured fitness scores.  \n4.  **If a piece of code is non-deterministic** (for example it uses randomness, thread timing, or unseeded PRNGs, or it invokes external services whose responses can vary), then:  \n    -  its “correctness” may fluctuate from run to run, so you can’t tell reliably whether it truly still meets the spec, and  \n    -  its timing measurements become noisy (one run looks fast, the next slow), so speed-up numbers are unstable.  \n5.  Unstable fitness estimates will cause the sample‐selection stage to mis‐rank patches (discard good ones or keep bad ones), and it may even fail to converge because the “best” samples keep changing on you.  \n\nAnswer:\nExecution‐based sample selection relies on repeatable correctness and timing measurements. With non‐deterministic outputs, both pass/fail outcomes and speed-up estimates become noisy, causing unreliable fitness scores, mis-ranking of samples, and extra overhead (more runs) needed to stabilize the measurements."
    },
    {
        "question": "What potential ethical considerations arise when using genetic operator-inspired chain-of-thought prompting to evolve code?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Genetic Operator-inspired Chain-of-thought Prompting"
        ],
        "id": 1830,
        "masked_question": "What potential ethical considerations arise when using [mask1] to evolve code?",
        "masked_number": 1,
        "masked_elements": [
            "Genetic Operator-inspired Chain-of-thought Prompting"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.12159v1_figure_1.png",
        "paperid": "2408.12159v1",
        "paper_path": "./MISS-QA/papers/2408.12159v1.json",
        "figure_id": "2408.12159v1_figure_1.png",
        "caption": "Figure 1: The overview of SBLLM.",
        "qtype": "Others",
        "response": "1.  Identifying “[mask1]”  \n    – In the figure, the red-boxed module on the right is labeled “Genetic Operator-inspired Chain-of-Thought Prompting” (GO-COT). This component instructs the LLM to perform crossover and mutation on earlier code variants to produce new, evolved code.  \n\n2.  Why genetic-operator prompting might raise ethical flags  \n    a. Intellectual-Property and Licensing Risks  \n       • Crossover of two snippets could inadvertently blend proprietary or GPL-licensed fragments into a new variant, creating unclear provenance.  \n       • Mutation steps might transplant small but critical pieces of copyrighted code from the model’s training data without proper attribution or permission.  \n\n    b. Security and Safety Vulnerabilities  \n       • Automated recombination can introduce unforeseen bugs, insecure API calls, or backdoors—particularly since the model is not formally verifying security invariants.  \n       • Because GO-COT “reasons” through multiple generations, it may amplify a latent vulnerability across descendants.  \n\n    c. Accountability & Transparency  \n       • When an evolved snippet causes a failure (e.g. a crash or data leak), it can be very hard to trace which crossover or mutation step introduced the flaw.  \n       • The chain-of-thought reasoning, while human-readable, may not map neatly to actual code transformations, making audits or legal forensics difficult.  \n\n    d. Bias and Fairness  \n       • If the training corpus contains biased or non-portable optimization patterns, GO-COT may repeatedly reproduce those biases (e.g. favoring a particular hardware vendor’s APIs).  \n       • The “fitness” criteria (speedup + correctness) may prioritize micro-optimizations that disadvantage maintainability or accessibility, with downstream social costs.  \n\n    e. Dual-Use & Malicious Applications  \n       • An adversary could weaponize GO-COT to evolve malware faster—using crossover to recombine known exploits and mutation to mask signatures.  \n       • Without strict guardrails, the same machinery that hunts for performance hotspots can hunt for security vulnerabilities in third-party code.  \n\n    f. Workforce & Socioeconomic Impact  \n       • Automating iterative optimization risks displacing human performance-engineers or demoting their role to merely supervising model outputs.  \n       • Overreliance on black-box evolution tools may erode deep expertise in secure, clean coding practices over time.  \n\n3.  In summary  \n   Embedding genetic operators into an LLM’s prompt template unlocks a powerful search-and-refine capability—but it also raises thorny questions about copyright infringement, code provenance, security vetting, explainability, bias amplification, potential for misuse, and the broader impact on developer roles and responsibilities."
    },
    {
        "question": "What are the limitations of using Foldseek for encoding protein 3Di sequences?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "The performance of structural pre-training heavily depends on Foldseek [foldseek], which aims to balance search efficiency and encoding accuracy. Thus there is still room for improving the representation capability of all methods building upon Foldseek, including ours."
        ],
        "final_answer": "Because Foldseek prioritizes a trade-off between search speed and encoding accuracy, its 3Di representations are not maximally detailed, and methods that rely on it (including S2ALM) have room to improve their structural representation capability.",
        "relevant_elements": [
            "Foldseek",
            "protein 3Di sequences"
        ],
        "id": 1831,
        "masked_question": "What are the limitations of using [mask1] for encoding [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Foldseek",
            "protein 3Di sequences"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.15215v1_figure_1.png",
        "paperid": "2411.15215v1",
        "paper_path": "./MISS-QA/papers/2411.15215v1.json",
        "figure_id": "2411.15215v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed hierarchical pre-training paradigm containing two stages. a, In stage I, S2ALM aims at general sequence-structure learning with protein sequences and structures. In stage II, S2ALM learns antibody specific multi-level knowledge using antibody sequences and structures. b, Masked Language Modeling (MLM) reconstructs the masked tokens based on the contextualized information. c, Sequence-Structure Matching (SSM) identifies the matching relationships between 1D and 3Di sequences. d, Cross-Level Reconstruction (CLR) reconstructs the corrupted tokens based on hybrid information from both 1D and 3Di sequences.",
        "qtype": "Others",
        "response": "First, let’s align the two highlighted regions to the text:\n\n1. The red‐boxed module is Foldseek – the lightweight structural‐search/encoding engine used in Stage I.  \n2. The blue‐boxed data stream is the “protein 3Di sequences” that Foldseek spits out (i.e. a discretized, 1D representation of a protein’s 3D fold).\n\nSo the question becomes:\n\n  “What are the limitations of using Foldseek for encoding protein 3Di sequences?”\n\nChain of thought:\n\n– In Stage I S²ALM we rely on Foldseek to turn full 3D atomic coordinates into a string of “3Di” tokens—that is, a coarse‐grained structural alphabet.  \n– Foldseek was explicitly designed to make structural searches very fast, even over millions of chains, by simplifying the geometry into an approximate alphabet.  \n– That very simplification means it gives up some representational fidelity. Subtle backbone twists, side‐chain orientations or long‐range tertiary contacts may collapse into the same discrete symbol.  \n– In practical terms, that limits how much fine‐grained structural information your downstream masked‐LM can learn from those 3Di sequences.  \n– As the authors note, “the performance of structural pre‐training heavily depends on Foldseek… thus there is still room for improving the representation capability of all methods building upon Foldseek.”\n\nAnswer:\n\nUsing Foldseek to encode protein 3Di sequences enforces a trade-off between speed and granularity. Its coarse structural alphabet makes searches super-fast but necessarily collapses many detailed geometric features into a handful of tokens. As a result, subtle conformational nuances and long-range tertiary interactions can be lost, capping the fidelity of any structural representations learned downstream and leaving headroom for more accurate (though likely slower) encodings."
    },
    {
        "question": "What biases might computationally predicted IgFold structures introduce into Sequence-Structure Matching?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "IgFold",
            "Sequence-Structure Matching"
        ],
        "id": 1832,
        "masked_question": "What biases might computationally predicted [mask1] structures introduce into Sequence-Structure Matching?",
        "masked_number": 1,
        "masked_elements": [
            "IgFold"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.15215v1_figure_1.png",
        "paperid": "2411.15215v1",
        "paper_path": "./MISS-QA/papers/2411.15215v1.json",
        "figure_id": "2411.15215v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed hierarchical pre-training paradigm containing two stages. a, In stage I, S2ALM aims at general sequence-structure learning with protein sequences and structures. In stage II, S2ALM learns antibody specific multi-level knowledge using antibody sequences and structures. b, Masked Language Modeling (MLM) reconstructs the masked tokens based on the contextualized information. c, Sequence-Structure Matching (SSM) identifies the matching relationships between 1D and 3Di sequences. d, Cross-Level Reconstruction (CLR) reconstructs the corrupted tokens based on hybrid information from both 1D and 3Di sequences.",
        "qtype": "Others",
        "response": "First, let’s pin down what “\\[mask1\\]” is referring to.  In the figure it’s the set of structures coming from ABodyBuilder2 and IgFold—i.e. computationally predicted antibody structures.  \n\nSo the question really is:\n\n  “What biases might computationally predicted antibody structures introduce into the Sequence–Structure Matching task?”\n\nHere’s a step-by-step reasoning:\n\n1.  **Role of predicted structures in SSM.**  \n    In Stage II, S2ALM is trained to tell whether a 1D antibody sequence and a 3Di “structure” really belong together.  To get enough structural examples they mix in both experimentally solved antibody structures (from SAbDab) and *predicted* structures (from IgFold, ABodyBuilder2).\n\n2.  **Nature of prediction‐model artifacts.**  \n    Any structure‐prediction engine has its own quirks.  – It may systematically smooth out or regularize certain loops (especially the highly variable CDR-H3).  \n    – It may over-rely on particular templates or statistical priors learned from its training set (e.g. more human or mouse antibodies).  \n    – It cannot represent conformational heterogeneity or dynamic motions that real antibodies exhibit.\n\n3.  **How those artifacts bias SSM.**  \n    Because SSM is a binary classifier trained to spot “match vs. mismatch” between sequence and 3Di tokens, it can end up keying off of the *idiosyncrasies* of predicted structures—  \n    – learning to discriminate on small but systematic differences in 3Di token patterns that are *prediction-model specific*, not biophysically real.  \n    – overfitting to the kind of folds that IgFold/ABodyBuilder tend to spit out, and thus failing when confronted with an unusual experimental structure or a truly novel CDR conformation.  \n    – inadvertently building in species or framework biases inherited from the prediction‐engine’s training data (e.g. it may do better on human‐like loops if IgFold was mostly trained on human antibodies).\n\n4.  **The bottom line.**  \n    By injecting large numbers of *predicted* antibody structures, SSM can pick up on “prediction‐model fingerprints” rather than genuine sequence–structure compatibility.  In practice this can  \n    – inflate matching accuracy on the *same* kind of predicted data,  \n    – hurt generalization to real experimental structures or to antibodies with rare CDR conformations,  \n    – and embed template- or species-derived biases from the structure predictor into the downstream representations.\n\nHence, the main biases are the **systematic modeling errors and idiosyncratic token patterns** that arise from the prediction pipelines (IgFold / ABodyBuilder2), which SSM may mistake for true structure–sequence correlations."
    },
    {
        "question": "What is the rationale for alternating 1D sequences and 3Di sequences in Stage I pre-training?",
        "relevant_section_ids": [
            "2.3.1"
        ],
        "relevant_context": [
            "Building on the multi-level vocabulary, we obtain 1D and 3Di sequences and feed them into the model alternately.",
            "During pre-training stage I, we train S2ALM using the BERT-style Masked Language Modeling (MLM) objective to integratively learn from the 1D and 3Di sequences, enabling support for both sequence-level and structure-level tasks.",
            "Pre-training stage I endows the model with the capability to simultaneously identify both 1D and 3Di sequences. Furthermore, the efficient utilization of protein data in pre-training stage I effectively alleviates issues arising from insufficient antibody structure data."
        ],
        "final_answer": "By alternating between 1D amino-acid sequences and 3Di structural tokens in each training batch, Stage I forces the model to learn from both sequence-level and structure-level information within the same masked-language-modeling framework. This design lets S2ALM simultaneously identify and predict sequence and structural tokens, leverages abundant protein data to compensate for scarce antibody structures, and lays a foundation of global structural constraints that benefits the antibody-specific learning in Stage II.",
        "relevant_elements": [
            "Stage I",
            "1D sequences",
            "3Di sequences"
        ],
        "id": 1833,
        "masked_question": "What is the rationale for alternating [mask1] and [mask2] in Stage I pre-training?",
        "masked_number": 2,
        "masked_elements": [
            "1D sequences",
            "3Di sequences"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.15215v1_figure_1.png",
        "paperid": "2411.15215v1",
        "paper_path": "./MISS-QA/papers/2411.15215v1.json",
        "figure_id": "2411.15215v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed hierarchical pre-training paradigm containing two stages. a, In stage I, S2ALM aims at general sequence-structure learning with protein sequences and structures. In stage II, S2ALM learns antibody specific multi-level knowledge using antibody sequences and structures. b, Masked Language Modeling (MLM) reconstructs the masked tokens based on the contextualized information. c, Sequence-Structure Matching (SSM) identifies the matching relationships between 1D and 3Di sequences. d, Cross-Level Reconstruction (CLR) reconstructs the corrupted tokens based on hybrid information from both 1D and 3Di sequences.",
        "qtype": "Design_Rationale",
        "response": "First, let’s be crystal clear about what [mask1] and [mask2] are:\n\n  1. [mask1] (red‐boxed) = “protein 1D sequences”  \n  2. [mask2] (blue‐boxed) = “protein 3Di sequences” (i.e. the pseudo‐structural sequences produced by Foldseek)  \n\nNow, in Stage I the paper says:\n\n  •  they build a *multi-level vocabulary* consisting of the 20 amino-acid tokens (1D) and 20 structural “3Di” tokens,  \n  •  they then perform standard BERT-style Masked Language Modeling (MLM) on *both* modalities, and  \n  •  to do so they literally toss the model alternately a batch of 1D protein sequences or a batch of 3Di sequences and mask 15% of its tokens each time.  \n\nWhy alternate?\n\n  1. **Joint Sequence–Structure Learning.** By literally interleaving (alternating) 1D and 3Di examples, the same transformer backbone learns to predict missing amino acids *and* missing structural interaction tokens.  \n  2. **Balanced Exposure.** It prevents the model from over-fitting to one modality (e.g. sequence only) and under-utilizing the structural vocabulary.  \n  3. **Generalization Foundation.** Protein structures outnumber antibody structures.  Alternating on large-scale protein data lets the model soak up broad, global structural priors that will later be fine-tuned on the much smaller antibody structural set in Stage II.  \n\nIn short, the red-box vs. blue-box alternation is purely the implementation of a two-headed MLM: one head on sequence tokens, one head on structure tokens, shuffled so the model learns both “languages” in one unified network."
    },
    {
        "question": "What motivates the integration of Sequence-Structure Matching with Cross-Level Reconstruction in Stage II?",
        "relevant_section_ids": [
            "2.3.2"
        ],
        "relevant_context": [
            "After pre-training stage I, S2ALM has thoroughly comprehended 1D and 3Di sequences across the general protein domain. Subsequently in pre-training stage II, we can primarily focus on multi-level representation learning in the target antibody sub-domain. To better absorb comprehensive knowledge of antibody sequences and structures, exploring new pre-training mechanisms is worthwhile.",
            "Two multi-level learning objectives are introduced to inject different granularities of antibody specific sequential and structural information into an ALM: Sequence-Structure Matching (SSM) and Cross-Level Reconstruction (CLR). The customized learning objectives facilitate the extraction of complex patterns and interdependency inherent in antibody sequences and structures.",
            "Sequence-structure matching captures the coarse-grained alignment between antibody sequential and structural information.",
            "Cross-level reconstruction focuses on improving fine-grained understanding in antibody sequence-structure pre-training, which differs in reconstruction conditions from MLM in Sec. 2.3.1."
        ],
        "final_answer": "The integration of Sequence-Structure Matching and Cross-Level Reconstruction in Stage II is motivated by the need to inject antibody-specific structural knowledge at multiple granularities: SSM provides a coarse-grained alignment between sequence and structure, while CLR enforces fine-grained cross-level interdependency, together enabling the model to extract complex sequence-structure patterns specific to antibodies.",
        "relevant_elements": [
            "Sequence-Structure Matching",
            "Cross-Level Reconstruction",
            "Stage II"
        ],
        "id": 1834,
        "masked_question": "What motivates the integration of [mask1] with Cross-Level Reconstruction in Stage II?",
        "masked_number": 1,
        "masked_elements": [
            "Sequence-Structure Matching"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.15215v1_figure_1.png",
        "paperid": "2411.15215v1",
        "paper_path": "./MISS-QA/papers/2411.15215v1.json",
        "figure_id": "2411.15215v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed hierarchical pre-training paradigm containing two stages. a, In stage I, S2ALM aims at general sequence-structure learning with protein sequences and structures. In stage II, S2ALM learns antibody specific multi-level knowledge using antibody sequences and structures. b, Masked Language Modeling (MLM) reconstructs the masked tokens based on the contextualized information. c, Sequence-Structure Matching (SSM) identifies the matching relationships between 1D and 3Di sequences. d, Cross-Level Reconstruction (CLR) reconstructs the corrupted tokens based on hybrid information from both 1D and 3Di sequences.",
        "qtype": "Design_Rationale",
        "response": "First, let’s identify what “[mask₁]” is.  If you look at the red‐boxed panel in the figure, it is exactly the “Sequence–Structure Matching” (SSM) task (Fig. 1c).  In Stage II we actually train on two complementary objectives:\n\n  1.  Sequence–Structure Matching (SSM), which asks “given a 1D sequence and a 3Di‐encoded structure, do they really pair?”  \n  2.  Cross-Level Reconstruction (CLR), which asks “if I mask out some residues in the sequence or the 3Di structure, can I recover them using the unmasked information from both levels?”\n\nWhy do we want to integrate SSM with CLR in Stage II?  Here is the step-by-step reasoning:\n\n  •  **Scarcity of high–quality antibody structures.**  There are millions of antibody sequences but only tens of thousands of experimentally determined structures, so we need to squeeze as much signal as we can out of every structure.  \n  •  **Multi-granular learning.**  SSM forces the model to align entire sequences with their corresponding structural “language” at a coarse, global level.  CLR then drills down: by masking some tokens at one level (sequence or structure) and asking the model to predict them from the other level, it enforces a fine-grained, residue-by-residue interdependency.  \n  •  **Complementary signals.**  Coarse matching (SSM) teaches “does this sequence belong to that fold?”  Cross-level reconstruction (CLR) teaches “given this fold, exactly which residues go here?”  Together they cover both the big picture and the nitty-gritty.  \n  •  **Better antibody representations.**  By learning both coarse alignment and fine rebuilding in tandem, the model builds richer, more accurate embeddings of antibody sequence-structure relationships—precisely what you need for downstream tasks like affinity prediction or design.\n\nIn short, we integrate SSM with CLR in Stage II to exploit both global sequence‐to‐structure consistency and local, residue-level interdependencies, overcoming the paucity of real antibody structures and producing a more faithful, multi-level antibody foundation model."
    },
    {
        "question": "What motivates combining low-rank approximation with dynamic eigenscaling during graph matching for enhanced object-level context?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "An intuitive approach would be simply aggregating  and  without any transformation. However, as shown in Fig. 3, this approach may transfer noise or irrelevant information, highlighting the need to extract features that emphasize object-level context.",
            "From this realization, we leverage the low-rank components of VFM, which contain distinct object patterns within the graph structure. Specifically, we (I) extract the critical object-level contextual structure of  via low-rank approximation and enhance the graph structure by dynamically scaling eigenvalues.",
            "In the decomposed eigenbasis, we identify key object-level features of each graph by searching an optimal number of eigenvalues  through an energy-based approach. This ensures that the chosen  eigenvalues capture a significant portion of the graph’s energy, retaining essential structural information while discarding noise and less relevant details.",
            "We refine the low-rank components with a scaling function , which dynamically amplifies larger eigenvalues and reduces smaller ones. Compared to the conventional shrinkage function, which only focuses on noise cutoff, our approach emphasizes essential structural information, particularly object-level context features, while suppressing noise and irrelevant details."
        ],
        "final_answer": "Because simply merging the raw VFM and CLIP attention graphs would mix in noise and irrelevant connections, the model first uses a low-rank approximation to keep only the most energy-rich, object-defining spectral components, and then applies dynamic eigenscaling to amplify those key components (and de-emphasize smaller, less meaningful ones). This combination ensures that the distilled graph strongly highlights true object-level context while filtering out noise.",
        "relevant_elements": [
            "Low-rank Approx.",
            "Dynamic Eigenscaling",
            "Graph Matching"
        ],
        "id": 1835,
        "masked_question": "What motivates combining [mask1] with dynamic eigenscaling during graph matching for enhanced object-level context?",
        "masked_number": 1,
        "masked_elements": [
            "Low-rank Approx."
        ],
        "figure_path": "./MISS-QA/figures/0_2411.17150v1_figure_1.png",
        "paperid": "2411.17150v1",
        "paper_path": "./MISS-QA/papers/2411.17150v1.json",
        "figure_id": "2411.17150v1_figure_1.png",
        "caption": "Figure 1: \nWe present CASS, object-level Context-Aware training-free open-vocabulary Semantic Segmentation model. (a) Overview:\nOur method distills the vision foundation model’s (VFM) object-level contextual spectral graph into CLIP’s attention and refines query text embeddings towards object-specific semantics.\n(b) Object-Level Context:\nThis result illustrates how incorporating object-level context improves segmentation accuracy by unifying object-wise components into a user-defined object class; for example, (top row) our approach precisely segments the truck’s body, wheels, and cargo area and (bottom row) accurately groups elements into a single object, such as the sheep, whereas baselines [16, 25] often fail to achieve this unified segmentation.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step‐by‐step reasoning that leads to the answer:\n\n1.  Goal of the pipeline  \n    – We want to transfer only the _object‐level_ contextual relationships discovered by the Vision Foundation Model (VFM) into CLIP’s attention, without dragging along all the noisy, low‐level connections.  \n\n2.  Why not just copy the full VFM graph?  \n    – The raw VFM attention graph A_VFM contains a mix of strong object‐centric links and a lot of weaker, noisier edges (e.g. spurious patch‐to‐patch interactions that don’t really help you “see” the object as one piece). If you simply add A_VFM onto CLIP’s graph, you end up transferring that noise, which actually hurts segmentation.  \n\n3.  Low-rank approximation ([mask1])  \n    – By doing an eigendecomposition of A_VFM and then keeping only the top k eigen‐components, you isolate the _dominant_ spectral modes of that graph—the ones carrying the bulk of the “object shape” and “object part” information. Everything else (low‐energy eigenvectors) mostly encodes noise or very fine‐grained details that aren’t essential to grouping the object together.  \n\n4.  Dynamic eigenscaling  \n    – Even among those top k modes, not all are equally important. A conventional shrinkage (hard‐threshold) would just cut off small modes; but here we want to _reweight_ them smoothly—boosting the very largest eigenvalues (to further exaggerate the strongest object cues) and gently damping the smaller ones (to suppress whatever residual noise remains).  \n\n5.  Why combine the two?  \n    – Low-rank approximation _selects_ the subspace of interest.  \n    – Dynamic eigenscaling _reweights_ within that subspace so that the true object‐level signals dominate.  \n    – Together, they produce a distilled VFM graph that (a) throws away unhelpful edges and (b) amplifies the edges that really encode “this patch belongs to that object,” making the subsequent graph‐matching step both cleaner and more accurate.  \n\nAnswer:  \nThey first apply a low‐rank approximation to strip out the noisy, low-energy modes and keep only the principal object‐level components of the VFM graph. Then they use dynamic eigenscaling to further amplify the strongest eigenmodes (and suppress the weaker ones) so that only the most salient, object-centric relationships survive. This two‐stage process ensures that when they match and merge the VFM graph into CLIP’s attention, they carry over _only_ the clean, object-level context and throw away the distracting noise."
    },
    {
        "question": "What motivates refining CLIP text embeddings into adjusted text embeddings for object-specific segmentation alignment?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Our model also leverages CLIP’s highly effective zero-shot object classification capability (i.e., object presence prior), widely validated in prior work [34, 19, 29], to capture detailed object-level context within scenes. As such, we adjust the text embeddings based on the object presence prior encoded by CLIP. This process involves refining the embeddings to better align with object-specific semantics.",
            "The spectral object-level context distillation in Sec. 3.2 enables precise object mask generation. However, due to the nature of OVSS, where users can provide arbitrary query prompts, different parts of the same object may still be assigned to closely related categories. Meanwhile, numerous studies [34, 19, 29] have demonstrated that CLIP excels in zero-shot object classification capability. Accordingly, we utilize the zero-shot object classification score encoded by CLIP (i.e., the object presence prior) to refine text embeddings and patch-text similarity, enhancing object-centered perspective."
        ],
        "final_answer": "Because arbitrary text prompts can cause parts of the same object to be misclassified into related categories, the model uses CLIP’s zero-shot object presence prior to refine the original text embeddings. This adjustment ensures the text features better align with actual object semantics, producing segmentation maps that are correctly centered on the specified object.",
        "relevant_elements": [
            "Text Embeddings",
            "Adjusted Text Embeddings"
        ],
        "id": 1836,
        "masked_question": "What motivates refining [mask1] into [mask2] for object-specific segmentation alignment?",
        "masked_number": 2,
        "masked_elements": [
            "Text Embeddings",
            "Adjusted Text Embeddings"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.17150v1_figure_1.png",
        "paperid": "2411.17150v1",
        "paper_path": "./MISS-QA/papers/2411.17150v1.json",
        "figure_id": "2411.17150v1_figure_1.png",
        "caption": "Figure 1: \nWe present CASS, object-level Context-Aware training-free open-vocabulary Semantic Segmentation model. (a) Overview:\nOur method distills the vision foundation model’s (VFM) object-level contextual spectral graph into CLIP’s attention and refines query text embeddings towards object-specific semantics.\n(b) Object-Level Context:\nThis result illustrates how incorporating object-level context improves segmentation accuracy by unifying object-wise components into a user-defined object class; for example, (top row) our approach precisely segments the truck’s body, wheels, and cargo area and (bottom row) accurately groups elements into a single object, such as the sheep, whereas baselines [16, 25] often fail to achieve this unified segmentation.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning, step by step:\n\n1.  In the red box (“mask1”) the model is simply using the raw CLIP text embeddings t_CLIP for each user‐provided class label.  \n2.  These off‐the‐shelf text embeddings know general word meaning, but they carry no information about which of those classes actually appear in the current image.  \n3.  Meanwhile, CLIP’s image encoder can also produce a zero‐shot “object presence” score for each class—effectively a prior saying “this class is likely in the picture.”  \n4.  If we leave the text embeddings unchanged, our segmentation head will treat every class equally and may spread related labels over parts of the same object or confuse similar categories.  \n5.  By taking t_CLIP and “adjusting” it (the blue box, “mask2”)—i.e. adding in the object‐presence prior F_CLIP—we steer each label embedding toward the classes that CLIP’s own classifier thinks are present.  \n6.  These adjusted text embeddings now carry two signals: (a) the semantic meaning of the word, and (b) how strongly CLIP believes that word’s object is in the image.  \n7.  When we recompute patch–text similarities with these refined embeddings, patches belonging to the true object get a boost, while off‐object or spurious labels are suppressed.  \n8.  The result is a segmentation map that (a) groups all parts of the same object under one user‐defined label and (b) avoids confusing adjacent or visually similar categories.\n\nIn short, we refine the raw CLIP text embeddings into “adjusted” embeddings in order to inject an image‐specific, object‐presence prior—this ensures that the final segmentation is truly object‐specific and tightly aligned with what actually appears in the input."
    },
    {
        "question": "How does Graph Matching compute the cost matrix from spectral distributions for attention head alignment?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "After obtaining eigenvalues for each head, we compute spectral distances to quantify structural differences, creating a cost matrix C for each graph pair from VFM and CLIP: where \\tilde{\\lambda}_i^v and \\tilde{\\lambda}_j^\\psi represent the normalized eigenvalues of the i-th head of VFM and the j-th head of CLIP, respectively, and C_{ij} denotes the Wasserstein distance.",
            "Here, the Wasserstein distance is computed as W(\\mu,\\nu) = \\sum_i |sort(\\mu)_i - sort(\\nu)_i|, where the sort function orders the values of \\mu and \\nu in ascending order."
        ],
        "final_answer": "Graph Matching forms the cost matrix C by taking the normalized eigenvalue vectors (spectral distributions) from each VFM attention head and each CLIP attention head and then computing the pairwise Wasserstein distance between them. Specifically, each entry C_{ij} is the Wasserstein distance between the sorted, normalized eigenvalues of the i-th VFM head and the j-th CLIP head.",
        "relevant_elements": [
            "Graph Matching",
            "Spectral Distribution"
        ],
        "id": 1837,
        "masked_question": "How does [mask1] compute the cost matrix from spectral distributions for attention head alignment?",
        "masked_number": 1,
        "masked_elements": [
            "Graph Matching"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.17150v1_figure_1.png",
        "paperid": "2411.17150v1",
        "paper_path": "./MISS-QA/papers/2411.17150v1.json",
        "figure_id": "2411.17150v1_figure_1.png",
        "caption": "Figure 1: \nWe present CASS, object-level Context-Aware training-free open-vocabulary Semantic Segmentation model. (a) Overview:\nOur method distills the vision foundation model’s (VFM) object-level contextual spectral graph into CLIP’s attention and refines query text embeddings towards object-specific semantics.\n(b) Object-Level Context:\nThis result illustrates how incorporating object-level context improves segmentation accuracy by unifying object-wise components into a user-defined object class; for example, (top row) our approach precisely segments the truck’s body, wheels, and cargo area and (bottom row) accurately groups elements into a single object, such as the sheep, whereas baselines [16, 25] often fail to achieve this unified segmentation.",
        "qtype": "Implementation_Details",
        "response": "Here’s how the red‐boxed “Graph Matching” module (your [mask1]) builds the cost matrix to align VFM heads with CLIP heads:\n\n1.  **Eigendecompose each attention graph.**  \n    – For each VFM head \\(i\\) you have an adjacency matrix \\(\\mathbf{A}^{\\text{VFM}}_{i}\\), and for each CLIP head \\(j\\) an adjacency \\(\\mathbf{A}^{\\text{CLIP}}_{j}\\).  \n    – Run a standard eigendecomposition on each to extract the top \\(K\\) eigenvalues  \n       \\(\\lambda^{\\text{VFM}}_{i} = [\\lambda_{i,1},\\dots,\\lambda_{i,K}]\\)  \n       and  \n       \\(\\lambda^{\\text{CLIP}}_{j} = [\\mu_{j,1},\\dots,\\mu_{j,K}]\\).  \n\n2.  **Normalize the spectral vectors.**  \n    – Scale each vector \\(\\lambda^{\\text{VFM}}_{i}\\) and \\(\\mu^{\\text{CLIP}}_{j}\\) so they form valid (e.g. sum–to–one) distributions.  \n\n3.  **Compute pairwise Wasserstein distances.**  \n    – For every pair \\((i,j)\\), compute  \n       \\[\n         C_{ij} \\;=\\; W\\!\\bigl(\\lambda^{\\text{VFM}}_{i},\\,\\mu^{\\text{CLIP}}_{j}\\bigr)\n         \\;=\\;\\lVert \\mathrm{sort}(\\lambda^{\\text{VFM}}_{i}) \\;-\\;\\mathrm{sort}(\\mu^{\\text{CLIP}}_{j})\\rVert_{1}\\,,\n       \\]  \n       i.e. the 1-D Wasserstein (earth-mover’s) distance between their sorted eigenvalue spectra.  \n\n4.  **Assemble the cost matrix.**  \n    – The result is an \\(H_{\\text{VFM}}\\times H_{\\text{CLIP}}\\) matrix \\(C\\).  \n    – You then feed \\(C\\) into the Hungarian algorithm to find the minimal-cost matching of heads.\n\nIn short, [mask1] builds the cost matrix by extracting and normalizing the top-\\(K\\) eigenvalues of each attention head’s adjacency graph, measuring pairwise Wasserstein distances between those spectral distributions, and arranging those distances into the matrix \\(C\\)."
    },
    {
        "question": "How does the corrupt module determine mask positions and maintain input-data integrity during partial corruption?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Corrupt Module"
        ],
        "id": 1839,
        "masked_question": "How does the [mask1] determine mask positions and maintain input-data integrity during partial corruption?",
        "masked_number": 1,
        "masked_elements": [
            "Corrupt Module"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.12730v1_figure_2.png",
        "paperid": "2409.12730v1",
        "paper_path": "./MISS-QA/papers/2409.12730v1.json",
        "figure_id": "2409.12730v1_figure_2.png",
        "caption": "Figure 2: An overview of proposed AEL.",
        "qtype": "Implementation_Details",
        "response": "Here is the step-by-step reasoning:\n\n1.  Identify the red-boxed module.  \n    –  In Fig. 2 that red box is labelled “Corrupt Module,” and the text immediately below tells us “Large also utilizes the corrupt module to corrupt the input x to x̃ using a mask-out technique.”\n\n2.  Recall the purpose of the corrupt module.  \n    –  Its job is to partially corrupt the raw input vector so that the autoencoder cannot simply learn the identity mapping.  At the same time it must leave enough of the original signal intact so that the model can learn to reconstruct the missing pieces.\n\n3.  How are the mask positions chosen?  \n    –  We generate a binary mask M of the same dimensionality as the input x.  \n    –  Each entry M_i is sampled (e.g. from a Bernoulli(ρ) distribution or by selecting exactly k positions at random), so that with probability ρ (the corruption rate) M_i = 0 (masked), and with probability 1–ρ M_i = 1 (kept).\n\n4.  How is the actual corruption performed?  \n    –  The module simply does an element-wise product:  \n         x̃  =  M ⊙ x  \n       All positions i for which M_i = 0 are set to zero (i.e. “masked out”), and all positions with M_i = 1 remain exactly as in the original x.\n\n5.  Why does this maintain data integrity on the unmasked entries?  \n    –  Because we never change the unmasked dimensions – we only zero out the masked ones – the remaining entries are the exact same values that came in.  No further noise or distortion is applied to them.\n\nPutting it all together:  \nThe corrupt module builds a random binary mask (via Bernoulli draws or by choosing k random slots), multiplies it element-wise with the input, zeroing out only those selected positions.  The unmasked entries are left completely untouched, so only a controlled subset of the data is corrupted while the rest of the original signal remains intact for the denoising autoencoder to learn from."
    },
    {
        "question": "How does the sparse gating network combine expert scores and Gaussian noise to generate adaptive weights for parent-AEs?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "The sparse gating network utilizes the Noisy Top-K Gating strategy for selecting experts. In this approach, tunable Gaussian noise is added before applying the softmax function. This helps in load balancing and ensures that only suitable experts are activated for each input.",
            "where W_g and W_n are trainable weight matrices, G′ is the output of the gating network, and the TopK function retains the top K values and sets the rest to 0."
        ],
        "final_answer": "The sparse gating network first computes raw scores for each parent-AE via trainable weight matrices, then adds tunable Gaussian noise to those scores. It next applies a softmax to produce a probability distribution and finally retains only the top-K highest probabilities (zeroing out the others). These resulting values serve as adaptive, input-dependent weights for mixing the parent-AE outputs.",
        "relevant_elements": [
            "Sparse Gating Network",
            "Parent-AEs"
        ],
        "id": 1840,
        "masked_question": "How does the [mask1] combine expert scores and Gaussian noise to generate adaptive weights for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Sparse Gating Network",
            "Parent-AEs"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.12730v1_figure_2.png",
        "paperid": "2409.12730v1",
        "paper_path": "./MISS-QA/papers/2409.12730v1.json",
        "figure_id": "2409.12730v1_figure_2.png",
        "caption": "Figure 2: An overview of proposed AEL.",
        "qtype": "Implementation_Details",
        "response": "The little red box in Figure 2 is nothing more exotic than a “Noisy Top–K” gating network (sometimes called a sparse mixture-of-experts gate).  Concretely, for each input x it does the following:\n\n 1.  It runs x through a small feed-forward net (two linear layers with a ReLU in between) to produce an un-normalized “score” vector s∈ℝ³—one score for each of the three parent-AEs.  \n 2.  It draws a small Gaussian noise vector n∼𝒩(0,σ²I) (σ is a learned or hand-tuned scalar) of the same size as s, and adds it to the scores:  s′=s+n.  \n 3.  It applies a softmax to s′, but only keeps the top-K entries (K=2 in their case), zeroing out the rest, and then re-normalizes so that the remaining weights still sum to one.  \n\nThe result is a sparse weight vector G (only two non-zeros) that is then used to take a weighted average of the three parent-AE reconstructions (the blue-boxed experts).  In this way the gate dynamically picks and blends the two most appropriate autoencoders for each x."
    },
    {
        "question": "What parallels exist between Corrupt Module’s mask-out and dropout in collaborative denoising autoencoders?",
        "relevant_section_ids": [
            "2",
            "2.1"
        ],
        "relevant_context": [
            "In the denoising module, we also introduce a corrupt module to improve robustness by partially corrupting initial input, preventing sub-AEs from simply learning the identity function.",
            "Large also utilizes the corrupt module to corrupt the input \\tilde{x} to x′ using a mask-out technique."
        ],
        "final_answer": "Both the Corrupt Module’s mask-out and the input dropout used in collaborative denoising autoencoders serve the same purpose: they randomly omit or \"mask\" portions of the input vector to corrupt it. This prevents the autoencoder from merely learning an identity mapping and forces it to discover more robust, generalizable representations.",
        "relevant_elements": [
            "Corrupt Module"
        ],
        "id": 1841,
        "masked_question": "What parallels exist between [mask1]’s mask-out and dropout in collaborative denoising autoencoders?",
        "masked_number": 1,
        "masked_elements": [
            "Corrupt Module"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.12730v1_figure_2.png",
        "paperid": "2409.12730v1",
        "paper_path": "./MISS-QA/papers/2409.12730v1.json",
        "figure_id": "2409.12730v1_figure_2.png",
        "caption": "Figure 2: An overview of proposed AEL.",
        "qtype": "Literature_Background",
        "response": "In that figure, the thing you’ve boxed in red is the “corrupt module,” which simply masks‐out (i.e. zeroes) a random subset of the input features before feeding them to the sub-autoencoders.  In essence it is doing exactly what “dropout” does in a Collaborative Denoising AutoEncoder (CDAE) or any other denoising autoencoder – corrupting part of the input by setting those dimensions to zero.  The parallels are thus:  \n\n  1.  Both techniques randomly choose a fraction of the input entries and force them to zero.  \n  2.  By doing so they prevent the network from trivially learning the identity mapping.  \n  3.  They act as a form of regularization, encouraging the model to discover more robust, higher-level correlations among the remaining (unmasked) features in order to reconstruct the missing ones."
    },
    {
        "question": "How does Sparse Gating Network adapt Mixture-of-Experts principles to dynamically weight Parent-AEs?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "To achieve adaptability to dynamic noise intensities, we introduce an improved sparse gating network [26] to manage these parent-AEs.",
            "The sparse gating network utilizes the Noisy Top-K Gating strategy for selecting experts. In this approach, tunable Gaussian noise is added before applying the softmax function. This helps in load balancing and ensures that only suitable experts are activated for each input.",
            "We combine the outputs of the gating network G and experts E to generate overall predictions:"
        ],
        "final_answer": "The sparse gating network implements a Mixture-of-Experts style controller over the three parent-AEs.  For each input it first adds tunable Gaussian noise to the gating logits, then applies a Top-K softmax (‘Noisy Top-K Gating’) to select and weight only the strongest two experts, zeroing out the rest.  The weighted expert outputs are then summed to form the final prediction.  Additional load-balancing losses ensure no single parent-AE dominates across training, so the gating weights adapt dynamically to each input’s noise level while maintaining balanced expert usage.",
        "relevant_elements": [
            "Sparse Gating Network",
            "Parent-AEs"
        ],
        "id": 1842,
        "masked_question": "How does [mask1] adapt Mixture-of-Experts principles to dynamically weight [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Sparse Gating Network",
            "Parent-AEs"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.12730v1_figure_2.png",
        "paperid": "2409.12730v1",
        "paper_path": "./MISS-QA/papers/2409.12730v1.json",
        "figure_id": "2409.12730v1_figure_2.png",
        "caption": "Figure 2: An overview of proposed AEL.",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning, step by step:\n\n1. In our design the three parent‐AEs (Strong, Moderate and Mild) play exactly the role of “experts” in a classical Mixture-of-Experts (MoE) framework (this is the blue box in Fig. 2).  Each parent-AE, when fed the (corrupted) input, produces its own reconstruction output, E₁, E₂ or E₃, which differ in how aggressively they denoise.\n\n2. The “brain” of the ensemble is the sparse gating network (the red box).  Its job is to look at the incoming data (or indeed at some running performance statistics) and decide which of those three experts to trust, and by how much, for each individual example.\n\n3. Concretely, the gating network is a small neural sub‐network with its own weight matrices.  Given an input, it computes a score for each expert, then adds a small, learnable amount of Gaussian noise to each score.\n\n4. Those noisy scores go through a softmax, but **only the top-K** (in our case K = 2) entries are kept—the rest are zeroed out.  This “Noisy Top-K Gating” both sparsifies the mixture (so we only ever combine two expert outputs per example) and encourages load balancing across experts.\n\n5. The nonzero softmax outputs become the gating weights G₁, G₂ (and implicitly G₃ = 0 if it isn’t in the top 2).  We then form the final denoised output by a sparse weighted sum of the expert reconstructions:\n     x̂  =  G₁·E₁  +  G₂·E₂  +  G₃·E₃.\n\n6. During training we also add auxiliary loss terms to make sure no single expert dominates (a coefficient‐of‐variation penalty on the gating probabilities), which keeps the system balanced.\n\nIn this way the red-boxed sparse gating network borrows the Mixture-of-Experts idea—noisy top-K gating plus sparsity—to dynamically assign instance-specific weights over the blue-boxed parent-AEs and thus adapt the overall denoising strength on the fly."
    },
    {
        "question": "How does gated fusion extend standard gating mechanisms to refine selective attention outputs?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Finally, as a popular technique, gating mechanisms [18, 4] are well suited for fusing features from different sources, so we use it to help the model decide how many cross features should be left.",
            "Given the textual feature H^Text_attn and visual feature H^Img_attn, the gate G and the fusion operation are defined as:\n  G = σ(A·H^Text_attn + B·H^Img_attn)\n  H^out = G ⊙ H^Text_attn + (1−G) ⊙ H^Img_attn",
            "By replacing H^Text_attn and H^Img_attn with the original H^Text and H^Img respectively, the gated outputs H^Text_out and H^Img_out are produced."
        ],
        "final_answer": "Gated fusion takes the outputs of the selective-attention blocks (H^Text_attn and H^Img_attn) as inputs to a standard sigmoid gate, computing G = σ(A·H^Text_attn + B·H^Img_attn). It then uses G to blend those attended features via H^out = G⊙H^Text_attn + (1−G)⊙H^Img_attn. In this way it adaptively filters and fuses the cross-modal attention outputs, rather than gating the raw embeddings directly.",
        "relevant_elements": [
            "Gated Fusion",
            "Selective Attention"
        ],
        "id": 1843,
        "masked_question": "How does [mask1] extend standard gating mechanisms to refine selective attention outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Gated Fusion"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.08709v1_figure_2.png",
        "paperid": "2408.08709v1",
        "paper_path": "./MISS-QA/papers/2408.08709v1.json",
        "figure_id": "2408.08709v1_figure_2.png",
        "caption": "Figure 2: The overall query-based entity-object transformer architecture.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In Fig. 2, after the model computes selective attention in both directions (text→image and image→text), it produces two intermediate feature sets:  \n   • Hᵗᵉˣᵗ_attn (textual features “attended” by the image)  \n   • Hᶦᵐᵍ_attn (visual features “attended” by the text)  \n2. Standard gating (e.g. Highway Networks or GRU‐style gates) normally takes two feature vectors x and y, computes a gate G = σ(Ax + By), and then fuses x and y via G ⊙ x + (1–G) ⊙ y.  \n3. In our “Gated Fusion” block (the red-boxed [mask1]), we reuse exactly that formula—but we let  \n   x = the selective‐attention output for one modality  \n   y = the original (or cross‐attended) features of the other modality  \n4. Concretely, for the text side:  \n   Gᵗ = sigmoid(A · Hᵗᵉˣᵗ_attn + B · Hᶦᵐᵍ)  \n   Hᵗᵉˣᵗ_out = Gᵗ ⊙ Hᵗᵉˣᵗ_attn + (1–Gᵗ) ⊙ Hᶦᵐᵍ  \n   And symmetrically for the image side.  \n5. By feeding in the *selective‐attention* outputs rather than the raw inputs, the gate learns exactly how much of the cross‐modality information to keep (and how much to suppress), yielding refined multimodal embeddings Hᵗᵉˣᵗ_out and Hᶦᵐᵍ_out.  \n\nAnswer  \n[mask1] is the “Gated Fusion” module.  It extends a standard sigmoid‐gate fusion by applying it *after* selective attention, using the attention‐refined features and the original unimodal features as its two inputs.  In this way the gate learns to weight and filter the cross‐modal signals, deciding exactly how much of the attended text or image information to incorporate into the final fused representations."
    },
    {
        "question": "How does transformer decoder cross-attention build upon encoder self-attention to extract query features?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We take feature F, which contains more information about the original image, as the input to the encoder, and each layer of the encoder is a standard multi-head self-attention (MSA) module and feed-forward network (FFN).",
            "The inputs for each decoder layer consist of multimodal sequences derived from the transformer encoder, along with defined queries. These queries undergo multi-head self-attention before cross-attention operations with the multimodal sequences. The reason for designing such a process is to allow the queries to discern the features obtained by other queries and subsequently determine the features they will extract from the multimodal sequences."
        ],
        "final_answer": "The encoder first applies multi-head self-attention to the fused multimodal sequence, producing contextualized representations of text and image features. In each decoder layer, the set of learned queries first self-attend to one another (multi-head self-attention), and then a cross-attention step uses those refined queries as queries and the encoder’s output as keys and values. This cross-attention lets each query selectively extract the specific multimodal features it needs from the encoder’s representations.",
        "relevant_elements": [
            "Transformer Encoder",
            "Transformer Decoder"
        ],
        "id": 1844,
        "masked_question": "How does [mask1] cross-attention build upon encoder self-attention to extract query features?",
        "masked_number": 1,
        "masked_elements": [
            "Transformer Decoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.08709v1_figure_2.png",
        "paperid": "2408.08709v1",
        "paper_path": "./MISS-QA/papers/2408.08709v1.json",
        "figure_id": "2408.08709v1_figure_2.png",
        "caption": "Figure 2: The overall query-based entity-object transformer architecture.",
        "qtype": "Literature_Background",
        "response": "In the paper’s terminology, the red‐boxed block is nothing magical – it is simply the standard Transformer “cross-attention” layer in the decoder, sitting on top of the encoder’s self-attention.  Concretely:\n\n 1.  The encoder ingests the fused text-image sequence (after selective attention and gated fusion) and runs a stack of multi-head **self-attention** + feed-forward layers to produce a memory tensor  \n      H_enc∈ℝQ×d  \n\n 2.  In the decoder you start with a fixed number of learnable query vectors Q_query∈ℝQ×d.  Each decoder layer does\n     a) multi-head **self-attention** among the queries themselves (so they can “talk to” each other),  \n     b) multi-head **cross-attention**, where  \n         • the **queries** are the Q_query (or the updated queries from the self-attention sublayer),  \n         • the **keys** and **values** are the encoder’s output H_enc.  \n     c) a position-wise feed-forward network  \n\n 3.  In that cross-attention step you compute  \n         A = softmax( Q_query · K_encᵀ / √d )  \n         H_query_out = A · V_enc  \n     so each query vector “pulls out” exactly the slice of the encoder’s global, multimodal representation that it needs.\n\nIn this way the decoder’s cross-attention *builds directly* on what the encoder has learned via self-attention, re-using the same attention machinery but now having the queries attend *to* the encoder’s feature map rather than to themselves.  The result is a set of query‐specific feature vectors that carry precisely the cross-modal information each query cares about."
    },
    {
        "question": "How does removing gated fusion alter selective attention's influence on cross-modal embedding generation?",
        "relevant_section_ids": [
            "3.2",
            "4.3"
        ],
        "relevant_context": [
            "Finally, as a popular technique, gating mechanisms [18, 4] are well suited for fusing features from different sources, so we use it to help the model decide how many cross features should be left.",
            "When we remove the gated-fusion module, all metrics significantly drop, indicating that the absence of dynamic feature selection can lead to the model indiscriminately accepting cross-modal features in a suboptimal state."
        ],
        "final_answer": "Without the gated-fusion module, the model loses its ability to dynamically screen and down-weight the outputs of the selective attention network. In effect, selective attention’s cross-modal signals are passed through unfiltered— ‘‘indiscriminately’’— resulting in noisier, suboptimal embeddings and a significant drop in performance.",
        "relevant_elements": [
            "selective attention",
            "gated fusion"
        ],
        "id": 1845,
        "masked_question": "How does removing [mask1] alter [mask2]'s influence on cross-modal embedding generation?",
        "masked_number": 2,
        "masked_elements": [
            "gated fusion",
            "selective attention"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.08709v1_figure_2.png",
        "paperid": "2408.08709v1",
        "paper_path": "./MISS-QA/papers/2408.08709v1.json",
        "figure_id": "2408.08709v1_figure_2.png",
        "caption": "Figure 2: The overall query-based entity-object transformer architecture.",
        "qtype": "Experimental_Results",
        "response": "Let’s unpack what each colored box is doing in Figure 2, then see what happens when we yank out the red one (the gated‐fusion) and how that changes the role of the blue one (the selective‐attention block).\n\n1. Blue‐box (“selective attention”)  \n   – Takes text features HText and image features HImg and computes cross‐attention in both directions (text attending to image patches and image attending to text tokens).  \n   – Produces two sets of “attended” features, HText_attn and HImg_attn, which highlight the pieces of each modality deemed most relevant to the other.\n\n2. Red‐box (“gated fusion”)  \n   – Takes the original modality feature (say HText) and its cross‐attended counterpart (HText_attn) and runs them through a small gating network:  \n        g = σ(A·HText + B·HText_attn)  \n        HText_out = g⊙HText + (1–g)⊙HText_attn  \n   – This gate learns to blend in exactly as much cross‐modal signal as is helpful, and suppress the rest.\n\n3. What if we remove the gated‐fusion entirely?  \n   – We still have the blue‐box pumping out HText_attn and HImg_attn via selective attention, but there’s no learnable gate to temper or weight those signals.  \n   – In practice (see the “w/o gated” ablation), the model simply concatenates or sums the attended features directly into the stream, “accepting” all cross‐modal cues indiscriminately.  \n   – As a result, the selective‐attention outputs dominate—or at least flood—the fused embedding, carrying along irrelevant or noisy activations that the gate would otherwise have dampened.  \n   – Empirically this hurts all downstream metrics, because without the gate you lose the ability to dynamically filter out spurious cross-modal correlations.\n\nSo, by removing the red‐box gated-fusion, the blue-box selective-attention still computes “what to look at” across modalities, but you no longer have a mechanism to dial its influence up or down. All attended signals flow straight into the joint embedding, leading to noisier, less discriminative cross-modal representations."
    },
    {
        "question": "How does modularizing annotation into Camera Movement Perception and Shot Size Perception improve labeling consistency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Camera Movement Perception",
            "Shot Size Perception"
        ],
        "id": 1847,
        "masked_question": "How does modularizing annotation into [mask1] and [mask2] improve labeling consistency?",
        "masked_number": 2,
        "masked_elements": [
            "Camera Movement Perception",
            "Shot Size Perception"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.10979v3_figure_2.png",
        "paperid": "2411.10979v3",
        "paper_path": "./MISS-QA/papers/2411.10979v3.json",
        "figure_id": "2411.10979v3_figure_2.png",
        "caption": "Figure 2: VidComposition comprises 15 categories of high-quality QA pairs, focusing on five aspects of compositions in compiled videos: cinematography, character, narrative, scene, and making. The correct answers are highlighted.",
        "qtype": "Experimental_Results",
        "response": "By splitting the shot‐by‐shot annotation interface into two dedicated modules—one for camera movements (the red box, e.g. “zoom in,” “pan left,” etc.) and one for shot sizes (the blue box, e.g. “close-up,” “medium shot,” etc.)—we force annotators to pick from a fixed, predefined label set for each subtask.  This modularization (1) narrows each question to a single well‐defined taxonomy, (2) prevents free-form answers or ambiguous phrasing, and (3) makes distractors and correct options directly comparable.  As a result, different annotators are far more likely to apply the same terminology in the same way, greatly reducing inter-annotator variance and boosting overall label consistency."
    },
    {
        "question": "What limitations emerge from fixed signal-strength threshold in Network's Graph Representation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Network's Graph Representation"
        ],
        "id": 1851,
        "masked_question": "What limitations emerge from fixed signal-strength threshold in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Network's Graph Representation"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.08479v1_figure_1.png",
        "paperid": "2407.08479v1",
        "paper_path": "./MISS-QA/papers/2407.08479v1.json",
        "figure_id": "2407.08479v1_figure_1.png",
        "caption": "Figure 1. \nRobustGANTT generates schedules for backscatter networks using a GNN-based Transformer model.\nStep 1: collect MAC and routing protocol information. Step 2: build the IoT network s graph representation, only including edges strong enough for carrier provisioning (e.g., -75 dBm). Step 3: generate the schedule through iterative one-shot node classification. Step 4: disseminate the schedule using existing network flooding mechanisms and append it to the IoT device s normal schedule.",
        "qtype": "Others",
        "response": "The red‐boxed region (“2. Network’s Graph Representation”) is nothing more than the IoT network’s connectivity graph, where you draw an edge between two nodes if – and only if – their measured RSSI exceeds a fixed cutoff (say −75 dBm).  Because that cutoff is hard–coded, the following practical problems immediately arise:\n\n 1.  **All‐or‐nothing links.**  A link that fluctuates just below the threshold is treated as if it didn’t exist, even though in practice it might still carry a usable carrier signal (albeit more weakly).  Conversely, borderline links that barely clear the threshold may be noisier or less reliable than the graph implies.  \n\n 2.  **Brittle topology under fading or mobility.**  In a real deployment RSSI drifts with time, interference, temperature or node movement.  The moment one link dips below your fixed cutoff you “lose” that edge in the graph, potentially fragmenting connectivity and forcing the scheduler onto a much worse solution.  \n\n 3.  **Lost carrier‐reuse opportunities.**  By throwing out any link deemed “too weak,” you throw away possible carrier‐provisioning paths that could have been harvested (perhaps at a slightly higher transmit power or just for a few tags).  That in turn drives up the total number of carrier slots and schedule length.  \n\n 4.  **Threshold tuning headache.**  Picking one universal RSSI threshold that works well in all parts of the building, all times of day and all hardware variants is essentially impossible.  Too high a cutoff → graph is too sparse; too low → you start scheduling carriers on really marginal (and unreliable) links.  \n\nIn short, by converting a continuously‐varying, time‐dependent link quality into a static binary graph using a fixed RSSI cutoff, you end up with a brittle, overly conservative and highly threshold‐sensitive model of your network—one that both misses usable carrier paths and flips edges on and off in an unpredictable way as conditions change."
    },
    {
        "question": "What privacy risks arise using Cloud/Edge Server for RobustGANTT scheduling?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Cloud/Edge Server",
            "RobustGANTT"
        ],
        "id": 1852,
        "masked_question": "What privacy risks arise using [mask1] for [mask2] scheduling?",
        "masked_number": 2,
        "masked_elements": [
            "Cloud/Edge Server",
            "RobustGANTT"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.08479v1_figure_1.png",
        "paperid": "2407.08479v1",
        "paper_path": "./MISS-QA/papers/2407.08479v1.json",
        "figure_id": "2407.08479v1_figure_1.png",
        "caption": "Figure 1. \nRobustGANTT generates schedules for backscatter networks using a GNN-based Transformer model.\nStep 1: collect MAC and routing protocol information. Step 2: build the IoT network s graph representation, only including edges strong enough for carrier provisioning (e.g., -75 dBm). Step 3: generate the schedule through iterative one-shot node classification. Step 4: disseminate the schedule using existing network flooding mechanisms and append it to the IoT device s normal schedule.",
        "qtype": "Others",
        "response": "First, let us identify the two masked regions in the figure:\n\n1.  (red box) is the “Cloud/Edge Server” that actually runs the scheduling algorithm.  \n2.  (blue box) is the “Scheduler” module itself (in our case, the RobustGANTT GNN-based Transformer).\n\nSo the question really asks:\n\n  “What privacy risks arise when I outsource my network’s tag-interrogation schedule to a remote Cloud/Edge Server running RobustGANTT?”\n\nHere is the step-by-step reasoning:\n\n1. To compute a schedule, the Cloud/Edge Server must be given your entire IoT network graph  \n   – that means every node’s identifier (MAC, Node‐ID), which tags live on which node, and the topology (who can talk to whom).  \n2. That data can be very sensitive:  \n   – MAC addresses and Node-IDs can be traced back to specific pieces of equipment or locations (e.g. which machine in a factory, which patient in a ward).  \n   – Tag-to-host mappings reveal exactly where sensors are placed (e.g. temperature, vibration sensors in critical spots).  \n   – The graph topology and scheduling patterns leak usage habits—how often you sample, when devices are active, etc.  \n3. Once all that is in the cloud, an honest-but-curious or malicious cloud operator (or an attacker who breaks in) can:  \n   – Reconstruct your facility layout or patient monitoring plan.  \n   – Infer when and where certain events happen (e.g. peak usage, equipment failures).  \n   – Perform ML-specific attacks (model inversion, membership inference) to recover training examples or even real time sensor readings.  \n4. Finally, because GNNs are notoriously “over-enthusiastic” about memorizing graph patterns, you also risk leaking proprietary graph motifs or operational protocols if your scheduler’s model is ever queried or stolen.\n\nIn short, by sending your full network graph and tag assignments off to a remote scheduler, you expose:\n\n • Physical deployment details (who is where)  \n • Sensor installation specifics (what is being measured, how often)  \n • Usage and activity patterns over time  \n • Potentially even raw or aggregate sensor data via ML-centric attacks  \n\nAll of which constitute clear privacy and confidentiality risks whenever you rely on a third-party (or even multi-tenant) cloud/edge server to do your RobustGANTT schedule computation."
    },
    {
        "question": "What justifies keeping the Verifier step post Well-Crafted Perturbations before feeding into the Decoder?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Verifier",
            "Well-Crafted Perturbations",
            "Decoder"
        ],
        "id": 1856,
        "masked_question": "What justifies keeping the [mask1] step post Well-Crafted Perturbations before feeding into the Decoder?",
        "masked_number": 1,
        "masked_elements": [
            "Verifier"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.06552v1_figure_1.png",
        "paperid": "2407.06552v1",
        "paper_path": "./MISS-QA/papers/2407.06552v1.json",
        "figure_id": "2407.06552v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed D L O V E𝐷𝐿𝑂𝑉𝐸DLOVEitalic_D italic_L italic_O italic_V italic_E attack leveraging Adversarial Machine Learning to a create well-crafted perturbation to overwrite the original watermark with the target watermark.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed step is simply the “Verifier” – i.e. the standard authenticity/format check that sits in front of the decoder.  We leave it in place even after Eve’s well-crafted perturbations because in a real deployment you can’t just hand your tampered pixels straight to the decoder – the image must still pass the system’s built-in verification (imperceptibility checks, metadata checks, etc.) before the decoder ever sees it.  By keeping the Verifier in the loop we both (a) model the true black-box scenario (where the attacker never gets to skip the front end) and (b) prove that the adversarially perturbed image remains “valid enough” to pass the verifier and still trigger the overwrite at decode time."
    },
    {
        "question": "How does the Unknown Encoder fuse the Original Watermark and Cover Image representations to maintain imperceptibility?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Unknown Encoder",
            "Original Watermark",
            "Cover Image"
        ],
        "id": 1857,
        "masked_question": "How does the [mask1] fuse the Original Watermark and Cover Image representations to maintain imperceptibility?",
        "masked_number": 1,
        "masked_elements": [
            "Unknown Encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.06552v1_figure_1.png",
        "paperid": "2407.06552v1",
        "paper_path": "./MISS-QA/papers/2407.06552v1.json",
        "figure_id": "2407.06552v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed D L O V E𝐷𝐿𝑂𝑉𝐸DLOVEitalic_D italic_L italic_O italic_V italic_E attack leveraging Adversarial Machine Learning to a create well-crafted perturbation to overwrite the original watermark with the target watermark.",
        "qtype": "Implementation_Details",
        "response": "In the figure you pointed out the red‐boxed block is nothing mystical at all but simply the “encoder” of the watermarking scheme – the network that actually *merges* the two inputs (the original logo or bit‐string watermark and the RGB cover‐image) into a single, nearly indistinguishable, watermarked output.  Concretely, what happens inside that box is:\n\n 1.  The cover‐image and the watermark go each through their own small convolutional front‐end (often just a handful of conv layers).  \n 2.  Those two sets of feature‐maps are then *fused* (almost always by concatenating them along the channel dimension, sometimes by element‐wise add or a learned attention mask).  \n 3.  A stack of further convolutional (and usually residual) blocks “decodes” that fused tensor back into a 3-channel image, effectively learning a tiny *residual* that, when added to the cover, carries the watermark bits.  \n 4.  During training the entire encoder is penalized with a pixel‐wise (L2/L1) or perceptual similarity loss (and often an adversarial loss) against the clean cover image, so it never learns to make visible changes—only the sub-threshold “noise” that encodes your watermark.  \n\nBecause the network literally learns to hide the logo as a small residual perturbation inside the cover image, trained end-to-end under a strong “cover-vs-watermarked” similarity constraint, the final watermarked image is visually indistinguishable from the original."
    },
    {
        "question": "How does EVE optimize well-crafted perturbations to manipulate the Decoder towards extracting the target watermark?",
        "relevant_section_ids": [
            "4.1.1",
            "4.2"
        ],
        "relevant_context": [
            "Having white-box access to the decoder gives the attacker enough information to simulate the network by devising a targeted adversarial attack and using the gradients of the decoder to create the desired perturbation δ, where mo is the original watermark, mt is the target watermark and ε is the perturbation limit. We minimize the loss (L), of D(·+δ) with respect to the target watermark while maximizing the loss of D(·+δ) with respect to the original watermark, i.e. we solve the optimization problem as shown in Eq. (4).",
            "The adversarial perturbation crafting algorithm is shown in Algo 1. Inputs to the algorithm are: a watermarked image x, the target decoder D, the target watermark mt, a perturbation δ (initialized as zero), and a limiting range ε. x+δ is passed into the decoder, which decodes the secret as s. The loss between s and mt is computed using the chosen loss function L. In each iteration of the loop, the optimizer tries to minimize the loss between s and mt and maximize the loss between s and mo. Accordingly, δ is updated. This process is repeated until the model converges and the desired δ is obtained, which is the realization of the DLOVE attack on D to overwrite mo with mt."
        ],
        "final_answer": "EVE formulates a constrained optimization over the added perturbation δ so that when the perturbed watermarked image (x + δ) passes through the decoder D, the extracted watermark is driven toward the attacker's target mt rather than the original mo. In the white-box variant, EVE uses the decoder’s gradients directly to minimize the decoder’s loss on the target watermark while maximizing the loss on the original watermark (Eq. 4), subject to a perturbation magnitude bound ε. In practice (Algo 1), EVE iteratively updates δ by computing the loss L between the decoder’s output and mt (minimizing) and between the output and mo (maximizing) until convergence, yielding imperceptible but effective perturbations that overwrite the embedded watermark with the target watermark.",
        "relevant_elements": [
            "EVE",
            "Well-Crafted Perturbations",
            "Decoder"
        ],
        "id": 1858,
        "masked_question": "How does [mask1] optimize [mask2] to manipulate the Decoder towards extracting the target watermark?",
        "masked_number": 2,
        "masked_elements": [
            "EVE",
            "Well-Crafted Perturbations"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.06552v1_figure_1.png",
        "paperid": "2407.06552v1",
        "paper_path": "./MISS-QA/papers/2407.06552v1.json",
        "figure_id": "2407.06552v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed D L O V E𝐷𝐿𝑂𝑉𝐸DLOVEitalic_D italic_L italic_O italic_V italic_E attack leveraging Adversarial Machine Learning to a create well-crafted perturbation to overwrite the original watermark with the target watermark.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be clear what the two masks refer to in the question:\n\n  • [mask1] is the red‐boxed entity in the figure—namely, the attacker “EVE.”  \n  • [mask2] is the blue‐boxed object—namely, the “well-crafted perturbations” δ that get added to the watermarked image.\n\nSo the question restated is:\n\n  “How does the attacker EVE optimize the well-crafted perturbations so that the Decoder, when run on the perturbed image, spits out the attacker’s chosen target watermark instead of the original one?”\n\nChain of thought:\n\n1.  In both the white-box and black-box setups the attacker’s goal is to force the decoder  to output a chosen target watermark  when fed   \n2.  To do that, EVE must find a small perturbation δ (the blue-boxed “well-crafted perturbation”) satisfying  \n     –      dec(xw+δ) ≈ t   (the target watermark)  \n     –      dec(xw+δ) ≠ w   (no longer the original watermark)  \n     –      ‖δ‖∞ ≤ ε    (imperceptibility constraint)  \n\n3.  In the white-box case EVE has direct access to the decoder’s parameters and can compute  \n     – the loss ℓ(dec(xw+δ),t) which measures how far the decoder’s output is from the target, and  \n     – optionally the loss ℓ(dec(xw+δ),w) which measures how well it still matches the original.  \n\n4.  EVE then runs a standard targeted adversarial‐example procedure (e.g. projected gradient descent or Adam) to solve  \n     minimize δ   ℓ(dec(xw+δ), t)  – ℓ(dec(xw+δ), w)  \n     subject to ‖δ‖∞ ≤ ε  \n\n   – at each iteration she takes a gradient step on δ to decrease the first term (make the decoder believe the target watermark) and to increase the second term (push it away from the original).  \n   – she then projects back into the allowed perturbation ball ‖δ‖∞≤ε.\n\n5.  In the black-box case she first trains a surrogate decoder via whatever limited (xw, w) pairs she can obtain, fine‐tunes it to mimic the real decoder, and then carries out exactly the same gradient‐based crafting on that surrogate.  Because adversarial examples transfer, the δ she finds will also fool the real decoder.\n\nAnswer (concise):\n\nEVE treats the watermark‐decoder exactly like a classifier and performs a targeted adversarial‐example attack.  By using the decoder (or a surrogate) to compute gradients of a loss that encourages the output to be the attacker’s chosen target watermark and discourages it from remaining the original, she iteratively adjusts the small “well-crafted perturbation” δ—while keeping ‖δ‖ below a tiny ε—so that the final perturbed image is decoded as the target watermark."
    },
    {
        "question": "How does Expert-guided integrate domain descriptors into prompts for fine-tuning to optimize LLM input formatting?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "(i) Expert-guided: In this approach, we replace the feature names with user-provided descriptors detailing the semantic context of each column in the tabular dataset while retaining the is qualifier from Baseline encoding.",
            "Although this requires additional human effort, our empirical study shows that this expanded context not only improves the efficacy of data generation but also provides significant gains in training efficiency, i.e., number of epochs for fine-tuning."
        ],
        "final_answer": "Expert-guided replaces each raw feature name in the Baseline prompt with a human-provided, domain-specific descriptor that explains the semantic meaning of that column, while keeping the original “is” qualifier. This enriched prompt formatting improves both the quality of generated samples and the efficiency of fine-tuning.",
        "relevant_elements": [
            "Expert-guided",
            "Prompts For Finetuning",
            "LLM"
        ],
        "id": 1859,
        "masked_question": "How does [mask1] integrate domain descriptors into [mask2] for fine-tuning to optimize LLM input formatting?",
        "masked_number": 2,
        "masked_elements": [
            "Expert-guided",
            "Prompts For Finetuning"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.03946v1_figure_1.png",
        "paperid": "2409.03946v1",
        "paper_path": "./MISS-QA/papers/2409.03946v1.json",
        "figure_id": "2409.03946v1_figure_1.png",
        "caption": "Figure 1: An overview of our approach for LLM-based tabular data generation. Our contributions include designing new prompt construction strategies and investigating their role in improving the quality of synthesized samples.",
        "qtype": "Implementation_Details",
        "response": "Chain of Thought  \n1.  In the figure, the red‐boxed module is the “Expert-guided” prompt‐construction protocol and the blue‐boxed module is the actual “Prompts for Fine-tuning” that get fed into the LLM during training.  \n2.  Under Expert-guided, a domain expert writes out full, semantically rich descriptions for each column (e.g. replacing “fLength” with “Major axis of the ellipse”) while still using the same “is”‐qualifier template from the baseline GReaT encoding.  \n3.  These expert-supplied descriptors are then slotted directly into the fine-tuning prompt template (the blue box). In other words, wherever the baseline would have placed a bare column name, the prompt now carries the expert’s full description.  \n4.  The result is that the LLM sees richly annotated, human-readable feature names in its training prompts, which leads to more efficient fine-tuning and better downstream data‐generation quality.  \n\nAnswer  \nThe Expert-guided protocol takes human-written, domain‐specific descriptors for each column (red box) and substitutes them for the original feature names inside the fine-tuning prompt template (blue box). By slotting these full, semantically clear descriptors into the “Prompts for Fine-tuning,” the LLM is trained on inputs that are formatted in natural language with precise context, improving both learning efficiency and generation accuracy."
    },
    {
        "question": "What procedure ensures Novel-mapping assigns consistent physics quantities when constructing prompts for fine-tuning?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "…we propose the use of the Novel-Mapping protocol, which will query an external LLM to generate a suitable feature name from an arbitrary domain (e.g., physics or life sciences); for example, one can use the query “I have a dataset that does not have meaningful names for features. Given the ranges of the columns are <list of ranges>, suggest a term/phenomenon from <field name> that can take values in each of the given ranges. Rules are: (i) the terms/phenomenon should be from the same field, (ii) no two suggestions can be identical.”",
            "Note, the <field name> can be arbitrarily chosen as long as the feature names remain consistent with the LLM’s prior knowledge (i.e., chosen from the same domain) and they have a similar range of feasible values (<list of ranges>)."
        ],
        "final_answer": "The Novel-Mapping protocol constructs prompts by querying an external LLM with the column value ranges and a chosen domain (e.g. physics) and enforces two rules: (i) all suggested terms must come from the same field, and (ii) no two suggestions can be identical. This ensures that each column is mapped to a consistent, unique physics quantity when fine-tuning the model.",
        "relevant_elements": [
            "Novel-mapping",
            "Prompts For Finetuning"
        ],
        "id": 1860,
        "masked_question": "What procedure ensures [mask1] assigns consistent physics quantities when constructing prompts for fine-tuning?",
        "masked_number": 1,
        "masked_elements": [
            "Novel-mapping"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.03946v1_figure_1.png",
        "paperid": "2409.03946v1",
        "paper_path": "./MISS-QA/papers/2409.03946v1.json",
        "figure_id": "2409.03946v1_figure_1.png",
        "caption": "Figure 1: An overview of our approach for LLM-based tabular data generation. Our contributions include designing new prompt construction strategies and investigating their role in improving the quality of synthesized samples.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red-boxed step in the diagram is the “Novel-Mapping” protocol.  In that protocol you:\n\n1. Pick an (arbitrary) physics sub-field—for instance, “particle physics.”  \n2. Supply the LLM with the numerical ranges of each column plus the chosen field name.  \n3. Instruct it to “Suggest a distinct term or phenomenon from [the same] field that can take values in each of the given ranges.  No two suggestions may be identical.”  \n\nBecause the prompt explicitly (i) fixes a single domain and (ii) enforces unique, range-compatible terms, the Novel-Mapping procedure guarantees that all generated feature names are consistent physics quantities."
    },
    {
        "question": "How does LLM-guided protocol's automated descriptor generation differ methodologically from Expert-guided prompt expansion?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "In this approach, we replace the feature names with user-provided descriptors detailing the semantic context of each column in the tabular dataset while retaining the is qualifier from Baseline encoding.",
            "As an alternative to expert guidance, we also explore the feasibility of leveraging a pre-trained Large Language Model (LLM) such as ChatGPT (OpenAI, 2024) to automate this process. More specifically, we query the ChatGPT API with the following prompt: “For a dataset named <name>, the given column names are <list of column names>. You need to provide a short one-line description of each feature.” The response from ChatGPT is then parsed and used in lieu of the original feature names during prompt construction for the fine-tuning step."
        ],
        "final_answer": "Expert-guided prompt expansion obtains detailed feature descriptors directly from human experts, who manually write semantic context for each column. In contrast, the LLM-guided protocol automates descriptor creation by sending the dataset and its column names to a large language model (e.g., ChatGPT) via an API prompt and using the model’s one-line feature descriptions in place of the original names.",
        "relevant_elements": [
            "LLM-guided",
            "Expert-guided"
        ],
        "id": 1861,
        "masked_question": "How does [mask1] protocol's automated descriptor generation differ methodologically from [mask2] prompt expansion?",
        "masked_number": 2,
        "masked_elements": [
            "LLM-guided",
            "Expert-guided"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.03946v1_figure_1.png",
        "paperid": "2409.03946v1",
        "paper_path": "./MISS-QA/papers/2409.03946v1.json",
        "figure_id": "2409.03946v1_figure_1.png",
        "caption": "Figure 1: An overview of our approach for LLM-based tabular data generation. Our contributions include designing new prompt construction strategies and investigating their role in improving the quality of synthesized samples.",
        "qtype": "Literature_Background",
        "response": "Mask1 (the red‐boxed “LLM‐guided” protocol) differs from Mask2 (the blue‐boxed “Expert‐guided” protocol) in that:\n\n• Expert‐guided  \n  – A human subject‐matter expert writes or selects rich, semantically precise descriptors for each column (e.g. “Major axis of the ellipse” for fLength), and those hand-crafted descriptors are slotted directly into the fine-tuning prompt.  \n  – Methodological hallmark: manual curation, expert effort, full control over wording.\n\n• LLM‐guided  \n  – Instead of a person, you automatically query a pre–trained LLM (e.g. ChatGPT) with a prompt like  \n    “For a dataset named X, the given column names are A, B, C. Provide a one‐line description of each feature.”  \n  – You parse the model’s reply and use those generated descriptions in your prompt.  \n  – Methodological hallmark: zero or minimal human writing, fully automated descriptor generation via an API."
    },
    {
        "question": "How does generated data from LLM fine-tuning interface with Random Forest evaluation methodology?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM",
            "Random Forest/Decision Tree Models"
        ],
        "id": 1862,
        "masked_question": "How does generated data from [mask1] fine-tuning interface with [mask2] evaluation methodology?",
        "masked_number": 2,
        "masked_elements": [
            "LLM",
            "Random Forest/Decision Tree Models"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.03946v1_figure_1.png",
        "paperid": "2409.03946v1",
        "paper_path": "./MISS-QA/papers/2409.03946v1.json",
        "figure_id": "2409.03946v1_figure_1.png",
        "caption": "Figure 1: An overview of our approach for LLM-based tabular data generation. Our contributions include designing new prompt construction strategies and investigating their role in improving the quality of synthesized samples.",
        "qtype": "Literature_Background",
        "response": "First, let’s pin down what the two masks point at:\n\n1.  [mask1] (the red box) is the LLM prompt‐based fine-tuning step – i.e. the Expert-guided / LLM-guided / Novel-Mapping prompts that we feed into the pre-trained model to produce synthetic rows.  \n2.  [mask2] (the blue box) is the downstream “machine learning efficiency” evaluation – namely, training Random Forests or Decision Trees on the synthetic data and measuring accuracy (for classification) or MSE (for regression) on held-out real data.\n\nWith that in place, the way they “interface” is straightforward:\n\n–  We take the table of synthetic samples that comes out of the fine-tuned LLM (mask1), and we hand it straight to our standard ML learners (mask2).  \n–  Those learners (RFs/DTs) are trained exclusively on the generated data.  \n–  Finally, we test those trained models on the real test set, computing accuracy or MSE.  \n–  That test score is exactly the “machine learning efficiency” (MLE) metric that we report.  \n\nIn other words, the red-boxed module produces synthetic records, and the blue-boxed module consumes them as a training set and evaluates how well models trained on them generalize back to true held-out data."
    },
    {
        "question": "How does Reference Selector transform Offline MCP's sampling-based bilevel plans for policy conditioning via task phase?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Reference Selector",
            "Offline MCP"
        ],
        "id": 1863,
        "masked_question": "How does [mask1] transform [mask2]'s sampling-based bilevel plans for policy conditioning via task phase?",
        "masked_number": 2,
        "masked_elements": [
            "Reference Selector",
            "Offline MCP"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.13817v1_figure_2.png",
        "paperid": "2410.13817v1",
        "paper_path": "./MISS-QA/papers/2410.13817v1.json",
        "figure_id": "2410.13817v1_figure_2.png",
        "caption": "Figure 2: The loco-manipulation planner [10] generates references in the form of multi-modal plans consisting of continuous trajectories 𝐗∗superscript𝐗\\bf{X}^{*}bold_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and manipulation schedules 𝐌∗superscript𝐌\\bf{M}^{*}bold_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. These are used by the demonstration-guided controller to select 𝐱∗superscript𝐱\\bf{x}^{*}bold_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and 𝐦∗superscript𝐦\\bf{m}^{*}bold_m start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT adaptively based on the task phase ϕitalic-ϕ\\phiitalic_ϕ and track them robustly.The controller receives full-state feedback and sends joint position commands to the robot.",
        "qtype": "Literature_Background",
        "response": "The component in the red box is the “Reference Selector,” and the component in the blue box is the “Offline MCP” (the sampling-based bilevel planner).  The Reference Selector’s job is simply to take the full, multi–contact trajectories and mode schedule ({X*, M*}) computed offline by the MCP and, at each control step, pick out the one state–mode pair 〈x*, m*〉 that corresponds to the current task‐phase ϕ.  In other words, it “slices” the MCP’s entire demonstration down into a single reference waypoint and contact mode (x*, m*) indexed by ϕ, and hands that to the policy as its conditioning input."
    },
    {
        "question": "How does Adaptive Phase Dynamics shape policy residual δv relative to demonstration tracking during robust interactions?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To this end, we propose the phase dynamics: φ̇ = v_ref(φ,x,x*,m*) + α δv.",
            "The term v_ref effectively pauses the phase evolution for large deviations from the current reference. As the tracking improves, it gradually approaches the nominal phase rate (v_nom).",
            "To enable motion recovery in such scenarios, we introduce a residual phase δv that allows potentially speeding up, slowing down, and even decreasing the phase whenever necessary. This residual phase is outputted from the policy, allowing it to adapt to the task phase dynamics via learnable parameters."
        ],
        "final_answer": "Adaptive Phase Dynamics defines the phase rate as φ̇ = v_ref + α δv, where v_ref slows or pauses progression when tracking errors are large and returns to nominal speed as the policy improves. The policy-produced residual δv then modulates this rate—speeding up, slowing down, or even reversing the phase—to recover from disturbances and maintain robust tracking of the demonstration.",
        "relevant_elements": [
            "Adaptive Phase Dynamics",
            "Policy"
        ],
        "id": 1864,
        "masked_question": "How does [mask1] shape [mask2] residual δv relative to demonstration tracking during robust interactions?",
        "masked_number": 2,
        "masked_elements": [
            "Adaptive Phase Dynamics",
            "Policy"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.13817v1_figure_2.png",
        "paperid": "2410.13817v1",
        "paper_path": "./MISS-QA/papers/2410.13817v1.json",
        "figure_id": "2410.13817v1_figure_2.png",
        "caption": "Figure 2: The loco-manipulation planner [10] generates references in the form of multi-modal plans consisting of continuous trajectories 𝐗∗superscript𝐗\\bf{X}^{*}bold_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and manipulation schedules 𝐌∗superscript𝐌\\bf{M}^{*}bold_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. These are used by the demonstration-guided controller to select 𝐱∗superscript𝐱\\bf{x}^{*}bold_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and 𝐦∗superscript𝐦\\bf{m}^{*}bold_m start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT adaptively based on the task phase ϕitalic-ϕ\\phiitalic_ϕ and track them robustly.The controller receives full-state feedback and sends joint position commands to the robot.",
        "qtype": "Literature_Background",
        "response": "Here “mask₁” is the red‐boxed Adaptive Phase Dynamics module and “mask₂” is the blue‐boxed Policy.  In our scheme the policy in the blue box only ever outputs a *residual* phase‐rate correction, δᵥ, on top of a *state-dependent* nominal phase rate φ̇_ref.  The Adaptive Phase Dynamics then use\n\n φ̇ = φ̇_ref(φ, x, x*, m*)  +  δᵥ\n\nbut also *clip* δᵥ (and φ̇) so that:\n\n  • when the robot is far from the demo (large tracking error) φ̇_ref → 0 and the clips force φ̇ ≈ 0, effectively stalling the demonstration,\n  \n  • as the robot closes in on the demo φ̇_ref→1 and the clips allow δᵥ to be positive (speeding up) or even slightly negative (backing up) to let the robot recover.\n\nIn this way the red‐boxed Adaptive Phase Dynamics “shapes” the blue‐boxed policy’s δᵥ: it turns the policy’s raw residual into a small, bounded tweak of the demo playback speed that automatically slows or pauses when tracking degrades and accelerates (or rewinds) only when the robot has recovered."
    },
    {
        "question": "How does Adaptive Phase Dynamics shape policy residual δv relative to demonstration tracking during robust interactions?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To this end, we propose the phase dynamics: φ̇ = v_ref(φ,x,x*,m*) + α δv.",
            "The term v_ref effectively pauses the phase evolution for large deviations from the current reference. As the tracking improves, it gradually approaches the nominal phase rate (v_nom).",
            "To enable motion recovery in such scenarios, we introduce a residual phase δv that allows potentially speeding up, slowing down, and even decreasing the phase whenever necessary. This residual phase is outputted from the policy, allowing it to adapt to the task phase dynamics via learnable parameters."
        ],
        "final_answer": "Adaptive Phase Dynamics defines the phase rate as φ̇ = v_ref + α δv, where v_ref slows or pauses progression when tracking errors are large and returns to nominal speed as the policy improves. The policy-produced residual δv then modulates this rate—speeding up, slowing down, or even reversing the phase—to recover from disturbances and maintain robust tracking of the demonstration.",
        "relevant_elements": [
            "Adaptive Phase Dynamics",
            "Policy"
        ],
        "id": 1864,
        "masked_question": "How does [mask1] shape [mask2] residual δv relative to demonstration tracking during robust interactions?",
        "masked_number": 2,
        "masked_elements": [
            "Adaptive Phase Dynamics",
            "Policy"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.13817v1_figure_2.png",
        "paperid": "2410.13817v1",
        "paper_path": "./MISS-QA/papers/2410.13817v1.json",
        "figure_id": "2410.13817v1_figure_2.png",
        "caption": "Figure 2: The loco-manipulation planner [10] generates references in the form of multi-modal plans consisting of continuous trajectories 𝐗∗superscript𝐗\\bf{X}^{*}bold_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and manipulation schedules 𝐌∗superscript𝐌\\bf{M}^{*}bold_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. These are used by the demonstration-guided controller to select 𝐱∗superscript𝐱\\bf{x}^{*}bold_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and 𝐦∗superscript𝐦\\bf{m}^{*}bold_m start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT adaptively based on the task phase ϕitalic-ϕ\\phiitalic_ϕ and track them robustly.The controller receives full-state feedback and sends joint position commands to the robot.",
        "qtype": "Literature_Background",
        "response": "Here is the short story of what’s happening in that red‐box → blue‐box interaction:  \n\n– The red‐boxed “Adaptive Phase Dynamics” is nothing more than a learnable timing governor for your demonstration.  Instead of marching the demo forward at a constant rate, it computes  \n     \n     φ̇ = r( tracking error )   +   δ_v  \n     \n  where  \n  •  r( tracking error ) is a clipped, reward‐dependent “slow‐down” term that goes to zero whenever you stray far from the reference, and  \n  •  δ_v is exactly the extra, learnable “knob” that the policy (blue box) is allowed to turn.  \n\n– In practice that means whenever you lose contact, get pushed off, or simply lag behind the demonstration, the first term collapses toward zero (i.e. φ̇≈0) so the demo doesn’t sprint on without you.  The policy then chooses a δ_v (positive, negative, or zero) to either pause φ, back it up, or even speed it up once it has recovered.  \n\nIn short, the adaptive phase dynamics block shapes δ_v so that the phase variable φ only advances along the demonstration when tracking is good, and gives the policy the freedom to slow, reverse or re-accelerate φ whenever robust recovery is required."
    },
    {
        "question": "How does the policy utilize residual phase from Adaptive Phase Dynamics to adjust joint command outputs adaptively?",
        "relevant_section_ids": [
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "In some instances, unforeseen slippage or large disturbances could render the object uncontrollable due to a complete loss of contact, resulting in significant deviations from the reference pose. In these situations, the term φ̇_ref is close to zero, and the robot cannot recover. To enable motion recovery in such scenarios, we introduce a residual phase φ̇_res that allows potentially speeding up, slowing down, and even decreasing the phase whenever necessary. This residual phase is outputted from the policy, allowing it to adapt to the task phase dynamics via learnable parameters.",
            "The actions a are interpreted as the residuals over the robot’s reference joint positions q_j^ref and the reference phase rate φ̇ from Sec. 3.2. The robot’s actions are sent to its actuators as joint position commands: q_j^cmd = q_j^ref + a_j, with a_j in [−Δ_j, Δ_j]."
        ],
        "final_answer": "The policy learns to output a residual phase rate φ̇_res alongside its joint‐offset actions. At each timestep the adaptive phase dynamics combine the nominal phase rate φ̇_ref with φ̇_res to decide how far along the demonstration to progress (speeding up, slowing down, or even reversing). That chosen phase then selects the reference joint positions q_j^ref. Finally, the policy adds its learned residuals a_j to those references to form the executed joint commands: q_j^cmd = q_j^ref + a_j, enabling the robot to adaptively adjust its motion in response to disturbances.",
        "relevant_elements": [
            "Adaptive Phase Dynamics",
            "Policy"
        ],
        "id": 1866,
        "masked_question": "How does the [mask1] utilize residual phase from Adaptive Phase Dynamics to adjust joint command outputs adaptively?",
        "masked_number": 1,
        "masked_elements": [
            "Policy"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.13817v1_figure_2.png",
        "paperid": "2410.13817v1",
        "paper_path": "./MISS-QA/papers/2410.13817v1.json",
        "figure_id": "2410.13817v1_figure_2.png",
        "caption": "Figure 2: The loco-manipulation planner [10] generates references in the form of multi-modal plans consisting of continuous trajectories 𝐗∗superscript𝐗\\bf{X}^{*}bold_X start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and manipulation schedules 𝐌∗superscript𝐌\\bf{M}^{*}bold_M start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT. These are used by the demonstration-guided controller to select 𝐱∗superscript𝐱\\bf{x}^{*}bold_x start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT and 𝐦∗superscript𝐦\\bf{m}^{*}bold_m start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT adaptively based on the task phase ϕitalic-ϕ\\phiitalic_ϕ and track them robustly.The controller receives full-state feedback and sends joint position commands to the robot.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is nothing more mysterious than the policy network itself.  Internally it both\n\n  1.  ingests the “residual phase” δᵥ that it just produced in the last step,  \n  2.  uses that δᵥ to push or pull the phase φ̇ away from its nominal rate,  \n\nand in that way it “decides” how fast or how slowly to march through the demonstration.  Concretely:\n\n  •  At each timestep the policy outputs two things:  \n     –  A small joint-position offset δq,  \n     –  A residual phase increment δᵥ.  \n\n  •  The δᵥ term is then added to the nominal φ̇ to form the actual phase‐rate φ̇=φ̇ref+δᵥ.  That φ̇ controls which point of the reference trajectory ⟨x*,m*⟩ the Reference Selector hands back to the policy next.  \n\n  •  Finally, the policy applies its learned δq on top of the freshly selected q* to yield the commanded joint positions q_cmd=q*+δq.\n\nBy letting δᵥ speed up, slow down or even step the phase backwards, the network is free to linger on a “sticky” part of the motion to recover from disturbances or to rush through easy segments—while always using the same underlying demonstration."
    },
    {
        "question": "How does metric monitoring & logging inform plan recommender’s cost-effectiveness optimization under multi-objective constraints?",
        "relevant_section_ids": [
            "2.1",
            "2.3.4"
        ],
        "relevant_context": [
            "Metric monitoring & logging. The performance and system resources are measured and logged. We allow both standard and customizable metrics to evaluate response quality, facilitating business-oriented benchmarking.",
            "Plan recommender. Given all the logged metrics and budget constraints, CEBench is able to recommend optimal plans, for both cost and effectiveness, capable of illustrating a Pareto front to demonstrate optimal configurations.",
            "When benchmarking locally deployed LLMs, CEBench tracks memory usage and processing latency. Memory usage indicates the minimal memory requirement for candidate server instances, while latency helps estimate the costs of running LLM pipelines on untested hardware resources."
        ],
        "final_answer": "Metric monitoring & logging gathers key performance metrics—such as generative quality, memory footprint, and end-to-end latency—alongside resource usage data. The plan recommender then takes these logged metrics together with budget constraints and instance quotes to estimate per-instance costs and identify Pareto-optimal configurations that balance quality, speed, and financial cost under multi-objective constraints.",
        "relevant_elements": [
            "Metric monitoring & logging",
            "Plan recommender"
        ],
        "id": 1868,
        "masked_question": "How does [mask1] inform plan recommender’s cost-effectiveness optimization under multi-objective constraints?",
        "masked_number": 1,
        "masked_elements": [
            "Metric monitoring & logging"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.12797v1_figure_1.png",
        "paperid": "2407.12797v1",
        "paper_path": "./MISS-QA/papers/2407.12797v1.json",
        "figure_id": "2407.12797v1_figure_1.png",
        "caption": "Figure 1: Workflow of benchmarking LLM pipelines using CEBench.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. In the diagram, the red box clearly labels the “Metric Monitoring & Logging” component.  \n2. From the text we know this module continuously records both:  \n   - Quality metrics (e.g. accuracy, coherence of LLM outputs)  \n   - System/resource metrics (e.g. latency, memory footprint, GPU performance translated into cost via instance quotes)  \n3. These logged metrics form the multi-objective performance profile of each benchmark run.  \n4. The plan recommender ingests exactly those profiles, along with any user-specified budget or resource constraints, to:  \n   a. Compare candidate configurations across axes like generative quality, time, memory, and monetary cost  \n   b. Identify the Pareto-optimal frontier under the given constraints  \n   c. Recommend the most cost-effective plan that balances all objectives  \n\nAnswer  \nThe Metric Monitoring & Logging component supplies the detailed quality, latency, memory and cost estimates that the plan recommender needs to perform multi-objective trade-off analysis. By feeding timestamped, per-run measurements into the planner, it enables the construction of Pareto fronts under budget or resource ceilings and thus drives the cost-effectiveness optimization."
    },
    {
        "question": "What limitations stem from using VectorDB for static embeddings when external knowledge evolves over time?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "VectorDB",
            "External knowledge"
        ],
        "id": 1869,
        "masked_question": "What limitations stem from using [mask1] for static embeddings when external knowledge evolves over time?",
        "masked_number": 1,
        "masked_elements": [
            "VectorDB"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.12797v1_figure_1.png",
        "paperid": "2407.12797v1",
        "paper_path": "./MISS-QA/papers/2407.12797v1.json",
        "figure_id": "2407.12797v1_figure_1.png",
        "caption": "Figure 1: Workflow of benchmarking LLM pipelines using CEBench.",
        "qtype": "Others",
        "response": "Chain-of-thought  \n1. In the figure the red box calls out the “VectorDB” component – that is, the vector database holding the precomputed embeddings of the external knowledge.  \n2. The paper explains that RAG augments a frozen LLM with an external store of document embeddings so the model can retrieve up-to-date, domain-specific facts.  \n3. However, once you compute and stash those embeddings in the VectorDB, they remain static. Any changes to the underlying documents (new policies, updated facts, fresh data) aren’t reflected until you re-compute and re-index all the embeddings.  \n4. Thus, static embeddings become stale over time, leading to retrieval of outdated or incomplete information, degraded relevance, and an onerous overhead of periodically rebuilding or refreshing the index.  \n5. These limitations are intrinsic to using a VectorDB of static embeddings when the external knowledge source is itself evolving.  \n\nAnswer  \nBecause the VectorDB holds fixed, precomputed embeddings, any updates in the source documents (new articles, corrections, emerging terminology, etc.) will not be reflected until you re-embed and re-index everything. In practice this means:  \n• The retrieved facts grow stale and may be inaccurate or incomplete.  \n• Retrieval relevance degrades as the knowledge base drifts from reality.  \n• You incur extra maintenance overhead to periodically rebuild or refresh all embeddings rather than getting instant, real-time updates."
    },
    {
        "question": "How could Inference Interface be enhanced to incorporate ethical safeguards for sensitive content generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Inference Interface"
        ],
        "id": 1870,
        "masked_question": "How could [mask1] be enhanced to incorporate ethical safeguards for sensitive content generation?",
        "masked_number": 1,
        "masked_elements": [
            "Inference Interface"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.12797v1_figure_1.png",
        "paperid": "2407.12797v1",
        "paper_path": "./MISS-QA/papers/2407.12797v1.json",
        "figure_id": "2407.12797v1_figure_1.png",
        "caption": "Figure 1: Workflow of benchmarking LLM pipelines using CEBench.",
        "qtype": "Others",
        "response": "Step 1: Identify what “[mask1]” refers to  \n– In the red box of Fig.1 we see the component labeled “Inference Interface,” i.e. the module in CEBench that sends assembled prompts (possibly augmented by RAG) into an LLM and returns responses.  \n\nStep 2: Understand its current role  \n– As described in the “Query execution” paragraph, the inference interface is a thin wrapper over an LLM service (by default via Ollama) that handles API calls, streaming or batched outputs, and basic metric logging. It does not, however, include any built-in ethical or safety checks on the content it generates.  \n\nStep 3: Introduce ethical safeguards  \nTo enhance the inference interface so that it actively prevents or flags sensitive, harmful or disallowed outputs, we could:  \n  1. Prepend a system-level “safety” prompt template  \n     • Automatically inject a high-level instruction (e.g. “You are a safe assistant. Do not produce hate speech, disallowed content or private personal data.”) before every user prompt.  \n  2. Integrate an external content-moderation API or filter  \n     • After the model returns its text, pipe it through a toxicity/bias/PII detector (e.g. Perspective API, OpenAI Moderation endpoint).  \n     • Block, redact or rewrite any segments that exceed configured toxicity or privacy thresholds.  \n  3. Incorporate a “policy vali­dator” module  \n     • Maintain a ruleset (e.g. GDPR, HIPAA, company code of conduct) and run generated text against it to automatically reject or flag violations.  \n  4. Add human-in-the-loop escalation  \n     • For any response that the filters label as “high risk” (e.g. medical advice, legal advice, self-harm content), pause the pipeline and queue it for a human reviewer.  \n  5. Log and monitor ethical metrics alongside performance  \n     • Track counts of blocked outputs, severity scores, demographic bias metrics, etc., in the same logging system already recording latency and cost.  \n  6. (Optional) Employ on-device differential privacy or secure enclaves  \n     • For particularly sensitive deployments, run the inference interface within a secure trusted execution environment that enforces no logging of certain user fields.  \n\nBy weaving these steps into the existing inference-interface component, CEBench would gain an end-to-end guardrail layer that prevents, flags or quarantines any ethically or legally sensitive content before it reaches end users."
    },
    {
        "question": "What potential failure modes exist in Patch Localization facing complex patch shapes or textures?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Patch Localization"
        ],
        "id": 1871,
        "masked_question": "What potential failure modes exist in [mask1] facing complex patch shapes or textures?",
        "masked_number": 1,
        "masked_elements": [
            "Patch Localization"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.09406v1_figure_2.png",
        "paperid": "2409.09406v1",
        "paper_path": "./MISS-QA/papers/2409.09406v1.json",
        "figure_id": "2409.09406v1_figure_2.png",
        "caption": "Figure 2: Pipeline of DIFFender.\nDIFFender utilizes a unified diffusion model to seamlessly coordinate the localization and restoration of adversarial patch attacks, integrating a prompt-tuning module to enable efficient and precise tuning.",
        "qtype": "Others",
        "response": "In that figure the red‐boxed module is the “patch localization” stage of DIFFender, where the system looks at differences between two one-step denoised versions of the image to produce a binary mask of the adversarial patch.  Because it relies on averaging noisy latent differences and a simple threshold + smoothing + dilation pipeline, it can fail when confronted with:\n\n  1. Highly irregular or non-rectilinear patch shapes  \n     •  The coarse binarization and subsequent Gaussian blur/dilation will tend to “round off” corners and thin features, so winding or star-shaped patches can be under-segmented (missed) or over-smoothed.  \n\n  2. Complex or high-frequency textures  \n     •  If the patch’s internal texture resembles natural image noise (for example, wood grain or foliage), the diffusion model may partially “explain it away” as natural content, yielding weak difference signals and holes in the mask (false negatives).  \n     •  Conversely, busy backgrounds with strong edges or high-frequency detail can light up the difference map and be mistaken for a patch (false positives).  \n\n  3. Small isolated islands of perturbation  \n     •  Tiny adversarial “speckles” may fall below the chosen threshold and be dropped entirely, or be merged into the background by the dilation step.  \n\n  4. Threshold‐ and prompt-sensitivity  \n     •  A single global threshold on the averaged difference map may not suit all images; highly textured patches may produce a lower peak response and so be buried below threshold, while very bright or high-contrast patches may bleed out and engulf nearby clean regions.  \n\nIn short, because localization hinges on a single difference‐map threshold and very light morphological cleanup, it can under- or over-segment when patches have elaborate shapes, camouflage‐like textures, or live in very busy scenes."
    },
    {
        "question": "How could Prompt Tuning adapt to domain shifts without employing an IDC token mechanism?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In line with the pipeline described, DIFFender leverages pre-trained diffusion models to enable efficient zero-shot defense. While this approach is effective in most instances, slight inaccuracies in the segmented masks can occasionally occur in certain challenging cases. Given that vision-language pre-training harnesses the power of large-capacity text encoders to explore an extensive semantic space [53], we introduce the prompt-tuning algorithm to effectively adapt these learned representations for adversarial defense tasks by only few-shot tuning.",
            "Learnable Prompts. We begin by replacing the textual vocabulary with learnable continuous vectors. Unlike text prompts, learnable prompts are a set of continuous vectors that allow for gradient computation to identify the most effective prompt.",
            "Tuning Process. Once the learnable vectors are established, we introduce three loss functions to guide prompt tuning. These losses are designed to jointly optimize the vectors, enhancing the model’s ability to identify adversarial regions and improving overall defense performance.",
            "Few-Shot Learning. During prompt tuning, DIFFender leverages a limited set of images for few-shot learning. Specifically, the model is fine-tuned on a limited number of attacked images (8-shot in our experiments) from a single attack type, enabling it to learn optimal prompts that generalize effectively across different scenarios and attacks."
        ],
        "final_answer": "Even without an IDC token, Prompt Tuning can handle domain shifts by replacing fixed text prompts with learnable continuous prompt vectors and then fine-tuning those vectors on a small, domain-specific set of examples. By optimizing the prompt embeddings via gradient descent—using losses for mask localization, pixel-level reconstruction, and feature alignment—the method adapts the learned textual guidance to new distributions in a few-shot manner.",
        "relevant_elements": [
            "Prompt Tuning",
            "IDC token"
        ],
        "id": 1872,
        "masked_question": "How could [mask1] adapt to domain shifts without employing an IDC token mechanism?",
        "masked_number": 1,
        "masked_elements": [
            "Prompt Tuning"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.09406v1_figure_2.png",
        "paperid": "2409.09406v1",
        "paper_path": "./MISS-QA/papers/2409.09406v1.json",
        "figure_id": "2409.09406v1_figure_2.png",
        "caption": "Figure 2: Pipeline of DIFFender.\nDIFFender utilizes a unified diffusion model to seamlessly coordinate the localization and restoration of adversarial patch attacks, integrating a prompt-tuning module to enable efficient and precise tuning.",
        "qtype": "Others",
        "response": "Here’s a step‐by‐step reasoning for how the *prompt‐tuning block* (the red‐boxed “mask₁” in your figure) can still handle a shift of domain (say from visible‐light images to infrared imagery) *without* ever introducing a special “IDC” token:\n\n1.  What mask₁ actually is  \n    – It is nothing more exotic than a small set of *continuous* (“soft”) prompt vectors V₁…Vₙ on which we do gradient‐based fine‐tuning.  \n    – At training time we minimize three losses jointly:  \n      •  L_CE to make the localization mask correct,  \n      •  L₁ on pixel‐wise reconstruction to make the restored patch look like the clean image,  \n      •  A feature‐alignment loss d(·) to ensure high‐level features of the restored image match those of the clean image.  \n\n2.  Why an IDC token is only a convenience, not a necessity  \n    – The IDC (Infrared Domain Context) token is really just *another* continuous vector in the same embedding space as V₁…Vₙ, and one that the authors manually reserve to carry “infrared‐ness.”  \n    – But there is nothing preventing us from *re–tuning* the *same* V₁…Vₙ prompt vectors on a handful of infrared examples so that they, too, learn to condition the diffusion model correctly on infrared data.  \n\n3.  Few‐shot prompt tuning alone adapts to new domains  \n    – Gather K ≈ 8 few‐shot examples in your new domain (infrared, thermal, medical scans, etc.).  \n    – Freeze the backbone diffusion model and text‐encoder; only back‐prop through V₁…Vₙ.  \n    – Continue minimizing L_CE, L₁ and feature‐loss *on those new domain examples.*  \n    – The prompt vectors will slide around in the embedding space so that “when I do denoising with this new prompt” the model learns to:  \n      •  correctly localize anomalies in infrared, and  \n      •  inpaint/restoration also looks plausible under infrared statistics.  \n\n4.  In effect, prompt‐tuning *is* a tiny, domain‐specific adapter  \n    – You never had to add a new token type; you simply re–fit the same low‐dimensional prompt.  \n    – Because the text‐encoder’s embedding space is rich and differentiable, it will absorb whatever shifts are needed to handle the new modality.  \n\n5.  Conclusion  \n    By treating the *learnable prompts* V₁…Vₙ themselves as the sole “hooks” into the diffusion model and by continuing few‐shot gradient updates on new‐domain data (using the same three losses), you automatically adapt the entire pipeline to domain shifts—*without* ever requiring a dedicated IDC token."
    },
    {
        "question": "What motivates integrating patch localization and patch restoration under a unified diffusion model?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Based on the AAP phenomenon, we further introduce DIFFender, a novel DIFfusion-based DeFender framework against adversarial patch attacks, utilizing the text-guided diffusion models. DIFFender operates by first localizing the adversarial patch through the analysis of discrepancies among various denoised images and then restoring the affected region while maintaining the integrity of the original content. These two stages are guided by a unified diffusion model, which allows for a synergistic interaction that enhances overall defense performance—precise localization aids in effective restoration, and successful restoration, in turn, validates and refines the localization process.",
            "Unified Defense Model. The two stages described above—patch localization and patch restoration—are seamlessly integrated into a unified diffusion model (e.g., Stable Diffusion), guided by the critical AAP phenomenon. This deliberate integration allows us to capitalize on the close interaction between these stages, significantly enhancing the overall defense mechanism."
        ],
        "final_answer": "Integrating localization and restoration under one diffusion model is motivated by the desire to exploit their tight interplay—accurate localization improves the quality of restoration, and successful restoration in turn validates and refines the localization—thereby significantly boosting overall defense effectiveness.",
        "relevant_elements": [
            "Unified Diffusion Model",
            "Patch Localization",
            "Patch Restoration"
        ],
        "id": 1873,
        "masked_question": "What motivates integrating [mask1] and [mask2] under a unified diffusion model?",
        "masked_number": 2,
        "masked_elements": [
            "Patch Localization",
            "Patch Restoration"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.09406v1_figure_2.png",
        "paperid": "2409.09406v1",
        "paper_path": "./MISS-QA/papers/2409.09406v1.json",
        "figure_id": "2409.09406v1_figure_2.png",
        "caption": "Figure 2: Pipeline of DIFFender.\nDIFFender utilizes a unified diffusion model to seamlessly coordinate the localization and restoration of adversarial patch attacks, integrating a prompt-tuning module to enable efficient and precise tuning.",
        "qtype": "Design_Rationale",
        "response": "Here “mask₁” is the red‐boxed Patch Localization module and “mask₂” is the blue‐boxed Patch Restoration module.  What drives us to fold both of these into one diffusion model is the discovery of the Adversarial Anomaly Perception (AAP) effect – namely, that a single diffusion network will naturally denoise normal image regions but fail to “explain’’ adversarial patches, so by comparing two denoised draws you get a very clean localization mask.  If you then feed that same mask back into the same network (now in an inpainting step), you can restore just the attacked patch without touching the rest of the image.  Tying both steps together in one model therefore:\n\n 1. Exploits the AAP phenomenon end‐to‐end  \n 2. Lets the localization and restoration stages inform one another (refining the mask as you inpaint)  \n 3. Avoids maintaining two separate networks or heavy post‐processing  \n\nThe net result is a much more accurate, semantically faithful, and computationally efficient patch defense."
    },
    {
        "question": "What motivates incorporating IDC token into prompt tuning for infrared extension?",
        "relevant_section_ids": [
            "4",
            "4.1"
        ],
        "relevant_context": [
            "However, transitioning to the infrared domain presents two primary challenges: (1) the domain transfer issue when the diffusion model moves from visible to infrared data, where the diffusion model is trained using dataset like LAION-5B, ensuring generalization across different scenes in the visible spectrum, with infrared images only constituting a small portion; (2) the inherent differences between infrared and RGB images, which lead to notable discrepancies in the patches generated for infrared attacks, such as the richer textures and colors in RGB images compared to the weaker textures in infrared images. These two issues make the method in Sec. 3 not directly deal with infrared adversarial patches.",
            "Diffusion models are trained using large-scale datasets to ensure generalization across various scenes in the visible spectrum, though infrared images constitute only a minor portion. To better adapt the diffusion model for infrared defense tasks and address the domain transfer issues from visible to infrared data, we designed the \"Infrared Domain Constrained Token\" (IDC token)."
        ],
        "final_answer": "Incorporating the IDC token is motivated by the need to overcome the domain shift when applying a diffusion model—originally trained mostly on visible‐spectrum images—to infrared inputs. By learning a dedicated infrared token, the model’s prompt is constrained to the infrared domain, ensuring that both localization and restoration work effectively on thermal imagery without degrading its performance on visible‐light images.",
        "relevant_elements": [
            "Prompt Tuning",
            "IDC token",
            "Infrared Extension"
        ],
        "id": 1874,
        "masked_question": "What motivates incorporating [mask1] into prompt tuning for infrared extension?",
        "masked_number": 1,
        "masked_elements": [
            "IDC token"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.09406v1_figure_2.png",
        "paperid": "2409.09406v1",
        "paper_path": "./MISS-QA/papers/2409.09406v1.json",
        "figure_id": "2409.09406v1_figure_2.png",
        "caption": "Figure 2: Pipeline of DIFFender.\nDIFFender utilizes a unified diffusion model to seamlessly coordinate the localization and restoration of adversarial patch attacks, integrating a prompt-tuning module to enable efficient and precise tuning.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed token in the prompt is the so-called “Infrared Domain Constrained (IDC) token.”  We introduce it for exactly the problem that our diffusion model was only ever trained on mostly visible–spectrum data, yet now we want it to clean up adversarial patches in the thermal/infrared domain.  In practice:\n\n 1.  A pre‐trained diffusion model has only seen a handful of IR images during its massive LAION-5B training run, so if you just point it at an IR patch attack it will tend to hallucinate visible‐looking colors and textures or simply fail to localize/restore correctly.  \n 2.  Infrared images are structurally different (low texture, edge‐driven, nonuniform heat spots), so you need to “tell” the model that everything it generates from here on must look like thermal imagery.  \n 3.  By learning a single embedding vector T_infrared that “stands in” for “this is an infrared scene,” and freezing it in front of your learnable prompt, you force the U-Net’s cross-attention to condition on an IR style.  \n\nIn short, the IDC token is incorporated into prompt tuning so that with only a handful of IR examples you can shift the diffusion model’s output space from RGB to genuine, physically plausible infrared images—thereby overcoming both (1) the domain‐transfer gap and (2) the visual/textural mismatch between RGB and IR."
    },
    {
        "question": "What motivates combining global retrieval and local retrieval outputs for robust fact retrieval?",
        "relevant_section_ids": [
            "3.1.2"
        ],
        "relevant_context": [
            "For more complex forgery types like image splicing and copy-move, a forged image may be derived from multiple data sources. Therefore, global retrieval plus additional retrieval of any overlaid objects is more effective than global retrieval alone.",
            "In particular, when the overlaid objects are small, relying solely on global retrieval is insufficient."
        ],
        "final_answer": "Because complex forgeries (e.g., splicing or copy-move) often involve small or separately sourced segments that global retrieval alone can miss, combining global retrieval with local (segment-level) retrieval ensures that both the full image context and the individual forged parts are correctly matched to their originals.",
        "relevant_elements": [
            "Global Retrieval",
            "Local Retrieval"
        ],
        "id": 1875,
        "masked_question": "What motivates combining [mask1] and [mask2] outputs for robust fact retrieval?",
        "masked_number": 2,
        "masked_elements": [
            "Global Retrieval",
            "Local Retrieval"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.18614v1_figure_1.png",
        "paperid": "2407.18614v1",
        "paper_path": "./MISS-QA/papers/2407.18614v1.json",
        "figure_id": "2407.18614v1_figure_1.png",
        "caption": "Figure 1: Upper half illustrates pipeline of our proposed open framework for image-based fact verification: It comprises two phases: forgery identification and fact retrieval. The modules therein can be replaced with almost any open-source toolbox. Lower half shows examples of two specific forgery types.",
        "qtype": "Design_Rationale",
        "response": "Here’s how we arrive at the answer step-by-step:\n\n1.  In the Fact Retrieval phase (Section III-A2), the authors split retrieval into two branches:\n    – Global Retrieval (red box / mask1): uses the entire image as a query.  \n    – Local Retrieval (blue box / mask2): uses only the detected forgery region (cropped by the mask) as a query.  \n\n2.  The text tells us:\n    – “Global retrieval … is sufficient for most forgery types, such as object removal and colorization.”  \n    – “But for more complex forgery types like image splicing and copy-move … when the overlaid objects are small, relying solely on global retrieval is insufficient.”  \n    – “Simultaneous retrieval of the detected forgery segments ensures that all related original images are identified.”  \n\n3.  Therefore, purely global search can miss small pasted or moved objects, and purely local search loses the overall scene context. By combining both outputs, the system:\n    – leverages whole-image cues when they suffice  \n    – and falls back on the cropped, localized patch to catch tiny or spliced inserts  \n\n4.  This combination makes the retrieval robust to the full spectrum of forgery types.  \n\nAnswer  \nBecause global retrieval alone can miss small, spliced or copy-moved regions and local retrieval alone lacks full-scene context, the two are fused so that whole-image features and cropped forgery patches jointly ensure all relevant originals are found across every forgery type."
    },
    {
        "question": "What is the rationale for integrating forgery segmentation outputs into fact retrieval?",
        "relevant_section_ids": [
            "3.1.2"
        ],
        "relevant_context": [
            "Local Retrieval: For more complex forgery types like image splicing and copy-move, a forged image may be derived from multiple data sources.",
            "Therefore, global retrieval plus additional retrieval of any overlaid objects is more effective than global retrieval alone.",
            "In particular, when the overlaid objects are small, relying solely on global retrieval is insufficient.",
            "Thus, simultaneous retrieval of the detected forgery segments ensures that all related original images are identified.",
            "Specifically, the detected forgery segments are cropped and input into the retrieval model to search for the corresponding original images."
        ],
        "final_answer": "Because complex forgeries (e.g., splicing or copy-move) often combine content from multiple source images—sometimes in small overlaid regions—using only the whole image may miss the true originals. By cropping and retrieving each detected forgery segment, the system can find all related source images, ensuring that even small or localized manipulations are correctly traced back to their authentic counterparts.",
        "relevant_elements": [
            "Forgery Segmentation",
            "Fact Retrieval"
        ],
        "id": 1876,
        "masked_question": "What is the rationale for integrating [mask1] outputs into fact retrieval?",
        "masked_number": 1,
        "masked_elements": [
            "Forgery Segmentation"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.18614v1_figure_1.png",
        "paperid": "2407.18614v1",
        "paper_path": "./MISS-QA/papers/2407.18614v1.json",
        "figure_id": "2407.18614v1_figure_1.png",
        "caption": "Figure 1: Upper half illustrates pipeline of our proposed open framework for image-based fact verification: It comprises two phases: forgery identification and fact retrieval. The modules therein can be replaced with almost any open-source toolbox. Lower half shows examples of two specific forgery types.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning, step by step:\n\n1.  In the first phase (“Forgery Identification”) the system not only decides “Real vs. Fake” and classifies the forgery type, but—if it’s forged—produces a precise segmentation mask (and/or bounding box) of the manipulated region.  \n2.  In the second phase (“Fact Retrieval”) there are two parallel searches:  \n    •  Global retrieval, which uses the entire image as a query, and  \n    •  Local retrieval, which uses just the manipulated region as a query.  \n3.  The segmentation mask from phase 1 is exactly what tells us *where* the forgery lives.  By cropping out the mask—that is, isolating just the tampered object or region—we can feed that smaller patch into the local‐retrieval branch.  \n4.  Local retrieval is critical for cases like spliced‐in objects or copy‐moved patches, especially when they’re small or would be “washed out” by background features in a global search.  \n5.  Therefore, we integrate the segmentation outputs into fact retrieval so that:  \n    –  All overlaid or transferred pieces are explicitly queried,  \n    –  Original source images for those pieces get retrieved,  \n    –  And together with the global‐query results, we recover the full provenance of the manipulated image.  \n\nIn short, the red-boxed “Forgery Segmentation” output feeds the *local retrieval* branch—without it, small or spliced‐in objects would be very hard to match against the reference corpus."
    },
    {
        "question": "How does forgery detection module output integrate with metadata generation for bounding box estimation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Forgery Detection",
            "Metadata"
        ],
        "id": 1877,
        "masked_question": "How does [mask1] module output integrate with [mask2] generation for bounding box estimation?",
        "masked_number": 2,
        "masked_elements": [
            "Forgery Detection",
            "Metadata"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.18614v1_figure_1.png",
        "paperid": "2407.18614v1",
        "paper_path": "./MISS-QA/papers/2407.18614v1.json",
        "figure_id": "2407.18614v1_figure_1.png",
        "caption": "Figure 1: Upper half illustrates pipeline of our proposed open framework for image-based fact verification: It comprises two phases: forgery identification and fact retrieval. The modules therein can be replaced with almost any open-source toolbox. Lower half shows examples of two specific forgery types.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought\n\n1. In the pipeline diagram, the red-boxed “Forgery Detection” is the module that actually pinpoints where in the image something has been tampered with—it produces a binary mask (or heat-map) of the forged pixels.  \n2. The blue-boxed “Metadata” block is not a neural net itself but rather a small post-processing step that gathers all of the detection outputs—namely, the forgery type label, the pixel-wise forgery mask, and derives a bounding‐box around that mask.  \n3. Concretely, once the detection module outputs its mask, the metadata generator computes the minimal enclosing rectangle over the non-zero pixels of that mask to get (x_min,y_min,x_max,y_max).  \n4. Those four coordinates become the “Bounding Box” entry in the metadata alongside the type and mask. That boxed region can then be cropped and sent into the local‐retrieval branch.  \n\nAnswer\n\nThe output of the red‐boxed Forgery Detection network is a binary (or probabilistic) mask of the tampered region. The blue‐boxed metadata‐generation step then takes that mask, computes its minimal enclosing rectangle (simply “fits” a box around the nonzero pixels), and packages those coordinates together with the mask and predicted forgery type into the metadata. This bounding‐box estimate is thus derived directly from the detection mask and made available for the downstream retrieval stage."
    },
    {
        "question": "How does Normal SDS compute gradients to optimize Learned Jacobians for coarse mesh deformation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Specifically, given the base mesh M₀ and deformation mapping F, we utilize a differentiable renderer Rₙ to render a normal map n = Rₙ(M₀, F; φ), where φ represents a camera pose that is arbitrarily sampled within the spherical coordinate system.",
            "Next, during the t-th timestep of the diffusion process, we encode the rendered normal map n into the latent space to obtain the latent code zⁿ, and add Gaussian noise ε to get zₜ.",
            "The typical latent-space SDS loss is thus utilized to optimize the deformation F by measuring the gradient with respect to the clean latent z₀ as: ∇_F L_sds = Eₜ,ε [ w(t)\n  (ε_θ(zₜ, t, c) − ε) · ∂z₀/∂F ], where ε_θ denotes the denoiser in Stable Diffusion."
        ],
        "final_answer": "Normal SDS first renders a normal map of the current mesh under a random camera pose, encodes that normal map into the Stable Diffusion latent space, and adds noise to obtain a noisy latent zₜ. It then applies the standard latent‐space SDS loss (the difference between the denoiser’s prediction ε_θ(zₜ, t, c) and the true noise ε) to compute a score‐distillation gradient with respect to the clean latent z₀. Finally, this gradient is backpropagated through the latent encoding and the differentiable renderer to the mesh deformation parameters (the per‐triangle Jacobians F), yielding ∇_F L_sds that drives the mesh to match the text prompt.",
        "relevant_elements": [
            "Normal SDS",
            "Learned Jacobians"
        ],
        "id": 1879,
        "masked_question": "How does [mask1] compute gradients to optimize [mask2] for coarse mesh deformation?",
        "masked_number": 2,
        "masked_elements": [
            "Normal SDS",
            "Learned Jacobians"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.07454v1_figure_2.png",
        "paperid": "2409.07454v1",
        "paper_path": "./MISS-QA/papers/2409.07454v1.json",
        "figure_id": "2409.07454v1_figure_2.png",
        "caption": "Figure 2: An overview of our DreamMesh that fully capitalizes on explicit 3D scene representation (triangle meshes) for text-to-3D generation in a coarse-to-fine scheme. In the first coarse stage, DreamMesh learns text-guided Jacobians matrices to deform a base mesh into the coarse mesh, and then textures it through a tuning-free process. In the second fine stage, both coarse mesh and texture are jointly optimized, yielding high-quality mesh with high-fidelity texture.",
        "qtype": "Implementation_Details",
        "response": "Here’s the key idea in plain language.  In the coarse stage we do not “guess’’ vertex positions directly, but instead optimize a small set of per‐triangle Jacobian matrices J​i (this is your [mask2]).  To drive those J​i toward a shape that matches your text prompt we use a normal‐map version of Score Distillation Sampling (the red‐boxed [mask1] in the figure).  Concretely:\n\n 1.  At each iteration we sample a random camera pose and render the current mesh (parameterized by the Jacobians J​i) into a normal map N.  \n 2.  We feed the downsampled normal map into the Stable Diffusion encoder to get a latent z, add Gaussian noise according to a randomly chosen diffusion timestep t, and run the pretrained diffusion denoiser εθ(zₜ,c) to predict that noise.  \n 3.  We form the usual SDS loss  \n      L_SDS = w(t) · ‖εθ(zₜ,c) − ε‖²  \n    where ε is the true noise we added and w(t) is a weighting schedule.  \n 4.  Crucially, because our renderer is differentiable, we can back–propagate the gradient ∂L_SDS/∂z through the encoder and then through the renderer (∂z/∂N, ∂N/∂J​i) all the way back to the Jacobians J​i.  \n\nIn short, the red-boxed “normal‐map SDS’’ module computes a per‐pixel score‐distillation gradient on the rendered normals, and then those gradients are pushed back through the differentiable rendering pipeline into the learnable per‐triangle Jacobian matrices (the blue box), deforming the mesh to better match the text."
    },
    {
        "question": "How are gradients from Image-to-Image Refiner propagated through the Differentiable Rendering Pipeline to update mesh and texture?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "By doing so, we employ a differentiable rendering pipeline, which includes a sequence of mesh operations, a rasterizer, and a deferred shading stage [14] to render a coarse colored image $I_c$ derived from the deforming mesh and parameterized texture map, conditioned on a random camera pose $\\xi$.",
            "This refined colored image $I_r$ is further utilized to guide the joint optimization of mesh and texture through Mean Squared Error (MSE) loss: $$L_{refine} = \\mathbb{E}_{\\text{views}}[\\|I_c - I_r\\|^2].$$"
        ],
        "final_answer": "We compute an MSE loss between the rendered coarse image $I_c$ and the refined image $I_r$. The resulting pixel-wise gradients are back-propagated through the differentiable rendering pipeline (mesh operations, rasterizer, deferred shading) to jointly update the mesh deformation parameters (Jacobian matrices) and the texture map.",
        "relevant_elements": [
            "Image-to-Image Refiner",
            "Differentiable Rendering Pipeline"
        ],
        "id": 1880,
        "masked_question": "How are gradients from [mask1] propagated through the [mask2] to update mesh and texture?",
        "masked_number": 2,
        "masked_elements": [
            "Image-to-Image Refiner",
            "Differentiable Rendering Pipeline"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.07454v1_figure_2.png",
        "paperid": "2409.07454v1",
        "paper_path": "./MISS-QA/papers/2409.07454v1.json",
        "figure_id": "2409.07454v1_figure_2.png",
        "caption": "Figure 2: An overview of our DreamMesh that fully capitalizes on explicit 3D scene representation (triangle meshes) for text-to-3D generation in a coarse-to-fine scheme. In the first coarse stage, DreamMesh learns text-guided Jacobians matrices to deform a base mesh into the coarse mesh, and then textures it through a tuning-free process. In the second fine stage, both coarse mesh and texture are jointly optimized, yielding high-quality mesh with high-fidelity texture.",
        "qtype": "Implementation_Details",
        "response": "First, let us fix what the two masked blocks are in the figure:\n\n  • [mask1] (red box) is the “Image‐to‐Image Refiner” that takes the coarse rendered image Icoarse and the text prompt and produces the refined image Irefined.  \n  • [mask2] (blue box) is the “Differentiable Rendering Pipeline” that, given the current mesh V(𝐽) (via the per‐triangle Jacobians 𝐽) and the texture atlas T, produces the coarse colored render Icoarse.\n\nIn the fine stage we introduce the loss\n\n Lrefine = ∥Icoarse – Irefined∥²\n\nwhere Irefined comes out of the frozen image refiner and is treated as a “target” image.  Because the rendering pipeline is fully differentiable, we can take\n\n ∂Lrefine\n———— = ∂Lrefine/∂Icoarse\n∂{V(𝐽), T}\n\nby first backpropagating through the pixel‐wise MSE to get gradients w.r.t. each pixel of Icoarse, and then pushing those image‐space gradients all the way through the rasterizer, shading, UV‐lookup, and finally through the Jacobian‐based mesh parameterization and the texture atlas.  In this way the loss coming from the refiner (mask1) flows backwards through the Differentiable Rendering Pipeline (mask2) and yields updates to both the mesh deformation parameters (the 𝐽’s) and the texture map T."
    },
    {
        "question": "How does Normal SDS-driven Learned Jacobians optimization extend DreamFusion’s SDS for explicit mesh deformation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Coarse Diffusion Guidance. To achieve text-driven deformation φ that aligns with input text prompt, we exploit the powerful text-to-image diffusion model (Stable Diffusion) as coarse diffusion guidance to facilitate Jacobians deformation.",
            "Specifically, given the base mesh M and deformation mapping φ, we utilize a differentiable renderer R to render a normal map N: N = R_normal(M, φ, θ) where θ represents a camera pose that is arbitrarily sampled within the spherical coordinate system.",
            "Next, during t-th timestep of diffusion process, we encode the rendered normal map N into the latent space to obtain the latent code z̃, and add Gaussian noise ε to get z_t. The typical latent space SDS loss is thus utilized to optimize the deformation φ by measuring the gradient w.r.t. z_t as: L_sds(φ) = E_{t,ε}[w(t)(ε_t - ε_θ(z_t, t, prompt))^T ∂z_t/∂φ]."
        ],
        "final_answer": "DreamMesh extends DreamFusion’s SDS by replacing the NeRF-based update of an implicit field with an SDS loss computed on normal‐map renderings of an explicit triangle mesh. In practice, it parameterizes mesh deformation via per-triangle Jacobian matrices, renders normal maps of the current mesh under random views, encodes and perturbs those normal maps in the diffusion latent space, and backpropagates the SDS gradient through the latent code into the Jacobian parameters. This “Normal SDS” thus drives explicit mesh deformation directly, rather than updating an implicit radiance field.",
        "relevant_elements": [
            "Normal SDS",
            "Learned Jacobians"
        ],
        "id": 1881,
        "masked_question": "How does [mask1]-driven [mask2] optimization extend DreamFusion’s SDS for explicit mesh deformation?",
        "masked_number": 2,
        "masked_elements": [
            "Normal SDS",
            "Learned Jacobians"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.07454v1_figure_2.png",
        "paperid": "2409.07454v1",
        "paper_path": "./MISS-QA/papers/2409.07454v1.json",
        "figure_id": "2409.07454v1_figure_2.png",
        "caption": "Figure 2: An overview of our DreamMesh that fully capitalizes on explicit 3D scene representation (triangle meshes) for text-to-3D generation in a coarse-to-fine scheme. In the first coarse stage, DreamMesh learns text-guided Jacobians matrices to deform a base mesh into the coarse mesh, and then textures it through a tuning-free process. In the second fine stage, both coarse mesh and texture are jointly optimized, yielding high-quality mesh with high-fidelity texture.",
        "qtype": "Literature_Background",
        "response": "Let’s first identify the two masked items:\n\n- Mask 1 (red box) is the phrase “Normal SDS.”  In the diagram it sits next to a rendered normal map and is used in place of DreamFusion’s usual image‐SDS.  \n- Mask 2 (blue box) is “Learned Jacobians,” the per‐triangle Jacobian matrices Jᵢ that parameterize mesh deformation.\n\nSo the question becomes:\n\n  How does Normal-SDS-driven Learned-Jacobians optimization extend DreamFusion’s SDS for explicit mesh deformation?\n\nStep-by-step reasoning:\n\n1. DreamFusion’s original SDS drives an implicit NeRF by sampling a rendered RGB image, adding noise, running the diffusion denoiser, and backpropagating the per-pixel score distillation gradient into the NeRF weights.  \n2. In DreamMesh, we have an entirely explicit mesh.  We do not have NeRF weights; we have vertex positions, but we do _not_ optimize them directly.  Instead we parameterize any deformation of the base mesh via a set of per-triangle Jacobian matrices {Jᵢ}.  \n3. To get an SDS signal suitable for surface geometry, DreamMesh replaces DreamFusion’s RGB-image-SDS with an SDS computed on a _normal map_ rendering of the mesh.  Hence “Normal SDS.”  \n4. We render the current mesh under a random camera as a normal map N, encode and noise it just like a diffusion latent, and compute the usual score‐distillation loss ∇θ ‖εθ(noised N) – added_noise‖².  \n5. Crucially, that gradient is back-propagated _through the differentiable renderer_ all the way into the Jacobian parameters {Jᵢ}.  In other words, the SDS loss on normals tells us how to tweak each triangle’s Jacobian so that the surface normals (and thus shape) move toward something that looks like the text prompt.  \n6. Finally, at each optimization step we solve a linear system that finds the new vertex positions whose per-triangle Jacobians best match the updated {Jᵢ} in a least‐squares sense.  \n\nIn sum:\nNormal-SDS-driven Learned-Jacobians optimization extends DreamFusion’s score-distillation sampling from optimizing an implicit volumetric field to directly _deforming an explicit triangle mesh_.  The key is that the SDS loss is computed on a _normal map_ and used to supervise the _per-triangle Jacobians_ which parameterize mesh deformation."
    },
    {
        "question": "How does joint optimization of mesh and texture using Differentiable Rendering Pipeline and Image-to-Image Refiner build upon coarse tuning-free texturing?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Coarse Texture Generation. Next, we target for producing realistic coarse textures for the learnt coarse mesh M_coarse. We apply a tuning-free approach to progressively generate coarse textures on the 3D triangle mesh with an interlaced use of pre-trained 2D diffusion models [...] We repeat this process until a complete coarse texture map T_coarse is formed.",
            "Stage II: Jointly Refine Mesh and Texture. Recall that at the first coarse stage, the optimization process of coarse mesh deformation solely focuses on the primary mesh irrespective of any texture. Such process might inevitably simulate textured results and lead to excessive modifications of meshes. Meanwhile, the coarse texture generation in first stage also encounters the inconsistency issue across all viewpoints.",
            "By doing so, we employ a differentiable rendering pipeline R, which includes a sequence of mesh operations, a rasterizer, and a deferred shading stage to render a coarse colored image I from the deforming mesh M and parameterized texture map T, conditioned on a random camera pose.",
            "Fine Diffusion Guidance. Instead, we excavate the fine diffusion guidance by additionally refining rendered coarse colored image I with diffusion-based image refiner. This refined colored image I_refined is further utilized to guide the joint optimization of mesh and texture through Mean Squared Error (MSE) loss: L_refine = ||I_refined - I||^2. By minimizing this objective, our DreamMesh enforces the rendered image I visually similar as the refined image I_refined that faithfully matches with text prompt, thereby yielding high-quality mesh with high-fidelity texture map."
        ],
        "final_answer": "The fine-stage joint optimization builds on the tuning-free coarse texturing by first taking the coarse texture atlas (produced without any parameter tuning) and explicitly parameterizing it alongside the mesh Jacobians. A differentiable renderer then produces colored renderings of the current mesh + texture under random views. These coarse renders are passed through a pre-trained image-to-image refiner, yielding refined target images. Finally, an MSE loss between refined images and the differentiably rendered images is back-propagated to update both mesh deformations and texture parameters. This procedure corrects viewpoint inconsistencies and texture artifacts left by the tuning-free stage, yielding high-fidelity, cross-view consistent meshes and textures.",
        "relevant_elements": [
            "Differentiable Rendering Pipeline",
            "Image-to-Image Refiner",
            "Tuning-free Coarse Texturing"
        ],
        "id": 1882,
        "masked_question": "How does joint optimization of mesh and texture using [mask1] and [mask2] build upon coarse tuning-free texturing?",
        "masked_number": 2,
        "masked_elements": [
            "Differentiable Rendering Pipeline",
            "Image-to-Image Refiner"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.07454v1_figure_2.png",
        "paperid": "2409.07454v1",
        "paper_path": "./MISS-QA/papers/2409.07454v1.json",
        "figure_id": "2409.07454v1_figure_2.png",
        "caption": "Figure 2: An overview of our DreamMesh that fully capitalizes on explicit 3D scene representation (triangle meshes) for text-to-3D generation in a coarse-to-fine scheme. In the first coarse stage, DreamMesh learns text-guided Jacobians matrices to deform a base mesh into the coarse mesh, and then textures it through a tuning-free process. In the second fine stage, both coarse mesh and texture are jointly optimized, yielding high-quality mesh with high-fidelity texture.",
        "qtype": "Literature_Background",
        "response": "Let’s unpack the two components and then see how, in Stage II, they get hooked up to turn our “tuning-free” atlas‐painting into a fully joint mesh + texture refinement.\n\n1.  Identify [mask1] and [mask2].  \n    – [mask1] (red box) is the “Differentiable Rendering Pipeline.”  This is the part that, given a current mesh (via its Jacobians) and the current UV atlas, spits out a coarse rendered color image under a random camera.  \n    – [mask2] (blue box) is the “Image-to-Image Refiner” (the diffusion-based image refiner).  It takes that coarse render, adds a bit of noise, and denoises/refines it back to a high-quality image that better matches the text prompt.\n\n2.  What coarse tuning-free texturing did.  \n    – In Stage I we painted the UV atlas by repeatedly rendering a depth or normal map, feeding it through off-the-shelf depth→image or inpainting diffusion models view by view, and pasting the colors back onto the UV.  No back-prop into the mesh or atlas—the texture was “tuning-free,” and the mesh was deformed by SDS on normals alone.  \n    – This got us a plausible but somewhat inconsistent, view-by-view texture and a coarse mesh.\n\n3.  How Stage II “builds upon” that with [mask1]+ [mask2].  \n    – We take the coarse mesh + atlas from Stage I, feed them through the differentiable renderer ([mask1]) to produce a rendered color image.  \n    – We then run that render through the pretrained image-to-image refiner ([mask2]) to produce a cleaner, text-aligned version of the same view.  \n    – Finally, we compute a simple MSE loss between the raw differentiable‐renderer output and the refiner output, and backpropagate it simultaneously into both the per-triangle Jacobians (i.e. the mesh deformation) and the UV‐atlas colors.  \n\nIn this way the rough, tuning-free UV painting is “polished” by a coherent 2D diffusion model, and both geometry and texture are nudged together until the differentiable render of the 3D model matches the refined 2D image.  The result is a high-fidelity texture and a mesh whose subtle shape cues are consistent with that texture—something the coarse, view-by-view inpainting alone cannot achieve."
    },
    {
        "question": "How does Pose Optimization extend differentiable renderer silhouette matching from prior pose estimation methods?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "For each retrieved template mesh, we perform pose optimization following the approach from [19] with some modifications. They initialize N camera hypotheses per template mesh model per batch to avoid local optima issues. The virtual camera parameters of a differentiable renderer are optimized to match the silhouette of the render to a given target silhouette.",
            "In our approach, we combine all masks from Mp into a single binary mask Ms, which is used as the target silhouette during training. Additionally, we modify the loss function from mean squared error (ℓ2 loss) to mean absolute error (ℓ1 loss), as employing ℓ1 loss yielded more consistent results in pose estimation.",
            "For the final selection from the N×E results, relying solely on IoU or part IoU does not lead to accurate poses due to significant divergences between our template meshes and the input. Instead, we propose selecting the result that minimizes the weighted sum of three losses.",
            "The first loss, ℓIoU, represents the IoU loss of the overall silhouette, aiming to ensure alignment of the overall structure, but being subject to ambiguous poses.",
            "The second loss, ℓpartIoU, is the part IoU loss averaged across the C semantic classes, which helps handling pose ambiguity by considering the accuracy of the semantic labels.",
            "The third loss is a normalized Euclidean distance between the centers of masks averaged across the C semantic classes. It alleviates the penalties of the other losses, by considering only the alignment of the center of the masks.",
            "Each component’s influence is controlled by a weight α. Finally, we retrieve the result that minimizes the total loss Ltotal."
        ],
        "final_answer": "Pose Optimization builds on prior differentiable‐renderer silhouette matching by (1) merging per-part masks into a single target silhouette, (2) switching from an ℓ2 to an ℓ1 loss for faster, more stable convergence, and (3) going beyond pure silhouette IoU by selecting the best pose via a weighted combination of three terms—overall silhouette IoU, semantic part IoU, and a normalized center-of-mask distance—thus using semantic and spatial cues to resolve ambiguities that pure silhouette matching cannot.",
        "relevant_elements": [
            "Pose Optimization"
        ],
        "id": 1883,
        "masked_question": "How does [mask1] extend differentiable renderer silhouette matching from prior pose estimation methods?",
        "masked_number": 1,
        "masked_elements": [
            "Pose Optimization"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.14001v2_figure_2.png",
        "paperid": "2407.14001v2",
        "paper_path": "./MISS-QA/papers/2407.14001v2.json",
        "figure_id": "2407.14001v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed solution for the Craft Assembly Task. Given the RGB image of the target object, the visible parts are segmented and classified. Labeled template meshes of the detected class are retrieved from a prepared database and their pose are optimized through a differentiable renderer using the segmentation results as the target. The parts of the best aligned mesh are simplified to primitive shapes. Finally each part is matched with an object in the scene (input) using a search algorithm, generating the final Craft Proposal.",
        "qtype": "Literature_Background",
        "response": "Here, “[mask1]” is the entire green “Pose Optimization” module in Fig. 2.  What we are doing in that box is still a silhouette‐based differentiable-renderer pose fit (as in [19]), but we extend it in four key ways:\n\n 1. Multi-hypothesis initialization  \n    – Instead of one random camera start, we spawn M different camera hypotheses per template per batch to escape local minima.  \n\n 2. Unified binary silhouette target  \n    – We take all of the per-part segmentation masks and merge them into one single binary mask  to drive the silhouette loss, rather than matching only the raw object outline.  \n\n 3. Switch from L2 to L1 silhouette loss  \n    – We found that mean absolute (“L1”) error on the silhouette converges more reliably across our truck, bus, chair and table classes than mean-squared (“L2”) loss.  \n\n 4. Richer final pose‐scoring function  \n    – Rather than picking the hypothesis with the highest plain IoU, we score each of the top M renders by a weighted sum of  \n      • global IoU loss (align overall shape)  \n      • mean part-IoU loss (enforce semantic alignment)  \n      • normalized centroid‐distance loss (penalize masks that are shifted)  \n\nBy picking the hypothesis that minimizes this three-term, weighted combination, we get much more robust and unambiguous poses than prior work that used only silhouette IoU or only an L2 loss."
    },
    {
        "question": "How does Primitive shape proposal integrate chamfer distance evaluation from point cloud reconstruction methodologies?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "Once all parts of this adjusted model are obtained, each one is simplified to a single type of primitive shape, either a cuboid or a cylinder. Each part is aligned to the axis of the coordinate system to approximate the minimal bounding box as the axis-aligned bounding box. Four primitive shapes candidates are generated based on the dimensions of this bounding box: a cuboid shape, or three possible rotated cylinders, where the planar faces are aligned with one of the three possible directions.",
            "Then, we sample separate uniform point clouds for each primitive shape candidate and for the part to calculate the chamfer distance. The candidate with the lowest distance is chosen as the simplified representation of that part."
        ],
        "final_answer": "In the Primitive shape proposal step, for each part of the reconstructed mesh the method generates several primitive candidates (one cuboid and three oriented cylinders), samples uniform point clouds from each candidate and from the original part geometry, computes the chamfer distance between these point clouds, and selects the primitive whose point cloud has the lowest chamfer distance as the simplified representation of that part.",
        "relevant_elements": [
            "Primitive shape proposal"
        ],
        "id": 1884,
        "masked_question": "How does [mask1] integrate chamfer distance evaluation from point cloud reconstruction methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "Primitive shape proposal"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.14001v2_figure_2.png",
        "paperid": "2407.14001v2",
        "paper_path": "./MISS-QA/papers/2407.14001v2.json",
        "figure_id": "2407.14001v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed solution for the Craft Assembly Task. Given the RGB image of the target object, the visible parts are segmented and classified. Labeled template meshes of the detected class are retrieved from a prepared database and their pose are optimized through a differentiable renderer using the segmentation results as the target. The parts of the best aligned mesh are simplified to primitive shapes. Finally each part is matched with an object in the scene (input) using a search algorithm, generating the final Craft Proposal.",
        "qtype": "Literature_Background",
        "response": "The module in the red box is nothing magical or new – it is simply the “primitive‐shape proposal” stage (described in III-E) where we turn each mesh‐part into one of a small family of axis–aligned primitives and then pick the one that best “looks like” the original part by borrowing the very same Chamfer‐distance machinery used in modern point‐cloud reconstruction papers.  \n\nConcretely, for each part we  \n 1. compute its tight, axis-aligned bounding box,  \n 2. generate a handful of candidate primitives (one cuboid and three cylinder orientations) whose dimensions match that box,  \n 3. sample a uniform point cloud on the surface of each candidate and a uniform point cloud on the actual part mesh,  \n 4. compute the bidirectional nearest‐neighbour distances (i.e. the Chamfer distance) between the two point sets,  \n 5. and finally select the primitive whose Chamfer distance to the original part is smallest.  \n\nIn this way the red-boxed block literally “plugs in” the point‐cloud Chamfer evaluation you see in reconstruction work as a fit-score for choosing between simple cuboids and cylinders."
    },
    {
        "question": "How does part segmentation quality influence pose optimization convergence?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Part Segmentation",
            "Pose Optimization"
        ],
        "id": 1885,
        "masked_question": "How does [mask1] quality influence [mask2] convergence?",
        "masked_number": 2,
        "masked_elements": [
            "Part Segmentation",
            "Pose Optimization"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.14001v2_figure_2.png",
        "paperid": "2407.14001v2",
        "paper_path": "./MISS-QA/papers/2407.14001v2.json",
        "figure_id": "2407.14001v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed solution for the Craft Assembly Task. Given the RGB image of the target object, the visible parts are segmented and classified. Labeled template meshes of the detected class are retrieved from a prepared database and their pose are optimized through a differentiable renderer using the segmentation results as the target. The parts of the best aligned mesh are simplified to primitive shapes. Finally each part is matched with an object in the scene (input) using a search algorithm, generating the final Craft Proposal.",
        "qtype": "Experimental_Results",
        "response": "Mask‐quality has a first‐order effect on how quickly and reliably the Pose‐Optimization stage settles on the right camera parameters.  Concretely, the red‐boxed “Part Segmentation” step produces the binary and semantic masks that become the optimization target in the blue “Pose Optimization” stage.  If those masks are crisp and correct, the differentiable renderer can match silhouettes and part labels with very low IoU‐ and centroid‐loss right away, so the gradient signals are strong and the camera pose parameters converge quickly to the true solution.  By contrast, if the masks are noisy, incomplete or mis‐labeled, the silhouette and part‐IoU losses become noisy, gradients point in the wrong directions, convergence slows dramatically (and often gets stuck in bad local minima), and one needs many more view‐initializations or iterations to recover a reasonable pose."
    },
    {
        "question": "How does primitive shape simplification affect correspondence accuracy in scene matching?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Primitive shape proposal",
            "Scene Matching"
        ],
        "id": 1886,
        "masked_question": "How does primitive shape simplification affect correspondence accuracy in [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Scene Matching"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.14001v2_figure_2.png",
        "paperid": "2407.14001v2",
        "paper_path": "./MISS-QA/papers/2407.14001v2.json",
        "figure_id": "2407.14001v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed solution for the Craft Assembly Task. Given the RGB image of the target object, the visible parts are segmented and classified. Labeled template meshes of the detected class are retrieved from a prepared database and their pose are optimized through a differentiable renderer using the segmentation results as the target. The parts of the best aligned mesh are simplified to primitive shapes. Finally each part is matched with an object in the scene (input) using a search algorithm, generating the final Craft Proposal.",
        "qtype": "Experimental_Results",
        "response": "Here is the chain of thought for how primitive shape simplification (the step boxed in red, i.e. “Primitive proposal” feeding into “Scene matching”) impacts correspondence accuracy:\n\n1.   What that red‐boxed module does:  \n     –  After we have a posed, part‐labelled mesh, we “flatten” each part to its best fitting cuboid or cylinder.  In practice that means we take an axis-aligned bounding box (for a cuboid) or the minimal enclosing cylinder (in one of the three principal directions), sample simple point-clouds on each candidate, pick the one with lowest Chamfer distance, and throw away all the other geometry.  \n\n2.   Why we do it:  \n     –  Our scene primitives are themselves only cuboids or cylinders.  By also reducing the template parts to that same domain, we turn a very high-dimensional, free-form shape-matching problem into a low-dimensional “match the three side-lengths (and orientation)” problem.  \n\n3.   The trade-off in correspondence accuracy:  \n     –  Pro:  Because every part is now represented by just (length, width, height) or (radius, length), we can very quickly normalize those numbers, compare volume- or ratio-errors, and run a small search rather than doing thousands of full point-cloud registrations.  \n     –  Con:  We have thrown away all the little curves, tapers and fillets that make, say, a real chair-leg a “chair-leg” rather than just a stick.  That coarsening inevitably introduces approximation errors.  Even if the bounding–box dimensions line up perfectly, the true part may have a flared foot or a chamfered corner that the box misses entirely.  \n\n4.   Empirical effect on accuracy:  \n     –  In our experiments (see Table II in the paper), this “primitive” simplification step only slightly lowers our final part‐IoU versus the baseline that optimizes a full mask overlap.  We still get within a few percentage points of the best possible 2D IoU, and our overall “success‐rate” (correct 6D pose, correct part counts, IoU ≥ 0.5) remains high.  \n     –  In other words, although we lose some fine-grained shape fidelity, the simplification achieves almost the same matching accuracy in practice—while making our correspondence search tractable on everyday hardware.  \n\nAnswer  \nPrimitive shape simplification trades fine geometric fidelity for a much simpler matching problem—by collapsing each part to a single cuboid or cylinder we discard little details, which does introduce some approximation error in correspondence, but in practice the loss of accuracy is small (only a few points of IoU) and the greatly reduced search dimensionality makes the matching both robust and efficient."
    },
    {
        "question": "How does leveraging public data with contrastive learning refine client-side feature discrimination under federated parameter aggregation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "public data",
            "contrastive learning",
            "parameter aggregation"
        ],
        "id": 1887,
        "masked_question": "How does leveraging [mask1] with [mask2] refine client-side feature discrimination under federated parameter aggregation?",
        "masked_number": 2,
        "masked_elements": [
            "public data",
            "contrastive learning"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.16289v1_figure_1.png",
        "paperid": "2407.16289v1",
        "paper_path": "./MISS-QA/papers/2407.16289v1.json",
        "figure_id": "2407.16289v1_figure_1.png",
        "caption": "Figure 1: Pipelines of federated learning-based face recognition methods including our proposed method. (a) The server collects class embedding of client c (e.g. FedFace). (b) Client c continuously downloads public data from the server (e.g. FedFR). (c) Our proposed method(FedFS), client c performs intra-subject self-supervised learning without any additional work.",
        "qtype": "Experimental_Results",
        "response": "Here’s what happens step by step:\n\n1.  In a vanilla FedAvg setup each client only ever sees its own (private) faces, so it has no notion of “what all the other classes look like.”  As a result, its embedding network can easily collapse or overfit to that single identity.\n\n2.  To remedy that, FedFR (the case in the middle of Figure 1) has the server broadcast a small, shared pool of “public data” (mask₁) to every client.  This gives each device a common set of “other” faces it can use as negatives.\n\n3.  On receipt of that public set each client then runs a contrastive‐learning stage (mask₂) on top of its own private images plus the downloaded ones.  Concretely:  \n    •  Positive pairs are built among views of the same private identity.  \n    •  Negative pairs are built between the private images and the various public images (which all carry different identities).  \n    A contrastive loss then pulls together features of the same person and pushes apart features of different people.\n\n4.  The net effect is that every client’s feature extractor learns to carve out tight clusters for its own identity while simultaneously creating large �margins� around all the “other” faces it has seen.  \n\n5.  Finally, when those locally refined parameters are averaged on the server (FedAvg), the global model inherits embeddings that are already well‐spread in feature space—i.e. high intra‐class compactness and high inter‐class separation—leading to much stronger, more discriminative face representations."
    },
    {
        "question": "How does intra-subject self-supervised learning within parameter-only sharing enhance personalized model convergence?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In intra-subject self-supervised learning, two major operations are performed simultaneously. 1) Training local information and reducing intra-class variation with intra-subject loss. 2) Preventing overfitting and bias with regularization loss.",
            "However, due to the nature of cross entropy, y<sub>ij</sub> is 0 except in cases where the input is the exactly same image within the online-batch. To address these limitations, our proposed method uses an adaptive soft label that reflects the correlation between all positive data to reformat labels within intra-instances and reduce intra-class variance, thereby more effectively training correlations for local data.",
            "Training only on local data without including negative data can easily lead to overfitting and biased results. To solve this problem, we perform regularizing between the global model that trains generalized facial features through sharing the parameters with the server and the personalized model, as follows: ..."
        ],
        "final_answer": "By integrating intra-subject self-supervised learning into a parameter-only sharing scheme, each client both (1) minimizes an intra-subject loss—computed via adaptive soft labels derived from cosine similarities among only its positive samples—to reduce intra-class variation and better capture local identity cues, and (2) applies a regularization loss between its personalized and the global model outputs to prevent overfitting on limited data. Together, these two losses let the personalized model extract stronger, less biased local representations and thus converge more reliably and quickly despite sharing only parameters.",
        "relevant_elements": [
            "intra-subject self-supervised learning",
            "parameter-only sharing"
        ],
        "id": 1888,
        "masked_question": "How does [mask1] within parameter-only sharing enhance personalized model convergence?",
        "masked_number": 1,
        "masked_elements": [
            "intra-subject self-supervised learning"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.16289v1_figure_1.png",
        "paperid": "2407.16289v1",
        "paper_path": "./MISS-QA/papers/2407.16289v1.json",
        "figure_id": "2407.16289v1_figure_1.png",
        "caption": "Figure 1: Pipelines of federated learning-based face recognition methods including our proposed method. (a) The server collects class embedding of client c (e.g. FedFace). (b) Client c continuously downloads public data from the server (e.g. FedFR). (c) Our proposed method(FedFS), client c performs intra-subject self-supervised learning without any additional work.",
        "qtype": "Experimental_Results",
        "response": "First, let’s pin down what “\\[mask1\\]” is by looking at the red‐boxed module in (c).  In that diagram the client takes its raw face images, runs them through the three networks (global, personalized and frozen pre‐trained), and then feeds all three outputs into the red‐boxed block labeled “Intra-subject Self-supervised Learning.”  In the text this is the section titled “Intra-subject self-supervised learning,” which consists of two pieces:\n\n 1. An **intra-subject loss** built on adaptive soft-labels among positive (same-identity) samples in the batch, and  \n 2. A **regularization loss** that ties the personalized branch back to the global model.\n\nHow this speeds up personalized convergence:\n\n  • By mining multiple augmented views of the *same* local face and building an adaptive soft-label across them, the client extracts a strong self-supervised signal **solely** from its private positives.  This reduces intra-class scatter much faster than vanilla cross-entropy on hard one-hot labels.  \n  • At the same time, the regularization term gently pulls the personalized weights toward the global weights, preventing the personal branch from drifting too far and overfitting.  \n  • The result is a rich, stable gradient that both sharpens local discriminability (via the intra-subject loss) and preserves generalization (via the regularizer).  Put together, those two forces drive the personalized model to its optimum more quickly and robustly—i.e. they accelerate and stabilize convergence without ever sharing raw data or requiring extra public images."
    },
    {
        "question": "What limitations might arise from relying solely on intra-subject self-supervised learning without negative samples?",
        "relevant_section_ids": [
            "3.2",
            "2"
        ],
        "relevant_context": [
            "Considering the client’s restriction to utilize only local data for privacy protection, each client trains the model using only positive data, excluding negative data.",
            "Training only on local data without including negative data can easily lead to overfitting and biased results.",
            "Unlike traditional contrastive learning approaches, in federated contrastive learning, clients can only have their data, so there are no other identities. To address this challenge, a variety of federated learning-based studies ... propose regularization loss to prevent overfitting and bias.",
            "However, due to the nature of cross entropy, \\(\\ell\\) is 0 except in cases where the input is the exactly same image within the online-batch."
        ],
        "final_answer": "Without any negative samples, training only on positive (intra-subject) data can easily lead to overfitting and biased representations, and the standard cross-entropy loss degenerates (yielding zero for all but identical inputs), limiting the model’s ability to learn discriminative features.",
        "relevant_elements": [
            "intra-subject self-supervised learning"
        ],
        "id": 1889,
        "masked_question": "What limitations might arise from relying solely on [mask1] without negative samples?",
        "masked_number": 1,
        "masked_elements": [
            "intra-subject self-supervised learning"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.16289v1_figure_1.png",
        "paperid": "2407.16289v1",
        "paper_path": "./MISS-QA/papers/2407.16289v1.json",
        "figure_id": "2407.16289v1_figure_1.png",
        "caption": "Figure 1: Pipelines of federated learning-based face recognition methods including our proposed method. (a) The server collects class embedding of client c (e.g. FedFace). (b) Client c continuously downloads public data from the server (e.g. FedFR). (c) Our proposed method(FedFS), client c performs intra-subject self-supervised learning without any additional work.",
        "qtype": "Others",
        "response": "Mask1 refers to the intra-subject self-supervised learning module – i.e. the loss that only ever “pulls” different views of the same identity together but never “pushes” any other identities apart.  If you tried to learn solely with that positive-only signal, you would run into two main issues:\n\n  • No inter-class repulsion → collapsed or overly‐crowded embeddings  \n    Since you never penalize two different identities for being similar, there is no force in your loss that spreads apart different faces.  In the extreme this can collapse all images to the same point in feature space.  \n\n  • Overfitting and bias toward local data  \n    With only positives to learn from, the model can easily memorize the few identities it sees on one client, producing features that do not generalize to unseen people or to the global population.  \n\nContrastive methods normally rely on negative samples to create both an attractive (positive) and a repulsive (negative) force in feature space.  By using only the intra-subject (positive-only) loss, you lose that repulsive component, so you end up with embeddings that are tight within each client but have poor overall discrimination and generalization."
    },
    {
        "question": "How can computational overhead be reduced when continuously downloading public data for contrastive learning?",
        "relevant_section_ids": [
            "2"
        ],
        "relevant_context": [
            "FedFR [Liu et al.(2022)Liu, Wang, Chien, and Lai] prevented bias by training personalized models using public data, demonstrating promising performance among federated learning–based face recognition models. However, this approach necessitates clients to continuously receive public data, posing significant resource constraints, especially in on-device environments like mobile platforms where computational resources are severely limited.",
            "In contrast, we do not use public data directly, because utilizing the data requires the client’s resources, which can be very taxing on the client’s devices."
        ],
        "final_answer": "By eliminating the need to download and process public data—i.e., performing contrastive learning solely on local (positive) data and sharing only model parameters—clients avoid the repeated transfer and handling of large public datasets, thereby substantially reducing computational overhead.",
        "relevant_elements": [
            "download public data",
            "contrastive learning"
        ],
        "id": 1890,
        "masked_question": "How can computational overhead be reduced when continuously [mask1] for contrastive learning?",
        "masked_number": 1,
        "masked_elements": [
            "download public data"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.16289v1_figure_1.png",
        "paperid": "2407.16289v1",
        "paper_path": "./MISS-QA/papers/2407.16289v1.json",
        "figure_id": "2407.16289v1_figure_1.png",
        "caption": "Figure 1: Pipelines of federated learning-based face recognition methods including our proposed method. (a) The server collects class embedding of client c (e.g. FedFace). (b) Client c continuously downloads public data from the server (e.g. FedFR). (c) Our proposed method(FedFS), client c performs intra-subject self-supervised learning without any additional work.",
        "qtype": "Others",
        "response": "Here’s the key insight: in the FedFR‐style pipeline (panel b) the server must push a fresh chunk of public images down to every client on each round so that they can form cross‐identity “negative” samples for contrastive training.  That constant image download becomes a heavy burden on a client’s CPU, memory and network.  In FedFS (panel c) we remove that step altogether and never ship public pictures to the clients.  Instead, each client:\n\n  1.  Only pulls down the global model parameters (no images).  \n  2.  Uses intra‐subject self‐supervised augmentations on its own local faces to create positive/negative pairs.  \n\nBy turning to purely local, intra-subject contrastive learning and only exchanging model weights, you eliminate the continual download of public data and thus drastically cut both the communication and compute overhead on each client."
    },
    {
        "question": "What scalability concerns exist for the EEG Diffusion Transformer in processing high-dimensional EEG data?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "EEG Diffusion Transformer"
        ],
        "id": 1891,
        "masked_question": "What scalability concerns exist for the [mask1] in processing high-dimensional EEG data?",
        "masked_number": 1,
        "masked_elements": [
            "EEG Diffusion Transformer"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.20253v2_figure_1.png",
        "paperid": "2407.20253v2",
        "paper_path": "./MISS-QA/papers/2407.20253v2.json",
        "figure_id": "2407.20253v2_figure_1.png",
        "caption": "Figure 1: The Illustration of the Proposed Method.",
        "qtype": "Others",
        "response": ""
    },
    {
        "question": "What limitations might label smoothing introduce to the Generated-Original Signals Reassemble augmentation pipeline?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Label Smooth",
            "Generated-Original Signals Reassemble"
        ],
        "id": 1892,
        "masked_question": "What limitations might [mask1] introduce to the Generated-Original Signals Reassemble augmentation pipeline?",
        "masked_number": 1,
        "masked_elements": [
            "Label Smooth"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.20253v2_figure_1.png",
        "paperid": "2407.20253v2",
        "paper_path": "./MISS-QA/papers/2407.20253v2.json",
        "figure_id": "2407.20253v2_figure_1.png",
        "caption": "Figure 1: The Illustration of the Proposed Method.",
        "qtype": "Others",
        "response": "In the architecture you pointed to, the little red‐boxed module is nothing more exotic than a label‐smoothing layer that sits between the raw, one‐hot “fake” EEG labels and the rest of the reassembly machinery.  In other words, before you crop and splice the generated traces into your real signals (and compute the KL‐vicinal‐risk on them), you first “soften” their labels.  That trick brings in two main drawbacks:\n\n  1.  Weakened Supervision  – By construction you are asking your classifier to treat every synthetic example as only weakly belonging to its purported class.  If you over‐smooth (i.e. pick λ too low), the model never learns a strong decision boundary around those generated samples and your network may under‐fit the very regions of feature‐space you are trying to fill in.  \n  2.  Hyperparameter Sensitivity  – The smoothing factor λ is a new knob you have to tune.  Push it too far toward uniform, and you’ll dilute the synthetic data’s usefulness; leave it too close to 1, and you’re back to giving the network highly confident (but potentially wrong) labels on low‐quality generated signals.  \n\nNeither of these issues is fatal, but they do force extra tuning and can blunt the discriminative power of the final classifier if handled carelessly."
    },
    {
        "question": "What is the rationale for randomly reassembling generated and original signals to form vicinal EEG?",
        "relevant_section_ids": [
            "1",
            "3.2.x"
        ],
        "relevant_context": [
            "Section 1: \"To effectively improve the performance of EEG classification networks, we propose an effective data augmentation method based on generated data. The proposed augmentation method randomly reassembles the generated data with the original data to obtain the vicinal data, which improves the model performance by minimizing the empirical risk and the vicinal risk (Chapelle et al. 2000).\"",
            "Section 3.2.x (Obtain Vicinal Data by Reassembling Generated-Original Signals): \"When the training data size is small, minimizing empirical risk can easily lead to overfitting. Data augmentation based on vicinal data can alleviate this issue by minimizing vicinal risk (Chapelle et al. 2000).... Inspired by (Zhang et al. 2018; Yun et al. 2019), we randomly reassemble original data and generated data to further enhance the diversity of training data distribution... The proposed GO loss function allows the EEG classification network to learn from a more diverse data distribution while being supervised by the distribution of original data.\""
        ],
        "final_answer": "By randomly reassembling segments of generated and original EEG signals we obtain vicinal samples that lie between real and synthetic data. This expands the diversity of the training set and, by minimizing both the empirical risk on real data and the vicinal risk on these mixed samples, regularizes the classifier and reduces overfitting.",
        "relevant_elements": [
            "Generated-Original Signals Reassemble",
            "Vicinal EEG"
        ],
        "id": 1894,
        "masked_question": "What is the rationale for randomly reassembling [mask1] to form vicinal EEG?",
        "masked_number": 1,
        "masked_elements": [
            "Generated-Original Signals Reassemble"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.20253v2_figure_1.png",
        "paperid": "2407.20253v2",
        "paper_path": "./MISS-QA/papers/2407.20253v2.json",
        "figure_id": "2407.20253v2_figure_1.png",
        "caption": "Figure 1: The Illustration of the Proposed Method.",
        "qtype": "Design_Rationale",
        "response": "Here “\\[mask1\\]” is simply the pair of EEG snippets inside the red box – one generated by the diffusion model and one real (original) recording.  We randomly cut and splice those two snippets in the time‐domain to produce what the paper calls a “vicinal” EEG sample because:\n\n 1.  Vicinal Risk Minimization  \n    •  Instead of treating the synthetic examples as wholly separate, mixing them piece‐wise with real data creates new samples that lie in the neighborhood (the “vicinity”) of the true data manifold (Chapelle et al. 2000).  \n    •  Training on those in-between points forces the classifier to learn smoother decision boundaries and reduces overfitting.  \n\n 2.  Diversity and Robustness  \n    •  Purely synthetic data can have subtle distributional mismatches that hurt performance if used alone.  \n    •  By splicing real with generated segments, you vastly expand the support of the training distribution without drifting too far from reality, so the network sees a richer variety of patterns.  \n\n 3.  Controlled Label Mixing  \n    •  Each new vicinal example carries a “soft” label interpolated according to how much came from the real versus the generated segment.  \n    •  This gently steers the model toward the correct class while still accounting for the synthetic portion.  \n\nTaken together, randomly reassembling the red‐boxed generated/original EEG signals gives you a continuum of training examples between pure real and pure synthetic.  That both widens your effective dataset and tethers your generative data to the true distribution, yielding better generalization."
    },
    {
        "question": "What motivates combining supervised and self-supervised prompt learning in the initial stage?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "For the SSL component, we follow the good practice in TTA (Chen et al., 2022a ###reference_b6###) to adopt contrastive learning. Effective classification requires that embeddings of the same class align relatively closely, while those from different classes remain distinct to ensure inter-class distinguishability. To achieve this, we introduce Contrastive Prompt Tuning (CPT).",
            "Initially, we integrate CPT with supervised learning (stage 1) and subsequently rely exclusively on CPT for new class adaptation (stage 2).",
            "Our empirical analysis reveals that CPT and classification tasks exhibit a consistently positive gradient correlation across 11 datasets. This correlation suggests that both tasks drive the model’s optimization in similar directions, allowing CPT to effectively act as a proxy for supervised learning during the adaptation phase."
        ],
        "final_answer": "The consistently positive gradient correlation between the contrastive (self-supervised) task and the supervised classification task motivates combining them in stage 1—by co-training CPT with the classification loss, the model learns prompts that both align same-class embeddings and separate different-class embeddings, and the self-supervised objective can serve as a proxy for supervised learning during test-time adaptation.",
        "relevant_elements": [
            "supervised prompt learning",
            "self-supervised prompt learning"
        ],
        "id": 1895,
        "masked_question": "What motivates combining [mask1] and [mask2] in the initial stage?",
        "masked_number": 2,
        "masked_elements": [
            "supervised prompt learning",
            "self-supervised prompt learning"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.05775v1_figure_1.png",
        "paperid": "2408.05775v1",
        "paper_path": "./MISS-QA/papers/2408.05775v1.json",
        "figure_id": "2408.05775v1_figure_1.png",
        "caption": "Figure 1: TPT versus Self-TPT. (a) TPT learns prompts from source data (stage 1), then adapts them to individual samples for prediction (stages 2&3). (b) Self-TPT employs text-oriented self-supervised learning (SSL) for joint training (stage 1) and for new class adaptation (stage 2), followed by direct predictions for each image (stage 3). (c) We present the frame per second (FPS) and graphics memory usage for each method when applied to CLIP-B/16 using the same A100-80G GPU. The y-axis represents the average cross-dataset accuracy.",
        "qtype": "Design_Rationale",
        "response": "In Self-TPT (Fig. 1 b) the very first stage (“Stage 1”) is shaded green because it is *not* just ordinary supervised prompt‐learning on the source labels any more, but a *joint* supervised + self-supervised (SSL) prompt‐learning phase.  We chose to do this for two very concrete reasons:\n\n  1.  Contrastive SSL on the *text* side naturally encourages each class’s prompt embedding to spread out from its neighbors while still remaining tightly clustered within the same class.  That is exactly the geometry you want for a classifier—good intra-class compactness and inter-class separability.\n\n  2.  Empirically, we measured the angle between the gradients of the two losses (cross‐entropy vs. our Contrastive Prompt Tuning loss) and found it to be *consistently positive* (8–10 of our 11 benchmarks).  In plain English, that means “whenever the supervised loss wants to move the prompt in a particular direction, the SSL loss is *helping* rather than *fighting*.”  \n\nPutting those two observations together, there is very little downside and a clear upside to co-training prompts with both the cross‐entropy (supervised) loss and the contrastive (self-supervised) loss right from the start.  You end up with an embedding space that (a) fits the source labels, (b) has the right class-to-class geometry to generalize, and (c) provides a far better launch-pad for the completely label-free, text-only adaptation step at test time."
    },
    {
        "question": "How are supervised and self-supervised prompt learning objectives jointly optimized during source data training?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The overall pipeline of Self-TPT, as depicted in Fig. 2, comprises three stages: prompt learning, test-time adaptation, and direct prediction. In Stage 1, we co-train the self-supervised task and the classification task: where Ψ is a SSL projection head, and θ denotes its parameters."
        ],
        "final_answer": "In the source-data training phase (Stage 1), Self-TPT jointly minimizes the standard cross-entropy classification loss and the self-supervised contrastive prompt tuning loss by summing them into a single objective.  Both the prompt parameters and the SSL projection head Ψ are updated together under this combined loss.",
        "relevant_elements": [
            "Sup. and self-sup. prompt learning on source data"
        ],
        "id": 1897,
        "masked_question": "How are [mask1] objectives jointly optimized during [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "supervised and self-supervised prompt learning",
            "source data training"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.05775v1_figure_1.png",
        "paperid": "2408.05775v1",
        "paper_path": "./MISS-QA/papers/2408.05775v1.json",
        "figure_id": "2408.05775v1_figure_1.png",
        "caption": "Figure 1: TPT versus Self-TPT. (a) TPT learns prompts from source data (stage 1), then adapts them to individual samples for prediction (stages 2&3). (b) Self-TPT employs text-oriented self-supervised learning (SSL) for joint training (stage 1) and for new class adaptation (stage 2), followed by direct predictions for each image (stage 3). (c) We present the frame per second (FPS) and graphics memory usage for each method when applied to CLIP-B/16 using the same A100-80G GPU. The y-axis represents the average cross-dataset accuracy.",
        "qtype": "Implementation_Details",
        "response": "Here, the “red‐boxed” objectives are simply the ordinary cross-entropy (supervised) loss for prompt learning on your labeled source set, and the self-supervised prompt‐tuning loss (Contrastive Prompt Tuning plus the Gradient-Matching term).  The “blue‐boxed” region is the Self-TPT pipeline, and it is in Stage 1 of Self-TPT that these two losses are married.  Concretely, during Stage 1 you optimize  \n   \n   Ltotal  =  LCE(prompt)  +  λ·LSSL(prompt)  \n   \nwhere  \n– LCE is the standard cross–entropy over your source classes,  \n– LSSL is the self-supervised loss (contrastive + gradient-matching), and  \n– λ balances the two terms.  \n\nBoth losses are back-propagated into the soft‐prompt parameters jointly, so that a single unified prompt is learned to serve both the classification and the SSL objectives."
    },
    {
        "question": "How does test-time prompt tuning on predefined class names interface with direct prediction for any input sample?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "As depicted in Fig. 1 (b), the adaptation process (stage 2) of Self-TPT operates solely on the predefined class names, allowing for direct predictions for any image without the need for prompt updates (stage 3), significantly reducing the computational load during inference.",
            "The prompts refined through Eq. 6 are directly applied to predict samples in Dₜ without further adjustments, thereby streamlining the test-time adaptation into a pre-processing step and significantly reducing computational costs during inference."
        ],
        "final_answer": "In Self-TPT, stage 2 uses only the list of predefined class names to tune the prompts (via a text-oriented SSL task). Once these class-specific prompts are adapted, they are frozen and then reused to make direct predictions on any test image (stage 3) without performing further per-sample prompt updates.",
        "relevant_elements": [
            "Test-time prompt tuning (adaptation) on predefined class names",
            "Directly predict for any input sample"
        ],
        "id": 1898,
        "masked_question": "How does [mask1] interface with direct prediction for any input sample?",
        "masked_number": 1,
        "masked_elements": [
            "test-time prompt tuning on predefined class names"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.05775v1_figure_1.png",
        "paperid": "2408.05775v1",
        "paper_path": "./MISS-QA/papers/2408.05775v1.json",
        "figure_id": "2408.05775v1_figure_1.png",
        "caption": "Figure 1: TPT versus Self-TPT. (a) TPT learns prompts from source data (stage 1), then adapts them to individual samples for prediction (stages 2&3). (b) Self-TPT employs text-oriented self-supervised learning (SSL) for joint training (stage 1) and for new class adaptation (stage 2), followed by direct predictions for each image (stage 3). (c) We present the frame per second (FPS) and graphics memory usage for each method when applied to CLIP-B/16 using the same A100-80G GPU. The y-axis represents the average cross-dataset accuracy.",
        "qtype": "Implementation_Details",
        "response": "In Self-TPT the red-boxed “Stage 2” no longer sees any individual test image at all but only the fixed list of class names.  It optimizes a single set of soft prompts purely over those text tokens, and then hands those “tuned” prompts straight over to Stage 3.  In Stage 3 you take your test image, encode it once through the frozen vision encoder, and simply score it against the pre-computed text embeddings (i.e. the prompts learned in Stage 2).  In this way the adaptation (Stage 2) is completely decoupled from prediction (Stage 3) – you never do another backward pass on an image, you just do one forward pass and match image features against your already-tuned class prompts."
    },
    {
        "question": "How does Semantic Unifier project varied Semantic Features into the unified node feature space?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To address semantic isolation across domains, we employ a semantic unifier module that preprocesses node features without requiring additional training.",
            "Given the feature matrix X, we utilize singular value decomposition (SVD) in extracting important latent features as follows:\n\n    \\[ \\widetilde{X} = \\mathrm{LayerNorm}(U_k^\\top X) \\]\n\nwhere LayerNorm represents layer normalization, ensuring numerical stability. If k is smaller than d, SVD will use a reduced rank to decompose X, with the remaining dimensions zero-padded to reach d. This ensures that the unified features \\(\\widetilde{X}\\) maintain a consistent dimensionality d across different graph data. Besides, the relative spatial distances between nodes are preserved in the unified features due to the nature of SVD."
        ],
        "final_answer": "The Semantic Unifier applies singular value decomposition (SVD) to the original node feature matrix X to extract its top-k latent components (i.e. the leading singular vectors), projects X onto these components, applies layer normalization for numerical stability, and then zero-pads (or truncates) to a fixed dimension d. This yields a unified node feature matrix of consistent size that preserves the relative distances among nodes.",
        "relevant_elements": [
            "Semantic Unifier",
            "Semantic Features"
        ],
        "id": 1899,
        "masked_question": "How does [mask1] project varied [mask2] into the unified node feature space?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic Unifier",
            "Semantic Features"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.12609v1_figure_2.png",
        "paperid": "2410.12609v1",
        "paper_path": "./MISS-QA/papers/2410.12609v1.json",
        "figure_id": "2410.12609v1_figure_2.png",
        "caption": "Figure 2. The proposed graph foundation reasoning engine SCORE transforms diverse graph tasks into zero-shot reasoning on multi-relational knowledge graphs with the integration of semantic features.",
        "qtype": "Implementation_Details",
        "response": "The “Semantic Unifier” module (red box) simply takes whatever raw semantic features you have (text embeddings, ontology counts, etc., shown in the blue “Semantic Features” stack), performs a thin‐rank SVD on that feature matrix, truncates or zero‐pads the resulting U·S factors to a fixed dimension d, and then applies layer normalization.  In this way it projects all of your varied input features into one consistent d-dimensional node feature space."
    },
    {
        "question": "How does SCMP combine semantic-augmented relation embeddings with unified node features during message passing?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Section 3.3: “In our SCORE framework, we refine the relation graph by supplementing the original triple data T with additional edges obtained through semantic augmentation. Specifically, we derive semantic interactions among entities from the unified features U. … The semantic interaction between i and each element in Sᵢ is regarded as an additional relation type e_s. Finally, the construction rules for the relation graph R_E can be formalized as follows: … ”",
            "Section 3.4: “Global–local Semantic Encoding: Although the improved relation graph and initialization function incorporate high-level semantic associations among entities, the original semantic features remain isolated from the CMP calculations. To address this, the SCMP module employs two CMP channels to encode global and local representations. The local representations are derived from the existing query-specific CMP process, while the global representations are encoded independently of the query, using all-ones vectors for relation-level initialization and semantic features for entity-level initialization. The complete two-channel calculations of SCMP are defined as follows: … The trainable parameters Θ and Φ are shared in two channels. Notably, the global representations can be precomputed and integrated into each query-specific CMP process, permitting SCMP to keep time and space complexities akin to basic CMP-based models.”"
        ],
        "final_answer": "SCMP first builds a semantic-augmented relation graph by adding new relation types derived from pairwise similarities in the unified feature space. It then runs a two-channel message-passing: 1) a local (query-conditioned) channel that uses the semantic-augmented relation embeddings for its MSG/AGG/UPD steps exactly like standard CMP, and 2) a global channel that is initialized with all-ones at the relation side and the unified node features at the entity side to inject raw semantic information. Finally, the two sets of node representations are merged by a small MLP, yielding node embeddings that fuse both the learned relation semantics and the unified node features.",
        "relevant_elements": [
            "SCMP",
            "Semantic-Augmented Relation Embeddings",
            "Unified Node Features"
        ],
        "id": 1900,
        "masked_question": "How does [mask1] combine semantic-augmented relation embeddings with unified node features during message passing?",
        "masked_number": 1,
        "masked_elements": [
            "SCMP"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.12609v1_figure_2.png",
        "paperid": "2410.12609v1",
        "paper_path": "./MISS-QA/papers/2410.12609v1.json",
        "figure_id": "2410.12609v1_figure_2.png",
        "caption": "Figure 2. The proposed graph foundation reasoning engine SCORE transforms diverse graph tasks into zero-shot reasoning on multi-relational knowledge graphs with the integration of semantic features.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. The red box in the figure calls out our SCMP module (“Semantic Conditional Message Passing”) from §3.4.  \n2. In SCMP we actually run two parallel CMP passes over the same graph:  \n   – a “local” (query-conditional) channel that behaves just like vanilla CMP but uses our semantic–augmented relation vectors as edge filters, and  \n   – a “global” (query-agnostic) channel that is seeded directly with the unified node features (from the SVD-based SemUnifier) and with trivial (all-ones) relation initials.  \n3. In both channels, each message from neighbor u to node v in layer l is formed by an element-wise product of u’s current node vector with the corresponding semantic-augmented relation embedding for the (u→v) edge; those messages are then summed (AGG) and passed through the usual UPD step.  \n4. Thus the “semantic-augmented relation embeddings” supply edge-specific weights, while the “unified node features” supply the starting node signals for the global channel.  \n5. At the end of L hops, SCMP simply concatenates (or otherwise combines via a small MLP) the local and the global node representations to yield the final, semantically enriched node embeddings.  \n\nAnswer  \nSCMP fuses the two sources of semantic signal by running a two-channel CMP:  \n• In the local stream, it passes messages exactly as in CMP but replaces each learned relation filter with the pre-computed semantic-augmented relation embedding.  \n• In the global stream, it does the same message-passing update rule, but initializes each node’s hidden state to its unified SVD-derived feature vector (and all relations to all-ones).  \nIn both streams, each neighbor message is computed by element-wise multiplying the neighbor’s hidden vector with the corresponding semantic-augmented relation embedding, summing over neighbors, and then applying the usual update. Finally, the two streams’ outputs are merged by a small MLP to produce node representations that jointly encode topology, query context, unified node semantics, and semantic-augmented relations."
    },
    {
        "question": "How does SCMP extend CMP's conditional message passing to incorporate semantic augmentations?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "To effectively leverage semantic features in the CMP process while avoiding these challenges, we propose a novel message passing framework called Semantic Conditional Message Passing (SCMP), including two core techniques: Semantic-injected Entity Initialization and Global-local Semantic Encoding.",
            "Instead of using the original semantic features, we inject the semantic neighbor labels into the entity initialization. The improved initialization function is defined as follows: h^{0}_{i|q} = I(i=q)·e_q + I(i∈N^{sem}_{x,q})·v_{sem}, where v_{sem} is a trainable vector shared across all semantic neighbors and N^{sem}_{x,q} comes from the unified feature matrix.",
            "In this schema, the initial representations of these neighbor entities are not all-zero vectors, enabling them to propagate high-order semantic messages from the very first layer of CMP.",
            "To address the remaining isolation of raw semantic features, SCMP employs two parallel CMP channels: a local channel that performs the standard query-conditional message passing, and a global channel that initializes relations with all-ones vectors and entities with their unified semantic features. The outputs of these two channels are then merged by a small MLP to produce the final conditional triple representations."
        ],
        "final_answer": "SCMP extends the standard CMP framework by (1) altering the node initialization step so that entities identified as semantic neighbors (via a precomputed, SVD-based unified feature space) receive a shared trainable ‘semantic neighbor’ embedding instead of zeros, and (2) running CMP in two parallel channels—a query-conditional (local) channel and a query-agnostic (global) channel seeded by the unified semantic features—and then merging their outputs via an MLP. Together, these augmentations allow SCMP to inject high-order semantic information into every layer of conditional message passing while preserving CMP’s theoretical expressiveness.",
        "relevant_elements": [
            "CMP",
            "SCMP"
        ],
        "id": 1901,
        "masked_question": "How does [mask1] extend CMP's conditional message passing to incorporate semantic augmentations?",
        "masked_number": 1,
        "masked_elements": [
            "SCMP"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.12609v1_figure_2.png",
        "paperid": "2410.12609v1",
        "paper_path": "./MISS-QA/papers/2410.12609v1.json",
        "figure_id": "2410.12609v1_figure_2.png",
        "caption": "Figure 2. The proposed graph foundation reasoning engine SCORE transforms diverse graph tasks into zero-shot reasoning on multi-relational knowledge graphs with the integration of semantic features.",
        "qtype": "Literature_Background",
        "response": "Let “​[mask1]” be the red‐boxed module in the encoder – namely the Semantic Conditional Message Passing (SCMP) layer.  Here is how SCMP extends a vanilla CMP to actually plug in all of our new semantic signals:\n\n1.  Semantic-injected Initialization  \n    – In ordinary CMP you only kick off the walk from your query entities (everything else starts at zero).  In SCMP we instead look up each node’s semantic neighbors (from the unified feature KNN graph) and give them a non-zero “semantic” start vector vsem as well.  Concretely, h⁰ᵢ = hᵢ⋅1{i∈{head,tail}} + vsem⋅1{i∈SemNbrs(q)}.  That way, high-order semantic relationships are already alive in the message stream from step one, without breaking the distinguishability guarantees of CMP.\n\n2.  Dual-channel (Global + Local) Message Passing  \n    – We actually run two parallel CMP passes:  \n      • A local, query-conditioned pass exactly like normal CMP (relations initialized by r|q, entities by the new semantic initialization above).  \n      • A global, query-agnostic pass where all relations start from an all-ones “global” vector but entities again use the unified semantic features.  \n    – At the end of each layer we merge the two streams (via a small MLP) so that every node’s new embedding carries both “local” (structural + query) and “global” (purely semantic) information.  \n\nBy (a) injecting semantic neighbor labels into the very first CMP step and (b) adding a second, semantic-only CMP channel and fusing it back into the query-conditioned channel, SCMP weaves your unified semantic features and the extra “semantic” edges seamlessly into every round of relational message passing."
    },
    {
        "question": "How do Semantic Unifier transformations influence Relation Graph's construction for semantic augmentation?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Here, the SemUnifier module transforms input features into a unified, fixed-dimensional feature space (\\mathbf{X}_U), addressing the feature heterogeneity across domains. (Section 3.2)",
            "In our SCORE framework, we refine the relation graph G_R by supplementing the original triple data T with additional edges obtained through semantic augmentation. Specifically, we derive semantic interactions among entities from the unified features X_U. For each entity e_i, we identify the top k spatially nearest entities in the unified feature space via pairwise similarities, while excluding its direct topological neighbors. The set of semantic neighbors N(i) is defined as follows: ... The semantic interaction between e_i and each element in N(i) is regarded as an additional relation type r_s. (Section 3.3)"
        ],
        "final_answer": "The Semantic Unifier first converts diverse node features into a single, fixed-dimensional embedding space (X_U). These unified embeddings are then used to compute pairwise similarities among entities; the top-K nearest neighbors (excluding existing graph neighbors) are treated as ‘semantic neighbors.’ Each semantic neighbor link is introduced as a new relation type in the Relation Graph (G_R), thereby augmenting it with edges derived from the unified feature space.",
        "relevant_elements": [
            "Semantic Unifier",
            "Relation Graph"
        ],
        "id": 1902,
        "masked_question": "How do [mask1] transformations influence [mask2]'s construction for semantic augmentation?",
        "masked_number": 2,
        "masked_elements": [
            "Semantic Unifier",
            "Relation Graph"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.12609v1_figure_2.png",
        "paperid": "2410.12609v1",
        "paper_path": "./MISS-QA/papers/2410.12609v1.json",
        "figure_id": "2410.12609v1_figure_2.png",
        "caption": "Figure 2. The proposed graph foundation reasoning engine SCORE transforms diverse graph tasks into zero-shot reasoning on multi-relational knowledge graphs with the integration of semantic features.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the “Semantic Unifier” block; Mask2 is the “Relation Graph” that we enrich with semantic edges.  In SCORE the unifier sits upstream of the relation‐graph builder and directly feeds it the only thing it needs to augment that graph: a common, fixed–dimensional embedding for every node.  Concretely:\n\n 1.  The Semantic Unifier (Sec 3.2) takes whatever node features you have (textual embeddings, ontology counts, etc.), applies layer-norm, does an SVD into a fixed d-dimensional space and zero-pads if necessary.  In doing so it (a) removes domain-specific scaling, (b) preserves relative distances between nodes, and (c) guarantees every KG, however heterogenous its raw features, now lives in the very same Euclidean space.  \n 2.  In Sec 3.3, those unified d-dimensional vectors are used to compute pairwise similarities.  For each entity e we pick its top-k nearest neighbors (outside of its original graph neighbors) whose cosine/dot-product similarity exceeds a threshold δ.  Each such “semantic neighbor” pair (e, e′) is then turned into an extra relation type r_sem and added as a node in the relation graph, with edges head-to-head, tail-to-tail, etc., just as with the original relations.  \n\nIn short, by turning all raw features into a single, distance‐meaningful embedding space, the Semantic Unifier makes it trivial to mine KNN “semantic” links—which become new edges (and new relation nodes) in the Relation Graph—thereby achieving semantic augmentation."
    },
    {
        "question": "How does CBAM draw on existing channel-attention and spatial-attention mechanisms for feature refinement?",
        "relevant_section_ids": [
            "3.2.1",
            "3.2.2",
            "3.2.3"
        ],
        "relevant_context": [
            "Fig. 2 illustrates the overview of the CBAM. It comprises two successive sub-modules: channel-attention and spatial-attention [37]. The CBAM adapts to enhance the intermediate feature map at each convolutional block of deep networks.",
            "The CBAM consecutively evaluates 1D channel attention map M_ch and a 2D spatial attention map M_sp for a given intermediate feature map F as input. The complete attention process can be described as: F' = M_ch(F) ⊙ F, F'' = M_sp(F') ⊙ F'.",
            "Each channel in a feature map serves as a feature detector, with channel attention focusing on 'what' is significant to an input image. As shown in Fig 3, the spatial size of the input feature is squeezed to optimize the channel attention. Therefore, CAM uses average-pooled and max-pooled features simultaneously.",
            "CAM focuses on 'what' is significant to an input image, whereas SAM focuses on 'where' an informative part of an image is located. The evaluation of spatial attention covers applying average-pooling and max-pooling along the channel axis and concatenating them to extract representative features. After concatenation, a convolutional layer is applied to generate SAM, M_sp."
        ],
        "final_answer": "CBAM refines features by chaining two attention mechanisms: first a channel-attention module (CAM) that pools spatial information (average and max), passes it through a shared MLP to produce a 1D channel-attention map and multiplies it with the feature map (focusing on ‘what’ is important), then a spatial-attention module (SAM) that pools channel information (average and max), concatenates the results, applies a 7×7 convolution to produce a 2D spatial-attention map and multiplies it with the intermediate feature (focusing on ‘where’ the important regions are).",
        "relevant_elements": [
            "CBAM",
            "channel-attention",
            "spatial-attention"
        ],
        "id": 1903,
        "masked_question": "How does [mask1] draw on existing channel-attention and spatial-attention mechanisms for feature refinement?",
        "masked_number": 1,
        "masked_elements": [
            "CBAM"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.09469v1_figure_1.png",
        "paperid": "2411.09469v1",
        "paper_path": "./MISS-QA/papers/2411.09469v1.json",
        "figure_id": "2411.09469v1_figure_1.png",
        "caption": "Figure 1: The proposed explainable Cervix-AID-Net model for high-risk and low-risk classification.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is nothing more exotic than a “Convolutional Block Attention Module” (CBAM) as introduced in Woo et al. (2018)—in other words, the network literally just “plugs in” two off-the-shelf attention blocks to every convolutional layer:\n\n 1.  **Channel Atten­tion (CAM)**  \n    – Squeeze: apply both global average‐pooling and global max‐pooling over H×W, turning each C×H×W feature‐map tensor into two C×1×1 context vectors.  \n    – Excitation: feed those two vectors through a shared two‐layer MLP (with a small hidden bottleneck) to produce a C×1×1 “what-to-look-at” weight vector.  \n    – Reweight: multiply that 1D vector back into the original C channels (Hadamard product), boosting “important” channels, suppressing noisy ones.\n\n 2.  **Spatial Atten­tion (SAM)**  \n    – Squeeze: apply max‐ and average‐pooling across channels to get two 1×H×W maps.  \n    – Combine & Convolve: concatenate them into a 2×H×W tensor, run a single 7×7 convolution to produce a 1×H×W spatial mask.  \n    – Reweight: multiply that 2D mask back into the C×H×W volume, sharpening “where” in each feature‐map the network should focus.\n\nBy simply cascading CAM then SAM at each convolutional block, the CBAM “draws on” (i.e. re-uses) these two proven attention recipes to refine the feature maps both in the channel dimension (“what” to amplify) and the spatial dimension (“where” to look), before handing them off to the next layer."
    },
    {
        "question": "How does CartoonX extend pixel RDE into the wavelet domain for smoother explanations?",
        "relevant_section_ids": [
            "3.4.4"
        ],
        "relevant_context": [
            "CartoonX is a novel explanation technique that is a special case of RDE. CartoonX first executes RDE in the discrete wavelet position-scale domain of an image X and then visualizes the wavelet mask M as a pixel-wise smooth picture. Wavelets efficiently represent 2D piece-wise smooth pictures, commonly known as cartoon-like images, along with providing optimum representations for piece-wise smooth 1D functions [45]. Algorithm 1 illustrates the steps for obtaining CartoonX explanations."
        ],
        "final_answer": "CartoonX extends pixel RDE by carrying out the rate‐distortion explanation not on raw pixels but on the image’s discrete wavelet coefficients. It first transforms the image into its wavelet position–scale domain, applies RDE to derive a sparse mask over those wavelet components, and then reconstructs that mask back into pixel space to produce a smooth, cartoon-like explanation.",
        "relevant_elements": [
            "CartoonX",
            "pixel RDE"
        ],
        "id": 1904,
        "masked_question": "How does [mask1] extend pixel RDE into the wavelet domain for smoother explanations?",
        "masked_number": 1,
        "masked_elements": [
            "CartoonX"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.09469v1_figure_1.png",
        "paperid": "2411.09469v1",
        "paper_path": "./MISS-QA/papers/2411.09469v1.json",
        "figure_id": "2411.09469v1_figure_1.png",
        "caption": "Figure 1: The proposed explainable Cervix-AID-Net model for high-risk and low-risk classification.",
        "qtype": "Literature_Background",
        "response": "Step 1 – Identify what’s in the red box (mask1):  \nIf you look at the bottom panel of the figure under “Explainable Artificial Intelligence,” the second module from the left (outlined in red) is labeled “CartoonX Explanations.”  \n\nStep 2 – Recall what pixel‐RDE does:  \n• Pixel‐RDE solves a constrained optimization over a sparse pixel‐mask so as to leave only those pixels that are most important to the network’s decision.  \n• In practice it relaxes the binary‐mask problem into a continuous one, but the end result is still a map of individual pixels.  \n• This pixel‐wise mask can be very noisy and jagged, because nothing enforces any spatial smoothness among neighboring pixels.  \n\nStep 3 – What CartoonX adds (and how it “extends” pixel‐RDE):  \n• CartoonX is in fact a special case of RDE, but carried out not in the raw‐pixel basis, but in a discrete wavelet basis (i.e. position–scale domain).  \n• It first takes the image, applies a wavelet transform to yield a set of coefficients at various positions and scales.  \n• It then performs the same sparse‐mask optimization (the RDE procedure) over those wavelet coefficients rather than pixels.  \n• Finally it inverts the masked wavelet coefficients back to the pixel domain, yielding a “cartoon‐like” (piecewise‐smooth) saliency map.  \n\nStep 4 – Why this gives smoother explanations:  \n• Wavelets are known to represent piecewise‐smooth structures very efficiently. Masking coefficients in wavelet space and then reconstructing forces the explanation to consist of large, smooth “blobs” rather than isolated noisy pixels.  \n\nAnswer:  \nCartoonX extends pixel‐RDE by moving the sparse‐mask optimization into a discrete wavelet (position–scale) domain. It masks out unimportant wavelet coefficients instead of raw pixels, and then inverts the wavelet transform to produce a smoothly varying, “cartoon-style” saliency map."
    },
    {
        "question": "How does using CBAM between each convolutional layer affect the model’s capacity to capture discriminative cervical features?",
        "relevant_section_ids": [
            "3.2.1",
            "3.2.3",
            "5"
        ],
        "relevant_context": [
            "Section 3.2.1: \"The CBAM adapts to enhance the intermediate feature map at each convolutional block of deep networks.\"",
            "Section 3.2.3: \"Each convolutional layer is followed by a CBAM block to extract relevant feature maps.\"",
            "Section 5: \"The proposed Cervix-AID-Net model generates highly discriminant features due to the CBAM module.\""
        ],
        "final_answer": "By inserting a CBAM block after every convolutional layer, the network is able to selectively emphasize the most informative channels and spatial regions in each intermediate feature map. This per-layer attention mechanism sharpens the features that matter most for cervical-precancer classification, thereby yielding more highly discriminative representations at every stage of the model.",
        "relevant_elements": [
            "CBAM",
            "convolutional layer"
        ],
        "id": 1905,
        "masked_question": "How does using [mask1] between each convolutional layer affect the model’s capacity to capture discriminative cervical features?",
        "masked_number": 1,
        "masked_elements": [
            "CBAM"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.09469v1_figure_1.png",
        "paperid": "2411.09469v1",
        "paper_path": "./MISS-QA/papers/2411.09469v1.json",
        "figure_id": "2411.09469v1_figure_1.png",
        "caption": "Figure 1: The proposed explainable Cervix-AID-Net model for high-risk and low-risk classification.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what “[mask1]” is.  If you look at the red‐outlined region in Fig. 1, it is the series of CBAM blocks (each consisting of a Channel-Attention Module followed by a Spatial-Attention Module) that have been “slotted in” between every pair of convolutional layers.\n\nStep-by-step reasoning:\n\n1.   Mask1 = the CBAM attention module inserted after each convolutional block.  \n2.   The channel-attention part of CBAM learns “what” feature‐maps (i.e. which channels) are most informative for discriminating low- versus high-risk cervical tissue.  \n3.   The spatial-attention part of CBAM learns “where” in each feature‐map the crucial cues (e.g. lesion boundaries, acetowhitening, vascular patterns) lie.  \n4.   By placing one CBAM after every convolutional layer, the network is repeatedly forced to re-weigh its feature channels and spatial locations at each depth.  \n5.   This continual re-calibration suppresses irrelevant patterns (e.g. specular highlights, background cervical anatomy) and amplifies subtle pathological cues.  \n6.   As a result, the learned feature hierarchy becomes more focused and discriminative, boosting the model’s ability to pick out the small color and texture changes that signal high-grade lesions.\n\nAnswer: By interleaving CBAM blocks between each convolutional layer, the network constantly refines both “what” it looks at (the most informative channels) and “where” it looks (the most salient spatial locations).  That repeated channel- and spatial-attention sharpening greatly enhances its capacity to capture the fine, discriminative cervical features needed for reliable high-risk versus low-risk classification."
    },
    {
        "question": "How does CartoonX methodology improve explanation smoothness compared to pixel RDE in model interpretation?",
        "relevant_section_ids": [
            "3.4.4",
            "5"
        ],
        "relevant_context": [
            "CartoonX is a novel explanation technique that is a special case of RDE. CartoonX first executes RDE in the discrete wavelet position-scale domain of an image X and then visualizes the wavelet mask M as a pixel-wise smooth picture. Wavelets efficiently represent 2D piece-wise smooth pictures, commonly known as cartoon-like images, along with providing optimum representations for piece-wise smooth 1D functions [45].",
            "Our analysis shows that explanations given by CartoonX lie around the cervix region, which is the most crucial part of the image. The reason for a more meticulous explanation of CartoonX is its ability to extract relevant piece-wise smooth parts of an image instead of relevant pixel sparse regions. The sparsity in the wavelet domain captures interpretable input features from the image compared to sparsity in the pixel domain, instance-based explanations, and output neuron activations. Our analysis shows that CartoonX captures piece-wise smooth explanations that can reveal relevant piece-wise smooth patterns that are not easily visible with existing grad-CAM that operates on the output feature maps and pixel-based methods like LIME and pixel RDE."
        ],
        "final_answer": "CartoonX improves explanation smoothness by performing RDE in the wavelet domain and then rendering the resulting mask as a pixel-wise smooth image. By exploiting wavelets’ ability to represent piece-wise smooth structures, CartoonX yields contiguous, smooth explanation regions, in contrast to the highly sparse, non-stationary pixel-level masks produced by pixel RDE.",
        "relevant_elements": [
            "CartoonX",
            "pixel RDE"
        ],
        "id": 1906,
        "masked_question": "How does [mask1] methodology improve explanation smoothness compared to [mask2] in model interpretation?",
        "masked_number": 2,
        "masked_elements": [
            "CartoonX",
            "pixel RDE"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.09469v1_figure_1.png",
        "paperid": "2411.09469v1",
        "paper_path": "./MISS-QA/papers/2411.09469v1.json",
        "figure_id": "2411.09469v1_figure_1.png",
        "caption": "Figure 1: The proposed explainable Cervix-AID-Net model for high-risk and low-risk classification.",
        "qtype": "Experimental_Results",
        "response": "Here’s the reasoning step-by-step:\n\n1. In the figure you pointed out, the blue-boxed module is labeled “pixel RDE Explanations,” and the red-boxed one is “CartoonX Explanations.”  \n\n2. From the text (section 3.4.3–3.4.4), pixel RDE works by learning a sparse mask directly in pixel space.  Because it enforces sparsity one pixel at a time, its resulting saliency maps tend to be “non-stationary” and noisy—bright speckles here and there rather than a clean, connected highlight.  \n\n3. CartoonX, in contrast, is a special case of RDE that solves the same masking problem in the discrete wavelet (position-scale) domain.  By enforcing sparsity on wavelet coefficients, CartoonX naturally picks out piece-wise smooth image components.  When you invert back to pixel space, you get a smooth, contiguous explanation map rather than a jittery one.  \n\n4. In short, CartoonX’s use of the wavelet basis enforces smoothness at reconstruction time, whereas pixel RDE’s pixel-level sparsity does not, so CartoonX explanations look far smoother and more visually coherent.  \n\nAnswer:  \nCartoonX improves smoothness by performing its sparse‐mask optimization in the wavelet domain (targeting piece-wise smooth wavelet coefficients) and then reconstructing back to the image.  This yields contiguous, smooth saliency regions.  Pixel RDE, by contrast, applies sparsity directly in pixel space, producing scattered, noisy masks."
    },
    {
        "question": "How does incorporating DepRoBERTa, Llama3-8B, and Qwen2-72B-Instruct ensure high-confidence pseudo-labels?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "We use a combination of Qwen2-72B-Instruct, along with fine-tuned DepRoBERTa and Llama3-8B models on the  labeled posts to generate pseudo-labels for unlabeled data, as depicted in Fig. 1 (a), rightmost.",
            "Specifically, we only kept posts for which all three models predicted the same labels, and discarded the rest in case of disagreement.",
            "By doing so, we retain only high-confidence pseudo-labels, which allow us to reduce model-specific biases, resulting in a cleaner dataset for fine-tuning later on."
        ],
        "final_answer": "By requiring that DepRoBERTa, Llama3-8B, and Qwen2-72B-Instruct all agree on the same label for each unlabeled post and discarding any posts where they disagree. This consensus‐based filtering yields only high‐confidence pseudo‐labels and reduces model‐specific noise.",
        "relevant_elements": [
            "DepRoBERTa",
            "Llama3-8B",
            "Qwen2-72B-Instruct"
        ],
        "id": 1907,
        "masked_question": "How does incorporating [mask1], [mask2], and Qwen2-72B-Instruct ensure high-confidence pseudo-labels?",
        "masked_number": 2,
        "masked_elements": [
            "DepRoBERTa",
            "Llama3-8B"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.04501v3_figure_1.png",
        "paperid": "2410.04501v3",
        "paper_path": "./MISS-QA/papers/2410.04501v3.json",
        "figure_id": "2410.04501v3_figure_1.png",
        "caption": "Figure 1: An overview of our approach. (a) pseudo-labels generation for unlabeled data. We first use 500500500500 labeled posts to fine-tune DepRoBERTa [5] and Llama3-8B [4] for the classification task. Then, we combine these models with Qwen2-72B-Instruct via prompting to annotate 1,50015001{,}5001 , 500 posts in the unlabeled dataset. We keep only ≈900absent900\\approx 900≈ 900 posts for which all three models predict the same and combine these with the 500500500500 labeled posts to form a new training set (Section IV-A). (b) LLMs fine-tuning. We then fine-tune Llama3-8B, Llama3.1-8B, and Gemma2-9B on the newly formed dataset with Macro Double Soft F1 loss (Section IV-B). (c) Model Ensembling. These fine-tuned models are combined with prompting Qwen2-72B-Instruct to create an ensemble model for classifying new user posts\n(Section IV-C).",
        "qtype": "Experimental_Results",
        "response": "Here’s the intuition, step by step:\n\n1.   You start with two very different classifiers that have each been trained on your small gold set of 500 posts:  \n     •  [mask1] = DepRoBERTa (an encoder‐only model fine‐tuned on depression/suicide signals)  \n     •  [mask2] = Llama3-8B (a decoder‐only language model fine-tuned to do the same task)  \n\n2.   You also have Qwen2-72B-Instruct running in few-shot, chain‐of-thought prompting mode.\n\n3.   You ask all three models to label each of the 1,500 unlabeled posts.\n\n4.   You then throw away any post where even one of the three disagrees.  Only those posts for which  \n     DepRoBERTa,  \n     Llama3-8B,  \n     and Qwen2-72B-Instruct  \n     all spit out the exact same suicide‐risk label survive.\n\n5.   By keeping only the unanimous predictions you:  \n     –  Filter out examples where a single model might be confused or overconfident in the wrong answer  \n     –  Mitigate model‐specific biases (encoder vs. decoder vs. prompted LLM)  \n     –  End up with a much cleaner, higher‐confidence pseudo‐labeled set (≈900 posts)  \n\nThat high‐precision “three‐way agreement” is exactly what gives you confidence in your pseudo‐labels before you fold them back into your larger training pool."
    },
    {
        "question": "How does weighted majority voting balance contributions from Llama3-8B and Qwen2-72B-Instruct?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Our ensemble model consisting of five individual models: Qwen2-72B-Instruct via prompting, and four fine-tuned large language models (LLMs) - Llama3-8B (with two variants differing in hyperparameters, denoted as Llama3-8B₁ and Llama3-8B₂), Llama3.1-8B, and Gemma2-9B, as shown in Fig. 1(c). From n labeled data, we assign the ensemble weight w_i to the models, with a weight of 2 for Qwen2-72B-Instruct, and a weight of 1 for the other four models. This means that the prediction of Qwen2-72B-Instruct counts twice, whereas all other models count once."
        ],
        "final_answer": "In the weighted majority vote, Qwen2-72B-Instruct is given a weight of 2 while each Llama3-8B (and the other models) is given a weight of 1, so Qwen’s vote effectively counts twice as much as any single Llama3-8B vote.",
        "relevant_elements": [
            "Llama3-8B",
            "Qwen2-72B-Instruct"
        ],
        "id": 1908,
        "masked_question": "How does weighted majority voting balance contributions from [mask1] and Qwen2-72B-Instruct?",
        "masked_number": 1,
        "masked_elements": [
            "Llama3-8B"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.04501v3_figure_1.png",
        "paperid": "2410.04501v3",
        "paper_path": "./MISS-QA/papers/2410.04501v3.json",
        "figure_id": "2410.04501v3_figure_1.png",
        "caption": "Figure 1: An overview of our approach. (a) pseudo-labels generation for unlabeled data. We first use 500500500500 labeled posts to fine-tune DepRoBERTa [5] and Llama3-8B [4] for the classification task. Then, we combine these models with Qwen2-72B-Instruct via prompting to annotate 1,50015001{,}5001 , 500 posts in the unlabeled dataset. We keep only ≈900absent900\\approx 900≈ 900 posts for which all three models predict the same and combine these with the 500500500500 labeled posts to form a new training set (Section IV-A). (b) LLMs fine-tuning. We then fine-tune Llama3-8B, Llama3.1-8B, and Gemma2-9B on the newly formed dataset with Macro Double Soft F1 loss (Section IV-B). (c) Model Ensembling. These fine-tuned models are combined with prompting Qwen2-72B-Instruct to create an ensemble model for classifying new user posts\n(Section IV-C).",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed model in the figure is Llama3-8B.  In the final ensemble (Sec. IV-C) we have five “voters”:\n\n  • Qwen2-72B-Instruct  \n  • Llama3-8B (two slightly different checkpoints)  \n  • Llama3.1-8B  \n  • Gemma2-9B  \n\nWhen we do weighted majority voting we assign  \n  – weight w=2 to Qwen2-72B-Instruct  \n  – weight w=1 to each of the fine‐tuned LLMs (including Llama3-8B)  \n\nAt inference time each model casts a vote for one of the four classes, and we sum up the weights of all models that voted for each class.  The class with the largest total weight wins.  In this way Qwen2-72B-Instruct “counts” as two votes, whereas Llama3-8B (and the others) each count as one vote, giving Qwen slightly more influence but still preserving substantial input from the fine-tuned LLMs."
    },
    {
        "question": "What potential ethical concerns arise from relying solely on Qwen2-72B-Instruct for pseudo-label generation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Qwen2-72B-Instruct"
        ],
        "id": 1909,
        "masked_question": "What potential ethical concerns arise from relying solely on [mask1] for pseudo-label generation?",
        "masked_number": 1,
        "masked_elements": [
            "Qwen2-72B-Instruct"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.04501v3_figure_1.png",
        "paperid": "2410.04501v3",
        "paper_path": "./MISS-QA/papers/2410.04501v3.json",
        "figure_id": "2410.04501v3_figure_1.png",
        "caption": "Figure 1: An overview of our approach. (a) pseudo-labels generation for unlabeled data. We first use 500500500500 labeled posts to fine-tune DepRoBERTa [5] and Llama3-8B [4] for the classification task. Then, we combine these models with Qwen2-72B-Instruct via prompting to annotate 1,50015001{,}5001 , 500 posts in the unlabeled dataset. We keep only ≈900absent900\\approx 900≈ 900 posts for which all three models predict the same and combine these with the 500500500500 labeled posts to form a new training set (Section IV-A). (b) LLMs fine-tuning. We then fine-tune Llama3-8B, Llama3.1-8B, and Gemma2-9B on the newly formed dataset with Macro Double Soft F1 loss (Section IV-B). (c) Model Ensembling. These fine-tuned models are combined with prompting Qwen2-72B-Instruct to create an ensemble model for classifying new user posts\n(Section IV-C).",
        "qtype": "Others",
        "response": "Step 1 – Resolve [mask1]  \nBy inspecting the red‐boxed text in the figure and cross-referencing the narrative (“We employ Qwen2-72B-Instruct to generate pseudo-labels…”), we see that [mask1] is the LLM named “Qwen2-72B-Instruct.”\n\nStep 2 – Why relying solely on Qwen2-72B-Instruct for pseudo-labels is ethically fraught  \n1. Model bias and unvetted judgments  \n   • Qwen2-72B-Instruct inherits biases from its pretraining data (news, web forums, etc.).  \n   • Left unchecked, it may systematically over- or under-label certain demographic or linguistic groups (e.g., non-native English speakers, youths, etc.).  \n2. Risk of false positives and false negatives  \n   • A false negative (failing to flag someone truly at risk) could mean missing an opportunity for intervention.  \n   • A false positive (mistakenly flagging a benign post) could lead to unnecessary alarm or stigmatization.  \n3. Lack of clinical oversight  \n   • Suicide risk assessment is a sensitive, high-stakes medical decision. Relying only on an LLM without human-expert review violates best practices for mental-health screening.  \n4. Accountability and transparency  \n   • If Qwen2-72B-Instruct makes an erroneous pseudo-label, there is no clear chain of custody or interpretability mechanism to trace or correct that error.  \n5. Privacy and consent concerns  \n   • Feeding user posts into a large, externally hosted LLM may contravene privacy regulations or user expectations about how their data will be used.\n\nIn sum, using Qwen2-72B-Instruct alone risks amplifying model biases, producing harmful misclassifications in a life-critical application, and sidestepping both clinical oversight and user privacy safeguards."
    },
    {
        "question": "How could alternative loss functions impact the fine-tuning efficacy of Llama3-8B and Gemma2-9B?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Since F1 Score is computed via precision and recall, it is not differentiable. To directly optimize the F1 score, we use Macro Double Soft F1, introduced by [33], as our loss function.",
            "We found that optimizing with this loss function in our experiment data gains some performance boost compared with other common choices such as Cross Entropy (Table VI)."
        ],
        "final_answer": "Using an alternative loss function like standard cross‐entropy instead of Macro Double Soft F1 would likely reduce fine-tuning efficacy for Llama3-8B and Gemma2-9B, since the paper reports that Macro Double Soft F1 yields a measurable performance boost over more common losses such as cross-entropy.",
        "relevant_elements": [
            "Llama3-8B",
            "Gemma2-9B"
        ],
        "id": 1910,
        "masked_question": "How could alternative loss functions impact the fine-tuning efficacy of [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Llama3-8B",
            "Gemma2-9B"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.04501v3_figure_1.png",
        "paperid": "2410.04501v3",
        "paper_path": "./MISS-QA/papers/2410.04501v3.json",
        "figure_id": "2410.04501v3_figure_1.png",
        "caption": "Figure 1: An overview of our approach. (a) pseudo-labels generation for unlabeled data. We first use 500500500500 labeled posts to fine-tune DepRoBERTa [5] and Llama3-8B [4] for the classification task. Then, we combine these models with Qwen2-72B-Instruct via prompting to annotate 1,50015001{,}5001 , 500 posts in the unlabeled dataset. We keep only ≈900absent900\\approx 900≈ 900 posts for which all three models predict the same and combine these with the 500500500500 labeled posts to form a new training set (Section IV-A). (b) LLMs fine-tuning. We then fine-tune Llama3-8B, Llama3.1-8B, and Gemma2-9B on the newly formed dataset with Macro Double Soft F1 loss (Section IV-B). (c) Model Ensembling. These fine-tuned models are combined with prompting Qwen2-72B-Instruct to create an ensemble model for classifying new user posts\n(Section IV-C).",
        "qtype": "Others",
        "response": "First, let’s ground what [mask1] and [mask2] are in the figure and in the fine‐tuning pipeline:\n\n  • In Fig. 1(b) the red box highlights Llama3-8B – that is [mask1].  \n  • The blue box highlights Gemma2-9B – that is [mask2].  \n\nBoth of these decoder-only LLMs are fine-tuned on the same four-class suicide‐risk task using the Macro Double Soft F1 loss.  Now: what would happen if we swapped in a different loss?\n\n1.  Cross-Entropy (CE) Loss  \n    – CE is the most common choice, but it optimizes log-likelihood rather than F1.  \n    – For both Llama3-8B and Gemma2-9B, CE would usually  \n      • converge faster early on (because the gradients are well-behaved),  \n      • but tend to favor the majority class (here “Indicator”),  \n      • and often hurt recall on the rarer “Attempt” and “Behavior” classes.  \n    – In practice you’d probably see higher overall accuracy but lower Macro-F1 than with Soft F1.\n\n2.  Class‐Weighted Cross-Entropy  \n    – By up-weighting the under-represented suicide‐risk labels in CE, you partially recover those minority‐class F1 gains.  \n    – Llama3-8B, which is relatively robust to gradient noise, might benefit noticeably—improved recall on “Attempt,” for example—while Gemma2-9B might still lag on the hardest classes unless weights are very carefully tuned.\n\n3.  Focal Loss  \n    – Focal adds a term that down-weights “easy” examples and focuses training on hard, ambiguous ones.  \n    – You would likely see both Llama3-8B and Gemma2-9B improve precision on the borderline posts (e.g. those between Ideation and Behavior), at the cost of a bit more training instability and slower convergence.\n\n4.  Dice or Tversky Loss  \n    – These are cousins of Soft F1.  They can be tuned (via an α / β hyperparameter) to trade off false positives vs. false negatives.  \n    – For Llama3-8B you might find a Dice loss with β > α boosts recall on “Attempt,” whereas Gemma2-9B might respond better to a slightly more symmetric setting.\n\n5.  Label-Smoothing + CE  \n    – A small amount of smoothing can regularize both models to prevent over-confident spikes on rare classes.  \n    – It often slightly lowers peak F1 but improves calibration and stability—Gemma2-9B in particular, which can be prone to overfitting, might generalize better.\n\n6.  Margin‐Based/Hinge Loss  \n    – Less common in multiclass LLM finetuning, but it enforces a margin between the correct class logit and the highest incorrect logit.  \n    – Llama3-8B, with its larger capacity, could exploit this to separate “Behavior” from “Ideation” more cleanly, but Gemma2-9B may struggle unless you very carefully normalize its logits.\n\nIn short, swapping out Macro Double Soft F1 for something like plain cross-entropy will in almost every case speed up initial convergence but drop macro-F1, especially on the rare “Attempt” and “Behavior” labels.  Weighting your CE or switching to focal or a parametric Dice loss can recoup much of that lost F1, at the cost of more hyperparameter tuning and potentially slower or less stable training—effects that tend to be more pronounced in the smaller-capacity Gemma2-9B than in the beefier Llama3-8B."
    },
    {
        "question": "What limitations might arise from separate spatial and temporal codebooks in handling complex facial motion?",
        "relevant_section_ids": [
            "9"
        ],
        "relevant_context": [
            "Figure 13 illustrates a failure case in which our method may introduce video blurring.",
            "This occurs when the character’s motion is excessively dynamic or when camera switching leads to discontinuities in facial features between frames."
        ],
        "final_answer": "Using separate spatial and temporal codebooks can fail to capture very dynamic facial motions or abrupt frame-to-frame changes, leading to video blurring and discontinuities in the restored facial features.",
        "relevant_elements": [
            "Spatial codebook",
            "Temporal codebook"
        ],
        "id": 1911,
        "masked_question": "What limitations might arise from separate [mask1] and [mask2] in handling complex facial motion?",
        "masked_number": 2,
        "masked_elements": [
            "Spatial codebook",
            "Temporal codebook"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.16468v1_figure_1.png",
        "paperid": "2411.16468v1",
        "paper_path": "./MISS-QA/papers/2411.16468v1.json",
        "figure_id": "2411.16468v1_figure_1.png",
        "caption": "Figure 1: Network architecture of Stage I.\nStage I uses HQ face videos to train HQ 3D-VQGAN (Ehsubscript𝐸ℎE_{h}italic_E start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and Dhsubscript𝐷ℎD_{h}italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) and spatial and temporal codebooks (𝒞Ssubscript𝒞𝑆\\mathcal{C}_{S}caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and 𝒞Tsubscript𝒞𝑇\\mathcal{C}_{T}caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT).\n(a) illustrates the quantization operation STLookUp through two codebooks in our proposed framework. (b) and (c) display the computation process of temporal attention and motion residual, respectively.\n(d) We leverage a pre-trained feature network DINOv2 and trainable multi-scale discriminator heads to construct a more powerful discriminator for stable training.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n\n1. In the figure, the red-boxed module ([mask1]) is the Spatial Codebook 𝒞_S, which independently quantizes per-frame appearance features.  \n2. The blue-boxed module ([mask2]) is the Temporal Codebook 𝒞_T, which independently quantizes motion-residual or attention-driven temporal features across frames.  \n3. By design, 𝒞_S and 𝒞_T operate in isolation—𝒞_S never “sees” long-range motion priors, and 𝒞_T never conditions on rich spatial detail beyond residuals.  \n4. When facial motion is simple (e.g. slight head turns or blinks), this separation suffices, but as soon as motion becomes highly nonrigid, rapid, or involves occlusion/camera cuts, neither codebook can jointly capture the fine‐grained spatio-temporal correlations.  \n5. In practice this leads to coarse quantization of fast motion, temporal misalignment of features (ghosting or flicker), and overall blurring in regions where strong cross-frame spatial cues and motion cues must be modeled together.\n\nAnswer:\n\nBecause spatial and temporal information are quantized in two independent codebooks ([mask1] vs. [mask2]), the model cannot learn rich joint spatio-temporal patterns. Highly dynamic or non-rigid facial motions (rapid expressions, occlusions, camera changes) will fall between codewords in one or the other book, leading to quantization artifacts, blurring, ghosting or flicker when motion is complex."
    },
    {
        "question": "What potential biases might DINOv2 pretraining introduce in the multi-scale discriminator heads?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pretrained DINOv2",
            "Multi-scale discriminator heads"
        ],
        "id": 1912,
        "masked_question": "What potential biases might [mask1] pretraining introduce in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Pretrained DINOv2",
            "Multi-scale discriminator heads"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.16468v1_figure_1.png",
        "paperid": "2411.16468v1",
        "paper_path": "./MISS-QA/papers/2411.16468v1.json",
        "figure_id": "2411.16468v1_figure_1.png",
        "caption": "Figure 1: Network architecture of Stage I.\nStage I uses HQ face videos to train HQ 3D-VQGAN (Ehsubscript𝐸ℎE_{h}italic_E start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and Dhsubscript𝐷ℎD_{h}italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) and spatial and temporal codebooks (𝒞Ssubscript𝒞𝑆\\mathcal{C}_{S}caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and 𝒞Tsubscript𝒞𝑇\\mathcal{C}_{T}caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT).\n(a) illustrates the quantization operation STLookUp through two codebooks in our proposed framework. (b) and (c) display the computation process of temporal attention and motion residual, respectively.\n(d) We leverage a pre-trained feature network DINOv2 and trainable multi-scale discriminator heads to construct a more powerful discriminator for stable training.",
        "qtype": "Others",
        "response": "Because the “backbone” features feeding into the trainable heads come from a frozen, pretrained DINOv2, any skew or blind spot that DINOv2 picked up during its own pretraining will get baked into those multi‐scale discriminator heads.  In practice that means things like uneven coverage of skin tones, age groups, lighting conditions, hair styles, face poses or other visual categories in DINOv2’s pretraining set can translate into a discriminator that systematically prefers (or penalizes) certain looks or appearances.  Those dataset‐induced biases thus propagate directly into the discriminator heads that sit on top of DINOv2."
    },
    {
        "question": "What motivates combining temporal attention and motion residual before quantization in STLookUp?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Compared to image face enhancement, the difficulty of video face enhancement lies in maintaining the consistency of facial features and background details between frames.",
            "The discretization nature of a codebook dictates that applying it directly to video tasks often results in poor temporal consistency, leading to the common flickering problem.",
            "In addition, conventional codebooks are limited to capturing spatial features and fail to account for the motion information embedded in the videos.",
            "To tackle the above challenges, we propose learnable motion-aware spatial-temporal codebooks, where the spatial codebook C_S records portrait features and the temporal codebook C_T stores motion residuals between frames.",
            "The spatial latents z_h,S are directly derived from z_h, whereas the temporal latents z_h,T incorporate inter-frame temporal-attention (TA) information and motion residuals, defined as follows:"
        ],
        "final_answer": "They combine temporal attention and motion residual so that the temporal codebook can capture both inter-frame correlations and explicit motion dynamics, thereby improving temporal consistency and reducing flickering when quantizing video latents.",
        "relevant_elements": [
            "temporal attention",
            "motion residual",
            "STLookUp"
        ],
        "id": 1913,
        "masked_question": "What motivates combining [mask1] and motion residual before quantization in STLookUp?",
        "masked_number": 1,
        "masked_elements": [
            "temporal attention"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.16468v1_figure_1.png",
        "paperid": "2411.16468v1",
        "paper_path": "./MISS-QA/papers/2411.16468v1.json",
        "figure_id": "2411.16468v1_figure_1.png",
        "caption": "Figure 1: Network architecture of Stage I.\nStage I uses HQ face videos to train HQ 3D-VQGAN (Ehsubscript𝐸ℎE_{h}italic_E start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and Dhsubscript𝐷ℎD_{h}italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) and spatial and temporal codebooks (𝒞Ssubscript𝒞𝑆\\mathcal{C}_{S}caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and 𝒞Tsubscript𝒞𝑇\\mathcal{C}_{T}caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT).\n(a) illustrates the quantization operation STLookUp through two codebooks in our proposed framework. (b) and (c) display the computation process of temporal attention and motion residual, respectively.\n(d) We leverage a pre-trained feature network DINOv2 and trainable multi-scale discriminator heads to construct a more powerful discriminator for stable training.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed module is nothing mystical – it’s simply the inter-frame Temporal Attention (TA) block.  In STLookUp we don’t feed the codebook a “raw” attention map or a “raw” frame difference on its own, but we first fuse them.  The reason is that TA gives you broad, context-aware cues about which regions in frame t − k should talk to which regions in frame t, while the motion residual supplies the precise pixel-level shift.  By adding the two together before you quantize, your temporal codebook learns atoms that carry both global attention-style correlations and fine–grained motion offsets.  That richer, motion-aware latent is what lets STLookUp snap to the right code entries and thereby eliminate flicker while still preserving detailed dynamics."
    },
    {
        "question": "Why leverage a pre-trained DINOv2 feature network with multi-scale discriminator heads for adversarial training?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Due to the instability and artifact-prone nature of training video-level VQGANs, we propose to leverage a partially initialized and more powerful discriminator.",
            "Specifically, the discriminator consists of a frozen pre-trained feature network, such as DINOv2 [33, 36, 37], paired with a set of trainable lightweight discriminator heads."
        ],
        "final_answer": "Because training video-level VQGANs tends to be unstable and prone to artifacts, using a frozen pre-trained DINOv2 feature network together with multi-scale discriminator heads provides a stronger, more stable discriminator that helps suppress artifacts and stabilize adversarial training.",
        "relevant_elements": [
            "DINOv2",
            "multi-scale discriminator heads"
        ],
        "id": 1914,
        "masked_question": "Why leverage a pre-trained [mask1] feature network with multi-scale discriminator heads for adversarial training?",
        "masked_number": 1,
        "masked_elements": [
            "DINOv2"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.16468v1_figure_1.png",
        "paperid": "2411.16468v1",
        "paper_path": "./MISS-QA/papers/2411.16468v1.json",
        "figure_id": "2411.16468v1_figure_1.png",
        "caption": "Figure 1: Network architecture of Stage I.\nStage I uses HQ face videos to train HQ 3D-VQGAN (Ehsubscript𝐸ℎE_{h}italic_E start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT and Dhsubscript𝐷ℎD_{h}italic_D start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT) and spatial and temporal codebooks (𝒞Ssubscript𝒞𝑆\\mathcal{C}_{S}caligraphic_C start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT and 𝒞Tsubscript𝒞𝑇\\mathcal{C}_{T}caligraphic_C start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT).\n(a) illustrates the quantization operation STLookUp through two codebooks in our proposed framework. (b) and (c) display the computation process of temporal attention and motion residual, respectively.\n(d) We leverage a pre-trained feature network DINOv2 and trainable multi-scale discriminator heads to construct a more powerful discriminator for stable training.",
        "qtype": "Design_Rationale",
        "response": "Here is the step-by-step reasoning:\n\n1.  Training a video-level VQGAN discriminator from scratch is notoriously unstable and prone to producing artifacts, especially when you demand both high-fidelity details and strict temporal consistency.  \n2.  A large pre-trained feature extractor (e.g. DINOv2) has already learned very rich, general–purpose visual representations at multiple levels of abstraction and across scales.  By freezing it, you inherit its stable, semantically meaningful features without having to relearn them.  \n3.  On top of those frozen features you attach several lightweight “multi-scale” heads—each one looks at a different resolution or feature-map size.  This lets you judge realism simultaneously at coarse (global layout, face shape) and fine (skin texture, micro-expressions, motion cues) scales.  \n4.  During adversarial training, the generator gets gradients from all of these heads.  The frozen pre-trained backbone ensures those gradients remain well-behaved and semantically grounded, while the multi-scale heads drive the generator to refine details and enforce consistency at every level.  \n   \nIn short, freezing a strong, pre-trained network like DINOv2 gives you stable, meaningful feature spaces right out of the box, and adding multiple discriminator heads ensures your adversarial loss “sees” both global structure and fine detail—together yielding much more stable training and far fewer flickers or artifacts."
    },
    {
        "question": "What motivates combining losses L1 and L2 within Prompt-aware Contrastive Learning?",
        "relevant_section_ids": [
            "3.3",
            "3.3.1",
            "3.3.2"
        ],
        "relevant_context": [
            "However, to further enhance the model’s understanding of the relationship between hatred and non-hatred at the feature level, we incorporate contrastive learning to improve the quality of feature distribution for samples.",
            "For mask feature vectors corresponding to samples of the same category, their distances in the feature space should tend to be close, while for mask feature vectors corresponding to samples of different categories, their distances in the feature space should tend to be increased.",
            "For each sample in a batch, the [mask] feature vector corresponding to the inference instance region in the sample’s sequence is considered as a positive example, paired with the label feature vector from the region of demonstrations with the same class. Simultaneously, it is treated as a negative example when paired with the label feature vector from the region of demonstrations with different class labels. This process serves to minimize the distance between positive examples and maximize the distance between negative examples, thereby expediting the aggregation and divergence process of z_mask.",
            "Finally, the overall loss for our approach is: L = L_cross + λ₁ L₁ + λ₂ L₂, where λ₁ and λ₂ are hyperparameters representing the weights assigned to different sub-losses."
        ],
        "final_answer": "They combine the two contrastive‐learning losses because each addresses a complementary aspect of feature‐space structure. L₁ (category‐oriented contrastive loss) pulls together mask representations of the same class and pushes apart those of different classes, while L₂ (prompt‐oriented contrastive loss) aligns each mask representation specifically with its correct prompt token and away from the wrong one. Together, they sharpen the overall feature distribution—both at the sample‐to‐sample level and at the sample‐to‐prompt level—thus improving discriminability and aiding the classifier.",
        "relevant_elements": [
            "Prompt-aware Contrastive Learning",
            "L1",
            "L2"
        ],
        "id": 1916,
        "masked_question": "What motivates combining losses [mask1] and [mask2] within Prompt-aware Contrastive Learning?",
        "masked_number": 2,
        "masked_elements": [
            "L1",
            "L2"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.07527v1_figure_2.png",
        "paperid": "2411.07527v1",
        "paper_path": "./MISS-QA/papers/2411.07527v1.json",
        "figure_id": "2411.07527v1_figure_2.png",
        "caption": "Figure 2: Overview of Pen Framework",
        "qtype": "Design_Rationale",
        "response": "First, let’s identify exactly what the two colored boxes represent in the figure under “Prompt‐aware Contrastive Learning”:\n\n  • The red box (which we’ll call [mask1]) is the *category-oriented* contrastive loss (often denoted L₁), where you take the [mask] embeddings of *different inference instances* in the same batch and pull together those of the *same* label, while pushing apart those of *different* labels.  \n  • The blue box (which we’ll call [mask2]) is the *prompt-oriented* contrastive loss (often denoted L₂), where for *a single* inference instance you pull its [mask] embedding closer to the special token embedding of a *same-label* demonstration (e.g. the “[bad]” token if the instance is hateful) and push it away from the opposite label’s token.\n\nWhy do we combine them?  \n– L₁ alone shapes the *global* feature space so that hateful examples cluster together and non-hateful examples cluster together, but it doesn’t explicitly tie your inference [mask] back to the actual prompt tokens you’re using as “anchors.”  \n– L₂ alone aligns each [mask] embedding with its corresponding prompt token, but it doesn’t enforce that two *different* hateful inferences should resemble each other, nor that hateful and non-hateful should be well separated in the global feature space.  \n\nBy summing them (L₁ + L₂) you therefore get the best of both worlds:  \n  1. a **coherent category-level** layout of your [mask] features across *all* samples, and  \n  2. a **tight, semantically grounded alignment** between each instance’s [mask] and the exact prompt token that signifies its label.  \n\nTogether this dual‐view contrastive objective both reduces *intra-class* variance and increases *inter-class* margins globally, while at the same time grounding each example in its local prompt context. This is exactly what motivates combining the two losses."
    },
    {
        "question": "How are the embeddings from Roberta fed into the LSTM modules to extract distinct region global features?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Next, we feed S into a PLM. Specifically, we employ the Roberta-large model Liu et al. (2019 ###reference_b16###) to obtain the overall embedding features H, where H represents the dimension of the hidden layers in the PLM, and n denotes the length of the entire sequence.",
            "Next, we employed Long Short-Term Memory (LSTM) networks to extract global information from the encoded representations of the three regions (X₁, X₂, and X₃), resulting in global information for inference instances and demonstrations: g_{ii}, g_{ht}, and g_{nht}."
        ],
        "final_answer": "After constructing the prompt-augmented input sequence S and passing it through Roberta-large to produce token-level embeddings H, the model splits H into three consecutive segments (X₁ for the inference instance, X₂ for the hateful demonstration, and X₃ for the non-hateful demonstration). Each of these three region-specific embedding sequences is then fed separately into its own LSTM network. The three LSTM modules process their respective region embeddings and output three fixed-length ‘global’ vectors (g_{ii}, g_{ht}, and g_{nht}), each summarizing the information in one of the regions.",
        "relevant_elements": [
            "Roberta",
            "LSTM",
            "Region global feature"
        ],
        "id": 1917,
        "masked_question": "How are the embeddings from [mask1] fed into the LSTM modules to extract distinct region global features?",
        "masked_number": 1,
        "masked_elements": [
            "Roberta"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.07527v1_figure_2.png",
        "paperid": "2411.07527v1",
        "paper_path": "./MISS-QA/papers/2411.07527v1.json",
        "figure_id": "2411.07527v1_figure_2.png",
        "caption": "Figure 2: Overview of Pen Framework",
        "qtype": "Implementation_Details",
        "response": "Here’s how it works, step by step:\n\n1.  The red‐boxed module in the figure is simply the RoBERTa encoder.  You take your full, concatenated input sequence  \n    (inference text + image caption + two demo captions + prompt) and feed it through RoBERTa.  \n    The output is a matrix  \n       E ∈ ℝn×d  \n    of token‐level embeddings (n tokens, d hidden size).\n\n2.  Because we deliberately laid out the sequence in four fixed‐length “regions” (blue=inference, red=hateful demo, green=non-hateful demo, orange=prompt), we know exactly which slice of E belongs to which region.\n\n3.  We then split E into three contiguous chunks  \n       E_blue ∈ ℝn_b×d,  \n       E_red  ∈ ℝn_r×d,  \n       E_green∈ ℝn_g×d  \n    by simply slicing along the token dimension at the pre-fixed boundaries.\n\n4.  Each of those three chunks is fed **independently** into its own LSTM module:\n     -  LSTM_blue(E_blue)  →   g_inference  \n     -  LSTM_red(E_red)    →   g_hateful_demo  \n     -  LSTM_green(E_green)→   g_nonhateful_demo  \n\n5.  The final hidden (or pooled) state from each LSTM is taken as the “region global feature” for that region.\n\nIn this way, the single RoBERTa embedding stream is routed—by simple slicing—to three parallel LSTMs, each of which collapses its slice into a fixed-size global representation."
    },
    {
        "question": "How does Option Order Sensitivity Test shuffle options to generate Shuffled Dataset from a Multichoice Dataset?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The list O_i is randomly shuffled to generate O_i', ensuring that the index of the correct answer A_i in O_i' differs from its index in O_i, thereby altering the correct answer's position.",
            "The final prompts, both before and after the shuffling, are the concatenation of the image, question, and choices: P_i = concat(I_i, Q_i, O_i) and P_i' = concat(I_i, Q_i, O_i'), where I_i and Q_i remain constant."
        ],
        "final_answer": "For each multiple-choice item, Option Order Sensitivity Test takes the original list of choices O_i and applies a random permutation to it, yielding a new choice list O_i'. It ensures that the correct answer’s position in O_i' is different from its original index in O_i. The shuffled dataset is then formed by keeping the same image and question but replacing O_i with the permuted choices O_i'.",
        "relevant_elements": [
            "Option Order Sensitivity Test",
            "Multichoice Dataset"
        ],
        "id": 1919,
        "masked_question": "How does [mask1] shuffle options to generate Shuffled Dataset from a Multichoice Dataset?",
        "masked_number": 1,
        "masked_elements": [
            "Option Order Sensitivity Test"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.03823v1_figure_1.png",
        "paperid": "2411.03823v1",
        "paper_path": "./MISS-QA/papers/2411.03823v1.json",
        "figure_id": "2411.03823v1_figure_1.png",
        "caption": "Figure 1: A description of Multimodal Data Contamination (left) and the overview of proposed MM-Detect framework (right).",
        "qtype": "Implementation_Details",
        "response": "Let’s walk through how the “Option Order Sensitivity Test” (the module boxed in red) turns a standard multiple‐choice set into its Shuffled Dataset:\n\n1. Start with your original dataset D of n examples.  Each example i provides  \n   - an image xᵢ  \n   - a question qᵢ  \n   - a list of m answer choices Cᵢ = [cᵢ,₁, cᵢ,₂, …, cᵢ,ₘ]  \n   - a correct‐answer index aᵢ (so cᵢ,ₐᵢ is the right choice).\n\n2. For each example i, you apply a random permutation π to the indices {1,…,m}.  \n   • You shuffle Cᵢ → Cᵢ′ = [cᵢ,_{π(1)}, cᵢ,_{π(2)}, …, cᵢ,_{π(m)}].  \n   • You check that π(aᵢ) ≠ aᵢ.  If the correct answer landed back in its original slot, you re-draw π until it moves.\n\n3. The result is a “shuffled” choice list Cᵢ′ in which the same answers appear but in a new, random order and the correct answer now sits at a different index aᵢ′=π(aᵢ).\n\n4. Collecting these (xᵢ, qᵢ, Cᵢ′) forms your Shuffled Dataset.  You then compare model accuracy on the original order versus this shuffled order to detect whether the model has memorized canonical option positions.\n\nIn short: mask1 (“Option Order Sensitivity Test”) simply randomizes the order of the m choices per question—re-sampling until the correct option moves—thereby producing the Shuffled Dataset."
    },
    {
        "question": "How does Slot Guessing for Perturbation Captions integrate back-translation with mask word prediction?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "This method is based on the intuition that if a model can predict a missing part of a sentence but fails with the back-translated version (from English to Chinese, then back to English), it likely indicates that the model has encountered the original sentence during training.",
            "We first apply a back-translation function111We use Google-Translate API for Python to implement the back-translation. to Ci: to obtain the back-translated sentence Ci'. Next, we perform keyword extraction222We employ the Stanford POS Tagger (Toutanvoa & Manning, 2000  ###reference_b27###), targeting nouns, adjectives, or verbs, as they encapsulate the sentences’ core meaning, on both Ci and Ci': where Ki and Ki' are the keywords extracted from Ci and Ci', respectively. We then use a masking function M to replace the keywords in the sentences with [MASK].",
            "The final prompt can be represented as the concatenation of the image, the instruction and the masked sentence: where I is the image and Instr is the instruction guiding the model to complete the mask word prediction task."
        ],
        "final_answer": "Slot Guessing for Perturbation Captions first back‐translates each original caption (e.g. English → Chinese → English) to obtain a paraphrased version, then extracts keywords from both the original and back‐translated captions, masks those keywords ([MASK]) in each sentence, and finally asks the model—given the image and a fill‐in‐the‐blank prompt—to predict the masked words. A drop in performance on the back‐translated masked prompts versus the original masked prompts signals potential data contamination.",
        "relevant_elements": [
            "Slot Guessing for Perturbation Captions",
            "Mask Word Prediction"
        ],
        "id": 1920,
        "masked_question": "How does [mask1] integrate back-translation with [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Slot Guessing for Perturbation Captions",
            "Mask Word Prediction"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.03823v1_figure_1.png",
        "paperid": "2411.03823v1",
        "paper_path": "./MISS-QA/papers/2411.03823v1.json",
        "figure_id": "2411.03823v1_figure_1.png",
        "caption": "Figure 1: A description of Multimodal Data Contamination (left) and the overview of proposed MM-Detect framework (right).",
        "qtype": "Implementation_Details",
        "response": "Chain of thought:\n\n1. The red‐boxed module is “Slot Guessing for Perturbation Captions,” which is our perturbation‐generation recipe for image captioning.  \n2. The blue‐boxed label beneath it reads “Mask Word Prediction,” which is the downstream fill‐in‐the‐blank task we ask the model to perform.  \n3. Inside the red box we see that we (a) take the original caption, (b) back‐translate it (English→Chinese→English) to produce a paraphrase, (c) extract keywords (“slots”) from both versions, and (d) replace those slots with [MASK].  \n4. We then feed these masked original and masked back‐translated captions (together with the image) into the model and ask it to recover the missing words—that is, to do a mask‐word prediction on both the clean and the back‐translated text.  \n5. In this way, the Slot Guessing module integrates back‐translation as the source of a semantically‐equivalent but lexically different perturbation, and Mask Word Prediction as the probe for whether the model truly understands the image–caption pair or merely memorized the exact training string.\n\nAnswer:\n\nSlot Guessing for Perturbation Captions integrates back‐translation by first using a translate-and-reverse step (English→Chinese→English) to generate a paraphrased caption, then masking the extracted keyword slots in both the original and back-translated captions, and finally asking the model to perform mask word prediction on those slots."
    },
    {
        "question": "How does Option Order Sensitivity Test shuffle options to generate Shuffled Dataset from a Multichoice Dataset?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "The list O_i is randomly shuffled to generate O_i', ensuring that the index of the correct answer A_i in O_i' differs from its index in O_i, thereby altering the correct answer's position.",
            "The final prompts, both before and after the shuffling, are the concatenation of the image, question, and choices: P_i = concat(I_i, Q_i, O_i) and P_i' = concat(I_i, Q_i, O_i'), where I_i and Q_i remain constant."
        ],
        "final_answer": "For each multiple-choice item, Option Order Sensitivity Test takes the original list of choices O_i and applies a random permutation to it, yielding a new choice list O_i'. It ensures that the correct answer’s position in O_i' is different from its original index in O_i. The shuffled dataset is then formed by keeping the same image and question but replacing O_i with the permuted choices O_i'.",
        "relevant_elements": [
            "Option Order Sensitivity Test",
            "Multichoice Dataset"
        ],
        "id": 1919,
        "masked_question": "How does [mask1] shuffle options to generate Shuffled Dataset from a Multichoice Dataset?",
        "masked_number": 1,
        "masked_elements": [
            "Option Order Sensitivity Test"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.03823v1_figure_1.png",
        "paperid": "2411.03823v1",
        "paper_path": "./MISS-QA/papers/2411.03823v1.json",
        "figure_id": "2411.03823v1_figure_1.png",
        "caption": "Figure 1: A description of Multimodal Data Contamination (left) and the overview of proposed MM-Detect framework (right).",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the diagram the red‐boxed module is the “Option Order Sensitivity Test.”  \n2. Its job is to take a standard multiple‐choice example (question + a list of choices) and perturb it by re‐ordering (“shuffling”) the choices.  \n3. Concretely, for each example i with choice list Oᵢ = [o₁, o₂, …, oₙ], you apply a random permutation σ to get Oᵢ′ = σ(Oᵢ).  \n4. You enforce that the correct answer’s position in Oᵢ′ is not the same as in Oᵢ (so the model can’t just rely on “always pick B,” say).  \n5. The newly ordered list Oᵢ′ then forms the Shuffled Dataset, which you feed back to the model to see if its accuracy drops.  \n\nAnswer  \n[mask1] randomly permutes (shuffles) the list of answer options for each multiple‐choice example—i.e. it draws a random permutation of the choices and reorders them so that the correct answer ends up in a different slot, yielding the Shuffled Dataset."
    },
    {
        "question": "How might Context Optimizer’s Token-based Encoding draw on subword tokenization methodologies to reduce vocabulary mismatches?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Context Optimizer",
            "Token-based Encoding"
        ],
        "id": 1923,
        "masked_question": "How might [mask1]’s Token-based Encoding draw on subword tokenization methodologies to reduce vocabulary mismatches?",
        "masked_number": 1,
        "masked_elements": [
            "Context Optimizer"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.09434v2_figure_1.png",
        "paperid": "2408.09434v2",
        "paper_path": "./MISS-QA/papers/2408.09434v2.json",
        "figure_id": "2408.09434v2_figure_1.png",
        "caption": "Figure 1: Hysem Architecture diagram",
        "qtype": "Literature_Background",
        "response": "Let’s unpack what’s going on in the Context Optimizer’s “Token‐based Encoding” and how it very much echoes the ideas behind subword tokenizers like BPE or WordPiece.\n\n1. The problem they’re solving  \n   • In domains like pharma you get long, unseen words (“Amoxycillin”, “preeclampsia”, GUIDs, dates, etc.) that the base LLM tokenizer doesn’t know.  \n   • As a result each of those words gets chopped into five, ten, fifteen little sub‐tokens, bloating your context window and hurting both speed and accuracy.\n\n2. How subword methods work at a high level  \n   • You start with characters (or bytes), then merge the most frequent pairs into “subwords,” iterating until you have a fixed-size vocabulary of common subwords.  \n   • New words are then decomposed into those subwords, guaranteeing you never see a completely unknown chunk but only familiar pieces.\n\n3. What HySem’s Token-based Encoding does similarly  \n   a. It first extracts every distinct cell string from the table and “tokenizes” it with the LLM’s own vocabulary.  \n   b. It then sorts those cells by how many tokens they would normally require—shortest first—so that the easiest mappings get claimed before the trickier, longer ones (this helps avoid collisions).  \n   c. For each cell, instead of accepting however many LLM‐tokens it originally needed, it tries to “pack” that entire cell into at most two tokens.  \n      – If it’s already one token, leave it alone (single‐token preservation).  \n      – If it’s more, look for the best two‐token split that (a) still uniquely identifies that string in the table and (b) ideally reuses whole subwords from the base tokenizer rather than chopping into new ones.  \n   d. It also has special bracket-aware logic so it doesn’t accidentally split inside “{…}” or “[…]” and create invalid JSON.\n\n4. Why this is just a dynamic, one‐off subword vocabulary  \n   • In BPE or WordPiece you build a static merge table once (offline) on a huge corpus.  \n   • Here, they build a tiny “merge table” on the fly for that single HTML table.  The procedure “learns” the best two‐token surrogates for each cell string, then records a mapping.  \n   • Downstream, the LLM sees only these short, familiar tokens (no repeated unknown sub‐splits), so it burns far fewer context slots on domain-specific jargon.\n\n5. The payoff  \n   • You end up reducing the overall token count—often by 30–40% or more—without sacrificing the exact string content (because the decoder simply looks up each surrogate token and restores the original text).  \n   • You’ve effectively applied the same core trick as subword tokenizers—“merge frequent or domain-specific sequences into single tokens”—but in a dynamic, per‐table manner so you never have to retrain your tokenizer on every new pharma data set.\n\nIn short, HySem’s Token-based Encoding is a just-in-time, table-specific variant of subword tokenization.  By greedily collapsing each high-cardinality cell string into one or two tokens drawn from the LLM’s own vocabulary—and by dynamically resolving collisions in ascending token‐length order—it minimizes out-of-vocabulary splits and slashes token count, yet still allows lossless reconstruction of the original table text."
    },
    {
        "question": "How does Syntax Corrector’s LLM–Code Executor interaction reflect iterative refinement methodologies?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Syntax errors in the LLM-generated JSON output render the table unusable for further processing, such as ingestion into databases. Consequently, correcting these syntax errors is a critical functionality, especially for enabling automated workflows in industrial settings.",
            "To address these challenges, we developed a Syntax Corrector, based on reflective agentic framework.",
            "Specifically, the Syntax Corrector accepts a syntactically invalid JSON as input and produces a syntactically valid JSON through iterative refinement. Through self-reflection..., iteratively refines the JSON output until a syntactically valid result is achieved or the maximum number of iterations is reached."
        ],
        "final_answer": "The Syntax Corrector implements an LLM–Code Executor loop in which the LLM proposes corrections to a piece of JSON, the Code Executor runs or validates that JSON to detect any syntax errors, and the resulting error feedback is fed back into the LLM. This cycle of “generate → execute → reflect → regenerate” is repeated—i.e., iteratively refined—until the JSON is syntactically valid or a preset iteration limit is reached.",
        "relevant_elements": [
            "Syntax Corrector",
            "LLM",
            "Code Executor"
        ],
        "id": 1924,
        "masked_question": "How does [mask1]’s LLM–Code Executor interaction reflect iterative refinement methodologies?",
        "masked_number": 1,
        "masked_elements": [
            "Syntax Corrector"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.09434v2_figure_1.png",
        "paperid": "2408.09434v2",
        "paper_path": "./MISS-QA/papers/2408.09434v2.json",
        "figure_id": "2408.09434v2_figure_1.png",
        "caption": "Figure 1: Hysem Architecture diagram",
        "qtype": "Literature_Background",
        "response": "First, let’s align the red‐boxed region in the figure with the text.  The red box encloses the “Syntax Corrector” module, which (per the caption and §2.3 of the paper) consists of two parts:\n\n  •  An LLM agent  \n  •  A Code Executor  \n\nWhen the Semantic Synthesizer spits out a piece of JSON that has syntax errors, that invalid JSON is fed into the Syntax Corrector.  Here’s how the LLM–Code Executor loop works, step by step:\n\n1.  The LLM reads the invalid JSON and proposes a set of edits (add missing commas, close brackets, fix quotes, etc.).  \n2.  The Code Executor takes the LLM’s corrected version and actually tries to parse it (or run lightweight validation code) to see if it is now valid.  \n3.  If the Code Executor still detects an error, it returns the failure message (e.g. “unexpected token at position 23”) back to the LLM.  \n4.  The LLM uses that feedback to further refine its next correction.  \n5.  Steps 2–4 repeat until the JSON parses cleanly or a maximum iteration count is hit.\n\nThis back‐and‐forth between “propose a fix” (LLM) and “check/validate the fix” (executor) is precisely an iterative‐refinement loop.  Each cycle uses the executor’s concrete error output to guide the LLM’s next edit, steadily improving the JSON’s syntax until it is guaranteed valid."
    },
    {
        "question": "How does Token-based Encoding reduce token count while preserving semantic uniqueness across table cells?",
        "relevant_section_ids": [
            "2.1.1"
        ],
        "relevant_context": [
            "Next, we apply our custom token-based encoding technique. The goal here is to represent the content of each cell with the minimum number of tokens while ensuring that each cell has a unique representation.",
            "Prior to encoding, we first sort the cells in ascending order based on the number of tokens they contain. This strategy allows us to resolve potential collisions more easily, as cells with fewer tokens are processed first. A collision occurs when two distinct cell contents map to overlapping token sequences.",
            "Our encoding process incorporates several high-level heuristics to enhance efficiency and accuracy:\n a. Single Token Preservation: If a cell’s content consists of a single token, it remains unchanged.\n b. Multi-Token Optimization: For cells with multiple tokens, we aim to represent the content using only two tokens whenever possible.\n c. Bracket Handling: We handle incomplete bracket sequences by checking if a token starts with an opening bracket (e.g., [, {) and lacks a matching closing bracket. In such cases, we concatenate subsequent tokens until the bracket is closed.",
            "The overall objective is to use the fewest tokens possible while maintaining uniqueness across all cell contents. By treating each tokenized cell as a unit, we can reduce the total number of tokens significantly without losing semantic integrity."
        ],
        "final_answer": "Token-based Encoding reduces token count by first sorting cells by their initial token length to avoid collisions, then applying heuristics that preserve single-token cells unchanged, represent multi-token cells in only two tokens where possible, and handle bracketed sequences in a way that prevents syntax errors. This approach ensures each cell is encoded into a minimal yet unique token sequence, thereby reducing overall tokens while retaining semantic distinctions between cells.",
        "relevant_elements": [
            "Context Optimizer",
            "Token-based Encoding"
        ],
        "id": 1925,
        "masked_question": "How does [mask1] reduce token count while preserving semantic uniqueness across table cells?",
        "masked_number": 1,
        "masked_elements": [
            "Token-based Encoding"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.09434v2_figure_1.png",
        "paperid": "2408.09434v2",
        "paper_path": "./MISS-QA/papers/2408.09434v2.json",
        "figure_id": "2408.09434v2_figure_1.png",
        "caption": "Figure 1: Hysem Architecture diagram",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought:\n\n1.  We know from the red‐boxed component that [mask1] is the “Token‐based Encoding” stage inside the Context Optimizer.  \n2.  Its goal is to shrink the number of tokens that an HTML table would generate under the LLM’s native tokenizer, without confusing one cell’s meaning for another.  \n3.  First, all cells are tokenized with the base tokenizer and then sorted by their original token‐length (shortest first).  This ordering helps catch and resolve any “collisions” (two different cells accidentally mapping to the same short token sequence) as early as possible.  \n4.  Next, for each cell:\n   •  If it already fits in one token, leave it alone.  \n   •  Otherwise, greedily attempt to re-encode it into a single token drawn from the existing vocabulary (often by merging sub‐tokens or domain‐terms) or, if that would lose too much meaning, into exactly two tokens.  \n   •  Special bracket-pair handling makes sure we don’t split JSON‐critical characters across encodings.  \n5.  At each step we check against all previously encoded cells to guarantee uniqueness (no two distinct cells get the same encoded token sequence).  \n6.  The result is an “encoded” HTML table where each cell is represented by as few as one or two tokens, yet every cell remains semantically distinguishable.  A side‐table of these mappings is kept so that, after the LLM finishes, we can decode the short tokens back into the original cell text.\n\nAnswer:\n\nToken‐based Encoding slashes token counts by collapsing multi‐word or out‐of‐vocabulary phrases into one or two compact tokens. It does so by (1) sorting cells by length, (2) trying a one‐token rewrite first (falling back to two tokens if needed), (3) handling brackets to avoid JSON errors, and (4) checking each new encoding against earlier ones to maintain a one‐to‐one mapping. This dynamic, per‐table re-encoding preserves uniqueness across all cells while dramatically reducing the total token budget."
    },
    {
        "question": "How does the Code Executor enhance iterative syntax refinement within the Syntax Corrector framework?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Syntax Corrector",
            "Code Executor"
        ],
        "id": 1926,
        "masked_question": "How does the [mask1] enhance iterative syntax refinement within the Syntax Corrector framework?",
        "masked_number": 1,
        "masked_elements": [
            "Code Executor"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.09434v2_figure_1.png",
        "paperid": "2408.09434v2",
        "paper_path": "./MISS-QA/papers/2408.09434v2.json",
        "figure_id": "2408.09434v2_figure_1.png",
        "caption": "Figure 1: Hysem Architecture diagram",
        "qtype": "Experimental_Results",
        "response": "First, note that the red-boxed component in the Syntax Corrector is the “Code Executor.”  In our agentic Syntax Corrector loop, the LLM proposes a corrected JSON, then immediately submits it to the Code Executor.  The executor tries to parse/execute that JSON, returns any syntax‐error messages or exceptions, and hands that feedback straight back to the LLM.  By running the candidate JSON through a real parser, the Syntax Corrector gets concrete, line‐and‐column–level error reports, which the LLM uses in the next iteration of self-reflection to fix just those mistakes.  In this way, the Code Executor closes the loop—surfacing precise syntax failures so the LLM can iteratively refine the JSON until it actually parses without error."
    },
    {
        "question": "How does global-local cooperation merge p_G and p_L to enhance feature alignment under frozen encoders?",
        "relevant_section_ids": [
            "5.1"
        ],
        "relevant_context": [
            "We enhance cooperation between the local and global learning processes by merging both local and global features to create the final text feature.",
            "The text feature is produced as follows: \\nwhere θ serves as a coefficient to balance the mix of the two features, which addresses the balancing between personalization and generalization. The variation in the parameter θ influences the outcomes of the inference. Specifically, when θ=0, the algorithm reverts to PrompFL, whereas at θ=1, it shifts to CoOp. Our approach consists of combining these features and using the resulting mixed feature to determine their similarity."
        ],
        "final_answer": "Global-local cooperation linearly combines the frozen‐encoder text features of the global prompt p_G and the local prompt p_L via a mixing weight θ. Concretely, it forms the final text feature as (1−θ)·h(p_G)+(θ)·h(p_L), thereby leveraging both global generalization and local personalization under frozen encoders.",
        "relevant_elements": [
            "global-local cooperation",
            "global prompt p_G",
            "local prompt p_L"
        ],
        "id": 1927,
        "masked_question": "How does [mask1] merge p_G and p_L to enhance feature alignment under frozen encoders?",
        "masked_number": 1,
        "masked_elements": [
            "global-local cooperation"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.19610v1_figure_1.png",
        "paperid": "2409.19610v1",
        "paper_path": "./MISS-QA/papers/2409.19610v1.json",
        "figure_id": "2409.19610v1_figure_1.png",
        "caption": "Figure 1: The image demonstrates the framework of the PromptFolio algorithm. The algorithm updates the global prompt and local prompt while keeping the weights of the fixed vision-language pretrained model unchanged. Additionally, it aggregates the global prompts from each client. The right side of the image intuitively demonstrates the advantages of global-local cooperation for performance when global and local are treated as two assets.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed “Global–Local Cooperation” block is nothing more exotic than the mixing step described in §4.2 of the paper.  In practice, we keep both the vision encoder g(⋅) and the text encoder h(⋅) frozen, and we learn two prompt vectors:\n\n  • p_G : the “global” prompt (aggregated across all clients)  \n  • p_L : the “local” prompt (trained on the current client)  \n\nAt inference time the cooperation block simply takes a convex‐combination of those two vectors,\n\n   p⋆  =  (1–θ)·p_G  +  θ·p_L \n\n(θ can be a fixed hyper‐parameter or the theoretically derived optimal mix).  That mixed prompt p⋆ is then fed through the frozen text encoder h(⋅) to yield a final text feature,\n\n   t⋆  =  h(p⋆)  \n\nwhich is directly compared to the frozen image feature g(x).  By blending p_G and p_L in this way, the method fuses the client‐specific “local” information with the more broadly trained “global” information, yielding a text embedding that better lines up with the frozen image embedding and thus improves classification accuracy under non‐i.i.d. data."
    },
    {
        "question": "What limitations arise when freezing the image encoder while updating global and local prompts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "image encoder",
            "global prompt",
            "local prompt"
        ],
        "id": 1929,
        "masked_question": "What limitations arise when freezing the [mask1] while updating global and local prompts?",
        "masked_number": 1,
        "masked_elements": [
            "image encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.19610v1_figure_1.png",
        "paperid": "2409.19610v1",
        "paper_path": "./MISS-QA/papers/2409.19610v1.json",
        "figure_id": "2409.19610v1_figure_1.png",
        "caption": "Figure 1: The image demonstrates the framework of the PromptFolio algorithm. The algorithm updates the global prompt and local prompt while keeping the weights of the fixed vision-language pretrained model unchanged. Additionally, it aggregates the global prompts from each client. The right side of the image intuitively demonstrates the advantages of global-local cooperation for performance when global and local are treated as two assets.",
        "qtype": "Others",
        "response": "Here’s how I reason about the question step‐by‐step:\n\n1.  Identify the [mask1]  \n    In the red box in the diagram sits the vision-language backbone’s image encoder (the block labeled “g Image Encoder” with the little snowflake beside it).  That is what they have frozen.\n\n2.  What “freezing” means here  \n    Freezing a module means we never change its weights during any of the client-side local updates or the server-side aggregation.  Only the text prompts (global and local) are allowed to move.\n\n3.  Consequences of freezing the image encoder  \n    –  **No adaptation of visual features.**  Any mismatch between the pre-trained encoder’s features and the actual data distribution on a client cannot be corrected by gradient steps in the backbone.  \n    –  **All learning must be “squeezed” into the prompts.**  If the frozen encoder does not already extract the perfectly discriminative features for a client’s peculiar skew (e.g. unusual lighting, background, or object shape) then the prompts alone are often too low-dimensional to compensate.  \n    –  **Limited expressivity and under-fitting.**  You can only shift the text embedding that pairs with the fixed image embedding.  You cannot reshape the image space itself, so there is a hard ceiling on how much the model can improve on truly new or heavily skewed visual patterns.  \n    –  **Potentially sub-optimal alignment.**  The theory hinges on a good alignment between image and text spaces.  If that alignment is imperfect for some client, freezing the image side forbids you from ever tightening it—any such correction would require fine-tuning the encoder.\n\n4.  Summary answer  \n    By freezing the image encoder, you save on compute and communication, but you also give up the ability to learn any new or better visual features.  All personalization and global knowledge transfer must happen through a small set of prompts.  In strongly non-IID settings or when the pre-trained backbone is imperfect for your task, that can leave you under-fitting (the prompts alone cannot fully bridge the gap)."
    },
    {
        "question": "What alternative metric learning techniques could replace the pixel-wise margin loss to enhance asymmetry sensitivity without additional projection head?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Projection Head",
            "Margin Loss"
        ],
        "id": 1932,
        "masked_question": "What alternative metric learning techniques could replace the [mask1] to enhance asymmetry sensitivity without additional [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Margin Loss",
            "Projection Head"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.18290v1_figure_2.png",
        "paperid": "2411.18290v1",
        "paper_path": "./MISS-QA/papers/2411.18290v1.json",
        "figure_id": "2411.18290v1_figure_2.png",
        "caption": "Figure 2: (a) Our SATs model begins with the encoder-decoder module, which uses shared weights to process two input signals and encode them into a discriminative representation. This representation is then further processed through a non-linear projection module and a distance metric learning module to produce feature maps.\n(b) A graphical representation of our metric learning strategy.\nCircles indicate individual CT images, I𝐼Iitalic_I, while red squares highlight the tumors. The tumors are composed of A and B, representing symmetrical and asymmetrical lesions relative to the central sagittal axis of symmetry, respectively.",
        "qtype": "Others",
        "response": "Here is a sketch of how you might answer the question step by step:\n\n1. Identify mask₁ and mask₂  \n   - From the red outline in the figure and the surrounding text we see that [mask₁] is the “pixel-wise margin loss” (i.e. the voxel-level distance metric).  \n   - From the blue outline in the figure and the text it is clear that [mask₂] is the small non-linear “projection head” that sits on top of the encoder before computing that margin loss.\n\n2. Restate the question in plain English  \n   “What other kinds of metric‐learning losses could I plug in place of that voxel-wise margin loss, so that I don’t even need the extra projection head module, but I still get strong sensitivity to left-vs-right asymmetries?”\n\n3. Brainstorm alternative metric‐learning losses  \n   What we want is something that will:\n\n   • Pull features from normal (symmetric) voxels together  \n   • Push features from tumor or asymmetric voxels apart  \n\n   without the two‐layer MLP that the authors call the projection head.  In the metric‐learning literature there are a number of off-the-shelf losses that do exactly that:\n\n   – **Triplet loss**: pick an anchor voxel in the normal region, a “positive” voxel from its flipped counterpart (should be similar), and a “negative” voxel in the tumor region (should be dissimilar).  \n   – **N-pair or proxy losses** (Proxy-NCA, N-pair loss): allow you to compare an anchor to multiple positives and negatives at once.  \n   – **Supervised contrastive (SupCon) loss**: you define “classes” = {normal vs. tumor}, pull all normals together, push all tumors apart.  \n   – **InfoNCE / NT-Xent**: a popular self-supervised contrastive objective which can also be turned into a supervised version if you provide labels for tumor vs. healthy.  \n   – **Angular-margin classification losses** (SphereFace, CosFace, ArcFace): you simply treat “normal” and “tumor” as two classes, use a standard classification head but add an angular margin on the final dot‐product.  This both separates the two populations and eliminates the need for a separate projection module.  \n\n4. Explain why no projection head is needed  \n   All of the above losses operate directly on the encoded feature vectors (once they’ve been normalized to unit length if you like).  You can take the output of your last shared decoder layer, L2-normalize it, and feed it straight into, say, a triplet-or-ArcFace-style loss.  There is no need for the extra 1×1×1 Conv–ReLU–norm “projection head” block.\n\n5. Sample answer  \n\n“Mask₁ is the voxel-level margin (pixel-wise) contrastive loss, and mask₂ is the small projection-head MLP that sits on top of the encoder.  If you want to remove that extra projection head yet still drive apart features on the two sides of the sagittal plane, you can simply switch to any of the standard metric‐learning losses that operate directly on normalized embeddings.  For example:  \n • A classical triplet loss (anchor-positive-negative) defined between matching normal voxels and tumor voxels  \n • An N-pair or proxy-NCA loss that compares each voxel embedding against multiple positives/negatives  \n • A supervised contrastive (SupCon) loss that groups all normal embeddings and pushes all tumor embeddings away  \n • An InfoNCE/NT-Xent contrastive objective  \n • Even an angular‐margin softmax (ArcFace/CosFace/SphereFace) with “normal” vs. “tumor” as two classes  \n\nAll of these can be applied to L₂-normalized features straight out of the encoder/decoder, so you can drop the projection head entirely and still get a strong asymmetry‐sensitive embedding.”"
    },
    {
        "question": "Why integrate a non-linear projection head before computing margin loss on symmetric encoding features?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "We utilize a non-linear projection P to transform the features before calculating the distance in margin loss, which aims to enhance the quality of the learned features.",
            "We hypothesize that directly applying metric learning to segmentation features might lead to information loss and diminish the model’s effectiveness.",
            "For example, some asymmetries in CT images are non-pathological and may stem from variations in the patient’s head positioning and posing, yet they are beneficial for segmentation.",
            "Utilizing a non-linear projection can filter out such irrelevant information from the metric learning process, ensuring it is preserved in the features used for segmentation."
        ],
        "final_answer": "The non-linear projection head is inserted so that the margin loss is applied to a purified feature space: it filters out non-pathological asymmetries (e.g. pose or angle variations) that would otherwise corrupt the contrastive signal, while preserving those features critical for segmentation.",
        "relevant_elements": [
            "Projection Head",
            "Margin Loss"
        ],
        "id": 1934,
        "masked_question": "Why integrate a non-linear [mask1] before computing margin loss on symmetric encoding features?",
        "masked_number": 1,
        "masked_elements": [
            "Projection Head"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.18290v1_figure_2.png",
        "paperid": "2411.18290v1",
        "paper_path": "./MISS-QA/papers/2411.18290v1.json",
        "figure_id": "2411.18290v1_figure_2.png",
        "caption": "Figure 2: (a) Our SATs model begins with the encoder-decoder module, which uses shared weights to process two input signals and encode them into a discriminative representation. This representation is then further processed through a non-linear projection module and a distance metric learning module to produce feature maps.\n(b) A graphical representation of our metric learning strategy.\nCircles indicate individual CT images, I𝐼Iitalic_I, while red squares highlight the tumors. The tumors are composed of A and B, representing symmetrical and asymmetrical lesions relative to the central sagittal axis of symmetry, respectively.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1.  What the network produces before the red-boxed module  \n    – The twin encoder–decoder (“Siamese”) processes the original CT and its left-right flipped copy and spits out two feature‐maps E and Eₙ (both of size H×W×D×C).  \n    – Those are the “symmetric encoding features” on which we want to impose a margin loss, so that truly pathological asymmetries are pulled apart from their mirror points.\n\n2.  Why not just apply the margin loss directly to E and Eₙ?  \n    – In a segmentation network, those raw features E/Eₙ carry everything needed to reconstruct the mask: anatomical shape, texture, even benign asymmetries caused by slight head tilt, patient posture, CT noise, etc.  \n    – If you force a contrastive/margin loss straight on them, you risk “over-correcting” or throwing away information that the decoder needs to segment correctly.\n\n3.  What the projection head (the red-boxed “[mask1]”) does  \n    – It is a tiny non-linear mapping (three 1×1×1 conv layers + ReLU + unit‐norm) that takes E/Eₙ → D/Dₙ.  \n    – In other words, it carves out a separate embedding sub-space specifically for the metric learning loss.\n\n4.  The big benefit of that detour  \n    – The margin loss now only shapes D and Dₙ, not the full segmentation features E/Eₙ.  \n    – Any “non-pathological” wiggles (patient tilt, scanner drift, benign asymmetries) can be filtered or deemphasized in D without harming the decoder’s ability to segment from E.  \n    – Meanwhile, the projection head still gets updated via back-prop from the margin loss, so it learns exactly the type of asymmetry (tumor vs. mirror) we care about.\n\nIn short: we insert a small non-linear projection head before computing the margin loss so that  \n  • the metric‐learning pressure is confined to a low‐dimensional embedding space,  \n  • irrelevant asymmetry cues are filtered out, and  \n  • the core segmentation features remain free to capture everything the decoder needs."
    },
    {
        "question": "What motivates balancing accuracy loss and consistency loss during diversification training?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "For each training epoch of the stacked U-Net, two paths, P₁ and P₂, are randomly selected for each input batch, yielding two separate predictions, ŷ₁ and ŷ₂. The loss function is then augmented with a regularization term that addresses the discrepancy between these two predictions, in addition to the standard error term that measures the deviation between the ground truth y and the prediction ŷ.",
            "In this manuscript, two losses are balanced using the coefficient λ. For simplicity, λ is set to 1, but it can be adjusted according to user requirements.",
            "During back-propagation, only the candidate blocks along the active path, P₂, responsible for the prediction ŷ₂, are updated. This focused updating approach helps in stabilizing the learning process and refining the model’s performance by iteratively adjusting only the actively involved blocks, illustrated in Algorithm 1."
        ],
        "final_answer": "Balancing the accuracy loss (deviation from ground truth) with the consistency loss (discrepancy between two model predictions) is motivated by the need to diversify and regularize the ensemble without sacrificing predictive performance. By weighting both terms, the model is encouraged to remain accurate while also maintaining agreement among its diversified sub-models, leading to a more stable optimization and improved overall performance.",
        "relevant_elements": [
            "accuracy loss",
            "consistency loss"
        ],
        "id": 1935,
        "masked_question": "What motivates balancing [mask1] and consistency loss during diversification training?",
        "masked_number": 1,
        "masked_elements": [
            "accuracy loss"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.05324v1_figure_1.png",
        "paperid": "2411.05324v1",
        "paper_path": "./MISS-QA/papers/2411.05324v1.json",
        "figure_id": "2411.05324v1_figure_1.png",
        "caption": "Fig. 1: The SASWISE pipeline efficiently estimates uncertainty while maintaining or enhancing pre-trained model performance. It begins by training a supervised model to convergence, followed by creating multiple candidate blocks in each block position. These blocks are shuffled and recombined into new models. In the diversification stage, two unique models are selected from the pool and trained on the same data sample. This stage involves calculating and utilizing the accuracy loss between the model being updated and the ground truth, along with the consistency loss between the two models, to only update the model being refined. After enough diversification training epochs, the best models from the partial or complete model pool are used to generate results from a single input. The final results for tasks with continuous or discrete data types are determined using median or majority voting methods, respectively, with uncertainty maps produced using standard deviation or majority ratio.",
        "qtype": "Design_Rationale",
        "response": "Here the authors are adding a second term—the “consistency‐loss” between two randomly sampled paths—to their usual supervised (accuracy) loss.  They then balance the two:\n\n L total = LACC (the standard error to the ground truth, i.e. accuracy loss)  \n    + λ·LCON (the disagreement or consistency loss between the two model outputs)\n\nBy tuning λ you force each newly trained path both (a) to stay close to the labelled data (so you don’t lose accuracy) and (b) to differ from the other path (so you grow diversity in your pool of blocks).  In short, you balance accuracy and consistency losses in order to get an ensemble that is simultaneously high‐performing and sufficiently diverse."
    },
    {
        "question": "What motivates using median on the result pool for fusing continuous predictions?",
        "relevant_section_ids": [
            "1",
            "2.3"
        ],
        "relevant_context": [
            "After empirically enough diversification training epochs, either the best selective model pool or the complete model pool is used to generate a results pool from a single input data point. For continuous or discrete data type tasks, the final result is produced using median or majority vote methods, respectively.",
            "In contrast, for continuous variables, the median of the outputs is used to achieve a central tendency, as below where  and  is sorted."
        ],
        "final_answer": "The median is used on the result pool for continuous predictions in order to capture the central tendency of the set of outputs, providing a robust single prediction that represents the ‘middle’ of the diverse continuous estimates.",
        "relevant_elements": [
            "Result Pool of 128 Predictions",
            "Median"
        ],
        "id": 1936,
        "masked_question": "What motivates using [mask1] on the [mask2] for fusing continuous predictions?",
        "masked_number": 2,
        "masked_elements": [
            "Median",
            "Result Pool of 128 Predictions"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.05324v1_figure_1.png",
        "paperid": "2411.05324v1",
        "paper_path": "./MISS-QA/papers/2411.05324v1.json",
        "figure_id": "2411.05324v1_figure_1.png",
        "caption": "Fig. 1: The SASWISE pipeline efficiently estimates uncertainty while maintaining or enhancing pre-trained model performance. It begins by training a supervised model to convergence, followed by creating multiple candidate blocks in each block position. These blocks are shuffled and recombined into new models. In the diversification stage, two unique models are selected from the pool and trained on the same data sample. This stage involves calculating and utilizing the accuracy loss between the model being updated and the ground truth, along with the consistency loss between the two models, to only update the model being refined. After enough diversification training epochs, the best models from the partial or complete model pool are used to generate results from a single input. The final results for tasks with continuous or discrete data types are determined using median or majority voting methods, respectively, with uncertainty maps produced using standard deviation or majority ratio.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In the figure, the red box highlights the word “Median” and the blue box highlights the “Result Pool of 128 Predictions.”  \n2. The accompanying text (under “Training and evaluation”) explicitly states that for continuous‐valued tasks, the ensemble’s outputs are fused by taking their median.  \n3. The rationale given is that the median provides a central‐tendency estimate that is robust to outlier predictions.  \n4. By applying the median to the pool of continuous outputs, one obtains a single fused prediction that is less skewed by any unusually high or low model outputs, thereby improving stability and overall performance.\n\nAnswer:\n\nThey use the median on the 128‐prediction result pool because, for continuous outputs, the median is a robust central‐tendency statistic that aggregates the ensemble’s predictions while minimizing the influence of outliers."
    },
    {
        "question": "How does diversification compute and apply gradients from accuracy and consistency losses to candidate blocks?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "For each training epoch of the stacked U-Net, two paths, P1 and P2, are randomly selected for each input batch, yielding two separate predictions, ŷ1 and ŷ2.",
            "The loss function is then augmented with a regularization term that addresses the discrepancy between these two predictions, in addition to the standard error term that measures the deviation between the ground truth y and the prediction ŷ.",
            "In this manuscript, two losses are balanced using the coefficient λ. For simplicity, λ is set to 1, but it can be adjusted according to user requirements.",
            "During back-propagation, only the candidate blocks along the active path, Ps, responsible for the prediction ŷ, are updated."
        ],
        "final_answer": "During diversification, for each input batch two distinct paths through the stacked model are sampled, producing two outputs ŷ1 and ŷ2. An accuracy loss is computed between one output and the ground truth (L_acc) and a consistency loss between the two outputs (L_con). These two terms are summed (with λ=1) to form the total loss. Gradients of this combined loss are then back-propagated, but only through the candidate blocks along the single active path being updated, leaving all other blocks unchanged.",
        "relevant_elements": [
            "accuracy loss",
            "consistency loss",
            "candidate blocks"
        ],
        "id": 1937,
        "masked_question": "How does diversification compute and apply gradients from [mask1] and consistency losses to candidate blocks?",
        "masked_number": 1,
        "masked_elements": [
            "accuracy loss"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.05324v1_figure_1.png",
        "paperid": "2411.05324v1",
        "paper_path": "./MISS-QA/papers/2411.05324v1.json",
        "figure_id": "2411.05324v1_figure_1.png",
        "caption": "Fig. 1: The SASWISE pipeline efficiently estimates uncertainty while maintaining or enhancing pre-trained model performance. It begins by training a supervised model to convergence, followed by creating multiple candidate blocks in each block position. These blocks are shuffled and recombined into new models. In the diversification stage, two unique models are selected from the pool and trained on the same data sample. This stage involves calculating and utilizing the accuracy loss between the model being updated and the ground truth, along with the consistency loss between the two models, to only update the model being refined. After enough diversification training epochs, the best models from the partial or complete model pool are used to generate results from a single input. The final results for tasks with continuous or discrete data types are determined using median or majority voting methods, respectively, with uncertainty maps produced using standard deviation or majority ratio.",
        "qtype": "Implementation_Details",
        "response": "Here is what happens inside that red “Epoch Overview” box during diversification:\n\n1.  **Select two models (paths)**  \n    From the pool of diversified candidates, pick two distinct models (we’ll call them Model A and Model B).  \n\n2.  **Forward‐pass the same sample through both**  \n    Feed the input x through both:\n\n    •  ŷ₁ = Model A(x)   ← “frozen” path  \n    •  ŷ₂ = Model B(x)   ← “active” path  \n\n3.  **Compute two losses**  \n    –  **Consistency loss** L₍con₎ = ℓ(ŷ₁, ŷ₂)  \n         (e.g. L₁ or L₂ distance between the two outputs)  \n    –  **Accuracy (supervised) loss** L₍acc₎ = ℓ(ŷ₂, y)  \n         (e.g. L₁ or L₂ distance to the ground truth y)  \n\n4.  **Form the total loss**  \n    With the balancing coefficient α set to 1, the joint loss is  \n       L = L₍acc₎ + L₍con₎  \n\n5.  **Back‐propagate only through Model B’s blocks**  \n    •  Gradients from L₍acc₎ and L₍con₎ are summed and flowed back  \n    •  **Only** the parameters in the active candidate blocks of Model B are updated  \n    •  Model A remains unchanged (it only supplies ŷ₁ for the consistency term)  \n\n6.  **Return both models to the pool**  \n    After the update step, both Model A and the newly updated Model B go back into the pool for future diversification steps.\n\nIn this way, each diversification iteration reinforces two objectives at once—making the updated candidate more accurate on the ground‐truth task and more consistent with its peers—while still only touching the weights of the one active model."
    },
    {
        "question": "How does Inter Cluster Sample compute Monte Carlo UCB scores using cluster variance estimates?",
        "relevant_section_ids": [
            "3.1.3"
        ],
        "relevant_context": [
            "Carpentier et al. [49] suggested an approach to progressively estimate variances. This method involves calculating the Monte Carlo Upper Confidence Bound (MC-UCB) for each cluster (treated as an ‘arm’ in a multi-armed bandit problem) and selecting the arm with the highest upper bound for subsequent sampling.",
            "At current search round t, the MC-UCB score of cluster k is computed as follows: where N_k is the cluster size, n^{(t)}_k is the number of points sampled in the previous round, s_k is the empirical standard deviation within each cluster, and a is a hyper-parameter."
        ],
        "final_answer": "At each round t, AcTracer treats each cluster as an arm and computes a Monte Carlo Upper Confidence Bound (MC-UCB) score by combining the cluster’s estimated standard deviation s_k, the total cluster size N_k and the number of points n^{(t)}_k already sampled in that cluster, along with a hyper-parameter a. The cluster with the highest MC-UCB score is then chosen for the next sample.",
        "relevant_elements": [
            "Inter Cluster Sample",
            "Cluster Variance Estimation"
        ],
        "id": 1939,
        "masked_question": "How does [mask1] compute Monte Carlo UCB scores using cluster variance estimates?",
        "masked_number": 1,
        "masked_elements": [
            "Inter Cluster Sample"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.03573v1_figure_1.png",
        "paperid": "2408.03573v1",
        "paper_path": "./MISS-QA/papers/2408.03573v1.json",
        "figure_id": "2408.03573v1_figure_1.png",
        "caption": "Figure 1: Overall Workflow of AcTracer.\n\\footnotesize1⃝: Internal and external information extraction from LLMs during inference.\n\\footnotesize2⃝: An automated search mechanism to identify a suitable number of clusters corresponding to a given LLM and test data. The geometry structure obtained through the clustering algorithm partitions the input test spaces for more efficient sampling.\n\\footnotesize3⃝: An intra-cluster and inter-cluster sampling strategy to actively select the next data point for labelling.",
        "qtype": "Implementation_Details",
        "response": "Here is how the red‐boxed “Inter Cluster Sample” block (your [mask1]) actually computes its Monte‐Carlo UCB scores:\n\n1.  Treat each cluster \\(C_i\\) as a separate “arm” in a multi–armed‐bandit.  \n2.  Keep track, for each cluster \\(i\\), of  \n   – \\(|C_i|\\): the total number of points in that cluster,  \n   – \\(k_i\\): how many points you have already sampled (and labeled) in \\(C_i\\),  \n   – \\(\\hat\\sigma_i\\): the empirical standard deviation of the performance metric over the \\(k_i\\) sampled points.  \n3.  At round \\(t\\), fix your failure‐probability parameter  \n   \\[\n     \\delta \\;=\\;\\frac{2}{t^2}\\quad(\\text{since our metric is bounded, this choice of }\\delta\\text{ gives a valid high‐probability bound).}\n   \\]\n4.  Compute the Monte‐Carlo UCB index for cluster \\(i\\) as\n   \\[\n     \\mathrm{MC\\text{-}UCB}_i(t)\n     \\;=\\;\n     |C_i|\\;\\hat\\sigma_i\\;\\sqrt{\\frac{2\\,\\ln\\bigl(2/\\delta\\bigr)}{\\,k_i\\,}}\n     \\;=\\;\n     |C_i|\\;\\hat\\sigma_i\\;\\sqrt{\\frac{2\\,\\ln\\bigl(t^2\\bigr)}{\\,k_i\\,}}\\,. \n   \\]\n5.  Pick the cluster with the largest \\(\\mathrm{MC\\text{-}UCB}_i(t)\\) to draw your next sample from—this automatically balances sampling more in large clusters and in clusters whose within‐cluster variance is still poorly known.\n\nIn short, the red box is simply the plug‐in UCB rule of Carpentier et al., where the “bonus” term grows with the cluster size and empirical standard deviation, and shrinks as more points in that cluster are observed."
    },
    {
        "question": "How does Intra Cluster Sample minimize discrepancy between sampled examples and LLM Confidence Distribution within clusters?",
        "relevant_section_ids": [
            "3.1.3"
        ],
        "relevant_context": [
            "Our intra-cluster sample is guided by the output confidence of the LLMs. While the internal states represent models’ knowledge, the output confidence reveals more information about models’ decisions.",
            "Our goal in this stage is to maintain the confidence distribution of the sample drawn to be as close as possible to the distribution of the entire cluster, aiming for an intra-cluster level unbiased sampling.",
            "This is achieved by selecting candidate sample points that greedily minimize the distance between the confidence distributions of the sampled points and the entire cluster. For measuring the distance between these distributions, the two-sample Kolmogorov-Smirnov test and the Wasserstein distance are applied."
        ],
        "final_answer": "Within each cluster, AcTracer picks new examples so that the confidence‐score histogram of the sampled points stays as close as possible to that of the entire cluster. It does so by greedily selecting candidates that minimize a statistical distance (Kolmogorov–Smirnov or Wasserstein) between the sample’s confidence distribution and the cluster’s overall confidence distribution.",
        "relevant_elements": [
            "Intra Cluster Sample",
            "LLM Confidence Distribution"
        ],
        "id": 1940,
        "masked_question": "How does [mask1] minimize discrepancy between sampled examples and [mask2] within clusters?",
        "masked_number": 2,
        "masked_elements": [
            "Intra Cluster Sample",
            "LLM Confidence Distribution"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.03573v1_figure_1.png",
        "paperid": "2408.03573v1",
        "paper_path": "./MISS-QA/papers/2408.03573v1.json",
        "figure_id": "2408.03573v1_figure_1.png",
        "caption": "Figure 1: Overall Workflow of AcTracer.\n\\footnotesize1⃝: Internal and external information extraction from LLMs during inference.\n\\footnotesize2⃝: An automated search mechanism to identify a suitable number of clusters corresponding to a given LLM and test data. The geometry structure obtained through the clustering algorithm partitions the input test spaces for more efficient sampling.\n\\footnotesize3⃝: An intra-cluster and inter-cluster sampling strategy to actively select the next data point for labelling.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be clear what the two masked blocks are:\n\n  • Mask 1 (red box) is the intra‐cluster sampler – the “sub‐sampling” procedure you run once you have picked a cluster.  \n  • Mask 2 (blue box) is the target confidence distribution of that cluster, as reported by the LLM’s output confidences.  \n\nThe goal of the intra‐cluster sampler is to make the empirical confidence–histogram of the few points you actually label look as much like the full cluster’s confidence–distribution as possible.  Concretely, at each sampling step you:\n\n  1. Look at the current set S of already‐picked points in the cluster and form its confidence–distribution D_S.  \n  2. For each as‐yet‐unlabeled candidate x in that cluster, imagine adding x to S to get S′ and compute the new distribution D_{S′}.  \n  3. Measure the distance between D_{S′} and the true cluster distribution D_cluster using a two‐sample statistic (either the Kolmogorov–Smirnov statistic or the Wasserstein‐distance).  \n  4. Pick the x that yields the smallest distance.  \n\nBy always greedily choosing the point that most reduces the distance between your sampled subset’s confidence–distribution and the full cluster’s confidence–distribution, the intra‐cluster sampler (mask 1) drives the discrepancy to zero as quickly as possible."
    },
    {
        "question": "How does cluster search’s adaptive sampling relate to the elbow-point heuristic in unsupervised clustering?",
        "relevant_section_ids": [
            "3.1.2"
        ],
        "relevant_context": [
            "Instead, the relationship between cluster number and inertia is more of a trade-off, where the elbow point of the cluster num-inertia curve is a widely used heuristic for appropriate cluster number search.",
            "In our study, we employ the Kneedle algorithm to automatically identify the elbow point as the proper number of clusters. To enhance the efficiency of the search process, we leverage adaptive sampling to intensively sample cluster number-inertia pairs in regions of rapid function change."
        ],
        "final_answer": "Cluster search’s adaptive sampling concentrates its limited evaluations on the region of the cluster-number vs. inertia curve where inertia drops most sharply—the elbow region. By sampling more intensively around this rapid-change area, it efficiently finds the elbow point (via the Kneedle algorithm) that indicates the optimal number of clusters.",
        "relevant_elements": [
            "Cluster Search",
            "Adaptive Sampling"
        ],
        "id": 1941,
        "masked_question": "How does [mask1]’s [mask2] relate to the elbow-point heuristic in unsupervised clustering?",
        "masked_number": 2,
        "masked_elements": [
            "Cluster Search",
            "Adaptive Sampling"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.03573v1_figure_1.png",
        "paperid": "2408.03573v1",
        "paper_path": "./MISS-QA/papers/2408.03573v1.json",
        "figure_id": "2408.03573v1_figure_1.png",
        "caption": "Figure 1: Overall Workflow of AcTracer.\n\\footnotesize1⃝: Internal and external information extraction from LLMs during inference.\n\\footnotesize2⃝: An automated search mechanism to identify a suitable number of clusters corresponding to a given LLM and test data. The geometry structure obtained through the clustering algorithm partitions the input test spaces for more efficient sampling.\n\\footnotesize3⃝: An intra-cluster and inter-cluster sampling strategy to actively select the next data point for labelling.",
        "qtype": "Literature_Background",
        "response": "Here “Cluster Search” is the step in which we decide how many clusters to use (i.e. we are searching for the knee or “elbow” of the k-inertia curve), and “Adaptive Sampling” is the trick we use to find that elbow quickly.  Concretely:\n\n 1.  We know that if you plot the total within-cluster inertia versus number of clusters k, you get a convex, strictly decreasing curve, and that the “right” k is often taken to be the elbow (i.e. the point of maximum curvature in that curve).\n\n 2.  Rather than evaluating inertia for every possible k (which would be wasteful), we use adaptive sampling to choose the next k at which to compute inertia based on where the curve is changing most rapidly.\n\n 3.  Once we have a handful of (k, inertia) pairs in those high-curvature regions, we feed them into the Kneedle algorithm to pinpoint the elbow.\n\nIn other words, the “Adaptive Sampling” inside our “Cluster Search” module is simply a way to home in on the elbow of the inertia–k curve with far fewer evaluations than a brute-force sweep, yet still recover the classic elbow-point heuristic that balances compactness (low inertia) against over-splitting (too many clusters)."
    },
    {
        "question": "How does Eq-Frame model leverage group equivariant network frameworks for canonical yaw frame estimation?",
        "relevant_section_ids": [
            "4.1",
            "4.3"
        ],
        "relevant_context": [
            "Section 4.1: “We see that choosing f(g·x)=g·f(x) satisfies this equality, leveraging the fact that φ is a homomorphism, i.e. φ(g₁g₂)=φ(g₁)φ(g₂). This equality puts a constraint on the neural network that estimates f, namely f(g·x)=g·f(x), i.e. f must be equivariant with respect to group actions by elements from G. Since G is a subgroup of O(3) we also say that f must be subequivariant with respect to G.”",
            "Section 4.3: “Inspired by Villar et al. (2021), we design our frame network to learn universally G equivariant outputs from invariant features alongside 2D vector features. We convert the sequence of N IMU measurements into S scalar features and V vector features. While we process scalar features with multilayer perceptrons and standard 1-D convolutions, we process vector features with specific linear and convolution layers, and combine scalar and vector features with specialized non-linear layers.”"
        ],
        "final_answer": "Eq-Frame enforces that its yaw‐frame predictor f commutes with every rotation or reflection in the subgroup G of transformations preserving gravity: f(g·x)=g·f(x). To do so it decomposes gravity‐aligned IMU readings into G‐invariant scalars and G‐equivariant 2D vectors, then processes them with G‐equivariant linear layers (Eq-L), G-equivariant 1D convolutions (Eq-Conv) over time, and gated nonlinearities—each designed so that their weights satisfy the equivariance constraint Wφ(g)=φ(g)W. This guarantees that the estimated canonical yaw frame transforms correctly under all rotations and reflections around the gravity axis, yielding a frame estimate that generalizes across arbitrary IMU orientations.",
        "relevant_elements": [
            "Eq. Frame model"
        ],
        "id": 1943,
        "masked_question": "How does [mask1] leverage group equivariant network frameworks for canonical yaw frame estimation?",
        "masked_number": 1,
        "masked_elements": [
            "Eq. Frame model"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.06321v3_figure_2.png",
        "paperid": "2408.06321v3",
        "paper_path": "./MISS-QA/papers/2408.06321v3.json",
        "figure_id": "2408.06321v3_figure_2.png",
        "caption": "Figure 2: \nEqNIO (a) processes gravity-aligned IMU measurements,\n{(ai,ωi)}i=1nsuperscriptsubscriptsubscript𝑎𝑖subscript𝜔𝑖𝑖1𝑛\\{(a_{i},\\omega_{i})\\}_{i=1}^{n}{ ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.\nAn equivariant network (blue) predicts a canonical equivariant frame F𝐹Fitalic_F into which IMU measurements are mapped, i.e. canonicalized, yielding invariant inputs {(ai′,ωi′)}i=1nsuperscriptsubscriptsubscriptsuperscript𝑎′𝑖subscriptsuperscript𝜔′𝑖𝑖1𝑛\\{(a^{\\prime}_{i},\\omega^{\\prime}_{i})\\}_{i=1}^{n}{ ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ω start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.\nA conventional neural network then predicts invariant displacement (d′superscript𝑑′d^{\\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT) and covariance (Σ′superscriptΣ′\\Sigma^{\\prime}roman_Σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT) which are mapped back yielding equivariant displacement (d𝑑ditalic_d) and covariance (ΣΣ\\Sigmaroman_Σ).\nThe equivariant network (b) takes as input n×C0s𝑛superscriptsubscript𝐶0𝑠n\\times C_{0}^{s}italic_n × italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT scalars, and n×C0v𝑛superscriptsubscript𝐶0𝑣n\\times C_{0}^{v}italic_n × italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT vectors:\nVectors are processed by equivariant layers (Eq-L, Eq-Conv, Eq-LN), while scalars are separately processed with conventional layers.\nEq-L (green) uses two weights W1,W2subscript𝑊1subscript𝑊2W_{1},W_{2}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for SO(2) equivariance, and only W1subscript𝑊1W_{1}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for O(2) equivariance. Eq-Conv (pink) uses Eq-L to perform 1-D convolutions over time. The equivariant non-linear layer (orange) mixes vector and scalar features.",
        "qtype": "Literature_Background",
        "response": "Here “[mask1]” is the little blue “Eq. Frame model” block boxed in red in Fig. 2(a).  In effect, that module is nothing more than a small SO(2)/O(2)–equivariant network whose sole job is to spit out a 3×3 yaw‐alignment matrix F that satisfies\n\n    F(g·x)  =  g F(x)  \n\nfor any pure yaw rotation or reflection g of the IMU body frame x.  Concretely it does this by\n\n  1. decomposing each accelerometer/gyro reading into  \n     – SO(2)–invariant scalars (z–components, norms, dot‐products) and  \n     – 2D vectors (the xy–components),  \n\n  2. passing the scalars through ordinary MLPs/1D‐convs and the 2D vectors through specially designed equivariant linear layers (Eq-L) and equivariant 1D convolutions (Eq-Conv),  \n\n  3. mixing them with a gated, pointwise nonlinearity that is itself equivariant to rotations and reflections in the xy–plane,  \n\nso that the entire “Eq. Frame model” is guaranteed to commute with any yaw rotation or reflection.  The output F therefore automatically “rotates” when the input is rotated, yielding a canonical yaw frame that generalizes to arbitrary yaw orientations."
    },
    {
        "question": "How does the equivariant non-linear layer integrate scalar and vector features while preserving equivariance?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Previous works (Weiler et al., 2018; Weiler & Cesa, 2019) propose various nonlinearities such as norm-nonlinearity, tensor-product nonlinearity, and gated nonlinearity for SO(2) and O(2) equivariance in an equivariant convolutional way; while Deng et al. (2021) applies per‐point nonlinearity for vector features only. Since we already apply convolutions over time we simply apply a non‐linearity pointwise. Unlike Deng et al. (2021), we need to mix scalar and vector features and thus adapt the gated nonlinearity (Weiler et al., 2018) to pointwise nonlinearity.",
            "Specifically, for C vector and C scalar features, we concatenate the norm features of the vectors with the scalar features. We run a single MLP with an output of size 2C, and split it into new norm features r and new activations a which we modulate with a non‐linearity σ. Finally, we rescale the original vector features according to the new norm: v' = v ⊙ (r · σ(a)), where ⊙ denotes element‐wise multiplication and the concatenation was along the feature dimension."
        ],
        "final_answer": "The equivariant non-linear layer first computes the norms of each vector feature and concatenates these invariant norms with the scalar features. This concatenated vector is fed through a shared MLP whose 2C outputs are split into two C-dimensional vectors: one r of \"new norms\" and one a of gating activations. A pointwise non-linearity σ is applied to a, and then each original vector feature v is rescaled by the element-wise product r·σ(a). Because r and σ(a) are computed from invariant quantities (scalars and norms), and scaling commutes with rotations, the resulting transformation of the vector features remains equivariant.",
        "relevant_elements": [
            "equivariant non-linear layer",
            "scalars and vectors"
        ],
        "id": 1946,
        "masked_question": "How does the [mask1] integrate [mask2] while preserving equivariance?",
        "masked_number": 2,
        "masked_elements": [
            "equivariant non-linear layer",
            "scalars and vectors"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.06321v3_figure_2.png",
        "paperid": "2408.06321v3",
        "paper_path": "./MISS-QA/papers/2408.06321v3.json",
        "figure_id": "2408.06321v3_figure_2.png",
        "caption": "Figure 2: \nEqNIO (a) processes gravity-aligned IMU measurements,\n{(ai,ωi)}i=1nsuperscriptsubscriptsubscript𝑎𝑖subscript𝜔𝑖𝑖1𝑛\\{(a_{i},\\omega_{i})\\}_{i=1}^{n}{ ( italic_a start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ω start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.\nAn equivariant network (blue) predicts a canonical equivariant frame F𝐹Fitalic_F into which IMU measurements are mapped, i.e. canonicalized, yielding invariant inputs {(ai′,ωi′)}i=1nsuperscriptsubscriptsubscriptsuperscript𝑎′𝑖subscriptsuperscript𝜔′𝑖𝑖1𝑛\\{(a^{\\prime}_{i},\\omega^{\\prime}_{i})\\}_{i=1}^{n}{ ( italic_a start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ω start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT.\nA conventional neural network then predicts invariant displacement (d′superscript𝑑′d^{\\prime}italic_d start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT) and covariance (Σ′superscriptΣ′\\Sigma^{\\prime}roman_Σ start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT) which are mapped back yielding equivariant displacement (d𝑑ditalic_d) and covariance (ΣΣ\\Sigmaroman_Σ).\nThe equivariant network (b) takes as input n×C0s𝑛superscriptsubscript𝐶0𝑠n\\times C_{0}^{s}italic_n × italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT scalars, and n×C0v𝑛superscriptsubscript𝐶0𝑣n\\times C_{0}^{v}italic_n × italic_C start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_v end_POSTSUPERSCRIPT vectors:\nVectors are processed by equivariant layers (Eq-L, Eq-Conv, Eq-LN), while scalars are separately processed with conventional layers.\nEq-L (green) uses two weights W1,W2subscript𝑊1subscript𝑊2W_{1},W_{2}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT for SO(2) equivariance, and only W1subscript𝑊1W_{1}italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT for O(2) equivariance. Eq-Conv (pink) uses Eq-L to perform 1-D convolutions over time. The equivariant non-linear layer (orange) mixes vector and scalar features.",
        "qtype": "Experimental_Results",
        "response": "In the paper’s terminology the red‐boxed block is nothing more exotic than their “equivariant non‐linear layer” and the blue box around the whole thing is the equivariant frame network in which it lives (i.e. the two separate streams of scalar features and 2D–vector features).  The key trick is that they want to *mix* those two streams without ever breaking equivariance, so they never do a direct, unconstrained dot–product or affine combination of vectors and scalars.  Instead they do the following, pointwise at each time step:\n\n 1.  Take the 2D vector features and compute their *norms* (which are scalars and hence rotation‐invariant).  \n 2.  Concatenate those norms onto the scalar‐feature stream.  \n 3.  Run that concatenation through a small MLP to get two things:  \n    •  A set of *gating* coefficients (one per vector), and  \n    •  A set of *new norms* for each 2D vector.  \n 4.  Finally, multiply each original 2D vector by its new norm (changing only its length, not its direction) and also modulate the scalar outputs by the gates.  \n\nBecause the only operations performed on the vectors are (a) measuring their lengths (an invariant), (b) scaling their lengths, and (c) leaving their directions untouched, the whole block commutes with any SO(2) (or O(2)) rotation.  In that way the equivariant non‐linear layer fuses the scalar and vector streams without ever losing equivariance."
    },
    {
        "question": "How does incremental addition of views influence initial PnP pose estimates from cube probe?",
        "relevant_section_ids": [
            "3.1",
            "4.3"
        ],
        "relevant_context": [
            "Incremental pose optimization. We employ an incremental pose optimization approach, introducing a new image into the training loop at fixed intervals. Given the input images and corresponding masks of the calibration object, the first image is designated as the reference image. Multiple projection views around the object are sampled to acquire mask images, and the view with the best matching mask is selected as the initial pose for the first frame. For each newly added frame, we first compute 2D correspondences with the previous image using SuperPoint and SuperGlue. The matching pixels in the image cast rays to locate corresponding 3D points on the object, leveraging the optimized pose for precise surface positioning. This forms 2D-3D correspondences between the newly added image and the object, allowing the PnP with RANSAC to calculate the initial pose of the new image.",
            "Incremental pose optimization improves initial poses for new frames by using the optimized poses from previous frames, making overall pose alignment easier. Removing this strategy results in a significant drop in model performance."
        ],
        "final_answer": "By adding views one at a time and using the already‐optimized poses from earlier frames to establish 2D–3D correspondences, the cube‐probe PnP initialization for each new view becomes progressively more accurate and stable.",
        "relevant_elements": [
            "incremental view addition",
            "PnP pose init probe"
        ],
        "id": 1948,
        "masked_question": "How does [mask1] influence [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "incremental view addition",
            "PnP pose init probe"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.16690v2_figure_2.png",
        "paperid": "2408.16690v2",
        "paper_path": "./MISS-QA/papers/2408.16690v2.json",
        "figure_id": "2408.16690v2_figure_2.png",
        "caption": "Figure 2: Method overview. We leverage generic objects in few-view input images as pose probes. The pose probe is automatically segmented by SAM with prompts, and initialized by a cube shape. The method does not introduce extra burden but successfully facilitates pose estimation in feature-sparse scenes.",
        "qtype": "Experimental_Results",
        "response": "By feeding in one image at a time, we turn the “PnP pose init” step into a very reliable incremental procedure.  Concretely, every time we “add a view incrementally” (mask₁), we match its keypoints against the immediately preceding, already-registered view, lift those 2D matches to 3D surface points on the probe (using the last view’s optimized pose), and then run PnP + RANSAC to get a fresh initialization for the new frame’s camera (mask₂).  In other words, the red-boxed “add views incrementally” module supplies exactly the 2D–3D correspondences and warm-start pose that the blue-boxed “PnP pose init” needs to succeed even in very sparse, feature-poor scenes."
    },
    {
        "question": "What limitations arise from cube init. on SDF grid when deforming complex probe shapes via DeformNet?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "cube init.",
            "SDF grid",
            "DeformNet"
        ],
        "id": 1949,
        "masked_question": "What limitations arise from [mask1] on SDF grid when deforming complex probe shapes via DeformNet?",
        "masked_number": 1,
        "masked_elements": [
            "cube init."
        ],
        "figure_path": "./MISS-QA/figures/1_2408.16690v2_figure_2.png",
        "paperid": "2408.16690v2",
        "paper_path": "./MISS-QA/papers/2408.16690v2.json",
        "figure_id": "2408.16690v2_figure_2.png",
        "caption": "Figure 2: Method overview. We leverage generic objects in few-view input images as pose probes. The pose probe is automatically segmented by SAM with prompts, and initialized by a cube shape. The method does not introduce extra burden but successfully facilitates pose estimation in feature-sparse scenes.",
        "qtype": "Others",
        "response": "Here the red‐boxed “mask” is simply the fact that we always start our SDF with a little cube (i.e. “cube init”).  That choice brings two related limitations when you try to morph very complicated probe shapes:\n\n  1. fixed, coarse domain and resolution  \n     – Because our explicit SDF is a non‐learnable voxel grid pegged to that initial cube’s bounds, any part of the true object that protrudes outside (or nestles into very thin or concave regions) can’t be faithfully represented.  \n     – Fine geometric detail finer than the cube’s voxel pitch is simply out of reach.\n\n  2. large, highly non‐linear deformations  \n     – DeformNet must learn enormous warp fields to drag a cube into, say, a spindly handle or a wiggly toy shape.  Under only a handful of views this strain often leads to degenerate solutions or “collapsed” geometry, because the network has to both invent new topology and respect the rigid boundaries of the cube’s grid.\n\nIn practice this means that while cube‐init SDFs work great for mostly convex, blob-like probes, they begin to fail (losing thin parts, suffering self‐intersections or collapsed regions) as the probe’s true shape departs further from the starting cube."
    },
    {
        "question": "What ethical concerns might using generic everyday pose probes raise in privacy-sensitive scene reconstructions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "pose probes"
        ],
        "id": 1950,
        "masked_question": "What ethical concerns might using generic everyday [mask1] raise in privacy-sensitive scene reconstructions?",
        "masked_number": 1,
        "masked_elements": [
            "pose probes"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.16690v2_figure_2.png",
        "paperid": "2408.16690v2",
        "paper_path": "./MISS-QA/papers/2408.16690v2.json",
        "figure_id": "2408.16690v2_figure_2.png",
        "caption": "Figure 2: Method overview. We leverage generic objects in few-view input images as pose probes. The pose probe is automatically segmented by SAM with prompts, and initialized by a cube shape. The method does not introduce extra burden but successfully facilitates pose estimation in feature-sparse scenes.",
        "qtype": "Others",
        "response": "First, let’s identify the masked term by aligning the diagram with the text:\n\n– In the figure the red‐boxed item is labelled “probe poses,” and throughout the paper they talk about using “generic everyday objects” (e.g. a Coke can or a box) as pose probes.  \n– So the full phrase is “generic everyday objects.”\n\nNow, what ethical concerns might arise from using generic everyday objects as calibration probes in privacy-sensitive scene reconstructions?\n\nChain of thought:\n\n1. The method deliberately exploits commonplace items already present in a scene—cups, cans, boxes, toys—as built-in calibration targets to recover camera poses and build a 3D model.  \n2. Because these objects are ubiquitous and unobtrusive, someone could slip into a private space (a home, office, or other sensitive environment), snap just a handful of photos, and reconstruct the geometry—and potentially the appearance—of that space without the occupants’ knowledge.  \n3. Even if faces and people are masked out, detailed geometry of the interior (furniture layouts, personal effects, artwork) could be reverse-engineered, revealing sensitive information about habits, possessions, or vulnerabilities.  \n4. Such reconstructions can be stored, shared, or analyzed further (e.g. for target profiling, real-estate spying, burglary scouting), all without explicit consent from the space’s owner.  \n5. Finally, the automatic segmentation (SAM) and self-supervised refinement make it trivial for a malicious actor to do this at scale—harvesting everyday objects as “free” calibration markers without alerting anyone.\n\nAnswer:\nUsing generic everyday objects as pose-probes makes it stealthily easy to turn ordinary snapshots of a private environment into a full 3D reconstruction. That in turn raises serious privacy and consent issues—people could have their homes or offices digitized without permission, exposing personal layouts, belongings, or other sensitive details for misuse by third parties."
    },
    {
        "question": "What biases might LLM-based Prompt introduce in spatial anomaly localization scenarios?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM-based Prompt"
        ],
        "id": 1951,
        "masked_question": "What biases might [mask1] introduce in spatial anomaly localization scenarios?",
        "masked_number": 1,
        "masked_elements": [
            "LLM-based Prompt"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.05905v2_figure_2.png",
        "paperid": "2408.05905v2",
        "paper_path": "./MISS-QA/papers/2408.05905v2.json",
        "figure_id": "2408.05905v2_figure_2.png",
        "caption": "Figure 2. The pipeline of our proposed STPrompt.",
        "qtype": "Others",
        "response": "First, let us be precise about what “[mask1]” is.  In the red‐boxed region of Fig. 2 the authors are calling out the “LLM-based Prompt” module.  Concretely, at test time they query a large language model for two sets of short captions:\n\n  •  A small list of “normal” background descriptions (e.g. “a picture of road,” “a picture of grass,” “a picture of chair,” etc.)  \n  •  A set of “abnormal” descriptions generated by prompting the LLM for synonyms or paraphrases of each high-level anomaly class (e.g. “people knockout someone,” “someone lying on the ground,” “someone ignite fire,” …)  \n\nThey then embed all of those phrases with CLIP and do a patch-to-text matching over the frame patches to build a spatial heatmap of likely “abnormal” regions.\n\nBecause those text prompts come directly out of an LLM, they carry all of the LLM’s implicit assumptions and gaps in coverage.  In practice this can introduce three main types of bias into the spatial localization:\n\n  1.  **Concept‐Coverage Bias**  \n     –  The LLM’s idea of what “normal” looks like is drawn from its massive but skewed web-text pretraining.  If it never saw, say, an overhead security-camera image of a subway platform at midnight, it may well omit that as a “normal” background and end up marking it anomalous.  \n     –  Likewise, its paraphrases for “fighting” or “arson” may miss less common or culturally-specific variants of those events, so genuine anomalies never match any abnormal prompt.  \n\n  2.  **Language/Cultural Bias**  \n     –  The phrases the LLM produces reflect the cultural and topical slant of its training data.  It might overemphasize the kinds of indoor furniture or roadway vocabulary seen in blockbuster film scripts and underrepresent, say, rural or warehouse backdrops.  \n     –  This language bias translates directly into over- and under-sensitive patch retrieval: regions matching well-represented prompts get boosted; under-represented but perfectly normal areas get penalized (false positives).  \n\n  3.  **Selection & Synonymy Bias**  \n     –  By choosing a handful of LLM-generated “normal”/“abnormal” captions, the authors implicitly pick a small subspace of all possible valid descriptions.  Any patch whose true content falls outside that subspace will tend to be ranked poorly.  \n     –  If the LLM leans heavily on one synonym (e.g. “people knock out someone” vs. “person strikes another”), one phrasing may align well while the other does not—even though they mean the same thing.  That inconsistency produces spurious hot-spots or misses real ones.  \n\nIn short, the “LLM-based Prompt” unit imports the LLM’s semantic priors—its skewed concept coverage, cultural slants, and idiosyncratic choice of synonyms—directly into the patch-level retrieval.  Those biases translate into spatial localization that is:\n\n  •  Overly confident on familiar, well-represented backgrounds or wordings  \n  •  Prone to false alarms in perfectly normal but under-described scene types  \n  •  Prone to misses whenever the anomaly manifests in a form the LLM didn’t enumerate  \n\nAll of these are specific instances of distributional and semantic bias introduced by feeding LLM-generated text directly into a vision-language patch retrieval pipeline."
    },
    {
        "question": "What alternative mechanisms to TopK could enhance robustness in variable-length anomaly detection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "TopK"
        ],
        "id": 1952,
        "masked_question": "What alternative mechanisms to [mask1] could enhance robustness in variable-length anomaly detection?",
        "masked_number": 1,
        "masked_elements": [
            "TopK"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.05905v2_figure_2.png",
        "paperid": "2408.05905v2",
        "paper_path": "./MISS-QA/papers/2408.05905v2.json",
        "figure_id": "2408.05905v2_figure_2.png",
        "caption": "Figure 2. The pipeline of our proposed STPrompt.",
        "qtype": "Others",
        "response": "First, let’s be very clear about what “[mask1]” is in the figure.  If you look at the red‐boxed module on the right‐hand side of Fig. 2 under “Binary Classifier → video‐level binary label,” you’ll see it says “Top-K.”  In other words, STPrompt’s classification branch aggregates its frame‐level anomaly scores by picking the top K most anomalous frames and then applies a loss on those K scores.  \n\nNow the question asks: “What alternative mechanisms to Top-K could enhance robustness in variable‐length anomaly detection?”  In other words, instead of always taking a fixed number K of highest‐scoring frames, what other pooling or aggregation strategies might we plug in to make the system more stable when videos have very different lengths or very different densities of anomalies?  \n\nHere is a short chain of thought:\n\n1.   **Why might fixed-K Top-K be brittle?**  \n     - If one video is 100 frames long and another is 10 000 frames long, “Top-K” might overemphasize outliers in one case and underemphasize them in the other.  \n     - If a video has a single brief glitch versus an extended anomalous segment, the same fixed K may either miss that single glitch or may overweight background in the longer case.  \n\n2.   **What properties do we want in an alternative?**  \n     - It should adapt to the length of the video or to the density of anomalies.  \n     - It should be learnable (so the network can decide which frames matter most).  \n     - It should be robust to noisy high scores that come from background or detector jitter.  \n\n3.   **Candidate replacements for Top-K:**  \n   a)  **Attention‐based MIL pooling** (a “gated attention” or “soft attention”)  \n       - Introduce a small network that, for each frame‐level feature, predicts a scalar attention weight αᵢ.  \n       - Aggregate via \\( \\sum_i α_i s_i \\) (where sᵢ is the anomaly score of frame i).  \n       - The network is trained jointly so that it learns to focus on true anomaly frames and downweight spurious high scores.  \n   b)  **Ratio-based Top-p pooling**  \n       - Instead of fixing K, pick the top p percent of frames (e.g. top 5%), so that the number of frames automatically scales with video length.  \n   c)  **Robust statistic pooling**  \n       - Use a trimmed‐mean or median of the top X% of scores (e.g. drop the top and bottom ε% and take the mean of the rest) to guard against both spurious spikes and outlier suppression.  \n   d)  **NetVLAD or VLAD-style pooling**  \n       - Cluster the frame embeddings into G “visual words” and then pool a VLAD descriptor over the entire video, turning a variable-length sequence into a fixed-size vector.  \n       - A downstream classifier then works on that fixed vector.  \n   e)  **Transformer-based set aggregator**  \n       - Feed the whole sequence of frame embeddings into a small self-attention module (a Set-Transformer or one or two layers of Transformer).  \n       - Use its learned [CLS] token (or a learned pooling token) as a summary embedding and attach your binary classifier to that.  \n   f)  **Learnable rank pooling**  \n       - Parameterize a smooth rank‐based pooling (e.g. “LogSumExp pooling” or “soft‐max pooling”):  \n           score = (1/β) · log ∑ᵢ exp(β · sᵢ).  \n       - As β→∞ it approximates max‐pooling; for moderate β it blends max and average.  β can be a learned parameter.  \n\nAny of these alternatives can adapt to the actual content and length of the video, learn to suppress background noise or spurious high‐score frames, and thus often yield more robust anomaly detection when the original Top-K may over‐ or under-represent anomalies."
    },
    {
        "question": "What motivates integrating Spatial Attention Aggregation with frame features before Temporal Adapter processing?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Although we explicitly disentangle WSVADL into two independent tasks, i.e., temporal anomaly detection and spatial anomaly detection, for the temporal anomaly detection task, we still require the critical spatial local anomalies as assistance information. This is because potential spatial anomalies can eliminate the noise effect caused by the irrelevant backgrounds, after all, most anomalies may occupy a small spatial region.",
            "No matter how the spatial region of abnormal events changes, these two features, i.e., the global frame feature and the aggregated spatial feature, can extract key abnormal information from the local and global perspectives. In other words, they are complementary."
        ],
        "final_answer": "Integrating Spatial Attention Aggregation with the frame‐level features is motivated by the need to filter out irrelevant background noise and to emphasize those small, localized regions where anomalies actually occur, thereby providing spatially focused cues to the temporal modeling stage.",
        "relevant_elements": [
            "Spatial Attention Aggregation",
            "Temporal Adapter"
        ],
        "id": 1953,
        "masked_question": "What motivates integrating [mask1] with frame features before [mask2] processing?",
        "masked_number": 2,
        "masked_elements": [
            "Spatial Attention Aggregation",
            "Temporal Adapter"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.05905v2_figure_2.png",
        "paperid": "2408.05905v2",
        "paper_path": "./MISS-QA/papers/2408.05905v2.json",
        "figure_id": "2408.05905v2_figure_2.png",
        "caption": "Figure 2. The pipeline of our proposed STPrompt.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through what those two colored boxes are doing in the diagram and why the red‐boxed output is folded into the blue‐boxed module.\n\n1. What is the red box ([mask1])?  \n   In §3.2.1 the authors introduce a “Spatial Attention Aggregation” (SA²) module.  It takes the per‐frame CLIP feature fₜ plus a motion‐prior based patch set, selects the Top-K most “moving” patches, and then does a little attention pooling over those patches to produce a compact spatial summary gₜ that highlights the likely anomaly regions in that frame.\n\n2. What is the blue box ([mask2])?  \n   In §3.2.2 they describe the “Temporal Adapter,” a lightweight Transformer‐style block that injects temporal context into CLIP’s frame features.\n\n3. Where do they get combined?  \n   Right before sending anything into the temporal adapter, they take the original global frame feature fₜ and add the spatially focused feature gₜ from SA².  That sum is then the input to the temporal adapter.\n\n4. Why do they do that?  \n   • Anomalies usually sit in a small part of the frame, so training a purely frame­-level temporal model mixes a lot of irrelevant background.  \n   • By first extracting a motion-aware “spotlight” on candidate anomaly patches (gₜ), they can bias the temporal module to pay attention to the “where it’s moving/strange.”  \n   • Adding gₜ to fₜ before temporal modeling both preserves the scene context (fₜ) and injects strong, spatially localized anomaly cues (gₜ), which helps the adapter learn temporal patterns of true anomalies rather than waste capacity modeling background noise.\n\nIn short, they integrate the Spatial Attention Aggregation output with the frame features so that the subsequent Temporal Adapter is fed a feature that already emphasizes the most suspicious spatial regions, making its job of spotting temporal anomalies both easier and more accurate."
    },
    {
        "question": "What advantage arises from combining Binary Classifier outputs with MIL-Align alignment for video-level labeling?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Throughout the whole process, we devise two modules to encourage the model to focus on anomalies from the spatial and temporal dimensions, which are illustrated in the following sections.",
            "Inspired by the pioneer work VadCLIP (Wu et al., 2024b), we also introduce a dual-branch framework, namely, classification branch and alignment branch. Specifically, given a video V, we employ a frozen image encoder of CLIP to extract the frame-level feature f, where T is the length of video V, and d is the feature dimension. Then these features are fed into two branches after a series of information enhancements, classification branch is to directly predict the anomaly confidence p_b by a binary classifier, another align branch is to compute the anomaly category probability p_a by means of the image-to-concept alignment. With p_b and p_a in hands, we adopt the typical TopK and the recent MIL-Align strategies to compute the video-level anomaly prediction and category prediction, respectively, these predictions are subsequently used to calculate losses and provide data support for model optimization."
        ],
        "final_answer": "By using both the binary classifier’s TopK-based anomaly score and the MIL-Align category alignment, the model obtains two complementary video-level labels—one indicating whether an anomaly occurs and one specifying its category. This dual-branch supervision enriches the training signals and leads to more robust anomaly detection and classification under weak video-level labels.",
        "relevant_elements": [
            "Binary Classifier",
            "MIL-Align"
        ],
        "id": 1954,
        "masked_question": "What advantage arises from combining [mask1] outputs with [mask2] alignment for video-level labeling?",
        "masked_number": 2,
        "masked_elements": [
            "Binary Classifier",
            "MIL-Align"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.05905v2_figure_2.png",
        "paperid": "2408.05905v2",
        "paper_path": "./MISS-QA/papers/2408.05905v2.json",
        "figure_id": "2408.05905v2_figure_2.png",
        "caption": "Figure 2. The pipeline of our proposed STPrompt.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1.  In the red box sits the C‐Branch “Binary Classifier,” which takes the TopK‐pooled frame‐level anomaly confidences and turns them into a single video‐level “anomalous or not” score via BCE.  \n2.  In the blue box sits the A-Branch “MIL-Align” module, which aligns each frame (or patch) feature against text prompts for all anomaly classes and then uses a MIL-style top-k pooling to yield a per-class probability for the entire video.  \n3.  If you use only the C-Branch you get a good yes/no anomaly signal but no idea which kind of anomaly.  If you use only the A-Branch you get class probabilities but they can be noisy when the video actually contains no anomaly.  \n4.  By *jointly* training and fusing both branches at the video level you get:  \n   - A reliable binary “is there an anomaly?” gate (so you don’t hallucinate classes when nothing is wrong)  \n   - A semantically grounded class prediction when there *is* an anomaly  \n   - Stronger, complementary gradient signals during training (binary+multiclass), which stabilizes learning under only video-level supervision  \n5.  In practice this means more accurate video‐level labels (both presence/absence and category) without ever having to collect frame- or pixel-level annotations.  \n\nAnswer  \nBy fusing the TopK‐based binary‐confidence output (red box) with the MIL-Align category‐alignment output (blue box), the model simultaneously learns “is there an anomaly?” and “if so, which kind?” from only video-level labels.  The binary branch keeps class predictions from firing on normal clips, the alignment branch injects semantic category information when anomalies are present, and together they deliver far more precise video-level anomaly detection and classification than either branch alone."
    },
    {
        "question": "How does joint sampling set design reduce total sampled entries compared to independent component-wise sampling?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "For any I such that their associated scopes are disjoint, i.e., X^P_i ∩ X^P_j = ∅, we can estimate the transitions for both components simultaneously using shared samples.",
            "Specifically, define the joint sampling set Z as: where the modulo operation ensures that we cycle through all possible values of each component’s state-action space. By sampling from each entry in this joint set for N times, we obtain samples that can be used to estimate both components with only D_max entries rather than |X^P_i|+|X^P_j| entries.",
            "This strategy improves sample efficiency by reducing the total number of samples needed compared to independently sampling each component."
        ],
        "final_answer": "By constructing a joint sampling set that cycles over each component’s values in parallel for D_max = max(|X^P_i|,|X^P_j|) entries, the same samples serve to estimate both disjoint components at once. This cuts the total sampled entries from the sum of their individual sizes to just the size of the largest component.",
        "relevant_elements": [
            "Independent Component-wise Sampling",
            "Joint Sampling Set"
        ],
        "id": 1955,
        "masked_question": "How does [mask1] reduce total sampled entries compared to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Joint Sampling Set",
            "Independent Component-wise Sampling"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.07591v1_figure_2.png",
        "paperid": "2411.07591v1",
        "paper_path": "./MISS-QA/papers/2411.07591v1.json",
        "figure_id": "2411.07591v1_figure_2.png",
        "caption": "Figure 2: Synchronous Sampling with Exclusive Scopes.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key point of comparison:\n\n  1. “Mask 2” (the blue‐boxed Independent Component‐wise Sampling) treats each factor completely separately.  You build one sample set of size |X₁ᵖ| for component 1 and another of size |X₂ᵖ| for component 2.  Total samples ⇨ |X₁ᵖ| + |X₂ᵖ|.  \n  2. “Mask 1” (the red‐boxed Joint/Synchronous Sampling Set Design) exploits the fact that the two components have disjoint scopes so you can draw from a single joint sampling set.  You only ever need as many draws as the larger of the two individual requirements.  Total samples ⇨ Dₘₐₓ = max(|X₁ᵖ|, |X₂ᵖ|).  \n\nIn other words, by sampling both factors together you replace the sum of their sample‐counts with the maximum of them.  That slash‐and‐reuse step cuts your sampling cost from |X₁ᵖ|+|X₂ᵖ| down to max(|X₁ᵖ|,|X₂ᵖ|)."
    },
    {
        "question": "Within Independent Component-wise Sampling, how does Sampling Set for Component 1 treat default entries?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "To estimate P̂ for any component j, let Z^P_j be defined as Z^P_j = { z : z_X_j ∈ X^P_j, z_{−X_j} = α }, where α is an arbitrary (but fixed) element from the rest of the state–action space.",
            "Importantly, while sampling from Z^P_j, we set the entries outside component j’s scope (i.e. z_{−X_j}) to this fixed default α so that we do not need to cover the rest of the state–action space."
        ],
        "final_answer": "All dimensions not in Component 1’s scope are held constant at a single fixed “default” value. In other words, for each sample in the Sampling Set for Component 1, any entry outside its scope is set to the chosen default element rather than varied.",
        "relevant_elements": [
            "Independent Component-wise Sampling",
            "Sampling Set for Component 1"
        ],
        "id": 1956,
        "masked_question": "Within [mask1], how does [mask2] treat default entries?",
        "masked_number": 2,
        "masked_elements": [
            "Independent Component-wise Sampling",
            "Sampling Set for Component 1"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.07591v1_figure_2.png",
        "paperid": "2411.07591v1",
        "paper_path": "./MISS-QA/papers/2411.07591v1.json",
        "figure_id": "2411.07591v1_figure_2.png",
        "caption": "Figure 2: Synchronous Sampling with Exclusive Scopes.",
        "qtype": "Implementation_Details",
        "response": "In the Independent Component‐wise Sampling block (the red box), each component only varies the coordinates in its own scope and “holds everything else at Default.”  In the blue‐boxed subfigure (the sampling set for component 1), you see exactly that: x[1] and x[2] are varied over all values in Z₁ᴾ={1,2}, while x[3] and x[4] are never sampled but are simply set to a fixed “Default” value in every entry."
    },
    {
        "question": "Within Synchronous Component-wise Sampling, how is Joint Sampling Set for Components 1&2 constructed to minimize sampling entries?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "For any two components whose associated scopes are disjoint, i.e., Z^P_i ∩ Z^P_j = ∅, we can estimate the transitions for both components simultaneously using shared samples. Specifically, define the joint sampling set Z^P = { z^(1), …, z^(D_max) } where D_max = max(|X^P_i|, |X^P_j|).",
            "Each joint sample z^(k) is constructed by taking for each component ℓ the ((k−1) mod |X^P_{I_ℓ}|)+1–th element of its independent sampling set X^P_{I_ℓ}, and filling all other coordinates with a fixed default value. The modulo operation ensures we cycle through each component’s entries without ever sampling more than D_max joint entries."
        ],
        "final_answer": "They take D_max = max(|X^P_1|,|X^P_2|) and build a joint sampling set of size D_max. For k=1,…,D_max, the k-th joint sample z^(k) uses the ((k−1) mod |X^P_1|)+1–th element of component 1’s set and the ((k−1) mod |X^P_2|)+1–th element of component 2’s set, with all other coordinates fixed to default. This way only D_max entries need to be sampled instead of |X^P_1|+|X^P_2|.",
        "relevant_elements": [
            "Synchronous Component-wise Sampling",
            "Joint Sampling Set for Components 1&2"
        ],
        "id": 1957,
        "masked_question": "Within [mask1], how is [mask2] constructed to minimize sampling entries?",
        "masked_number": 2,
        "masked_elements": [
            "Synchronous Component-wise Sampling",
            "Joint Sampling Set for Components 1&2"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.07591v1_figure_2.png",
        "paperid": "2411.07591v1",
        "paper_path": "./MISS-QA/papers/2411.07591v1.json",
        "figure_id": "2411.07591v1_figure_2.png",
        "caption": "Figure 2: Synchronous Sampling with Exclusive Scopes.",
        "qtype": "Implementation_Details",
        "response": "Here, [mask1] is the red‐boxed panel on the right (“Synchronous Component‐wise Sampling”), and [mask2] is the blue‐boxed subpanel inside it (“Joint Sampling Set for Components 1 & 2”).  What the figure is telling us is:\n\n1.  Instead of doing |X₁|+|X₂| separate samples (one block for component 1 and one for component 2), we form a single joint sampling table of only  \n       Dₘₐₓ = max(|X₁|,|X₂|)  \n   rows.\n\n2.  We then “cycle” through each component’s values in lockstep by using modulo indexing.  Concretely, if we label the rows i=1,…,Dₘₐₓ, then in row i we set  \n     • component 1’s coordinates to the i-th element of X₁  \n     • component 2’s coordinates to the ((i−1) mod |X₂|)+1-th element of X₂  \n     • all other components to a fixed “default.”  \n\n3.  Because one of the two scopes is no larger than Dₘₐₓ, the modulo wrap‐around guarantees that every value of the smaller scope appears at least once over the Dₘₐₓ rows, and every value of the larger scope appears exactly once.  Thus with only Dₘₐₓ total samples we cover both sub‐spaces fully, minimizing the total number of sampling entries."
    },
    {
        "question": "How does the multi-scale transformer block coordinate scale-specific self-attention maps to maintain consistent semantic context?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "Specifically, for an input feature X_in with 128-channel after dimensionality reduction, a layer normalization LN1 is first used to produce a tensor X0, that is, X0 = LN1(X_in). Next, our MTB generates different scale Qi (= Q1, Q2, Q3), Ki (= K1, K2, K3) and Vi (= V1, V2, V3) projections through adopting 1×1 point-wise convolution (PWConv) followed 3×3 depth-wise convolution (DWConv). Based on the experience of the RFB method, the kernel sizes are set to 3, 5, and 7, respectively.",
            "Subsequently, we independently reshape Qi¯ (Q_i) and Ki¯ (K_i) projections such that their dot-product interaction produces three transpose-attention maps (T_i). Finally, we perform attention map activation and concatenate three attention features with different scales to generate the first stage feature H1.",
            "To improve the expressive ability of the feature, we introduce a multi-scale operation in the feed-forward network (FFN) and then use residual connections to fuse the multi-scale features with the original input, i.e., H = X_in + H2."
        ],
        "final_answer": "The MTB first applies a shared layer normalization to the input feature, then projects that normalized feature into three parallel Q/K/V streams—each at a different kernel size (3, 5, 7)—to capture distinct receptive fields. It computes independent dot-product self-attention maps for each scale, applies the same activation to each, and finally concatenates these three scale-specific attention outputs. A multi-scale feed-forward network and a residual connection then fuse the concatenated result with the original input, ensuring that although attention is performed at multiple scales, the overall semantic context remains consistent across the combined feature.",
        "relevant_elements": [
            "MTB"
        ],
        "id": 1958,
        "masked_question": "How does the [mask1] coordinate scale-specific self-attention maps to maintain consistent semantic context?",
        "masked_number": 1,
        "masked_elements": [
            "MTB"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.09588v1_figure_2.png",
        "paperid": "2409.09588v1",
        "paper_path": "./MISS-QA/papers/2409.09588v1.json",
        "figure_id": "2409.09588v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of our GLCONet method. We use ResNet-50/Swin Transformer/PVT as the encoder and propose a collaborative optimization strategy (COS) that contains a global perception module (GPM), a local refinement module (LRM) and a group-wise hybrid interaction module (GHIM) to simultaneously model long-range dependencies and local details. In addition, we design an adjacent reverse decoder (ARD) to integrate the complementary information with different layers through cross-layer aggregation and reverse optimization.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is in fact the Global Perception Module (GPM), i.e. the stack of multi‐scale transformer blocks (MTBs) marked in red.  In each MTB the incoming 128-D feature is first layer–normalized and then projected into three “views” at different spatial extents by a 1×1 convolution followed by depth–wise convolutions of kernel size 3, 5 and 7.  Each of those three projections yields its own Q/K pair, whose dot–product produces a separate self–attention map A₁,A₂,A₃.  To prevent the three attentions from drifting apart in magnitude or semantics, each attention‐weighted value AᵢVᵢ is multiplied by a small learned scalar βᵢ, then all three are concatenated and passed through a single 1×1 fusion convolution W₀.  Because they all share the same normalization, scaling parameters βᵢ and final projection W₀, the three scale‐specific attention outputs are automatically aligned into one common 128-D semantic space.  This coordinated normalize–scale–concatenate–project recipe is precisely how the GPM maintains consistent semantic context across multiple self–attention scales."
    },
    {
        "question": "How do GPM and LRM collaboratively optimize global relationships and local details in COS?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1",
            "3.2.2"
        ],
        "relevant_context": [
            "we propose the COS that contains three components, that is, a global perception module (GPM), a local refinement module (LRM) and a group-wise hybrid interaction module (GHIM). The first two modules explore global and local perception representations through different structures, while the latter is utilized to integrate the global-local information.",
            "Technically, we design a global perception module (GPM), which utilizes the multi-scale transformer block (MTB) to obtain the relationship of all pixels from a global angle.",
            "Similarly, we introduce a multi-scale operation in the feed-forward network (FFN) ... ultimately, the second stage feature with abundant global contexts is generated via concatenating features at different scales.",
            "Unlike RFB and RMFE that directly combine all features, our LRM captures local spatial details by utilizing the progressive convolution block (PCB) of two stages, which aims to obtain multi-source local information from different operations.",
            "Through the two-stage operations, the feature contains abundant local details."
        ],
        "final_answer": "Within the Collaborative Optimization Strategy (COS), the Global Perception Module (GPM) and the Local Refinement Module (LRM) operate in parallel on the same encoder features to produce complementary representations. GPM uses a multi-scale transformer block (MTB)—featuring multi-scale self-attention and a multi-scale feed-forward network—to model long-range relationships among all pixels across different scale spaces, thereby enriching global semantic context. LRM employs a two-stage progressive convolution block (PCB) that leverages atrous and depth-wise convolutions at multiple receptive fields to extract and progressively aggregate fine-grained local spatial details. By jointly optimizing features through GPM and LRM, COS fuses both global and local cues for stronger discriminative feature learning in camouflaged object detection.",
        "relevant_elements": [
            "GPM",
            "LRM"
        ],
        "id": 1960,
        "masked_question": "How do [mask1] and [mask2] collaboratively optimize global relationships and local details in COS?",
        "masked_number": 2,
        "masked_elements": [
            "GPM",
            "LRM"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.09588v1_figure_2.png",
        "paperid": "2409.09588v1",
        "paper_path": "./MISS-QA/papers/2409.09588v1.json",
        "figure_id": "2409.09588v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of our GLCONet method. We use ResNet-50/Swin Transformer/PVT as the encoder and propose a collaborative optimization strategy (COS) that contains a global perception module (GPM), a local refinement module (LRM) and a group-wise hybrid interaction module (GHIM) to simultaneously model long-range dependencies and local details. In addition, we design an adjacent reverse decoder (ARD) to integrate the complementary information with different layers through cross-layer aggregation and reverse optimization.",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In the COS (Collaborative Optimization Strategy), there are two parallel streams operating on the same set of encoder features: one in the red box and one in the blue box.  \n2. The red‐boxed stream is the Global Perception Module (GPM).  It consists of a stack of Multi‐scale Transformer Blocks (MTBs) that (a) project the incoming 128-channel feature into several “scale” streams via point-wise + depthwise convolutions, (b) compute multi-scale self-attention maps to capture long-range pixel relations, (c) fuse those attention outputs through concatenation and feed-forward layers (again across multiple scales), and (d) add back the original feature via a residual connection.  This module thus builds rich global context.  \n3. The blue-boxed stream is the Local Refinement Module (LRM).  It uses a two-stage Progressive Convolutional Block (PCB).  In each stage it (a) applies atrous or depthwise convolutions at several kernel sizes to extract multi-receptive-field local cues, (b) aggregates them in a progressive (chain-like) fashion, and (c) again uses a residual link.  This focuses the network on fine spatial details.  \n4. After GPM and LRM produce their respective global feature maps G₂…G₅ and local maps L₂…L₅, a Group-wise Hybrid Interaction Module (GHIM) splits them into channel groups, adds and gates them to yield fused global–local representations.  \n5. By design, the MTBs in the red box learn long-distance dependencies, the PCBs in the blue box learn local texture edges and shapes, and the GHIM ties them together so the network’s final features carry both broad context and sharp detail.  \n\nAnswer  \nMask₁ (the red‐boxed Global Perception Module) uses multi-scale transformer blocks to model long-range pixel relationships and build coarse global context.  \nMask₂ (the blue‐boxed Local Refinement Module) uses progressive convolutional blocks to mine multi-scale local spatial details.  \nThey run side by side on the same encoder outputs, then their outputs are group-wise fused (via the GHIM) so that every pixel’s feature is simultaneously informed by global semantics and local texture."
    },
    {
        "question": "How does GHIM's fused feature inform ARD's adjacent reverse decoding process?",
        "relevant_section_ids": [
            "3.2.3",
            "3.3"
        ],
        "relevant_context": [
            "III-B3 Group-wise hybrid interaction module: Given a global feature G_i and a local feature L_i, we propose a group-wise hybrid interaction module (GHIM) that aims to integrate global-local information through a grouping fusion with different channel spaces. ... perform a residual connection to generate feature F_i with abundant global-local information.",
            "III-C Adjacent Reverse Decoder: After obtaining the optimized feature F_i, we need to decode the feature F_i to generate the predicted map. ... Subsequently, we input feature maps from different layer F5, F4 and F3 into the ARD to generate a feature map D6 ... we generate a reversed attention map by using a reversed attention acting on features U5 and P^g for optimizing feature U5 to generate optimized feature D6. Finally, the feature D6 and F3 are concatenated and dimensionally reduced, and then two feature maps D5 and D4 are added to generate the final feature map D3."
        ],
        "final_answer": "The GHIM fuses the global perception output (G_i) and the local refinement output (L_i) at each level into a single enhanced feature F_i. These GHIM‐fused features F_i are then directly fed into the adjacent reverse decoder (ARD). In the ARD, successive levels of F_i (e.g. F5, F4, F3) are upsampled, aligned in spatial size and channels, and concatenated with a coarse prediction. A reversed‐attention mechanism uses this concatenation to refine and produce the decoded feature maps (D_i). In this way, the GHIM’s rich, global‐local fused representations serve as the starting inputs that the ARD iteratively refines via cross‐layer aggregation and reverse attention to produce the final camouflaged object segmentation.",
        "relevant_elements": [
            "GHIM",
            "ARD"
        ],
        "id": 1961,
        "masked_question": "How does [mask1]'s fused feature inform [mask2]'s adjacent reverse decoding process?",
        "masked_number": 2,
        "masked_elements": [
            "GHIM",
            "ARD"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.09588v1_figure_2.png",
        "paperid": "2409.09588v1",
        "paper_path": "./MISS-QA/papers/2409.09588v1.json",
        "figure_id": "2409.09588v1_figure_2.png",
        "caption": "Figure 2: Overall architecture of our GLCONet method. We use ResNet-50/Swin Transformer/PVT as the encoder and propose a collaborative optimization strategy (COS) that contains a global perception module (GPM), a local refinement module (LRM) and a group-wise hybrid interaction module (GHIM) to simultaneously model long-range dependencies and local details. In addition, we design an adjacent reverse decoder (ARD) to integrate the complementary information with different layers through cross-layer aggregation and reverse optimization.",
        "qtype": "Literature_Background",
        "response": "Here is the reasoning in two steps:\n\n1. Identify the two masked regions in the figure:  \n   – **[mask1]** (the red‐boxed block) is the Group‐wise Hybrid Interaction Module (GHIM), whose job is to take the global features from the MTB and the local features from the PCB, split them into several channel‐wise groups, fuse them (via addition, 3×3 conv, gating, residual link) and output a set of “global–local” fused feature maps \\(F_2,F_3,F_4,F_5\\).  \n   – **[mask2]** (the blue‐boxed block) is the Adjacent Reverse Decoder (ARD), which decodes those fused features into the final segmentation map through a series of adjacent cross‐layer refinements using “reverse attention.”\n\n2. How the GHIM outputs drive the ARD:  \n   – The fused outputs \\(F_i\\) (from GHIM) become the multi‐level inputs to the ARD.  At each decoding stage \\(i\\):  \n     • You take the coarse prediction (or the previous stage’s refined feature) and upsample it to the spatial size of \\(F_i\\).  \n     • You concatenate that upsampled prediction with the GHIM’s fused feature \\(F_i\\).  \n     • From that concatenation you compute a “reversed attention map,” which learns to suppress background and highlight the subtle camouflaged edges.  \n     • You multiply (“reverse‐gate”) the coarse feature by that attention map to get a refined feature for this level.  \n     • Finally you pass that refined feature on (and also supervise it) before moving to the next, deeper level.  \n\nIn short, the **GHIM’s fused global–local features** are exactly what feeds into each Adjacent Reverse Decoder block.  The ARD uses them—together with upsampled coarse predictions—to generate reverse‐attention masks that re‐weight and polish the segmentation at every layer, achieving the cross‐layer, reverse‐optimization the paper describes."
    },
    {
        "question": "How does the LLM incorporate clinical entities compared to previous report generation methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Clinical Entities",
            "LLM"
        ],
        "id": 1962,
        "masked_question": "How does the [mask1] incorporate clinical entities compared to previous report generation methods?",
        "masked_number": 1,
        "masked_elements": [
            "LLM"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.13523v1_figure_1.png",
        "paperid": "2410.13523v1",
        "paper_path": "./MISS-QA/papers/2410.13523v1.json",
        "figure_id": "2410.13523v1_figure_1.png",
        "caption": "Figure 1: \nComparison of real image-text datasets and synthetic datasets.\n(a): The real image-text dataset, MIMIC-CXR (Johnson et al., 2019b), while authentic, often contains imperfections such as long-tailed data distribution, unpaired images and text, and low-quality CXR images, which limit the performance of MedVLP models pretrained on this dataset.\n(b): The synthetic dataset generation process uses clinical entities as prompts to an LLM (e.g., Llama3.1 (AI@Meta, 2024)) to generate synthetic reports. These reports are then used to create synthetic images through RoentGen (Bluethgen et al., 2024). We propose an automated pipeline to control the dataset distribution, ensuring it is balanced and includes paired image-text samples.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in (b) is not your garden‐variety image‐to‐text generator – it is an off‐the‐shelf LLM (Llama3.1-70B-Instruct) that we drive with a carefully sampled list of clinical entities rather than with raw images or free‐form prompts.  Concretely, before every report generation we:\n\n 1.  Extract a pool of Abnormality, Non-Abnormality, Disease, Non-Disease and Anatomy entities via an NER tool (RaTE).  \n 2.  Sample from each category in a balanced fashion so that no single entity dominates (avoiding the long tail).  \n 3.  Feed that exact set of entities (with their types) into the LLM prompt and ask it to produce both FINDINGS and IMPRESSION.  \n 4.  Run the generated text back through RaTE to verify it contains – and only contains – the sampled entities, repeating generation if necessary.\n\nBy contrast, almost all prior medical report generators either condition directly on images (with no explicit entity list) or on loosely‐defined captions, and have no mechanism to enforce entity coverage or balance.  Our LLM therefore “knows” exactly which clinical concepts to include up front, giving us tight control over both the content and the distribution of terms in the synthetic reports."
    },
    {
        "question": "How does the balanced dataset strategy mitigate long-tailed distribution issues compared to prior balancing methods?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Since we aim to build a synthetic dataset without a long-tailed distribution, we design a balanced sampling strategy to ensure that the appearance frequency of each entity type is approximately equal across the synthetic dataset.",
            "For each generation, we sample: where Ni is the number of entities sampled from the first four categories. Additionally, we sample Nd where Nd is the number of entities sampled from the ANATOMY category. Thus, the total sampled entity set for each generation is: [entities].",
            "We impose a maximum frequency threshold, f_max, for each entity. If an entity in S reaches this threshold, we resample it while keeping the remaining entities unchanged. This ensures a balanced distribution of entities across the synthetic dataset."
        ],
        "final_answer": "The balanced dataset strategy explicitly enforces an approximately uniform frequency of each clinical entity in the synthetic CXR reports by (1) sampling a fixed number of entities from each of the five categories on every report generation and (2) imposing a hard cap (f_max) on how often any single entity can appear and resampling when that cap is reached. This direct, entity‐level control stands in contrast to prior approaches that did not systematically regulate the long‐tailed frequencies of medical findings, thereby preventing a few common entities from dominating the dataset and flattening out the long tail.",
        "relevant_elements": [
            "Balanced Dataset",
            "Long-tailed Dataset"
        ],
        "id": 1963,
        "masked_question": "How does the [mask1] strategy mitigate long-tailed distribution issues compared to prior balancing methods?",
        "masked_number": 1,
        "masked_elements": [
            "Balanced Dataset"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.13523v1_figure_1.png",
        "paperid": "2410.13523v1",
        "paper_path": "./MISS-QA/papers/2410.13523v1.json",
        "figure_id": "2410.13523v1_figure_1.png",
        "caption": "Figure 1: \nComparison of real image-text datasets and synthetic datasets.\n(a): The real image-text dataset, MIMIC-CXR (Johnson et al., 2019b), while authentic, often contains imperfections such as long-tailed data distribution, unpaired images and text, and low-quality CXR images, which limit the performance of MedVLP models pretrained on this dataset.\n(b): The synthetic dataset generation process uses clinical entities as prompts to an LLM (e.g., Llama3.1 (AI@Meta, 2024)) to generate synthetic reports. These reports are then used to create synthetic images through RoentGen (Bluethgen et al., 2024). We propose an automated pipeline to control the dataset distribution, ensuring it is balanced and includes paired image-text samples.",
        "qtype": "Literature_Background",
        "response": "First, note that the little “bar‐chart” in the red box in Figure 1(b) is not a fancy icon but actually stands for the *balanced sampling strategy* we use when we generate our 200 K synthetic reports and images.  In effect, instead of taking the raw, heavily skewed MIMIC‐CXR entity frequencies as‐is, we build our synthetic dataset so that each medical entity appears roughly the same number of times.  Concretely:\n\n 1.  We partition all extracted entities into five buckets (ABNORMALITY, NON-ABNORMALITY, DISEASE, NON-DISEASE, ANATOMY).  \n 2.  For every synthetic report, we draw the same fixed number of entities from each of the first four buckets, plus a fixed number from the anatomy bucket.  \n 3.  We impose a hard cap \\(f_{\\max}\\) on how many times *any* single entity may appear across the entire synthetic corpus.  Whenever sampling would push an entity over \\(f_{\\max}\\), we drop that selection and resample.  \n\nBy *baking* this even‐frequency constraint into the *data-generation* process, we never recover the extreme “long tail” that plagues MIMIC-CXR.  Contrast that with classical remedies—reweighting losses, oversampling/undersampling at *training* time, or heuristic up/down‐sampling of real examples—which can only approximate balance and often lead to duplicate or noisy examples.  In our case, the data are produced *balanced by construction* and thus head and tail entities each get equal representation from the very start."
    },
    {
        "question": "How do clinical entities guide the LLM to generate a balanced synthetic dataset?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We query the LLM using prompts that include the entity list, as shown in Fig 6.",
            "Since we aim to build a synthetic dataset without a long-tailed distribution, we design a balanced sampling strategy to ensure that the appearance frequency of each entity type is approximately equal across the synthetic dataset.",
            "We impose a maximum frequency threshold, r_max, for each entity e. If an entity e in S reaches this threshold, we resample e while keeping the remaining entities in S unchanged. … This ensures a balanced distribution of entities across the synthetic dataset.",
            "After sampling, we input the selected entities S into the LLM and indicate their type. Let the output of the LLM be denoted as R, which represents the synthetic report generated by the model based on the sampled entities."
        ],
        "final_answer": "Clinical entities are first extracted and grouped into five categories. A balanced sampling strategy then selects a predefined number of entities from each category and enforces a per-entity maximum frequency. This sampled, balanced list of entities (with their types indicated) is passed as the LLM’s prompt. The LLM generates synthetic reports containing exactly those entities, producing a dataset whose clinical concepts are evenly distributed.",
        "relevant_elements": [
            "Clinical Entities",
            "LLM",
            "Balanced Dataset"
        ],
        "id": 1964,
        "masked_question": "How do [mask1] guide the [mask2] to generate a balanced synthetic dataset?",
        "masked_number": 2,
        "masked_elements": [
            "Clinical Entities",
            "LLM"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.13523v1_figure_1.png",
        "paperid": "2410.13523v1",
        "paper_path": "./MISS-QA/papers/2410.13523v1.json",
        "figure_id": "2410.13523v1_figure_1.png",
        "caption": "Figure 1: \nComparison of real image-text datasets and synthetic datasets.\n(a): The real image-text dataset, MIMIC-CXR (Johnson et al., 2019b), while authentic, often contains imperfections such as long-tailed data distribution, unpaired images and text, and low-quality CXR images, which limit the performance of MedVLP models pretrained on this dataset.\n(b): The synthetic dataset generation process uses clinical entities as prompts to an LLM (e.g., Llama3.1 (AI@Meta, 2024)) to generate synthetic reports. These reports are then used to create synthetic images through RoentGen (Bluethgen et al., 2024). We propose an automated pipeline to control the dataset distribution, ensuring it is balanced and includes paired image-text samples.",
        "qtype": "Experimental_Results",
        "response": "The key point is that nothing magical happens inside the LLM – we “steer” it by carefully choosing and feeding it the little building blocks (the clinical entities) we want to see in each report, in such a way that over the 200 K examples every entity type shows up the same number of times.  Concretely:\n\n1.  We start with our giant vocabulary of extracted clinical entities (e.g. “atelectasis,” “consolidation,” “left pleural effusion,” etc.), each tagged as one of the five classes (ABNORMALITY, NON-ABNORMALITY, DISEASE, NON-DISEASE, ANATOMY).  \n2.  For each synthetic report we decide to build, we **sample** a fixed number of entities from each of the first four classes plus a fixed number from the ANATOMY class, ensuring that all five buckets contribute equally.  \n3.  We also enforce a global **frequency cap** Tf on every entity: once an entity has been used Tf times in our growing corpus, we never sample it again.  \n4.  We hand the resulting small set of entities—plus their type labels—as a **prompt** to the LLM (e.g. Llama3.1).  The LLM’s job is simply to weave those exact entities into a plausible Findings section.  \n5.  We then run the LLM’s output back through our NER tool (RaTE) to verify that it includes exactly the entities we asked for, no more no less.  If it doesn’t, we re-prompt the LLM until it does.  \n\nBy repeating that process 200 000 times, we guarantee that each entity appears roughly the same number of times in the final synthetic corpus—and hence that our MedVLP model will be trained on a perfectly **balanced** distribution of clinical concepts."
    },
    {
        "question": "How does the Text2Image Model produce high-fidelity images for synthetic image-text pairs?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We use RoentGen’s (Bluethgen et al., 2024) official pretrained weights to generate images. Following their implementation, we use only the ‘IMPRESSION’ section from the synthetic reports as the text prompt for the T2I model. The generation process is controlled using the official hyperparameters provided by RoentGen, where the classifier-free guidance (CFG) is set to 4 and the number of denoising steps is set to 50.",
            "To prevent the synthetic images from exhibiting the same issues found in the real dataset (as discussed in Sec. 3.1), we apply a similar curation procedure. First, we use the MLLM to filter synthetic images, and then we compute the similarity of visual features between synthetic images and the problematic samples identified from the real dataset. If the visual similarity exceeds a threshold ε, we regenerate the images by re-querying the T2I model with the same text prompt until they pass the curation procedure."
        ],
        "final_answer": "The Text2Image model (RoentGen) produces high-fidelity synthetic CXR images by using the ‘IMPRESSION’ section of each synthetic report as the text prompt with its official pretrained weights under classifier-free guidance (CFG=4) and 50 denoising steps. Each generated image is then vetted by a Multimodal LLM filter and compared via RAD-DINO visual features to known problematic samples; any image exceeding a similarity threshold is regenerated with the same prompt until it passes the quality checks, ensuring high fidelity.",
        "relevant_elements": [
            "Text2Image Model",
            "High Fidelity Image"
        ],
        "id": 1965,
        "masked_question": "How does the [mask1] produce [mask2] for synthetic image-text pairs?",
        "masked_number": 2,
        "masked_elements": [
            "Text2Image Model",
            "High Fidelity Image"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.13523v1_figure_1.png",
        "paperid": "2410.13523v1",
        "paper_path": "./MISS-QA/papers/2410.13523v1.json",
        "figure_id": "2410.13523v1_figure_1.png",
        "caption": "Figure 1: \nComparison of real image-text datasets and synthetic datasets.\n(a): The real image-text dataset, MIMIC-CXR (Johnson et al., 2019b), while authentic, often contains imperfections such as long-tailed data distribution, unpaired images and text, and low-quality CXR images, which limit the performance of MedVLP models pretrained on this dataset.\n(b): The synthetic dataset generation process uses clinical entities as prompts to an LLM (e.g., Llama3.1 (AI@Meta, 2024)) to generate synthetic reports. These reports are then used to create synthetic images through RoentGen (Bluethgen et al., 2024). We propose an automated pipeline to control the dataset distribution, ensuring it is balanced and includes paired image-text samples.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step‐by‐step reasoning:\n\n1. In our pipeline (Fig. 1 (b)), the red‐boxed module is the Text-to-Image model—specifically RoentGen—into which we feed the synthetic report’s “IMPRESSION” text.  \n2. We use RoentGen’s official weights and settings (classifier-free guidance scale = 4, 50 denoising steps) so that it knows how to paint a chest X-ray from that text.  \n3. That alone gives us a rough synthetic CXR, but to guarantee high‐fidelity outputs (the blue box), we automatically screen each generated image via two checks:  \n   • An MLLM‐based filter that reruns our six quality queries, and  \n   • A RAD-DINO feature-similarity check against known “bad” images (threshold < 0.5).  \n4. Any image that fails either check is regenerated (same prompt) until it passes both.  \n\nAfter this loop, the surviving outputs are our “High Fidelity Image” samples."
    },
    {
        "question": "How does GMM distribution modelling inform class-conditional prompt sampling for contrastive learning?",
        "relevant_section_ids": [
            "3.2",
            "3.2.1",
            "3.2.2",
            "3.2.4"
        ],
        "relevant_context": [
            "We adopt the Gaussian Mixture Models (GMMs) as such generative model [46  ###reference_b46###], which improves the intra-class variability and increases robustness to class imbalances when compared to the alternative approaches mentioned before. Before delving into the methodology of the CPM, we first introduce the generation process of the class-conditional query features.",
            "In our method, the goal of GMM is to model the data distribution of the joint-modal mask embedding  for each class  in the -dimensional space by employing a weighted mixture of  multivariate Gaussians, defined as follows: ...",
            "Before delving into the audio recovery process, we first sample a set of class-conditional prompts  (where  represent the indices for ground truth labels) via the GMM model from the last iteration according to the target semantic classes  that we want to recover.",
            "By taking advantage of class-conditional distribution modelling, we can overcome this limitation by utilising the predicted spectrogram saliency mask  and its associated class label  of each sound source, denoted as ."
        ],
        "final_answer": "The GMMs are trained to fit, for each semantic class, the empirical distribution of the joint-modal mask embeddings produced by the transformer decoder. At every iteration, class-conditional query features (“prompts”) are then drawn by sampling from the specific Gaussian mixture corresponding to each target class. These sampled prompts drive the model to predict per-class saliency masks, which are used to extract class-specific audio and visual features via masked average pooling. Those region-level features form anchors and positives/negatives for supervised InfoNCE contrastive learning, thereby disentangling and sharpening class-specific representations.",
        "relevant_elements": [
            "GMM",
            "Class-conditional Prompt",
            "Dense Contrastive Learning"
        ],
        "id": 1967,
        "masked_question": "How does [mask1] distribution modelling inform class-conditional prompt sampling for contrastive learning?",
        "masked_number": 1,
        "masked_elements": [
            "GMM"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.05358v3_figure_1.png",
        "paperid": "2407.05358v3",
        "paper_path": "./MISS-QA/papers/2407.05358v3.json",
        "figure_id": "2407.05358v3_figure_1.png",
        "caption": "Figure 1: Comparing conventional AVS methods [7, 13] with our CPM approach, CPM inherits the class-agnostic query from transformer-based methods and integrates class-conditional prompts sampled from the learned joint-modal data distribution to achieve three objectives: 1) learn disentangled audio partitioning, 2) facilitate semantic-guided object identification, and 3) promote more explicit audio-visual contrastive learning.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. In Fig. 2 the red box encloses a small Gaussian‐Mixture‐Model (GMM) operating on the joint‐modal mask embeddings for each semantic class.  \n2. Section 3.2.1 (“Class Conditional Distribution Modelling”) explains that after Hungarian matching we collect all predicted mask embeddings paired with their class labels and fit a GMM per class via EM.  \n3. At each training iteration (both in the ACP / VCP stages and in the Prompting‐based Contrastive Learning (PCL) stage), we draw new “class‐conditional prompts” by sampling embedding vectors from the GMM component corresponding to each ground‐truth class.  \n4. These sampled embeddings are injected as query prompts into the transformer decoder, producing class‐specific saliency masks on the audio‐spectrogram and image feature maps.  \n5. In PCL, those saliency masks let us isolate region‐level audio (and visual) features per class via masked average pooling. We then form anchor–positive–negative triplets for a supervised InfoNCE loss.  \n6. In short, the GMM distribution model provides a generative reservoir of diverse, class‐specific query embeddings, which are sampled on‐the‐fly to drive the contrastive learning of disentangled, class‐conditioned audio–visual representations.  \n\nAnswer  \nBy fitting a GMM over the joint‐modal mask embeddings for each class, the model learns the full, multimodal distribution of “what a pink (cello), purple (violin), etc.” embedding looks like.  During training we then sample new prompt vectors directly from the Gaussian component assigned to each ground‐truth class.  Those sampled class‐conditional prompts are used to query the decoder, generate class‐specific saliency masks, and extract per‐class audio/visual feature regions for the supervised InfoNCE contrastive loss."
    },
    {
        "question": "Suggest alternatives to GMM for enhancing distribution modelling of class-conditional prompts.",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GMM",
            "Distribution Modelling"
        ],
        "id": 1968,
        "masked_question": "Suggest alternatives to [mask1] for enhancing distribution modelling of class-conditional prompts.",
        "masked_number": 1,
        "masked_elements": [
            "GMM"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.05358v3_figure_1.png",
        "paperid": "2407.05358v3",
        "paper_path": "./MISS-QA/papers/2407.05358v3.json",
        "figure_id": "2407.05358v3_figure_1.png",
        "caption": "Figure 1: Comparing conventional AVS methods [7, 13] with our CPM approach, CPM inherits the class-agnostic query from transformer-based methods and integrates class-conditional prompts sampled from the learned joint-modal data distribution to achieve three objectives: 1) learn disentangled audio partitioning, 2) facilitate semantic-guided object identification, and 3) promote more explicit audio-visual contrastive learning.",
        "qtype": "Others",
        "response": "First, let us restate what “​[mask1]​” is referring to in the figure and in the text:\n\n– In §3.2.1 (“Class Conditional Distribution Modelling”) the authors say “​We adopt the Gaussian Mixture Models (GMMs) as such generative model…, which improves the intra-class variability and increases robustness to class imbalances…​.”  \n– In the red-boxed inset of Fig. 1 you can see the “GMM” node whose outputs are sampled to produce class-conditional prompts.\n\nSo the question really asks:\n\n  “If you didn’t want to use a GMM to model the joint-modal mask-embedding distribution, what other distribution-modelling techniques could you plug in instead, in order to sample class-conditional prompts?”  \n\nHere is a short chain-of-thought followed by a list of concrete alternatives:\n\n1. **Why think beyond GMM?**  \n   – GMMs are a classical, parametric way to model a multi-modal embedding distribution, but they assume mixtures of Gaussians and full-covariance estimation can be costly in high dimensions.  \n   – Modern deep generative models can capture more complex, highly non-Gaussian densities; they can interpolate smoothly between modes and can scale to very high dimensions.\n\n2. **What properties do we need?**  \n   – A model that, once trained, lets us sample diverse but class-consistent embedding vectors (“prompts”).  \n   – It should handle multi-modality within each class (e.g. a violin sound under different timbres, intensities, room acoustics).  \n   – Ideally it updates online or incrementally as the underlying transformer-mask embeddings evolve.\n\n3. **Families of replacements:**\n\n   A. **Variational Auto-Encoders (VAEs)**  \n      – Train a conditional VAE per class (or a single CVAE with a one-hot class code).  \n      – At sampling time you condition on class label and draw from the learned latent Gaussian, then decode to a prompt embedding.  \n      – Pros: scalable, amortized inference, can capture complex manifolds.\n\n   B. **Normalizing Flows**  \n      – E.g. RealNVP, Glow, Masked Autoregressive Flow (MAF).  \n      – These learn an invertible mapping from a simple base distribution (e.g. N(0,I)) to the mask-embedding space.  \n      – You attach a class-condition either via concatenation or via FiLM layers.  \n      – Pros: exact likelihoods, flexible multi-modal densities, cheap sampling.\n\n   C. **Conditional Generative Adversarial Networks (cGANs)**  \n      – Learn a generator \\(G(z,c)\\) that, given noise \\(z\\sim N(0,I)\\) and class-code \\(c\\), produces an embedding in the prompt space.  \n      – Pros: often sharper samples, very flexible; cons: training stability.\n\n   D. **Diffusion / Score-Based Models**  \n      – Train a small conditional diffusion model in the embedding space: start from noise and iteratively denoise towards a class-specific prompt.  \n      – Pros: state-of-the-art density modelling; cons: slower sampling.\n\n   E. **Non-parametric Density Estimation**  \n      – Kernel Density Estimation (Parzen windows) over a memory bank of embeddings per class.  \n      – At sample time draw a random memory point and perturb it by a small Gaussian.  \n      – Pros: very simple; cons: scales poorly in high D, choice of kernel bandwidth is critical.\n\n   F. **Dirichlet-Process / Bayesian Nonparametrics**  \n      – Replace a fixed-K GMM with an infinite-mixture model, automatically inferring the number of modes in each class.  \n      – Pros: flexible number of components; cons: more complicated inference (Gibbs, variational Bayes).\n\n   G. **Neural Mixture-of-Experts**  \n      – A small routing network decides which expert network (or which sub-distribution) to sample from for a given class.  \n      – Pros: can condition on context beyond class label; cons: more moving parts.\n\n4. **Concrete suggestion**  \n   – If you want to stay “light,” swap the GMM for a **conditional VAE**: you collect mask embeddings for class k, train a class-conditioned encoder/decoder that maps them to a low-dim Gaussian latent, then at each CPM iteration you sample \\(z\\sim N(0,I)\\), decode \\(\\hat e = \\text{Decoder}(z, k)\\) and use \\(\\hat e\\) as your prompt.  \n   – If you need more expressive power, use a **conditional Glow** or **MAF** in place of the GMM — in practice you would fit the flow to the same memory bank of embeddings and then draw as many prompts per class as you like with trivial sampling.\n\nAll of these approaches would slot into the pipeline exactly where “GMM” is now, replacing the red-boxed module with a different generative modelling block."
    },
    {
        "question": "What are potential limitations of bypassing bipartite matching with fixed GT assignment?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Bipartite Matching to GT",
            "Fixed GT Assignment"
        ],
        "id": 1969,
        "masked_question": "What are potential limitations of bypassing [mask1] with fixed GT assignment?",
        "masked_number": 1,
        "masked_elements": [
            "Bipartite Matching to GT"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.05358v3_figure_1.png",
        "paperid": "2407.05358v3",
        "paper_path": "./MISS-QA/papers/2407.05358v3.json",
        "figure_id": "2407.05358v3_figure_1.png",
        "caption": "Figure 1: Comparing conventional AVS methods [7, 13] with our CPM approach, CPM inherits the class-agnostic query from transformer-based methods and integrates class-conditional prompts sampled from the learned joint-modal data distribution to achieve three objectives: 1) learn disentangled audio partitioning, 2) facilitate semantic-guided object identification, and 3) promote more explicit audio-visual contrastive learning.",
        "qtype": "Others",
        "response": "Here is a step-by-step breakdown of why blindly “hard-wiring” each prompt to a fixed ground-truth (GT) slot—i.e. completely skipping the Hungarian bipartite matching and simply saying “prompt 1 always means class A,” “prompt 2 always means class B,” and so on—can backfire in practice:\n\n1. loss of flexibility in the number of objects  \n   – In the usual bipartite set‐matching you can cope with images that have 0, 1 or many instances of a class by simply matching whichever queries the model has “discovered.”  \n   – With a fixed GT-assignment you have to reserve exactly one (or some predetermined number of) query slots per class.  If an image has more objects of class X than you allotted queries, you can’t assign them.  If it has fewer, some prompts will be “left dangling” and contribute noise.\n\n2. brittle to missing or noisy annotations  \n   – Real datasets often contain annotation omissions or label noise.  Under bipartite matching the model can in principle ignore spurious GT boxes or “drop” a missing object by assigning it to the no‐object null class.  \n   – With fixed assignments, every prompt must find its exact partner in the GT.  If that ground–truth label is wrong or missing, you force the model to learn from pure noise.\n\n3. reduced permutation‐invariance and generalization  \n   – The Hungarian matcher intrinsically respects that queries are permutation‐invariant: you don’t care which query gets which object, as long as each object is covered exactly once.  This encourages the model to discover objects in any order.  \n   – A hard-wired assignment breaks that principle: the model learns a brittle ordering (“object A always lives in slot 3”), which rarely transfers when object counts, aspect ratios or backgrounds shift at test time.\n\n4. inability to handle variable instance counts per class  \n   – Some images may have two violins and no cellos, others one cello and three violins.  A fixed one-query-per-class scheme cannot adapt to those per‐image count variations.  \n   – Bipartite matching, by contrast, simply matches as many queries to GT objects as needed and dumps the rest as “no‐object.”\n\n5. over-constraint and reduced robustness  \n   – By forcing an exact prompt⇄GT mapping you inject a very strong “curriculum” but you also remove the model’s ability to self-correct.  If during early training the model’s “slot 2” is badly initialized, that error will persist and hurt convergence rather than being “re-matched” away by the Hungarian solver.\n\nIn short, while fixed GT assignments can stabilize one piece of the training pipeline, they do so at the cost of flexibility, robustness to noisy or variable annotations, and the permutation‐invariant object discovery that makes transformer-based set predictors so powerful."
    },
    {
        "question": "How could Self-Validating Filtering and Adaptive Latent Re-attention reinforce each other's errors in filtering toxic tokens?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Self-Validating Filtering",
            "Adaptive Latent Re-attention"
        ],
        "id": 1970,
        "masked_question": "How could [mask1] and Adaptive Latent Re-attention reinforce each other's errors in filtering toxic tokens?",
        "masked_number": 1,
        "masked_elements": [
            "Self-Validating Filtering"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.12761v1_figure_2.png",
        "paperid": "2410.12761v1",
        "paper_path": "./MISS-QA/papers/2410.12761v1.json",
        "figure_id": "2410.12761v1_figure_2.png",
        "caption": "Figure 2: Illustration of SAFREE framework. Based on proximity analysis between the masked token embeddings and the toxic subspace 𝒞𝒞\\mathcal{C}caligraphic_C, we detect unsafe tokens and project them into orthogonal to the toxic concept (in red), but still be in the input space ℐℐ\\mathcal{I}caligraphic_I (in green). SAFREE adaptively controls the filtering strength in an input-dependent manner, which also regulates a latent-level re-attention mechanism. Note that our approach can be broadly applied to various image and video diffusion backbones.",
        "qtype": "Others",
        "response": "Chain-of‐thought  \n1. In SAFREE the red‐boxed module is its “self‐validating filtering” (SVF), which at each denoising step decides whether to use the original embedding p or the detoxified embedding p_proj based on a cosine‐similarity threshold.  \n2. Downstream, the adaptive latent re-attention (ALR) module uses whichever embedding SVF picked to attenuate low-frequency components in the UNet or DiT latent features.  \n3. If SVF under-filters (i.e. mistakenly judges an unsafe token as safe because the similarity stays above threshold), it will continue feeding a “toxic” embedding into ALR. ALR then fails to sufficiently suppress harmful features, letting toxic patterns slip through—so the image latents remain contaminated.  \n4. Conversely, if SVF over-filters (mistakenly projects a benign token embedding away), ALR will over-attenuate low-frequency content, warping or erasing perfectly safe structures. Those distortions in turn change the cosine similarity back in SVF’s next check—making SVF even more prone to mis-classify safe versus unsafe.  \n5. In short, a misstep in SVF feeds ALR the wrong embedding, so ALR attenuates the wrong frequency bands, which then shifts the embedding–latent similarity and causes SVF’s subsequent decisions to drift further. Over consecutive steps this feedback loop can amplify errors in both token-level filtering and latent re-attention, either leaking toxicity or wrecking image quality."
    },
    {
        "question": "What alternative projection strategies could replace Selective Orthogonal Projection for toxic concept subspace mitigation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Selective Orthogonal Projection",
            "toxic concept subspace"
        ],
        "id": 1971,
        "masked_question": "What alternative projection strategies could replace [mask1] for toxic concept subspace mitigation?",
        "masked_number": 1,
        "masked_elements": [
            "Selective Orthogonal Projection"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.12761v1_figure_2.png",
        "paperid": "2410.12761v1",
        "paper_path": "./MISS-QA/papers/2410.12761v1.json",
        "figure_id": "2410.12761v1_figure_2.png",
        "caption": "Figure 2: Illustration of SAFREE framework. Based on proximity analysis between the masked token embeddings and the toxic subspace 𝒞𝒞\\mathcal{C}caligraphic_C, we detect unsafe tokens and project them into orthogonal to the toxic concept (in red), but still be in the input space ℐℐ\\mathcal{I}caligraphic_I (in green). SAFREE adaptively controls the filtering strength in an input-dependent manner, which also regulates a latent-level re-attention mechanism. Note that our approach can be broadly applied to various image and video diffusion backbones.",
        "qtype": "Others",
        "response": "Here’s a quick sketch of some of the most obvious “drop‐the‐toxic‐subspace” alternatives you could swap in for the pure orthogonal projection in SAFREE:\n\n  1. Oblique (non‐orthogonal) projection  \n     • Instead of projecting exactly onto the subspace’s orthogonal complement, use an oblique projector  \n       P = I – U (W)⁻¹ Uᵀ  \n       with W = UᵀA U for some positive–definite metric A.  \n     • This lets you “steer” the removal along particular directions (e.g. weighted by token importance).    \n\n  2. Soft/attenuated removal  \n     • Compute the toxic‐subspace coordinate α = Uᵀp, then replace p ← p – λ·Uα with λ∈[0,1]  \n       (λ=1 is full removal, λ<1 just dials it back).  \n     • Allows you to trade off coherence vs. aggressiveness.  \n\n  3. Householder reflections  \n     • Build a reflector H that flips the toxic direction to its negative:  \n       H = I – 2vvᵀ/‖v‖² for some v spanning the toxic subspace,  \n       then p←H p.  \n     • This uses a single rank-1 “mirror” rather than a full orthonormal basis.  \n\n  4. PCA / singular‐value thresholding  \n     • Run a quick SVD or PCA on your toxic‐keyword embeddings to find principal axes,  \n       then zero-out the top k singular components in p.  \n     • Equivalent to an approximate projection but often more stable if your U’s are noisy.  \n\n  5. Learned linear debiasing layer  \n     • Fit a small ridge-regression (Tikhonov) layer R: p_safe = R p that minimizes alignment with U while staying close to p.  \n     • This can be done in closed form at inference time, no backprop through the big model.  \n\n  6. Nearest‐neighbor “safety” replacement  \n     • Instead of projecting, simply swap p for its nearest neighbor in a precomputed set of “safe” embeddings.  \n     • Preserves naturalness, but is a coarser, discrete step.  \n\nAll of these strategies can be slotted in place of the I–UUᵀ orthogonal projection in SAFREE’s Sec. 3.2. They vary in how sharply they excise the toxic directions, how much residual coherence they leave in the prompt embedding, and how much extra precomputation (PCA, regression, nearest‐neighbor tables) they require."
    },
    {
        "question": "What motivates Self-Validating Filtering to dynamically adjust denoising steps?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "While our approach so far adaptively controls the number of token embeddings to be updated, it sometimes lacks flexibility in preserving the original generation capabilities for content outside the target concept. Recent observations (Kim et al., 2024a; Ban et al., 2024a) suggest that different denoising timesteps in T2I models contribute unevenly to generating toxic or undesirable content.",
            "Based on this insight, we propose a self-validating filtering mechanism during the denoising steps of the diffusion model that automatically adjusts the number of denoising timesteps conditioned on the obtained embedding."
        ],
        "final_answer": "It is motivated by the finding that different denoising timesteps have uneven influence on the creation of toxic or undesirable content, so dynamically adjusting when to apply the filtered embeddings lets the method strengthen filtering for unsafe prompts while preserving quality for safe prompts.",
        "relevant_elements": [
            "Self-Validating Filtering",
            "denoising steps"
        ],
        "id": 1972,
        "masked_question": "What motivates [mask1] to dynamically adjust [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Self-Validating Filtering",
            "denoising steps"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.12761v1_figure_2.png",
        "paperid": "2410.12761v1",
        "paper_path": "./MISS-QA/papers/2410.12761v1.json",
        "figure_id": "2410.12761v1_figure_2.png",
        "caption": "Figure 2: Illustration of SAFREE framework. Based on proximity analysis between the masked token embeddings and the toxic subspace 𝒞𝒞\\mathcal{C}caligraphic_C, we detect unsafe tokens and project them into orthogonal to the toxic concept (in red), but still be in the input space ℐℐ\\mathcal{I}caligraphic_I (in green). SAFREE adaptively controls the filtering strength in an input-dependent manner, which also regulates a latent-level re-attention mechanism. Note that our approach can be broadly applied to various image and video diffusion backbones.",
        "qtype": "Design_Rationale",
        "response": "Self‐validating filtering is driven by the empirical finding that not all diffusion timesteps are equally responsible for “injecting” toxic or unwanted content.  In practice one finds that early, middle, or late denoising steps each contribute unevenly to the final image’s semantics, so applying a one‐size‐fits‐all filter over every step either fails to catch all the bad bits or else over–suppresses and ruins perfectly good parts of the image.  By measuring how “detoxified” the prompt embedding already is (via a cosine‐similarity check between the original and the projected–safe embedding), the self‐validating filter can automatically dial up or down the number of guarded denoising steps in the Image/Video Diffusion block.  In this way, it tightens the reins when the prompt still looks unsafe and backs off when it’s already been cleansed, preserving as much of the original model’s generative power as possible."
    },
    {
        "question": "What is the reasoning for applying Selective Orthogonal Projection after Toxic Token Detection?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "However, directly removing or replacing these tokens with irrelevant ones, such as random tokens or replacing the token embeddings with null embeddings, disrupts the coherence between words and sentences, compromising the quality of the generated image to the safe input prompt, particularly when the prompt is unrelated to the toxic concepts.",
            "To address this, we propose projecting the detected token embeddings into a space orthogonal to the toxic concept subspace while keeping them within the input space to ensure that the integrity of the original prompt is preserved as much as possible."
        ],
        "final_answer": "Selective Orthogonal Projection is used to eliminate the toxic concept associations from the detected tokens in the embedding space while preserving the semantic coherence and overall integrity of the original prompt, thereby avoiding the quality degradation that would occur if tokens were simply removed or replaced.",
        "relevant_elements": [
            "Selective Orthogonal Projection",
            "Toxic Token Detection"
        ],
        "id": 1973,
        "masked_question": "What is the reasoning for applying [mask1] after [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Selective Orthogonal Projection",
            "Toxic Token Detection"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.12761v1_figure_2.png",
        "paperid": "2410.12761v1",
        "paper_path": "./MISS-QA/papers/2410.12761v1.json",
        "figure_id": "2410.12761v1_figure_2.png",
        "caption": "Figure 2: Illustration of SAFREE framework. Based on proximity analysis between the masked token embeddings and the toxic subspace 𝒞𝒞\\mathcal{C}caligraphic_C, we detect unsafe tokens and project them into orthogonal to the toxic concept (in red), but still be in the input space ℐℐ\\mathcal{I}caligraphic_I (in green). SAFREE adaptively controls the filtering strength in an input-dependent manner, which also regulates a latent-level re-attention mechanism. Note that our approach can be broadly applied to various image and video diffusion backbones.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition in three steps:\n\n1.  Mask-and-measure (blue box):  For each input token we temporarily mask it out, pool the remaining token embeddings, and see how “close” that pooled vector is to our pre-built toxic subspace 𝐶.  Any token whose removal makes the pooled embedding jump far away from 𝐶 is flagged as concept-relevant (i.e. likely to be carrying nudity/violence/etc.).\n\n2.  Selective projection (red box):  Now that we know exactly which tokens are problematic, we take only those token embeddings and project them (a) into the learned input subspace ℐ to keep them coherent with the rest of the prompt and then (b) orthogonally away from the toxic subspace 𝐶.  This “detoxifies” them without scrubbing away the entire prompt or wrecking its semantics.\n\n3.  Why that order?  If you tried to project blindly before knowing which tokens matter, you’d either (a) alter harmless tokens unnecessarily or (b) miss some of the really toxic ones.  By detecting first (blue) and then projecting (red), you guarantee a minimal, targeted intervention that preserves overall prompt quality while removing unsafe directions."
    },
    {
        "question": "What reasoning underlies combining Gram matrices and MLPs for weather-aware feature extraction?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Weather variations can be viewed as distinct image “styles”, which are inherently decoupled from the image content.",
            "The Gram matrix, which represents correlations within feature maps, is commonly used to define image styles.",
            "Yet, the original form of the Gram matrix fails in the context of multi-weather restoration, as it represents artistic styles rather than weather-relevant features.",
            "To address this, we append trainable projection layers—multi-layer perceptrons (MLPs)—on top of the vanilla Gram matrix, to learn weather-specific “style”."
        ],
        "final_answer": "The authors treat each weather degradation as an image “style” independent of content, so they use the Gram matrix to capture feature-correlation statistics (style) and then place MLPs on top to learn to map those raw style descriptors into weather-specific embeddings. This combination lets the network decouple weather effects from scene content and produce a compact, learned weather-aware feature vector.",
        "relevant_elements": [
            "Gram Matrix",
            "MLP"
        ],
        "id": 1974,
        "masked_question": "What reasoning underlies combining [mask1] and [mask2] for weather-aware feature extraction?",
        "masked_number": 2,
        "masked_elements": [
            "Gram Matrix",
            "MLP"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.17226v1_figure_2.png",
        "paperid": "2411.17226v1",
        "paper_path": "./MISS-QA/papers/2411.17226v1.json",
        "figure_id": "2411.17226v1_figure_2.png",
        "caption": "Figure 2: The architecture of MWFormer. The main image processing network consists of a Transformer encoder, a Transformer decoder, and convolution tails.\n(a) A feature extraction network learns to generate some of the parameters of the Transformer blocks and intra-patch Transformer blocks in the main network, thereby partially controlling the production of intermediate feature maps. (b) The Transformer block in the encoder of the main network, which is guided by the feature vector. (c) Transformer decoder of the main network, whose queries are learnable parameters.",
        "qtype": "Design_Rationale",
        "response": "Here is the step‐by‐step reasoning that links the red‐boxed module ([mask1]) to the blue‐boxed module ([mask2]) and explains why we combine them to get a weather‐aware restoration network:\n\n1.  Analogy to Style and Noise  \n    -  In blind image denoising one often gets a big boost by telling the network what the noise level or type is (i.e. “non-blind” denoising).  \n    -  Likewise, different weather effects (rain, snow, haze, etc.) can be viewed as different “styles” or “degradations” that ought to be treated differently, even though the underlying image content is the same.\n\n2.  Extracting a Compact Weather “Style” Vector ([mask1], red box)  \n    -  We tap the first two scales of the Transformer encoder and compute Gram matrices over their feature maps.  A Gram matrix measures channel‐wise correlations, which in the style‐transfer literature corresponds to image “style.”  \n    -  Because vanilla Gram matrices capture artistic style rather than weather‐specific patterns, we follow each matrix with a small MLP (the so-called projection layers) to learn a 64-D embedding of the weather degradation present in the input.  \n    -  We do this at two scales, concatenate the two 64-D vectors, and then project down to one final weather feature vector z∈ℝ^D.  \n    -  This entire block is trained with a contrastive loss so that images sharing the same weather cluster together in z-space and images with different weather pull apart.\n\n3.  Conditioning the Restoration Network on z ([mask2], blue box)  \n    Once we have z, we turn our single Transformer‐based restoration backbone into a *conditional* network that adapts itself to exactly that weather style:\n    a.  Spatially **local** adaptivity  \n        – We insert a depth-wise convolution between the two MLP layers in each encoder block’s FFN.  \n        – Instead of using a fixed set of kernels, we use a tiny HyperMLP that takes z as input and *generates* the weights of that depth-wise convolution.  \n        – Different weather embeddings ⇒ different local filters.  \n\n    b.  Spatially **global** adaptivity  \n        – In the self-attention layers we normally have three learned projection matrices W_Q, W_K, W_V.  \n        – Here again we feed z into another HyperMLP to *produce* these projection parameters on the fly.  \n        – This means the attention heads themselves adapt to the current weather type.  \n\n    c.  **Channel-wise** feature modulation  \n        – Before each patch‐embedding layer we generate a set of scale-and‐shift (γ, β) parameters from z.  \n        – We apply them channel‐wise to the feature maps (an affine transform), re-calibrating each channel based on the weather.  \n\n4.  Why Combine Them?  \n    -  The red box alone only *extracts* a weather descriptor; the blue box alone is a vanilla Transformer with no weather awareness.  \n    -  By marrying the two, however, we turn our backbone into a *non-blind* (weather-aware) restoration network:  \n      – we first *measure* the weather style,  \n      – then *inject* that information at every level of the encoder (and decoder queries) to adapt both local filters, global attention, and channel strengths.  \n    -  This joint design disentangles weather artifacts from image content and lets a single set of weights handle rain, snow, haze, or any mix thereof—simply by changing the feature vector z that flows from the red‐box extractor into the blue‐box Transformer."
    },
    {
        "question": "What is the rationale for generating depthwise convolution kernels via HyperMLP for local adaptivity?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Since vanilla Transformer architectures lack inductive biases expressive of local pixel interactions, we add a depthwise convolution layer between the two MLPs in each feed-forward network (FFN) in the Transformer blocks.",
            "Unlike previous models, however, we leverage the predicted weather type features z computed by the hyper-network h to generate the parameters of the depthwise convolution layers, so that pictures degraded by different weather types will be processed by different filters adaptively.",
            "The feature vector z is fed into a 2-layer projection MLP (named HyperMLP in Fig. 2) since it is intended to generate the parameters of other modules, then reshaped to the 2D depthwise convolution kernels W (omitting the batch dimension) that are used to convolve the input X."
        ],
        "final_answer": "Vanilla Transformers do not inherently model local pixel interactions. To introduce this local inductive bias, MWFormer inserts a depthwise convolution into each Transformer feed-forward block. Moreover, since different weather degradations require different local filtering, the network uses the weather feature vector z and a small HyperMLP to dynamically generate the depthwise convolution kernels. This lets the model adapt its local filtering to the predicted weather condition, improving restoration under varied local degradations.",
        "relevant_elements": [
            "Depthwise Convolution",
            "HyperMLP"
        ],
        "id": 1975,
        "masked_question": "What is the rationale for generating [mask1] kernels via HyperMLP for local adaptivity?",
        "masked_number": 1,
        "masked_elements": [
            "Depthwise Convolution"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.17226v1_figure_2.png",
        "paperid": "2411.17226v1",
        "paper_path": "./MISS-QA/papers/2411.17226v1.json",
        "figure_id": "2411.17226v1_figure_2.png",
        "caption": "Figure 2: The architecture of MWFormer. The main image processing network consists of a Transformer encoder, a Transformer decoder, and convolution tails.\n(a) A feature extraction network learns to generate some of the parameters of the Transformer blocks and intra-patch Transformer blocks in the main network, thereby partially controlling the production of intermediate feature maps. (b) The Transformer block in the encoder of the main network, which is guided by the feature vector. (c) Transformer decoder of the main network, whose queries are learnable parameters.",
        "qtype": "Design_Rationale",
        "response": "Mask₁ turns out to be the depth‐wise convolution kernels that sit in the FFN of each Transformer block.  The paper makes two points:\n\n 1.  Vanilla Transformers have no built-in “local” bias, yet removing, say, rain streaks is fundamentally a local operation.\n 2.  Different weather degradations demand different local filters (rain streaks vs. raindrops vs. snowflakes).\n\nSo instead of using one fixed set of depth‐wise conv kernels, they take the weather‐type embedding f from the hyper‐network, feed it through a tiny MLP (“HyperMLP”), and reshape its output into the actual 2D convolution kernels.  In this way the model can generate a bespoke set of local filters, on the fly, that are tailored to whichever weather degradation it is currently seeing."
    },
    {
        "question": "How does HyperMLP reshape the weather vector into 2D depthwise convolution kernels?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The feature vector c is fed into a 2-layer projection MLP (named HyperMLP in Fig. 2 since it is intended to generate the parameters of other modules), then reshaped to the 2D depthwise convolution kernels W (omitting the batch dimension) that are used to convolve the input x.",
            "W denotes the weights of the depthwise convolution generated by reshaping the projection of the c vector, x denotes the input of the spatially local operation (i.e., depthwise convolution), dwconv denotes depthwise convolution, and σ denotes nonlinear activation."
        ],
        "final_answer": "HyperMLP takes the weather feature vector as input, passes it through a two-layer projection MLP, and then reshapes the MLP’s output into the 2D depthwise convolution kernels (omitting the batch dimension) which are used to convolve the input feature map.",
        "relevant_elements": [
            "HyperMLP",
            "Depthwise Conv"
        ],
        "id": 1976,
        "masked_question": "How does [mask1] reshape the weather vector into 2D [mask2] kernels?",
        "masked_number": 2,
        "masked_elements": [
            "HyperMLP",
            "Depthwise Conv"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.17226v1_figure_2.png",
        "paperid": "2411.17226v1",
        "paper_path": "./MISS-QA/papers/2411.17226v1.json",
        "figure_id": "2411.17226v1_figure_2.png",
        "caption": "Figure 2: The architecture of MWFormer. The main image processing network consists of a Transformer encoder, a Transformer decoder, and convolution tails.\n(a) A feature extraction network learns to generate some of the parameters of the Transformer blocks and intra-patch Transformer blocks in the main network, thereby partially controlling the production of intermediate feature maps. (b) The Transformer block in the encoder of the main network, which is guided by the feature vector. (c) Transformer decoder of the main network, whose queries are learnable parameters.",
        "qtype": "Implementation_Details",
        "response": "In the diagram the little red‐boxed “HyperMLP” is exactly what takes your 1D weather embedding z and turns it into a stack of 2D convolution filters (the blue‐boxed depth‐wise conv kernels).  Concretely, you feed z into a two-layer projection MLP (that’s your HyperMLP), it spits out a flat weight vector of length C·k·k, and then you simply apply a reshape operator R to that vector so that it becomes C separate k×k kernels.  Those are exactly the depth-wise convolution kernels shown in the blue box."
    },
    {
        "question": "How are upper triangular Gram matrices transformed by MLPs to produce normalized weather embeddings?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We utilize the first two scales of the Transformer encoders, where a Gram matrix is computed at each scale. Since Gram matrices are symmetric, only the upper triangular parts of the two matrices are vectorized to save computation.",
            "These vectors are further fed to the two projection layers (MLPs), thereby generating two 64-dimensional embeddings.",
            "Finally, the two embeddings are concatenated and projected onto a single feature vector , which encodes the weather-degradation information from the input image."
        ],
        "final_answer": "At each of the first two encoder scales a Gram matrix is formed and only its upper-triangular entries are extracted into a vector. Each of these two vectors is then passed through its own MLP to produce a 64-dimensional embedding. Finally, the two 64-D outputs are concatenated and linearly projected into a single weather-degradation feature vector (which is then L₂-normalized in the contrastive learning pipeline).",
        "relevant_elements": [
            "Gram Matrix",
            "MLP"
        ],
        "id": 1977,
        "masked_question": "How are upper triangular [mask1] transformed by [mask2] to produce normalized weather embeddings?",
        "masked_number": 2,
        "masked_elements": [
            "Gram Matrix",
            "MLP"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.17226v1_figure_2.png",
        "paperid": "2411.17226v1",
        "paper_path": "./MISS-QA/papers/2411.17226v1.json",
        "figure_id": "2411.17226v1_figure_2.png",
        "caption": "Figure 2: The architecture of MWFormer. The main image processing network consists of a Transformer encoder, a Transformer decoder, and convolution tails.\n(a) A feature extraction network learns to generate some of the parameters of the Transformer blocks and intra-patch Transformer blocks in the main network, thereby partially controlling the production of intermediate feature maps. (b) The Transformer block in the encoder of the main network, which is guided by the feature vector. (c) Transformer decoder of the main network, whose queries are learnable parameters.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ is simply the upper‐triangular entries of the Gram matrix at each of the two scales.  What the blue‐boxed MLPs (Mask₂) do is:\n\n 1.  Flatten (vectorize) those upper‐triangular entries at scale 1 into a 1D vector, and feed that into a small two‐layer MLP.  Its output is a 64-dimensional vector.  \n 2.  Do exactly the same for the upper‐triangular entries at scale 2, with its own two‐layer MLP, yielding another 64-dimensional vector.  \n 3.  Concatenate those two 64-dim vectors into a 128-dim vector, pass it through one more linear-projection MLP, and then L₂–normalize the result.  \n\nThe end result is a single, unit–length “weather embedding” φ that the rest of the network uses to condition all its blocks."
    },
    {
        "question": "How does the validator mechanism evaluate and filter local model updates before miner processing?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Subsequently, each validator v examines worker transactions τ_k, one at a time, in a sequential manner until all transactions are verified. Different validators should share the worker transactions they received with peers, ensuring that each validator receives all worker transactions for the current communication round. Subsequently, v discards the transaction data without a digital signature, proceeds to extract m_k^t from τ_k with a digital signature and evaluate its validity using the DFLoc validator mechanism. Afterward, v issues either a positive or negative vote, denoted as α_k^t, based on the outcome of the validation process.",
            "In the t-th communication round, a validator v typically evaluates the quality of the update model m_k^t by comparing its testing localization accuracy Acc(m_k^t) against that of a single-epoch trained local model, denoted as m̃_k, on the worker’s test dataset D_k^test, as suggested by [26]. If noise distorts m_k^t, Acc(m_k^t) will differ, leading to a decline in accuracy compared to m̃_k. Conversely, unaltered m_k^t yields minimal differences between Acc(m_k^t) and Acc(m̃_k). Notably, v lacks access to m_k^t, so it cannot directly obtain the value pair (Acc(m_k^t),Acc(m̃_k)).",
            "A viable solution to address this issue involves validator v initially conducting a single-epoch of local learning by using global model m_g^t and its train dataset D_v^train to obtain a local update model m̃_v, and computing the performance of m̃_v and m_g^t under v’s test dataset D_v^test, denoted as Acc(m̃_v) and Acc(m_g^t), respectively. Subsequently, they serve as the proxy evaluation for Acc(m_k^t) and Acc(m̃_k).",
            "In BFC, validator v evaluates the potential distortion of m_k^t by calculating the validation accuracy difference, denoted as Δ̃_k = Acc(m_g^t) − Acc(m̃_v), and comparing it to a validator-threshold value τ. The hypothesis behind this is that the value of Δ̃_k will differ between an update sent by a legitimate worker and that by a malicious worker. If Δ̃_k exceeds τ, indicating that the accuracy drop exceeds v’s tolerance threshold, validator v assigns a negative vote to m_k^t; otherwise, v issues a positive vote.",
            "In the LLR part, validator v calculates the validation loss ratio, denoted as Γ̃_k = L(m̃_v)/L(m_g^t), following the computation of L(m̃_v) and L(m_g^t). This ratio is then compared to a validator-threshold value σ to assess the potential distortion of m_k^t. Similarly, v issues a positive vote if Γ̃_k is below σ and a negative vote otherwise."
        ],
        "final_answer": "Each validator first discards any unsigned worker transactions, then extracts the submitted local update m_k^t and applies the DFLoc validator mechanism to decide whether to accept or reject it. Since v cannot directly measure the worker’s true update, v trains its own single‐epoch model m̃_v from the current global model on its local data and evaluates both m̃_v and the global model on its test set. For classification (BFC), v computes the proxy accuracy drop Δ̃_k and compares it to a threshold τ; for regression (LLR), v computes the proxy loss ratio Γ̃_k and compares it to a threshold σ. If the metric exceeds its threshold, the update is deemed malicious and receives a negative vote; otherwise it receives a positive vote. Only updates with more positive than negative votes are forwarded to miners for aggregation.",
        "relevant_elements": [
            "Validator",
            "Local Model"
        ],
        "id": 1978,
        "masked_question": "How does the [mask1] mechanism evaluate and filter local model updates before miner processing?",
        "masked_number": 1,
        "masked_elements": [
            "Validator"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.07921v1_figure_1.png",
        "paperid": "2407.07921v1",
        "paper_path": "./MISS-QA/papers/2407.07921v1.json",
        "figure_id": "2407.07921v1_figure_1.png",
        "caption": "Figure 1: DFLoc system overview. Clients handle tasks including model training, verification, and block mining, constituting a blockchain that can aggregate and distribute the global model, replacing the central server.",
        "qtype": "Implementation_Details",
        "response": "In the figure, the little hour‐glass‐and‐shield icon boxed in red is nothing mystical at all but the DFLoc “validator” check that every fresh local update must pass before any miner will even consider including it in a block.  Concretely, each validator takes the candidate update fₖ,ᵢ and does the following:\n\n 1.  Starting from the current global model gₖ, it runs exactly one epoch of local training on its own private data to produce a “baseline” update ĥₖ,ᵢ.  \n 2.  It then evaluates both the candidate update fₖ,ᵢ and the baseline ĥₖ,ᵢ on the validator’s hold‐out (test) set, measuring  \n     •  The accuracy gap ΔAₖ,ᵢ = A(ĥₖ,ᵢ) – A(fₖ,ᵢ), and  \n     •  The loss ratio Γₖ,ᵢ = (L(fₖ,ᵢ) / L(ĥₖ,ᵢ)).  \n 3.  Each of those two numbers is compared to a small, pre‐set threshold (δ for accuracy drop, ζ for loss‐ratio).  If either metric exceeds its threshold, the update is deemed “suspicious” (i.e. potentially poisoned or simply aberrant) and the validator casts a negative vote; if both metrics lie within tolerance, it casts a positive vote.  \n 4.  Only those local‐model updates that collect more positive than negative votes across all validators are passed on to the miners for inclusion in the next block.  \n\nIn this way the red‐boxed “validator mechanism” automatically filters out bad or malicious updates before any miner even looks at them."
    },
    {
        "question": "How do miners apply PoS consensus to finalize block selection based on validator votes?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Afterward, each miner i undertakes the consolidation of summarized results, including all local updated models {\\tilde{w}^k_t}_{k=1}^K and corresponding summarized votes {V_t(\\tilde{w}^k_t)}_{k=1}^K, all worker rewards {R^w_{t,k}}_{k=1}^K, all validator rewards {R^{val}_{t,i}}_{i=1}^V, and its own reward r^{mining}_{t,i}, to assemble a candidate block denoted as B^i_t. Subsequently, miner i engages in the mining process specific to the Proof of Stake (PoS) consensus which involves hashing the complete content of the block and signing the hash by using its private key. Upon B^i_t being mined, miner i propagates the mined block to all the other miners in the network.",
            "Following this dissemination, each miner j leverages the stake information recorded on its blockchain to identify and select the block generated by the miner with the highest stake among the set {B^i_t}_{i=1}^N as the legitimate block \\hat{B}_t. Only this legitimate block is deemed suitable for extracting the records of rewards and summarized votes, along with their corresponding model updates."
        ],
        "final_answer": "Each miner first constructs and ‘mines’ a candidate block by hashing and signing it under PoS rules. Once all candidate blocks are broadcast, every miner examines the stake information on its local chain and chooses the block proposed by the miner holding the highest stake among those candidates. That highest-stake block is then accepted as the legitimate block for that round.",
        "relevant_elements": [
            "Miner",
            "PoS Consensus"
        ],
        "id": 1979,
        "masked_question": "How do [mask1] apply [mask2] to finalize block selection based on validator votes?",
        "masked_number": 2,
        "masked_elements": [
            "Miner",
            "PoS Consensus"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.07921v1_figure_1.png",
        "paperid": "2407.07921v1",
        "paper_path": "./MISS-QA/papers/2407.07921v1.json",
        "figure_id": "2407.07921v1_figure_1.png",
        "caption": "Figure 1: DFLoc system overview. Clients handle tasks including model training, verification, and block mining, constituting a blockchain that can aggregate and distribute the global model, replacing the central server.",
        "qtype": "Implementation_Details",
        "response": "In DFLoc, it is the miners (mask1) that actually “run” the PoS-consensus step (mask2).  Concretely, once every miner has collected all of the validator votes and packaged them into its own candidate block, each miner does the following:\n\n 1.  Hashes and signs the entirety of its candidate block (i.e. the set of local model updates plus the positive/negative vote tallies).  \n 2.  Examines its local copy of the chain’s stake ledger to see how much stake each block‐proposing miner has locked up.  \n 3.  Compares the stakes of all competing candidate blocks and selects the one proposed by the miner with the largest stake.  \n 4.  Broadcasts that “highest‐stake” block as the single, legitimate block for the round.  \n\nBy choosing the candidate block whose proposer holds the most stake, every honest miner converges on the exact same block, thus “finalizing” the selection based on the aggregated validator votes."
    },
    {
        "question": "How does miner aggregation of local model updates improve security over centralized server aggregation?",
        "relevant_section_ids": [
            "1",
            "2.2",
            "3.2"
        ],
        "relevant_context": [
            "On the one hand, as for single-point failure, opting for introducing decentralized technology to ease the over-reliance on the central server is a favorable solution. To this end, we introduce blockchain techniques due to their attributes of decentralization, traceability, and immutability. On the other hand, to counter malicious attacks, we design an update verification mechanism to differentiate between legitimate and malicious model updates, safeguarding our trained model from malicious alterations.",
            "As a result of the integration of blockchain and FL, Blockchain-based federated learning (BCFL) can mitigate the single-point failure and malicious attacks.",
            "Finally, each device, regardless of its previous role, is tasked with two pivotal responsibilities to finish the t-th communication round. Firstly, it engages in the aggregation of locally updated models in the legitimate block whose count of positive votes is not less than that of negative votes. This aggregation process yields a new global model. … This ensures that models suspected of poor quality or tampering are excluded from the model aggregation process."
        ],
        "final_answer": "By having miners collect only those locally trained models that have been signed, verified by validators, and voted ‘positive’ more often than ‘negative,’ DFLoc replaces a single, central aggregator with a decentralized, stake-based consensus. Each miner bundles vetted updates into a block via Proof-of-Stake mining and the network then agrees on the block with highest stake. Since only updates passing the validator checks make it into the block, and no single server holds all the power, this approach eliminates the single-point failure risk and excludes malicious or tampered model updates.",
        "relevant_elements": [
            "Miner",
            "Local Model"
        ],
        "id": 1981,
        "masked_question": "How does [mask1] aggregation of [mask2] updates improve security over centralized server aggregation?",
        "masked_number": 2,
        "masked_elements": [
            "Miner",
            "Local Model"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.07921v1_figure_1.png",
        "paperid": "2407.07921v1",
        "paper_path": "./MISS-QA/papers/2407.07921v1.json",
        "figure_id": "2407.07921v1_figure_1.png",
        "caption": "Figure 1: DFLoc system overview. Clients handle tasks including model training, verification, and block mining, constituting a blockchain that can aggregate and distribute the global model, replacing the central server.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the miner-based (i.e. blockchain) aggregation step, and mask2 are the client’s local-model updates.  \n\nBy having a pool of miners collect only those local updates that have already passed the validator votes and then use a PoS consensus to bundle them into an on-chain block, you\n\n1. remove the single trust anchor (no more one central server that can be attacked or go down),  \n2. ensure that every included update carries a signed, positive‐vote record (so poisoned or malformed updates never make it into the global model), and  \n3. pin the aggregation itself into an immutable, auditable ledger.  \n\nTogether, these guarantees make the system far harder to subvert than a classic federated server that simply sums or averages raw client updates."
    },
    {
        "question": "How does Adaptive Frequency Weight Matrix modify Frequency Distance Matrix to address DCT’s energy concentration bias?",
        "relevant_section_ids": [
            "3.1.2",
            "3.1.3"
        ],
        "relevant_context": [
            "In FFL (Jiang et al. 2021), frequency distance measures image differences, but DCT’s magnitude spectrum differs from DFT’s. The spectrum weight in (Jiang et al. 2021) ignores the frequency values’ dynamic range, where high frequencies are much smaller than low ones. To improve this, we adjust the frequency weighting function using the absolute value of the logarithm of the spectrum distance. This adjustment increases the weight for lower scale distances. The frequency distance between reference points in image pairs is defined in Euclidean space at spectral position (u,v) as the frequency distance matrix F. Here, μ serves as a control factor to adjust changes in each section. Similar to FFL, we normalize the weight matrix by dividing F by its maximum value Fmax, and refer to it as F̂ from now on.",
            "As shown in Fig. 3, the distribution of DCT is different from DFT, being more energy-concentrated with a single low-frequency point at the upper left corner. For effective image reconstruction, emphasizing high-frequency details is crucial. To address this, we introduce a control coefficient α to enforce the model in focusing on high-frequency components. Thus, we use a weight mask M that aligns with the spectrum distribution of DCT, defined as follows:  M(u,v)=1 if √(u²+v²)>τ, else α, where τ is a threshold for controlling the magnitude of the frequency spectrum."
        ],
        "final_answer": "The Adaptive Frequency Weight Matrix takes the normalized Frequency Distance Matrix and re-weights it with a DCT-aligned mask: it applies a thresholded mask M(u,v) that assigns a higher weight (e.g. 1) to coefficients whose frequency radius exceeds a preset threshold τ, and a lower control coefficient α to those below τ. This element-wise multiplication boosts the relative contribution of high-frequency positions, counteracting the DCT’s tendency to concentrate energy at low frequencies.",
        "relevant_elements": [
            "Adaptive Frequency Weight Matrix",
            "Frequency Distance Matrix"
        ],
        "id": 1982,
        "masked_question": "How does [mask1] modify [mask2] to address DCT’s energy concentration bias?",
        "masked_number": 2,
        "masked_elements": [
            "Adaptive Frequency Weight Matrix",
            "Frequency Distance Matrix"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.13716v1_figure_2.png",
        "paperid": "2408.13716v1",
        "paper_path": "./MISS-QA/papers/2408.13716v1.json",
        "figure_id": "2408.13716v1_figure_2.png",
        "caption": "Figure 2: Overview of FreqINR. The inference process for INR-based models (light blue) is guided by our core component, Adaptive DCT Frequency Loss (dark blue), which leverages the Frequency Distance Matrix (light green) and the Adaptive Frequency Weight Matrix (dark green) to dynamically enhance fine detail learning.",
        "qtype": "Literature_Background",
        "response": "In FreqINR’s training pipeline (see Fig. 2 and Sec. “Adaptive Frequency Weighting Matrix”), the thing in the red box is the Adaptive Frequency Weighting Matrix (AFWM), and the thing in the blue box is the Frequency Distance Matrix (FDM).  The AFWM is nothing more than the FDM re-scaled (element-wise multiplied) by a simple “high-frequency boosting” mask.  Concretely, after you compute the raw spectral-distance weights in FDM (which, because of DCT’s energy concentration, are overwhelmingly large at very low frequencies), you introduce a small control coefficient λ and form a DCT-shaped mask that grows with frequency.  You then do:  \n\n   AdaptiveFrequencyWeight = FrequencyDistanceMatrix ⊙ BoostMask(λ)  \n\nThis has the effect of down-weighting the massive low-frequency entries that DCT naturally concentrates energy into, and up-weighting the mid- and high-frequency entries you really want the network to learn.  By multiplying FDM by this adaptive mask, FreqINR corrects for DCT’s single low-frequency bias and forces the model to pay attention to finer details."
    },
    {
        "question": "How does Adaptive DCT Frequency Loss interact with Spatial Loss to balance spatial and spectral objectives?",
        "relevant_section_ids": [
            "1",
            "3.2.1",
            "3.4.2"
        ],
        "relevant_context": [
            "During training, FreqINR integrates frequency consistency and global spatial correlations between HR and ground-truth images into the Implicit Neural Function framework through a unified objective function that combines pixel-wise loss with our Adaptive Discrete Cosine Transform Frequency Loss (ADFL).",
            "Eventually, after passing through a decoder, a spatial loss and a global frequency loss defined as the final objective function that could improve widely used in Implicit Neural Function (INF) for SR, where λ indicates the hyper-parameter balanced the trade-off between the contribution of two losses.",
            "Finally, we combine both spatial loss and frequency loss to form the objective function for training the network."
        ],
        "final_answer": "Adaptive DCT Frequency Loss (ADFL) is simply added to the usual per-pixel spatial (pixel-wise) loss to form a single, unified training objective. A scalar hyperparameter λ controls how much weight the network gives to the spectral (frequency) term versus the spatial term, thereby balancing the model’s attention between matching high-frequency details and preserving overall image fidelity.",
        "relevant_elements": [
            "Adaptive DCT Frequency Loss",
            "Spatial Loss"
        ],
        "id": 1983,
        "masked_question": "How does [mask1] interact with [mask2] to balance spatial and spectral objectives?",
        "masked_number": 2,
        "masked_elements": [
            "Adaptive DCT Frequency Loss",
            "Spatial Loss"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.13716v1_figure_2.png",
        "paperid": "2408.13716v1",
        "paper_path": "./MISS-QA/papers/2408.13716v1.json",
        "figure_id": "2408.13716v1_figure_2.png",
        "caption": "Figure 2: Overview of FreqINR. The inference process for INR-based models (light blue) is guided by our core component, Adaptive DCT Frequency Loss (dark blue), which leverages the Frequency Distance Matrix (light green) and the Adaptive Frequency Weight Matrix (dark green) to dynamically enhance fine detail learning.",
        "qtype": "Literature_Background",
        "response": "Here’s the core of the interaction, step by step:\n\n1.   Mask 1 is the “Adaptive DCT Frequency Loss” (ADFL).  It lives in the DCT (frequency) domain and is computed by  \n     a.  taking the DCT of the network’s output and of the ground truth,  \n     b.  measuring their point‐wise spectral distance via the Frequency Distance Matrix (FDM),  \n     c.  re‐weighting that distance with the Adaptive Frequency Weighting Matrix (AFWM) so that “hard” (high-frequency) bands are emphasized.\n\n2.   Mask 2 is the ordinary per-pixel “Spatial Loss” (e.g. ℓ₁ or ℓ₂ on the RGB pixels).  It ensures that low-frequency, coarse image structures are recovered accurately.\n\n3.   These two losses are simply summed (with a tunable scalar λ controlling their relative strength) to form the final training objective:  \n        Lₜₒₜₐₗ  =  L_spatial  +  λ · L_ADFL.  \n    By adjusting λ, one trades off pure pixel-space fidelity against spectral fidelity in the high-frequency bands.\n\n4.   In practice this means the network simultaneously learns to minimize the usual per-pixel error (mask 2) and to close the spectral gap at the frequencies identified by mask 1, producing reconstructions that are both sharp (thanks to the frequency term) and spatially accurate (thanks to the pixel term).\n\nThat weighted sum is exactly how the Adaptive DCT Frequency Loss (red box) “interacts” with the Spatial Loss (blue box) to balance the two objectives."
    },
    {
        "question": "How does Frequency Distance Matrix guide Adaptive Frequency Weight Matrix to emphasize high-frequency components?",
        "relevant_section_ids": [
            "3.1",
            "3.1.2",
            "3.1.3"
        ],
        "relevant_context": [
            "In this section, we describe the key techniques of FreqINR: Adaptive DCT Frequency Loss (ADFL) for training and Enhanced Receptive Field Encoder for inference.  The overall architecture of FreqINR is illustrated in Fig. 2.",
            "During training, we introduce Adaptive DCT Frequency Loss (ADFL).  First, we represent image by DCT bases.  Then, we employ the Frequency Distance Matrix (FDM) to guide the Adaptive Frequency Weighting Matrix (AFWM) in dynamically minimizing spectral discrepancies of generated HR and ground-truth.",
            "The frequency distance between reference points in image pairs I_t and I_g is defined in Euclidean space at spectral position k as the frequency distance matrix.  Similar to FFL, we normalize the weight matrix by dividing W by its maximum value W_max, and refer to it as FDM from now on.",
            "As shown in Fig. 3, the distribution of DCT is difference from DFT, being more energy-concentrated with a single low-frequency point at the upper left corner.  For effective image reconstruction, emphasizing high-frequency details is crucial.  To address this, we introduce a control coefficient α, to enforce the model in focusing on high-frequency components.  Thus, we use a weight mask M that aligns with spectrum distribution of DCT, defined as follows: where T is a constant that represents the threshold for controlling the magnitude of the frequency spectrum."
        ],
        "final_answer": "The Frequency Distance Matrix measures per-frequency spectral discrepancies between the generated and ground-truth images and, once normalized, becomes FDM.  The Adaptive Frequency Weighting Matrix then uses FDM together with a DCT-aligned mask and a control coefficient α: frequencies with larger FDM values—i.e. the higher-frequency components where spectral gaps are greatest—are given higher weight (above a threshold T), thereby dynamically emphasizing high-frequency details during training.",
        "relevant_elements": [
            "Frequency Distance Matrix",
            "Adaptive Frequency Weight Matrix"
        ],
        "id": 1984,
        "masked_question": "How does [mask1] guide [mask2] to emphasize high-frequency components?",
        "masked_number": 2,
        "masked_elements": [
            "Frequency Distance Matrix",
            "Adaptive Frequency Weight Matrix"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.13716v1_figure_2.png",
        "paperid": "2408.13716v1",
        "paper_path": "./MISS-QA/papers/2408.13716v1.json",
        "figure_id": "2408.13716v1_figure_2.png",
        "caption": "Figure 2: Overview of FreqINR. The inference process for INR-based models (light blue) is guided by our core component, Adaptive DCT Frequency Loss (dark blue), which leverages the Frequency Distance Matrix (light green) and the Adaptive Frequency Weight Matrix (dark green) to dynamically enhance fine detail learning.",
        "qtype": "Experimental_Results",
        "response": "Here, the red‐boxed “Frequency Distance Matrix” (FDM) is literally the per‐frequency error map between the fake and real DCT spectra.  During training we take that FDM, apply a simple log‐scaling and normalization, and then feed it straight into the blue‐boxed “Adaptive Frequency Weight Matrix” (AFWM).  Concretely, the FDM tells the AFWM exactly which frequency bins are still off, so the AFWM raises the loss weight on those bins (and since high frequencies tend to have the largest spectral gaps, they end up with the biggest weights).  In this way the FDM “guides” the AFWM to spotlight—i.e. to emphasize—high‐frequency components in the overall loss."
    },
    {
        "question": "How does the spatial loss combine with Adaptive DCT Frequency Loss to form the final objective during training?",
        "relevant_section_ids": [
            "3.1",
            "3.2.1",
            "3.4.2"
        ],
        "relevant_context": [
            "Finally, we integrate ADFL into the per-pixel spatial loss to form our final objective function.",
            "Eventually, after passing through a decoder, a spatial loss and a global frequency loss defined as the final objective function that could improve widely used in Implicit Neural Function (INF) for SR, where λ indicates the hyper-parameter balanced the trade-off between the contribution of two losses.",
            "Finally, we combine both spatial loss and frequency loss to form the objective function for training the network."
        ],
        "final_answer": "During training, the network is supervised by the sum of the per-pixel spatial loss and the Adaptive DCT Frequency Loss. A hyper-parameter λ is used to balance the relative contributions of the spatial and frequency losses in the final objective.",
        "relevant_elements": [
            "Spatial Loss",
            "Adaptive DCT Frequency Loss"
        ],
        "id": 1985,
        "masked_question": "How does the [mask1] combine with [mask2] to form the final objective during training?",
        "masked_number": 2,
        "masked_elements": [
            "Spatial Loss",
            "Adaptive DCT Frequency Loss"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.13716v1_figure_2.png",
        "paperid": "2408.13716v1",
        "paper_path": "./MISS-QA/papers/2408.13716v1.json",
        "figure_id": "2408.13716v1_figure_2.png",
        "caption": "Figure 2: Overview of FreqINR. The inference process for INR-based models (light blue) is guided by our core component, Adaptive DCT Frequency Loss (dark blue), which leverages the Frequency Distance Matrix (light green) and the Adaptive Frequency Weight Matrix (dark green) to dynamically enhance fine detail learning.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed “Spatial Loss” (mask1) and the blue‐boxed “Adaptive DCT Frequency Loss” (mask2) are simply added together (with a scalar trade‐off weight) to form the final training objective.  In other words, if Lspatial is the pixel‐wise reconstruction loss and Lfreq is the adaptive DCT frequency loss, then the overall loss is  \nLtotal = Lspatial + λ · Lfreq  \nwhere λ is a hyper‐parameter that balances the two terms."
    },
    {
        "question": "How does dataset construction account for differences in sequence length among TF, MC, and DG outputs?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To elucidate the impact of different editing objectives on the performance of the edited model, we created a Multi-Question Dataset (MQD) based on the ATOMIC commonsense database (Sap et al., 2019). This dataset comprises three question types: true/false, multiple-choice, and direct generation. The corresponding editing objectives are yes/no, a/b/c/d, and entity/event, respectively. Each question type consists of 4000 samples.",
            "According to our statistical analysis, the average length of the input tokens for the three question types is 23.44, 35.03, and 13.38, respectively, while the average length of the editing objectives tokens is 1, 1, and 3.88, respectively.",
            "The true/false questions have two possible output types: yes or no. The multiple-choice questions have four editing objectives: a, b, c, and d. In contrast, the directly generated questions have more diverse editing objectives, including entities or events, with the number of tokens for events typically exceeding 1."
        ],
        "final_answer": "In constructing the MQD dataset, the authors created three parallel question‐format branches—TF, MC, and DG—each with the same core prompt material but distinct output objectives whose token lengths naturally differ. For TF and MC they use single‐token answers (yes/no or a–d), whereas DG outputs are full entities or event phrases averaging 3.88 tokens. They controlled for these length differences by (a) giving all three formats the same number of samples, (b) measuring and reporting both input lengths (23.44, 35.03, 13.38 tokens) and output lengths (1, 1, 3.88 tokens), and (c) designing each format’s prompt structure (binary logic checks, multiple‐choice distractors, or direct concatenation) so that sequence lengths vary only in the editing objective.",
        "relevant_elements": [
            "Dataset construction",
            "Sequence length"
        ],
        "id": 1986,
        "masked_question": "How does [mask1] account for differences in sequence length among TF, MC, and DG outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Dataset construction"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.23843v1_figure_1.png",
        "paperid": "2410.23843v1",
        "paper_path": "./MISS-QA/papers/2410.23843v1.json",
        "figure_id": "2410.23843v1_figure_1.png",
        "caption": "Fig. 1: This framework outlines the comprehensive approach to understanding the performance decline of edited models. On the left, traditional knowledge editing tasks are categorized into different types, each with distinct editing objectives: yes/no, a/b/c/d, and entity/event. On the right, our experiments are structured from both data and model perspectives. From the data perspective, we conduct three experiments: (a) a comprehensive performance evaluation of the model, (b) the construction of a Multi-Question Dataset (MQD), and (c) an assessment of the impact of editing different target outputs on model performance. From the model perspective, we design four experiments: (d) an evaluation of the edited model’s forgetting ability, (e) an identification of the current knowledge editing method’s bottleneck and an exploration of the correlation between editing probability values and parameter layer norms, and (f) a proposal of a sequence editing method, which effectively enhances the performance of the edited model.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the “Dataset construction” block (experiment b), i.e. our creation of the Multi-Question Dataset (MQD).  In that stage we deliberately build three sub-datasets that all draw on the same underlying ATOMIC facts, but ask:\n\n  1. A true/false question whose answer is either “yes” or “no” (TF)  \n  2. A multiple-choice question whose answer is one of {a, b, c, d} (MC)  \n  3. A direct-generation prompt whose answer is a free-form entity or event (DG)  \n\nBy holding the knowledge source constant and only varying the question format, we can measure exactly how the length of the model’s target output changes across TF, MC and DG.  In fact, we compute:\n\n  • TF and MC both collapse to a single‐token output (yes/no or a/b/c/d)  \n  • DG averages about 3.88 output tokens  \n\nWe also report the average input‐prompt lengths for each format (23.44, 35.03, 13.38 tokens, respectively) so that downstream analyses of perplexity and L₁‐norm explicitly control for—and therefore “account for”—these sequence‐length differences."
    },
    {
        "question": "How does Dump for sequence mitigate norm growth compared to standard edit sequence methods?",
        "relevant_section_ids": [
            "4.3.2",
            "4.3.3"
        ],
        "relevant_context": [
            "The D4S method is designed to save the editing history in O(d²) space and apply batch editing methods in sequence editing situations.",
            "So we just need to save the two matrices above. For each new edit with ΔK and ΔW, we can integrate it into edit history with a simple addition operation: H_K ← H_K + ΔK, H_W ← H_W + ΔW. This approach requires just O(d²) storage space and allows us to convert sequence editing methods into batch editing methods, thus reducing the damage to the edited model during sequence editing.",
            "Due to ΣB_i being positive definite, intuitively, the inverse of ΣB_i is expected to have smaller numerical values compared to each B_i. Therefore, the norm of (ΣA_i)(ΣB_i)⁻¹ is smaller than that of Σ(A_i B_i⁻¹). The experimental results in Figures 6 also demonstrate the effectiveness of the D4S method in mitigating L1-norm growth."
        ],
        "final_answer": "Instead of applying each edit one-by-one (which yields a series of individual updates A_i B_i⁻¹ whose norms sum and explode), D4S accumulates all of the edit numerators (ΣA_i) and denominators (ΣB_i) in two small matrices and then applies a single batch update (ΣA_i)(ΣB_i)⁻¹. Because ΣB_i is larger and positive definite, its inverse has smaller entries, so the overall update has a much smaller norm than the sum of individual updates. This batching-and-invert-once strategy prevents the explosive norm growth seen in standard sequential editing.",
        "relevant_elements": [
            "Edit sequence",
            "Dump for sequence"
        ],
        "id": 1987,
        "masked_question": "How does [mask1] mitigate norm growth compared to standard [mask2] methods?",
        "masked_number": 2,
        "masked_elements": [
            "Dump for sequence",
            "Edit sequence"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.23843v1_figure_1.png",
        "paperid": "2410.23843v1",
        "paper_path": "./MISS-QA/papers/2410.23843v1.json",
        "figure_id": "2410.23843v1_figure_1.png",
        "caption": "Fig. 1: This framework outlines the comprehensive approach to understanding the performance decline of edited models. On the left, traditional knowledge editing tasks are categorized into different types, each with distinct editing objectives: yes/no, a/b/c/d, and entity/event. On the right, our experiments are structured from both data and model perspectives. From the data perspective, we conduct three experiments: (a) a comprehensive performance evaluation of the model, (b) the construction of a Multi-Question Dataset (MQD), and (c) an assessment of the impact of editing different target outputs on model performance. From the model perspective, we design four experiments: (d) an evaluation of the edited model’s forgetting ability, (e) an identification of the current knowledge editing method’s bottleneck and an exploration of the correlation between editing probability values and parameter layer norms, and (f) a proposal of a sequence editing method, which effectively enhances the performance of the edited model.",
        "qtype": "Experimental_Results",
        "response": "Let’s first align the two highlighted regions with the text:\n\n  • [mask2] (blue-box) is our “Edit sequence” or “bottleneck of sequence edit” analysis (Sect. 4.2 and Fig. 5), where each new edit is applied one after another, and you literally add up ΔW₁, then ΔW₂, …, so the L₁‐norm of the total change keeps ratcheting up until the model “blows out.”  \n  • [mask1] (red-box) is our new “Dump for Sequence” (D4S) method (Sect. 4.3), in which we merge all past edits into two low‐rank matrices and do one single matrix inversion instead of inverting at every step.\n\nNow—step by step—why D4S curbs norm growth:\n\n1.  In the standard “edit sequence” setup you solve for each edit ΔWᵢ by inverting the same Hessian‐approximation matrix A (or closely related) and then stacking up rank-one updates:\n     ΔW_total = ΔW₁ + ΔW₂ + … + ΔW_t  \n   Each inversion A⁻¹ has a fixed operator norm, so each new ΔWᵢ has roughly the same scale, and summing them drives ∥ΔW_total∥ ever larger (as seen in Fig. 5b).\n\n2.  In D4S we notice that if you collect all your “uᵢ uᵢᵀ” contributions from each edit, you can write the joint update in closed form as\n     ΔW_D4S = (A + ∑ᵢ uᵢ uᵢᵀ)⁻¹ (∑ᵢ vᵢ uᵢᵀ)\n   rather than\n     ΔW_seq = A⁻¹ v₁u₁ᵀ + A⁻¹ v₂u₂ᵀ + ⋯ + A⁻¹ v_tu_tᵀ.\n\n3.  Because A and ∑ᵢ uᵢuᵢᵀ are both positive definite, their sum has larger eigenvalues than A alone.  Inverting a “larger” matrix produces a “smaller” inverse (in operator‐norm terms).  Hence\n     ∥(A + ∑uᵢuᵢᵀ)⁻¹∥ < ∥A⁻¹∥\n   and so ∥ΔW_D4S∥ is strictly smaller than the naïve ∥ΔW_seq∥ that accumulates t copies of A⁻¹.\n\n4.  Empirically (Fig. 6), the L₁‐norm of the editing layer under D4S grows far more slowly and never “explodes” the way the standard edit‐sequence norm does.\n\nIn short, [mask1] (D4S) merges all past edits into one batched inversion step, which—by increasing the spectrum of the matrix being inverted—yields much smaller‐magnitude updates than the repeated per-edit inversions of a fixed A in the standard [mask2] (“Edit sequence”) pipeline."
    },
    {
        "question": "How might Dump for sequence jeopardize model interpretability or raise ethical concerns in knowledge editing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Dump for sequence",
            "Knowledge editing"
        ],
        "id": 1989,
        "masked_question": "How might [mask1] jeopardize model interpretability or raise ethical concerns in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Dump for sequence",
            "Knowledge editing"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.23843v1_figure_1.png",
        "paperid": "2410.23843v1",
        "paper_path": "./MISS-QA/papers/2410.23843v1.json",
        "figure_id": "2410.23843v1_figure_1.png",
        "caption": "Fig. 1: This framework outlines the comprehensive approach to understanding the performance decline of edited models. On the left, traditional knowledge editing tasks are categorized into different types, each with distinct editing objectives: yes/no, a/b/c/d, and entity/event. On the right, our experiments are structured from both data and model perspectives. From the data perspective, we conduct three experiments: (a) a comprehensive performance evaluation of the model, (b) the construction of a Multi-Question Dataset (MQD), and (c) an assessment of the impact of editing different target outputs on model performance. From the model perspective, we design four experiments: (d) an evaluation of the edited model’s forgetting ability, (e) an identification of the current knowledge editing method’s bottleneck and an exploration of the correlation between editing probability values and parameter layer norms, and (f) a proposal of a sequence editing method, which effectively enhances the performance of the edited model.",
        "qtype": "Others",
        "response": "Here’s how I arrive at the answer:\n\n1.  Identify mask₁ and mask₂  \n    -  In the annotated figure, the red‐boxed panel (f) is labeled “Dump for Sequence” (our D4S method).  \n    -  The blue box on the far left surrounds the words “Knowledge Editing,” i.e. the broader task in which D4S operates.  \n\n2.  Recall what D4S does  \n    -  D4S accumulates all past edits into two small matrices so that each new edit can be applied in “batch” rather than sequentially rewriting the model’s parameters one step at a time.  \n    -  The benefit is that it dramatically slows the explosive growth of parameter norms and lets you make hundreds or thousands of edits with minimal loss of general performance.  \n\n3.  Think through interpretability risks  \n    -  Because D4S folds every single edit into a compact, opaque summary (two matrices), you lose a fine‐grained record of exactly *what* was changed at each step.  \n    -  That makes it much harder for an auditor or developer to inspect “which edit caused what behavioral change,” eroding model transparency and traceability.  \n\n4.  Think through ethical/privacy concerns  \n    -  In a typical knowledge‐editing scenario the edits themselves may encode sensitive facts (e.g. medical updates, personal details, proprietary data).  \n    -  By dumping *all* of these into a couple of dense matrices, you risk inadvertently retaining—and potentially leaking—private or copyrighted information that you no longer have an obvious way to excise.  \n    -  Furthermore, because the edit history is no longer human‐readable, it’s harder to spot whether any of those cumulative edits have introduced biased or harmful content.  \n\nPutting that together:\n\n“By using the Dump for Sequence (D4S) mechanism inside a Knowledge Editing workflow, you collapse an entire chain of edits into just two learned matrices.  While that tames parameter‐norm growth, it also obliterates a step-by-step log of what changed when.  As a result, developers lose the ability to trace individual edits and their effects, making the model far less interpretable.  Moreover, since D4S hides all prior edits in an opaque form, you can accidentally entrench—or even expose—sensitive or biased information with no straightforward way to audit or remove it, raising serious ethical and privacy concerns.”"
    },
    {
        "question": "What limitations arise from discrete symmetry hypotheses sampling for diverse object geometries in symmetry decoder?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Symmetry Hypotheses",
            "Symmetry Decoder"
        ],
        "id": 1990,
        "masked_question": "What limitations arise from discrete [mask1] sampling for diverse object geometries in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Symmetry Hypotheses",
            "Symmetry Decoder"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.17763v1_figure_2.png",
        "paperid": "2411.17763v1",
        "paper_path": "./MISS-QA/papers/2411.17763v1.json",
        "figure_id": "2411.17763v1_figure_2.png",
        "caption": "Figure 2: Overview of Reflect3D, our zero-shot single-image symmetry detector. Top: Our transformer-based feed-forward symmetry detector (Section 3.2) predicts symmetry planes from a single RGB image. Bottom: Our multi-view symmetry enhancement pipeline (Section 3.3) leverages multi-view diffusion to resolve the inherent single-view ambiguity in symmetry detection. Aggregating symmetry predictions from multiple synthesized views results in more precise and comprehensive symmetry predictions.",
        "qtype": "Others",
        "response": "First, let’s align the picture with the text:\n\n- The red‐boxed module in the top‐half of Figure 2 is the set of “Symmetry Hypotheses” – i.e. a fixed, discrete sampling of unit normals spanning the hemisphere.  \n- The blue‐boxed module is the “Symmetry Decoder,” which takes those hypotheses plus the image features and (1) classifies which hypotheses actually correspond to symmetry planes in the object, and (2) regresses a small residual rotation (quaternion) off each hypothesis to land on the true plane normal.\n\nNow, what is the drawback of using a *discrete* set of hypotheses in that decoder when you have to deal with objects of wildly different or very fine‐grained symmetry geometries?\n\nChain-of-thought:\n\n1. By construction, the decoder can only ever *start* its search on one of the N pre‐sampled directions.  \n2. If the true symmetry plane of an object lies in between two of those samples, the model must (a) recognize that one of the nearby hypotheses is “close enough,” and then (b) make a potentially large regression correction.  \n3. Coarse hypothesis grids therefore introduce *quantization error*: planes that fall in the “gaps” between hypotheses are harder to catch, or force big, error‐prone residuals.  \n4. To reduce that error you must either greatly increase N (more compute, more memory) or accept that *rare*, *oblique* or *closely spaced* symmetry planes will be missed or localized inaccurately.  \n5. In practice, on highly complex or heavily curved objects—where symmetries don’t neatly align with a handful of canonical directions—this coarse sampling can lead to lower recall (missed planes) or lower precision (poorly regressed normals).\n\nAnswer:  \nBecause the decoder only ever reasons about a *fixed, discrete* set of normals, any true symmetry plane that falls between those samples incurs quantization error.  In other words, rare or finely angled symmetries can be missed outright or only recovered by a large—and therefore error‐prone—regression step.  Scaling to very diverse or subtle geometries would require an impractically dense sampling of hypotheses (or accept degraded accuracy)."
    },
    {
        "question": "How could alternative multi-view diffusion strategies improve aggregation accuracy for highly occluded views?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Multi-view Diffusion",
            "Aggregation"
        ],
        "id": 1991,
        "masked_question": "How could alternative [mask1] strategies improve [mask2] accuracy for highly occluded views?",
        "masked_number": 2,
        "masked_elements": [
            "Multi-view Diffusion",
            "Aggregation"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.17763v1_figure_2.png",
        "paperid": "2411.17763v1",
        "paper_path": "./MISS-QA/papers/2411.17763v1.json",
        "figure_id": "2411.17763v1_figure_2.png",
        "caption": "Figure 2: Overview of Reflect3D, our zero-shot single-image symmetry detector. Top: Our transformer-based feed-forward symmetry detector (Section 3.2) predicts symmetry planes from a single RGB image. Bottom: Our multi-view symmetry enhancement pipeline (Section 3.3) leverages multi-view diffusion to resolve the inherent single-view ambiguity in symmetry detection. Aggregating symmetry predictions from multiple synthesized views results in more precise and comprehensive symmetry predictions.",
        "qtype": "Others",
        "response": "First, let’s unpack what the two masked blocks in the bottom row of Figure 2 are:\n\n  1. The red-boxed module is the “Multi-view Diffusion” front end that hallucinates a fixed set of novel views from a single input image.  \n  2. The blue-boxed module is the “Aggregation” step that clusters the symmetry‐plane predictions from all those synthesized views into a final set of normals.\n\nQuestion:  \n“How could alternative [mask1] strategies improve [mask2] accuracy for highly occluded views?”\n\n→ In plain English:  \n“How could using other multi-view synthesis methods help the aggregation step give more accurate symmetry estimates, especially when parts of the object are heavily occluded in the original image?”\n\nChain‐of‐thought:\n\n1.  In our pipeline we generate 8 novel views with a pre‐trained diffusion model (e.g. Stable-Zero123), then feed each view through the same feed‐forward symmetry detector.  Because some regions of the object might be occluded or poorly imagined by the diffusion model, the symmetry predictions on those views can be noisy or even completely wrong.\n\n2.  During aggregation, we cluster all predicted normals across all views and take cluster‐centers as final symmetry axes.  The cleaner and more consistent each individual view’s prediction, the tighter those clusters, and the more accurate the final symmetry normals.\n\n3.  Therefore, anything that makes the synthesized views look more geometrically faithful—especially in regions that were originally hidden—will directly reduce the scatter in the per‐view symmetry predictions and lead to higher‐confidence, sharper clusters at aggregation time.\n\n4.  Concretely, we could swap in “alternative multi‐view diffusion strategies” that:\n     • Incorporate explicit depth or normal‐map conditioning so the back‐side geometry is more plausible.  \n     • Use NeRF‐based or geometry‐aware pipelines which enforce multi‐view photometric and geometric consistency.  \n     • Train a learned inpainting network on 3D datasets to hallucinate only the occluded regions, rather than a full unconditional diffusion.  \n     • Generate a much denser set of views or sample viewpoints according to an uncertainty metric (more views in areas of high occlusion).  \n\n5.  All of these variants would tend to produce fewer hallucination errors on the hidden parts of the object.  When we then run the feed‐forward symmetry detector on those “cleaner” views, the predicted plane normals will agree more tightly across views.  During the clustering (“Aggregation”) step, that leads to higher precision (tighter clusters, fewer spurious modes) and higher recall (fewer true symmetries thrown out as noise).\n\nAnswer:  \nBy replacing our off‐the‐shelf diffusion model with a geometry-aware or depth-conditioned view-synthesis network—in other words, by using alternative multi-view diffusion strategies that explicitly enforce 3D consistency and better inpainting of occluded regions—we can produce more accurate, less noisy synthetic views of the object’s hidden surfaces.  When those higher‐fidelity views are fed through the symmetry detector, their normals agree more closely, so the final clustering in the Aggregation module yields much more precise symmetry estimates, even for heavily occluded viewpoints."
    },
    {
        "question": "What motivates sampling fixed unit vectors spanning a hemisphere as symmetry hypotheses for cross-attention decoding?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We discretize the space of possible symmetry plane normals by evenly sampling fixed unit vectors spanning a hemisphere, which serve as symmetry hypotheses.",
            "Each hypothesis represents a small neighborhood containing all normal vectors closest to this normal. Hence our hypotheses cover all possible normals.",
            "Note that our symmetry hypotheses only span a hemisphere rather than the entire unit sphere because a normal vector n and its opposite –n represent the same plane."
        ],
        "final_answer": "Sampling fixed unit vectors spanning a hemisphere discretizes the continuous space of symmetry‐plane normals into a finite set of hypotheses that collectively cover all possible orientations, and by limiting the samples to a hemisphere it avoids duplicating equivalent planes (since a normal and its opposite define the same symmetry plane).",
        "relevant_elements": [
            "Symmetry Hypotheses",
            "Symmetry Decoder"
        ],
        "id": 1992,
        "masked_question": "What motivates sampling fixed unit vectors spanning a hemisphere as [mask1] for cross-attention decoding?",
        "masked_number": 1,
        "masked_elements": [
            "Symmetry Hypotheses"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.17763v1_figure_2.png",
        "paperid": "2411.17763v1",
        "paper_path": "./MISS-QA/papers/2411.17763v1.json",
        "figure_id": "2411.17763v1_figure_2.png",
        "caption": "Figure 2: Overview of Reflect3D, our zero-shot single-image symmetry detector. Top: Our transformer-based feed-forward symmetry detector (Section 3.2) predicts symmetry planes from a single RGB image. Bottom: Our multi-view symmetry enhancement pipeline (Section 3.3) leverages multi-view diffusion to resolve the inherent single-view ambiguity in symmetry detection. Aggregating symmetry predictions from multiple synthesized views results in more precise and comprehensive symmetry predictions.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought reasoning:\n\n1.  We want our transformer-style decoder to “know about” all possible orientations of a reflection plane’s normal, but working directly in the continuous sphere of normals would be intractable.  \n2.  A classic trick in this setting is to turn a continuous direction estimation into (a) a coarse, discrete classification over a fixed set of anchor directions plus (b) a fine “delta” regression around whichever anchor wins.  \n3.  To do that we pick a set of fixed unit vectors that (i) uniformly cover the hemisphere of possible normals and (ii) do not duplicate normals (we only need a hemisphere because +n and –n define the same plane).  \n4.  Each of these sampled unit vectors becomes one “symmetry hypothesis” query in the cross-attention decoder: the decoder first classifies which hypothesis bin contains the true normal, then regresses a small residual (as a quaternion) to recover the exact direction.  \n5.  This scheme (fixed‐anchor classification plus residual regression) both simplifies learning and guarantees we have coverage of all possible normals.  \n\nAnswer: We sample fixed unit vectors spanning a hemisphere to create a discrete, uniformly-spaced set of “symmetry hypothesis” anchors that cover every possible plane normal once (since +n/–n are the same), enabling a straightforward classification-plus-residual-regression decoding via cross-attention."
    },
    {
        "question": "What motivates incorporating multi-view diffusion prior to aggregation of symmetry predictions?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "One of the major challenges in single-image symmetry detection is single-view ambiguity [48], where depth, perspective distortion, and occlusion obscure symmetry cues.",
            "We propose to address this problem by leveraging a generative prior. We apply multi-view diffusion models [21] to synthesize surrounding views of the target object, providing a more complete and less ambiguous perspective on symmetry. By aggregating symmetry predictions across multiple views, we achieve comprehensive and accurate symmetry detection.",
            "A major difficulty in single-image symmetry prediction is the single-view ambiguity—the high uncertainty of the back surface of objects makes the learning ambiguous, especially under a regression framework. We explore using a multi-view diffusion model to resolve this ambiguity and enhance the robustness of our symmetry prediction."
        ],
        "final_answer": "Incorporating multi-view diffusion is motivated by the need to resolve single-view ambiguity—uncertainties in depth, occlusion, and unseen back‐surface geometry. By synthesizing surrounding views with a diffusion model, the method obtains more complete and less ambiguous symmetry cues, which when aggregated yield more robust and accurate symmetry predictions.",
        "relevant_elements": [
            "Multi-view Diffusion",
            "Aggregation"
        ],
        "id": 1993,
        "masked_question": "What motivates incorporating [mask1] prior to aggregation of symmetry predictions?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-view Diffusion"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.17763v1_figure_2.png",
        "paperid": "2411.17763v1",
        "paper_path": "./MISS-QA/papers/2411.17763v1.json",
        "figure_id": "2411.17763v1_figure_2.png",
        "caption": "Figure 2: Overview of Reflect3D, our zero-shot single-image symmetry detector. Top: Our transformer-based feed-forward symmetry detector (Section 3.2) predicts symmetry planes from a single RGB image. Bottom: Our multi-view symmetry enhancement pipeline (Section 3.3) leverages multi-view diffusion to resolve the inherent single-view ambiguity in symmetry detection. Aggregating symmetry predictions from multiple synthesized views results in more precise and comprehensive symmetry predictions.",
        "qtype": "Design_Rationale",
        "response": "Step 1 – Identify the red‐boxed module  \nIf you look at the bottom row of Fig. 2, the red box encloses exactly the “Multi-view Diffusion” block.\n\nStep 2 – Recall why we need multi-view diffusion  \nEarlier (end of Sec 1 and in Sec 3.3) the authors stress that a single RGB view is inherently ambiguous – the back side of an object is occluded, depth and perspective cues are missing, and that makes a purely single-view symmetry detector uncertain.  \n\nStep 3 – What does the diffusion module buy us?  \nBy running a learned multi-view diffusion model on the single input image, we “hallucinate” or synthesize plausible surrounding views. Those extra views expose parts of the object that were invisible in the original shot, reducing depth/perspective ambiguity.  \n\nStep 4 – Why do it before aggregation?  \nOnly once you have a set of more complete, diverse views can you run your feed-forward symmetry detector on each and then cluster/aggregate the results into a final, high-confidence symmetry prediction. Without those extra views, the aggregation step would be pooling noisy, under-constrained single-view guesses.  \n\nAnswer  \nThey insert the multi-view diffusion step first because it provides a learned generative prior – it “fills in’’ all the missing viewpoints and thus resolves the inherent single-view ambiguity. Aggregating symmetry hypotheses across these synthesized views then yields much more robust, accurate final predictions."
    },
    {
        "question": "How does placing FCE module at P5 influence feature representation before upsampling stages?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "To enhance global feature extraction and capture comprehensive information from medical images, we have designed three improved methods by incorporating different FCE modules into different components of YOLOv8 network architecture, as illustrated in Figs. 2, 3, and 4, respectively.",
            "Specifically, in the improved method 1 (M1), FCE modules (i.e., SE, GC, GE, and GCT) are added after the Spatial Pyramid Pooling - Fast (SPPF) layer in the Backbone component.",
            "SE enhances the representational capacity of neural networks, including YOLOv8, by dynamically adjusting the weights of feature channels based on their importance.",
            "GC … aggregates the global context across all locations, enabling the capture of long-range dependencies.",
            "GE can efficiently aggregate the global context information while maintaining spatial resolution, without significantly increasing the model computational demands.",
            "GCT can enhance the ability of YOLOv8 to capture context information by incorporating the Gaussian filter."
        ],
        "final_answer": "By inserting an FCE module at P5 (immediately after SPPF in the backbone), the network enriches its high-level feature map before any upsampling. Specifically, SE reweights channel features by importance, GC and GE gather and distribute global context (including long-range dependencies) while preserving spatial detail, and GCT adds stable, Gaussian-based context modeling. Together these yield a stronger, more informative feature representation for the subsequent upsampling stages.",
        "relevant_elements": [
            "FCE module",
            "P5",
            "Upsample"
        ],
        "id": 1994,
        "masked_question": "How does placing [mask1] at [mask2] influence feature representation before upsampling stages?",
        "masked_number": 2,
        "masked_elements": [
            "FCE module",
            "P5"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.01031v2_figure_2.png",
        "paperid": "2410.01031v2",
        "paper_path": "./MISS-QA/papers/2410.01031v2.json",
        "figure_id": "2410.01031v2_figure_2.png",
        "caption": "Figure 2: Network architecture of improved method-1 (M1): adding one FCE module to the Backbone component of YOLOv8.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through what we see in the picture and in the text, and then answer the question step by step.\n\n1.  Identify [mask2]: the blue box is drawn around the 20×20 “P5” feature‐map that comes out of the backbone (right after SPPF).  \n2.  Identify [mask1]: the red box is drawn around the “FCE Module” that Method-1 injects into the network.  \n3.  In M1, the network takes that coarse, low-resolution 20×20 feature (P5), sticks an FCE block on it, and only then upsamples.  \n4.  What does an FCE block do?  Depending on which variant you pick (SE, GC, GE or GCT), it will  \n   - perform channel‐wise re‐weighting (SE, GCT),  \n   - gather global context or long‐range dependencies (GC, GE), and  \n   - preserve fine spatial layout while fusing global information (GE).  \n5.  By putting that block right at P5, before any top-down upsampling, you ensure that the very highest‐level, most semantically rich feature map is already been “enhanced” with global attention/context.  \n\nAnswer to the question:  \n\nBy placing the FCE module at P5 (i.e. immediately after the backbone’s 20×20 output and before upsampling), you inject global‐context modeling and channel-wise attention into the coarsest feature map.  This recalibration sharpens the semantic cues and suppresses irrelevant channels in that 20×20 tensor so that when you later upsample and fuse with lower‐level maps, each scale carries stronger, more discriminative, globally aware features."
    },
    {
        "question": "What rationale supports concatenating FCE outputs with original features via Concat rather than stacking ConvModule?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "FCE module",
            "Concat",
            "ConvModule"
        ],
        "id": 1995,
        "masked_question": "What rationale supports concatenating FCE outputs with original features via [mask1] rather than stacking ConvModule?",
        "masked_number": 1,
        "masked_elements": [
            "Concat"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.01031v2_figure_2.png",
        "paperid": "2410.01031v2",
        "paper_path": "./MISS-QA/papers/2410.01031v2.json",
        "figure_id": "2410.01031v2_figure_2.png",
        "caption": "Figure 2: Network architecture of improved method-1 (M1): adding one FCE module to the Backbone component of YOLOv8.",
        "qtype": "Design_Rationale",
        "response": "Here is the step-by-step reasoning that leads to the answer:\n\n1.  Where the red box (“[mask1]”) appears in the figure, you can see that after an FCE module has processed the higher‐level map (P5), its output is upsampled and then concatenated (yellow “Concat” block) with the next‐lower‐level backbone feature (P4).  A very similar pattern happens again one level down (FCE → upsample → concat with P3).\n\n2.  In contrast, you might imagine simply stacking yet another ConvModule (i.e. more 3×3 conv + BN + SiLU) in that location, thereby “mixing” the two streams purely through depth, rather than by channel‐wise concatenation.\n\n3.  The authors explicitly point out elsewhere that one of YOLOv8’s neck design goals is to\n    – preserve as much of the **original** feature (especially low‐level, localization‐sensitive detail) as possible,  \n    – keep the number of convolutions low so as not to blur or overly smooth small fracture cues, and  \n    – leverage multi-scale information by fusing high- and low-level maps.\n\n4.  By concatenating the FCE-enhanced map with the untouched backbone map you get:\n    • A **richer** feature tensor that carries both the un-altered spatial details (from the backbone) and the globally-enhanced contextual cues (from the FCE).  \n    • **Zero** information loss of the original feature — you know it will be there, in full fidelity, in half of the channels of the new tensor.  \n    • No added depth in the convolutional chain at that point, which keeps parameters/FLOPs down and preserves the sharp edges and thin lines so crucial for detecting subtle pediatric wrist fractures.\n\n5.  If, instead, you simply “stacked” a ConvModule on top of the FCE output _and then_ fed that into the next stage, you would:\n    – force the network to learn how to _re-create_ or _re-inject_ those low-level details into a purely transformed stream,  \n    – add more parameters and more non-linearities at the very place where the paper argues fewer non-linear mixing steps are better for localization, and  \n    – depart from the tried-and-true FPN/PAN pattern in YOLOv8 where concatenation is used as the primary fusion mechanism.\n\nSo in short, the **rationale** is:\n\n– Concatenation preserves the original backbone feature _verbatim_ while at the same time appending the richer, globally contextualized channels from the FCE.  \n– It avoids the extra convolutional depth (and attendant parameter/FLOP cost and potential blur) that would come from “stacking” yet another ConvModule.  \n– It stays faithful to the YOLOv8 neck’s philosophy of fusing multi-scale features via channel-wise concatenation, thereby giving the detector both high-level context and low-level localization cues bundled together."
    },
    {
        "question": "How does Upsample merge FCE Module outputs with P4 features before Concat?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Upsample",
            "FCE Module",
            "Concat"
        ],
        "id": 1996,
        "masked_question": "How does [mask1] merge [mask2] outputs with P4 features before Concat?",
        "masked_number": 2,
        "masked_elements": [
            "Upsample",
            "FCE Module"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.01031v2_figure_2.png",
        "paperid": "2410.01031v2",
        "paper_path": "./MISS-QA/papers/2410.01031v2.json",
        "figure_id": "2410.01031v2_figure_2.png",
        "caption": "Figure 2: Network architecture of improved method-1 (M1): adding one FCE module to the Backbone component of YOLOv8.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step of what the red‐boxed module does to merge the FCE (blue‐boxed) output with the P4 features:\n\n1. It takes the 20×20 feature map coming out of the FCE module.  \n2. An Upsample layer (green) applies a 2× spatial interpolation (e.g. nearest‐neighbor or bilinear) to that 20×20 map, producing a 40×40 feature map.  \n3. Because P4 is already 40×40, the upsampled FCE output and the P4 feature map now share the same spatial dimensions.  \n4. Finally, the Concat operator (yellow) concatenates them channel‐wise, ready for the downstream C2f block."
    },
    {
        "question": "How does C2f distribute incoming channels across DarknetBottleneck layers before concatenation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "C2f",
            "DarknetBottleneck",
            "Concat"
        ],
        "id": 1997,
        "masked_question": "How does [mask1] distribute incoming channels across DarknetBottleneck layers before concatenation?",
        "masked_number": 1,
        "masked_elements": [
            "C2f"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.01031v2_figure_2.png",
        "paperid": "2410.01031v2",
        "paper_path": "./MISS-QA/papers/2410.01031v2.json",
        "figure_id": "2410.01031v2_figure_2.png",
        "caption": "Figure 2: Network architecture of improved method-1 (M1): adding one FCE module to the Backbone component of YOLOv8.",
        "qtype": "Implementation_Details",
        "response": "First, let’s restate what the red‐boxed module (the “C2f” block) is doing, step by step:\n\n1.  Input has c_in channels.  \n2.  A ConvModule projects that to c_out channels.  \n3.  A Split operation then chops those c_out channels into **equal slices of size 0.5·c_out**.  \n4.  One of those 0.5·c_out-sized slices simply “bypasses” (keeps as a skip connection).  \n5.  The remaining slices (there are n of them) each pass through a separate DarknetBottleneck, and each bottleneck also outputs 0.5·c_out channels.  \n6.  You now have (n + 1) tensors, each of shape [0.5·c_out, H, W].  These are concatenated along the channel axis into a single volume of shape [0.5·(n+2)·c_out, H, W] (the “+2” comes from the n bottlenecks plus the one initial skip path).  \n7.  A final ConvModule then fuses that concatenation back down to c_out channels.  \n\nSo in answer to the question:\n\n“How does [mask1] distribute incoming channels across DarknetBottleneck layers before concatenation?”\n\n**Answer:**  \nIt first splits the c_out feature map into equal channel‐sized chunks of size 0.5·c_out.  One chunk is held out as a skip, the other n chunks each feed one DarknetBottleneck (so every bottleneck sees exactly 0.5·c_out channels).  After processing, all (n+2) chunks (the skip plus n bottleneck outputs) are concatenated along the channel axis."
    },
    {
        "question": "How does the Cross Modality Alignment / Translation Module transform image and audio token representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Cross Modality Alignment / Translation Module",
            "Image Encoder",
            "Audio Encoder"
        ],
        "id": 1998,
        "masked_question": "How does the [mask1] transform image and audio token representations?",
        "masked_number": 1,
        "masked_elements": [
            "Cross Modality Alignment / Translation Module"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.05036v1_figure_1.png",
        "paperid": "2407.05036v1",
        "paper_path": "./MISS-QA/papers/2407.05036v1.json",
        "figure_id": "2407.05036v1_figure_1.png",
        "caption": "Figure 1: Text-centric multimodal alignment, which converts different modalities into text to serve as input prompts for LLMs, is a common method for aligning large multimodal language models when pairwise multimodal data is limited.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the “traditional embedding methods” (the top half of the figure) separate encoders turn each raw modality (image, audio, video) into its own sequence of tokens or embeddings.  \n2. Those per‐modality token streams aren’t yet directly comparable — you need to “translate” or “align” them so that an image token and an audio token that mean the same thing end up in the same region of representation‐space.  \n3. That is exactly what the red‐boxed module does: it takes each encoder’s token outputs and runs a small cross‐modal network over them (often implemented with cross‐attention or contrastive projection heads) to project all of them into one shared embedding space.  \n4. After this cross‐modality alignment/translation, the formerly heterogeneous image and audio tokens become unified, semantically aligned vectors that can be fused or concatenated for any downstream multimodal task.  \n\nAnswer  \nThe red‐boxed block is a learned cross‐modality alignment/translation module.  It ingests the separate image and audio token streams coming out of their individual encoders and projects or “translates” them into a single, shared embedding space (often via cross‐attention layers or contrastive projection heads).  In this way an image token and an audio token that convey the same concept end up as nearby vectors, so they can be fused and passed on to the downstream task."
    },
    {
        "question": "What mechanism converts textual outputs from LLM into text token representations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "LLM",
            "Text Token Rep."
        ],
        "id": 1999,
        "masked_question": "What mechanism converts textual outputs from [mask1] into [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "LLM",
            "Text Token Rep."
        ],
        "figure_path": "./MISS-QA/figures/1_2407.05036v1_figure_1.png",
        "paperid": "2407.05036v1",
        "paper_path": "./MISS-QA/papers/2407.05036v1.json",
        "figure_id": "2407.05036v1_figure_1.png",
        "caption": "Figure 1: Text-centric multimodal alignment, which converts different modalities into text to serve as input prompts for LLMs, is a common method for aligning large multimodal language models when pairwise multimodal data is limited.",
        "qtype": "Implementation_Details",
        "response": "Let’s walk through the figure step by step:\n\n1. In the red‐boxed region ([mask1]) the LLM produces _plain text_ (e.g. captions, descriptions, reports).  \n2. Those raw text strings (shown as document icons) then feed into the blue‐boxed module ([mask2]), which is labeled “Text Token Rep.”  \n3. What happens inside that blue box is simply standard _text tokenization_ followed by an _embedding lookup_:  \n   - First, the text is split into discrete tokens by the LLM’s tokenizer.  \n   - Then each token ID is mapped via the model’s embedding matrix into a continuous‐valued vector.  \n\nIn short, the mechanism that converts the LLM’s textual outputs into the vector‐valued “Text Token Rep.” is the _tokenizer plus text‐embedding layer_ (i.e. the text encoder) of the language model."
    },
    {
        "question": "How does Image Encoder token representation integration draw on contrastive learning like CLIP?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Image Encoder",
            "Image Token Rep."
        ],
        "id": 2000,
        "masked_question": "How does [mask1] token representation integration draw on contrastive learning like CLIP?",
        "masked_number": 1,
        "masked_elements": [
            "Image Encoder"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.05036v1_figure_1.png",
        "paperid": "2407.05036v1",
        "paper_path": "./MISS-QA/papers/2407.05036v1.json",
        "figure_id": "2407.05036v1_figure_1.png",
        "caption": "Figure 1: Text-centric multimodal alignment, which converts different modalities into text to serve as input prompts for LLMs, is a common method for aligning large multimodal language models when pairwise multimodal data is limited.",
        "qtype": "Literature_Background",
        "response": "Here’s how I arrived at the answer step by step:\n\n1. What is [mask1]?  \n • In the red‐boxed region of the “Traditional Embedding Methods” diagram sits the Image Encoder block.  \n • That encoder spits out what the figure calls “Image Token Rep.”  \n\n2. What does CLIP do?  \n • CLIP has two streams — an image encoder and a text encoder — trained with a contrastive loss so that matching image–text pairs are pulled together in the same vector space, and non‐matching pairs are pushed apart.  \n\n3. How do we “draw on contrastive learning like CLIP” when integrating the Image Token Rep.?  \n • We pretrain (or fine-tune) the image encoder with a CLIP-style objective:  \n  – Encode an image into a sequence of tokens (the “Image Token Rep.”).  \n  – Encode a caption or text into its own token sequence.  \n  – Use a contrastive loss (e.g. normalized temperature‐scaled cross‐entropy) to maximize similarity of the correct image–text pair and minimize similarity of all other (negative) pairs.  \n • After this stage, the Image Token Rep. lives in the same embedding space as the text tokens.  \n\n4. How is it then integrated in the multimodal system?  \n • Those CLIP-aligned image tokens are fed, alongside audio and video tokens, into a shared Cross-Modality Alignment module.  \n • Because the image encoder was trained contrastively, its token outputs already line up semantically with text (and by extension with the other modalities once they, too, are mapped into that space).  \n\nAnswer, in one sentence:  \nThe image encoder is trained exactly as in CLIP—using a dual‐encoder, image-vs-text contrastive loss—so that its output “image token” vectors live in the same semantic space as text; those aligned image tokens are then co‐embedded with audio/video tokens in the downstream cross-modality alignment module."
    },
    {
        "question": "How does LLM-generated text tokenization leverage chain-of-thought reasoning frameworks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM",
            "Text Token Rep."
        ],
        "id": 2001,
        "masked_question": "How does [mask1]-generated text tokenization leverage chain-of-thought reasoning frameworks?",
        "masked_number": 1,
        "masked_elements": [
            "LLM"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.05036v1_figure_1.png",
        "paperid": "2407.05036v1",
        "paper_path": "./MISS-QA/papers/2407.05036v1.json",
        "figure_id": "2407.05036v1_figure_1.png",
        "caption": "Figure 1: Text-centric multimodal alignment, which converts different modalities into text to serve as input prompts for LLMs, is a common method for aligning large multimodal language models when pairwise multimodal data is limited.",
        "qtype": "Literature_Background",
        "response": "Here, the red‐boxed module in the diagram is simply the LLM itself – it is the component that turns every modality (image, audio, video) into plain text.  The trick is that when you ask that LLM to “describe what’s going on” you don’t stop at a one‐line caption.  Instead, you use Chain‐of‐Thought (CoT) prompting (per Wei et al. 2022) so that the LLM produces a little reasoning trace – a sequence of intermediate steps and justifications – and then you break that entire trace into tokens.  \n\nConcretely:\n\n 1.  You feed the LLM the raw captions/descriptions from your expert encoders (or drop‐in examples), plus a few “think‐aloud” demonstrations showing how to unpack complex observations into multi‐step rationales.  \n 2.  The LLM emits a structured explanation (“I see a dog.  The dog has a collar, which suggests it’s a pet.  Pets often live indoors, etc.”).  \n 3.  You tokenize that whole chain of thought.  Each token now carries not just pixel or audio content but the latent inference path that the model used to arrive at its conclusions.  \n\nBy converting every modality into a CoT‐style text dump, you end up with token sequences that embed explicit reasoning steps.  When you then feed those tokens into your downstream classifier or MLP, you get far more robust, noise-tolerant representations—because you’re not just feeding in raw captions but the model’s own internal “thinking,” laid out step by step."
    },
    {
        "question": "How does classifier aggregation via historical dependencies improve efficiency over FedPAC's optimization-based weighting?",
        "relevant_section_ids": [
            "1",
            "4.3"
        ],
        "relevant_context": [
            "Yet, acquiring the optimal weights in FedPAC necessitates solving a complex optimization problem, significantly prolonging training time.",
            "The primary purpose of setting the critical co‐learning period is (1) to simplify the computational cost in the later phase, and (2) to maintain personalization.",
            "With the help of dependency map T, we can perform classifier collaboration directly via Equation (10). In the experiments presented later in Section 5.2, we can observe that this design effectively reduces computational complexity and training time."
        ],
        "final_answer": "Instead of solving a costly optimization problem to compute per‐client combination weights as in FedPAC, FedReMa records and reuses historical peer‐selection counts as aggregation weights. This direct use of the dependency map eliminates the need for runtime optimization, thereby simplifying computation and reducing training time.",
        "relevant_elements": [
            "Historical dependencies",
            "Aggregation"
        ],
        "id": 2003,
        "masked_question": "How does classifier [mask1] via historical dependencies improve efficiency over FedPAC's optimization-based weighting?",
        "masked_number": 1,
        "masked_elements": [
            "Aggregation"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.01825v2_figure_2.png",
        "paperid": "2411.01825v2",
        "paper_path": "./MISS-QA/papers/2411.01825v2.json",
        "figure_id": "2411.01825v2_figure_2.png",
        "caption": "Figure 2: Workflow of FedReMa. (1) Local training. (2) Upload model to server. (3) Aggregate global feature extractors. (4) Determine whether the current period is a critical learning period, if so, go to (5), otherwise go to (8). (5) Measure the task relevance 𝒮𝒮\\mathcal{S}caligraphic_S using similarity of their logits, then perform MDS to filter out irrelevant clients. At the same time, the server will record the client’s historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (6) Update CCP status. (7) Aggregate personalized classifiers by 𝒜k(t)superscriptsubscript𝒜𝑘𝑡\\mathcal{A}_{k}^{(t)}caligraphic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. (8) Aggregate personalized classifiers based on historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (9) Server communicates personalized models to all clients.",
        "qtype": "Literature_Background",
        "response": "First, let’s align the red‐boxed operation in the diagram with the text.  The box around  \n   \n   1/|D| · ∑_{i∈𝒦} |D_i| θ̂_i^(t)  \n   \nis simply the *uniform, dataset‐size‐weighted aggregation* of feature‐extractor parameters – i.e. the FedAvg step for θ.  However, the question is about the *classifier* step *outside* the critical co‐learning period, which the paper calls “Personalization via Historical Peer Matching.”  In that phase (step 8 in Fig 2) the server no longer runs the expensive MDS optimization every round; instead it reuses the *historical dependency map* 𝒢, whose entries are just counts of “how often client k picked client j during CCP.”  Those counts become the aggregation weights in a single weighted‐average of the classifiers φ_j→φ_k.  \n\nBy contrast, FedPAC formulates a new convex optimization in each round to find *optimal* mixing weights over all peers’ classifiers, which is accurate but costly.  FedReMa’s “historical‐dependency” version simply says  \n   \n   φ_k^(t+1)  =  ∑_j  G_{kj}  · φ_j^(t)  \n   \n(plus normalization)  \n   \nwhere G_{kj} is just a running count or frequency, rather than the solution of an optimization.  This cuts out the per‐round solver altogether and reduces classifier‐aggregation to one fast matrix–vector multiply.  In practice this slashes the extra compute and wall‐time overhead compared to FedPAC’s optimization‐based weighting."
    },
    {
        "question": "How does the CCP determination affect the switch between MDS-based and dependency-based classifier aggregation?",
        "relevant_section_ids": [
            "4.2",
            "4.3"
        ],
        "relevant_context": [
            "Once we are unable to differentiate relevant clients based on similarities, the co-learning in this stage becomes ineffective. If the MDS algorithm continues to be applied, there will be a degradation in accuracy. Another way to effectively aggregate the classifiers must be used. (Section 4.2)",
            "When CCP is determined to be beyond the critical co-learning period, we adopt an algorithm that leverages the historical matching decisions. ... With the help of dependency map H, we can perform classifier collaboration directly via Equation (10). (Section 4.3)"
        ],
        "final_answer": "The CCP threshold marks the cutoff between two aggregation modes. While the training round is within the critical co-learning period (i.e. MDS is still effective in distinguishing peers), the server uses the MDS-based matching to select and aggregate client classifiers. Once the average maximum similarity difference falls below the CCP threshold—indicating that MDS no longer reliably identifies relevant peers—the server switches to dependency-based aggregation, using the historical dependency map (H) built during CCP to combine classifiers.",
        "relevant_elements": [
            "CCP",
            "MDS",
            "classifier aggregation"
        ],
        "id": 2004,
        "masked_question": "How does the [mask1] determination affect the switch between [mask2]-based and dependency-based classifier aggregation?",
        "masked_number": 2,
        "masked_elements": [
            "CCP",
            "MDS"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.01825v2_figure_2.png",
        "paperid": "2411.01825v2",
        "paper_path": "./MISS-QA/papers/2411.01825v2.json",
        "figure_id": "2411.01825v2_figure_2.png",
        "caption": "Figure 2: Workflow of FedReMa. (1) Local training. (2) Upload model to server. (3) Aggregate global feature extractors. (4) Determine whether the current period is a critical learning period, if so, go to (5), otherwise go to (8). (5) Measure the task relevance 𝒮𝒮\\mathcal{S}caligraphic_S using similarity of their logits, then perform MDS to filter out irrelevant clients. At the same time, the server will record the client’s historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (6) Update CCP status. (7) Aggregate personalized classifiers by 𝒜k(t)superscriptsubscript𝒜𝑘𝑡\\mathcal{A}_{k}^{(t)}caligraphic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. (8) Aggregate personalized classifiers based on historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (9) Server communicates personalized models to all clients.",
        "qtype": "Experimental_Results",
        "response": "Here is a step-by-step of what happens in Fig. 2:\n\n1. After every round of local training each client uploads its updated classifier and feature–extractor.  \n2. The server always aggregates feature–extractors by vanilla FedAvg (step 3).  \n3. Before it aggregates classifiers, however, it hits the red‐boxed module (step 4) labelled “CCP?” – that is, it asks “are we still in the Critical Co‐learning Period?”  \n4. That question is answered by monitoring the average maximum similarity difference Δ̄ across all clients (Eqn 9).  If Δ̄ is above a preset threshold we remain in CCP; once Δ̄ falls below threshold the CCP ends.  \n5. If CCP is still true, the control flows along the “yes” arrow down into the blue‐boxed block (steps 5–7), where the server runs the MDS (maximum‐difference segmentation) relevant-matching procedure to pick each client’s peers and then aggregates classifiers using only those peers.  \n6. If CCP has ended, the server takes the “no” arrow out to step 8 instead, where it foregoes MDS and simply aggregates each classifier via the accumulated historical dependency map 𝒢 (i.e. dependency-based aggregation).  \n\nIn short: the red‐boxed CCP check gates whether you use on-the-fly MDS-based peer selection (blue box) or switch over to the simpler, dependency-based classifier aggregation once co-learning is no longer effective."
    },
    {
        "question": "How does feature extractor aggregation complement personalized classifier aggregation in FedReMa workflow?",
        "relevant_section_ids": [
            "4",
            "4.1"
        ],
        "relevant_context": [
            "To realize this, we propose FedReMa, an adaptive method that leverages a novel relevant matching algorithm (named as MDS shown in Algorithm 1) for classifier synergy and carefully identified critical co-learning period (CCP), adapting to the dynamics of clients’ prediction expertise; the feature extractor aggregation is the same as standard FedAvg.",
            "Then, the server performs distinct aggregation methods on the uploaded feature extractors and classifiers: The server aggregates a robust global feature extractor in equation (6), while equation (7) ensures the personalized aggregation of classifiers."
        ],
        "final_answer": "In FedReMa, feature extractors are aggregated via standard FedAvg to form a single, robust global backbone that captures generalizable representations across all clients. This shared feature extractor mitigates feature‐level heterogeneity and provides a consistent embedding space. Complementing this, each client’s classifier is aggregated in a personalized fashion—using relevance‐based peer matching (MDS) during the critical co‐learning period and historical peer weights thereafter—to preserve local dominant‐class preferences while importing knowledge on non‐dominant classes. In combination, the global feature extractor supplies a stable shared representation, and the personalized classifier aggregation tailors decision boundaries to each client’s data skew, jointly balancing generalization and personalization.",
        "relevant_elements": [
            "feature extractor aggregation",
            "personalized classifier aggregation"
        ],
        "id": 2005,
        "masked_question": "How does [mask1] complement [mask2] in FedReMa workflow?",
        "masked_number": 2,
        "masked_elements": [
            "feature extractor aggregation",
            "personalized classifier aggregation"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.01825v2_figure_2.png",
        "paperid": "2411.01825v2",
        "paper_path": "./MISS-QA/papers/2411.01825v2.json",
        "figure_id": "2411.01825v2_figure_2.png",
        "caption": "Figure 2: Workflow of FedReMa. (1) Local training. (2) Upload model to server. (3) Aggregate global feature extractors. (4) Determine whether the current period is a critical learning period, if so, go to (5), otherwise go to (8). (5) Measure the task relevance 𝒮𝒮\\mathcal{S}caligraphic_S using similarity of their logits, then perform MDS to filter out irrelevant clients. At the same time, the server will record the client’s historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (6) Update CCP status. (7) Aggregate personalized classifiers by 𝒜k(t)superscriptsubscript𝒜𝑘𝑡\\mathcal{A}_{k}^{(t)}caligraphic_A start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT. (8) Aggregate personalized classifiers based on historical dependencies 𝒢𝒢\\mathcal{G}caligraphic_G. (9) Server communicates personalized models to all clients.",
        "qtype": "Experimental_Results",
        "response": "In FedReMa the little red box is simply the feature‐extractor θₖ, and the big blue box is the entire client-side “local training” module (dataset Dₖ plus feature-extractor θₖ plus classifier φₖ).  What FedReMa does is split the model into two pieces:\n\n 1.  A shared, **global** feature extractor θ that all clients train locally but then upload and **aggregate by straight FedAvg** (step 3 in Fig. 2).  \n 2.  A **personalized** classifier head φₖ that each client tunes on its own data and then refines via the “relevant matching” routine or, once past the CCP, via its own historical peer‐dependency map.\n\nBecause of that split:\n\n - The **red-boxed** feature extractor learns a **universal** representation across every client (it is averaged in exactly the FedAvg way).  \n - The **blue-boxed** local‐training loop then **consumes** that common feature backbone to produce logits, which are in turn used both for ordinary classification losses and for the similarity–based peer matching.\n\nIn this way the shared feature extractor (mask 1) underpins and **complements** the per‐client training pipelines (mask 2): it guarantees every client starts from the same robust embedding space, while the classifier heads remain free to specialize and to borrow only the most relevant peer knowledge."
    },
    {
        "question": "How does geometric recycling alter pose update dynamics across iterations?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, the outputs of the stacked attention modules—namely, the predicted poses \\(\\{\\mathbf{R}_l^n, \\mathbf{t}_l^n\\}\\) and node features \\(\\{h_l^n\\}\\)—are fed back into the network as contextual information for further refinement.",
            "Position Recycling. To improve the network’s awareness of part positions after each transformation, we apply each part’s predicted pose to its point cloud and then re-extract features from this transformed cloud. These transformed features are incorporated into the node features.",
            "Pose Recycling. We also recycle the predicted pose itself. The predicted rotation matrix is converted to axis-angle form, and we apply trigonometric functions to the angle vector before concatenating it with the translation vector.",
            "During inference, recycling forms a recurrent network with shared weights that iteratively refines the output."
        ],
        "final_answer": "Geometric recycling turns the pose updater into a multi-round, recurrent refinement process. After each attention pass, the network’s own predicted poses are (1) used to re-transform the input point clouds and re-extract updated spatial features (position recycling) and (2) embedded again as part of the input (pose recycling, via axis-angle + trigonometric encoding). By feeding back both the newly extracted features and the last predicted pose into the next round, the model dynamically corrects and refines its pose estimates across iterations rather than predicting them in a single shot.",
        "relevant_elements": [
            "Geometric Recycling",
            "Update"
        ],
        "id": 2006,
        "masked_question": "How does geometric recycling alter pose update dynamics across [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Update"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.17788v1_figure_1.png",
        "paperid": "2411.17788v1",
        "paper_path": "./MISS-QA/papers/2411.17788v1.json",
        "figure_id": "2411.17788v1_figure_1.png",
        "caption": "Figure 1: Overview of our model architecture. Given the point clouds of each part, we first use a feature extractor to generate part features and pairwise features. These features, along with the initial poses, are updated in a stack of geometric point attention modules. The predicted poses and positions are recycled for the next round of predictions in the geometric recycling module.",
        "qtype": "Experimental_Results",
        "response": "First, let me clarify what “[mask1]” is.  In the diagram that question refers to, the red‐boxed region encloses the entire “apply–transform–extract–update” loop – in other words, it is exactly the loop that is being executed over multiple recycling rounds (sometimes called “iterations” or “recycling steps”).  Thus “[mask1]” simply means “recycling rounds.”\n\nNow:  “How does geometric recycling alter pose‐update dynamics across recycling rounds?”\n\nChain‐of‐thought:\n\n1.  **One‐shot versus iterative.**  \n    -  Without recycling, the network would extract features from the raw input parts, run them through the stacked attention layers once, and spit out a single guess for each 6-DoF pose.  \n    -  With geometric recycling, we turn that once-through into a small recurrent loop: once you’ve gotten a pose estimate, you feed it back in as part of your next input.\n\n2.  **What gets fed back.**  \n    -  **Position recycling:** we actually take each part’s point cloud, rigidly transform it by the *predicted* pose, run that transformed cloud back through the backbone (PointNet/DGCNN) to get new part‐features, and recompute pairwise geometric invariants (e.g. center-of-mass distances via RBF).  \n    -  **Pose recycling:** we also take the predicted rotation (as an axis–angle) and translation, compute sin/cos on the angle and concatenate it with the translation, and re-inject that vector into the node‐ and pair‐embedders.\n\n3.  **Effect on pose updates.**  \n    -  In each subsequent round the *input* to the attention blocks carries progressively more precise geometry: the point clouds are closer to their target positions, and the embedded pose‐vector already encodes roughly where each part should go.  \n    -  As a result, the *relative* transformations the network predicts at each attention layer (those quaternions and translations it appends onto the input pose) become smaller and more fine‐grained.  In early rounds you see larger “coarse” corrections, and in later rounds the network learns to make tiny “polishing” adjustments.\n\n4.  **Equivariance and stability.**  \n    -  Because each round’s pose update is predicted as a *relative* transformation (left-multiplied onto the current pose), equivariance to any global rigid motion is maintained across rounds.  \n    -  Moreover, the network never has to learn a big jump from identity→correct pose in one shot.  Instead it learns a sequence of incremental refinements, which is empirically much easier to optimize and yields more stable convergence.\n\nPutting that all together in one concise answer:\n\nAnswer:\n\nGeometric recycling turns the static one‐pass pose predictor into a small recurrent loop over “recycling rounds.”  After each round, the predicted poses are used to re-transform the part point clouds and regenerate both part-level and pairwise geometric features (position recycling), and to repack the predicted rotation–translation into the node embeddings (pose recycling).  Feeding these updated features back into the next pass causes the network’s predicted *relative* pose corrections to shrink and sharpen over successive rounds: the early iterations make coarse, large‐magnitude adjustments, and the later iterations learn to apply fine, small‐magnitude refinements, all while preserving equivariance to global rigid transforms."
    },
    {
        "question": "How do part attention and pair attention synergize during update to capture inter-part geometry?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Part Attention. The node features are transformed into query, key, and value vectors, and the squared multiplication between the query and key vectors represents the part-level attention weights.",
            "Pair Attention. After modeling part-level attention, we introduce cross-part pair features, which are incorporated as an additional term in the attention calculation to regulate the cross-part relative weights. … To achieve this, we include geometric invariant distances and orientations between parts in the edge module.",
            "Feature Update. After obtaining attention weights from the part, pair, and point attention modules, we use these weights to update part features and poses. The final attention weights are computed by combining the different weights, followed by the softmax function. Here, w^p represents the contributions of global features from neighboring nodes, w^e incorporates edge information and geometric invariant relationships, while w^{pt} encapsulates relative transformation information. Note that we add part attention weights and pair attention weights but subtract the point attention weights."
        ],
        "final_answer": "Part attention computes high-level, global part‐to‐part interactions by comparing node features, while pair attention brings in explicit geometric consistency through invariant distances and orientations between part centers. During the feature update, their respective attention scores (w^p for part attention and w^e for pair attention) are summed and normalized (with point‐attention terms subtracted) before being used to update both part features and poses. This additive fusion ensures that each part’s update reflects both abstract semantic context and concrete geometric relationships between parts.",
        "relevant_elements": [
            "Part Attention",
            "Pair Attention",
            "Update"
        ],
        "id": 2007,
        "masked_question": "How do [mask1] and [mask2] synergize during update to capture inter-part geometry?",
        "masked_number": 2,
        "masked_elements": [
            "Part Attention",
            "Pair Attention"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.17788v1_figure_1.png",
        "paperid": "2411.17788v1",
        "paper_path": "./MISS-QA/papers/2411.17788v1.json",
        "figure_id": "2411.17788v1_figure_1.png",
        "caption": "Figure 1: Overview of our model architecture. Given the point clouds of each part, we first use a feature extractor to generate part features and pairwise features. These features, along with the initial poses, are updated in a stack of geometric point attention modules. The predicted poses and positions are recycled for the next round of predictions in the geometric recycling module.",
        "qtype": "Experimental_Results",
        "response": "mask1 is the “Part Attention” branch and mask2 is the “Pair Attention” branch of the Geometric Point Attention module.  During the update they work hand‐in‐hand:  \n\n1. Part Attention (mask1) computes a soft “who speaks to whom” weight purely from the node (part) features – it tells you which other parts are contextually important for each part.  \n2. Pair Attention (mask2) computes an extra bias term from the learned pair‐features plus the true geometric invariants between parts (center‐to‐center distances + dihedral angles encoded with RBFs).  In other words it tells you how “geometrically compatible” each pair of parts is.  \n3. These two signals are simply added (along with the invariant Point Attention term) before the softmax, so that the final attention weight on any neighbor is high only if (a) that neighbor is contextually important and (b) their relative geometry is valid.  \n4. Those combined weights then drive the weighted sums that update both the part features and ultimately the 6-DoF pose updates.  \n\nIn this way, Part Attention provides the global, semantic context and Pair Attention injects precise inter-part geometric constraints, and by adding them together they jointly steer the feature and pose updates to respect both."
    },
    {
        "question": "What trade-offs emerge when replacing the feature extractor with a graph neural network?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Feature Extractor"
        ],
        "id": 2008,
        "masked_question": "What trade-offs emerge when replacing the [mask1] with a graph neural network?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Extractor"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.17788v1_figure_1.png",
        "paperid": "2411.17788v1",
        "paper_path": "./MISS-QA/papers/2411.17788v1.json",
        "figure_id": "2411.17788v1_figure_1.png",
        "caption": "Figure 1: Overview of our model architecture. Given the point clouds of each part, we first use a feature extractor to generate part features and pairwise features. These features, along with the initial poses, are updated in a stack of geometric point attention modules. The predicted poses and positions are recycled for the next round of predictions in the geometric recycling module.",
        "qtype": "Others",
        "response": "When we say “replace the red‐boxed feature‐extractor” with a GNN, what we really mean is: swap out the two separate MLP-style embedders (one that looks at each part in isolation, one that builds a fixed “pair” feature for every part–part pair) and instead do everything by passing messages on a graph whose nodes are parts and whose edges link parts that “should” talk to one another.\n\nHere’s what you gain, and what you pay for, in that swap:\n\n1. Stronger relational bias  \n   • Gain  – A GNN naturally encodes which parts are “neighbors” and forces information to flow only along those edges.  You no longer have to manually build pair-features and concatenate them in; the message-passing layers learn an edge function for you.  \n   • Pay   – You lose the ability to cleanly separate “part features” from “pair features” and to inject our pre-computed geometric invariants (distances, angles) directly.  All of that must now be re-learned inside the GNN.\n\n2. Fewer explicit geometric invariants  \n   • Gain  – Cleaner code: you have one graph-module rather than two embedder MLPs plus a bespoke Gaussian-RBF encoder for distances and dihedrals.  \n   • Pay   – In our original design we bake in exactly the SE(3)–invariant/RBF-encoded distances and angles so that the downstream attention can make use of them.  A vanilla GNN won’t automatically discover those invariants, so you either lose them or have to re-engineer them back in.\n\n3. Coarser part-level features  \n   • Gain  – If your scene really is about “which parts touch which,” a GNN’s message passing can be very efficient at propagating that combinatorial structure.  \n   • Pay   – You give up point-cloud–level expressivity inside each part.  Our PointNet/DGCNN backbone followed by separate part- and pair-embedders can still see the 1,000 raw points of each fragment and learn very fine geometric cues; a pure “parts as nodes” GNN can only see whatever summary you give it at each node.\n\n4. Computational & optimization costs  \n   • Gain  – In small assemblies (say 5–10 parts) the overhead of building a graph and running a GNN can be modest.  \n   • Pay   – In larger assemblies (tens of parts) the GNN’s message-passing (and especially deeper graph stacks) becomes a memory and time sink.  Conversely, our tiny per-part MLPs and pair-embedders scale linearly in the number of parts and remain very lightweight.\n\n5. Integration with the attention-and-pose pipeline  \n   • Gain  – You can in principle plug a GNN into the place of “red-box” and still run the geometric attention modules on top of it.  \n   • Pay   – Because our attention heads expect split “node features” and “edge (pair) features” plus explicit geometric invariants, you will have to undo or reassemble the GNN’s internal representations before each attention layer.  The bookkeeping can become quite a bit more complex than the clean MLP→attention interface we started with.\n\nIn short, swapping in a GNN makes your feature extractor more relationally structured at the node‐and‐edge level, but you trade away the fine‐grained point‐cloud geometry, the hand‐crafted geometric invariants, and a very clean separation of part-vs-pair features—and you incur extra graph‐construction overhead and potential oversmoothing in deep message‐passing stacks."
    },
    {
        "question": "What alternative sampling strategies could mitigate bias inherent in the Negative Sampler for diverse users?",
        "relevant_section_ids": [
            "3.1.2",
            "3.1.3",
            "3.1.4",
            "3.2.2",
            "3.4.3"
        ],
        "relevant_context": [
            "Predefined SNS incorporates the pre-defined negative samples from the dataset into the recommender’s training process (Song et al., 2015; Yu et al., 2018; Sun et al., 2021; Zhang et al., 2024). For example, DRN employs real user behaviors (e.g., skipped, clicked, and ordered actions) within the dataset to delineate positive and negative samples.",
            "Popularity-based SNS selects negative samples based on the popularity of items, that is, the more popular the item is, the more likely it is to be selected as the negative sample (Gantner et al., 2012; Quadrana et al., 2017; Cheng et al., 2021; Ma et al., 2018; Rendle and Freudenthaler, 2014; Togashi et al., 2021; Wang et al., 2019b; Li et al., 2018; He et al., 2016). Relying on the assumption that the popularity of items may demonstrate users’ global preferences, a series of popularity-based SNS methods typically assign sampling weights to items based on their frequency.",
            "Non-sampling SNS considers the unobserved instances from the whole training data for recommender learning, thus avoiding negative sampling (Chen et al., 2020d; Chen et al., 2020c; Chen et al., 2019d; Chen et al., 2019b; Li et al., 2021). These related works argue that negative sampling strategies are highly sensitive to the data distribution and the number of negative samples, making them difficult to achieve the optimal performance in large-scale RS.",
            "User-similarity DNS identifies similar users based on their preferences from their historical behaviors and then dynamically selects items as negative samples according to this similarity association (Wu et al., 2019b; Wang et al., 2021b; Giobergia, 2022; Chen et al., 2019b). It can capture the dynamic user interest with historical user behaviors and approximate the user’s conditional preference state with the correlation between these users.",
            "Debiased IRW identifies and corrects the ubiquitous biases that exist in RS (e.g., popularity bias, exposure bias) and assigns higher weights to items that have been overlooked in the past to deliver more equitable and diverse recommendations (Shen et al., 2021; Yu et al., 2020; Chen et al., 2023b). For example, FairNeg proposes a negative sampling distribution mixup mechanism, which incorporates both the importance-aware negative sampling distribution and the fairness-aware negative sampling distribution to simultaneously strengthen the feature representations and item-oriented group fairness."
        ],
        "final_answer": "Rather than relying solely on uniform random negatives, one can reduce sampler bias by:\n1) Predefined SNS – use explicit negative signals (e.g. low ratings, skips) when these real behaviors are available.  \n2) Popularity-based SNS – weight the sampling probability by item popularity so under-represented items are more fairly considered.  \n3) Non-sampling SNS – leverage all unobserved items (no sampling) to avoid any sampling bias altogether.  \n4) User-similarity DNS – dynamically draw negatives from items favored by users with similar profiles, thus better matching each user’s preference space.  \n5) Debiased Importance Re-weighting – apply a fairness-aware weighting (e.g. FairNeg’s mix of importance- and fairness-aware distributions) to counteract long-standing exposure or popularity biases.",
        "relevant_elements": [
            "Negative Sampler"
        ],
        "id": 2010,
        "masked_question": "What alternative sampling strategies could mitigate bias inherent in the [mask1] for diverse users?",
        "masked_number": 1,
        "masked_elements": [
            "Negative Sampler"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.07237v1_figure_1.png",
        "paperid": "2409.07237v1",
        "paper_path": "./MISS-QA/papers/2409.07237v1.json",
        "figure_id": "2409.07237v1_figure_1.png",
        "caption": "Figure 1. Illustration of the training stage of a personalized recommender system.",
        "qtype": "Others",
        "response": "First, let me unpack the question.  In the figure the red‐boxed module is simply the “Negative Sampler,” i.e. the piece that picks un‐observed items to serve as negatives during training.  A very naive sampler (for example pure uniform sampling) will tend to re-produce all of the well‐known biases in recommendation (popularity bias, conformity bias, group‐fairness gaps, etc.) and will under‐serve under-represented users or tail items.\n\nIf your goal is to reduce that bias — to make sure that “diverse” users (fairness‐constrained, long‐tail, cold‐start, minority-group users, etc.) all get treated equitably in the negative sampling step — you can swap out the simple uniform sampler for one of the many more sophisticated alternatives reviewed in the literature.  In particular:\n\n 1.  Dynamic negative sampling (DNS)  \n    •  User‐aware DNS: pick negatives by looking at each user’s nearest neighbors or pseudo-social graph so you sample items that are informative to that user’s profile rather than always grabbing the same global pool.  \n    •  Knowledge‐aware DNS: condition your negatives on side-information (item attributes, user demographics, location, time, etc.) so that the sampler can adapt to different user segments.  \n    •  Distribution‐based DNS: monitor the score‐distribution of candidate negatives and choose samples with “just right” hardness for each user—instead of always picking the top-1 or a pure random draw.  \n    •  Mixed strategies: for instance, interleave uniform draws with hard draws, or blend user‐similarity and popularity signals in a single sampler.  \n\n 2.  Adversarial negative generation (ANG)  \n    •  Generative ANG: train a little adversarial model that learns, per user, how to invent plausible negatives—this pushes the recommender to distinguish subtle edge-cases that uniform sampling would never surface.  \n    •  Sampled ANG (e.g. IRGAN‐style): use a mini‐GAN whose “generator” selects real items as negatives in a way that maximally confuses the “discriminator” (your recommendation model).  Because you’re sampling real items, you avoid some of the stability issues of pure GANs.  \n\n 3.  Importance re-weighting (IRW)  \n    •  Debiased IRW: measure each negative sample’s contribution to known fairness metrics (e.g. group recall gaps, exposure rates) and up-weight negatives for under-represented groups.  FairNeg is one recent example that mixes an importance-aware distribution with a fairness-aware one.  \n    •  Attention-based IRW: compute a per-user or per-item attention score over the negative candidates and then rescale your loss so that the model sees some negatives as “more important” than others—this automatically pushes learning to focus on slices of data where the base sampler under-performs.  \n    •  Knowledge-based IRW: use side-information (social graphs, KG embeddings) to infer a confidence or “trust” score on each negative; you then weight your loss by that confidence so that the sampler effectively “sees” a more balanced set of negatives.  \n\n 4.  Knowledge-enhanced negative sampling (KNS)  \n    •  General KNS: first filter your negative pool by semantic categories or latent contexts (e.g. only sample from the same movie genre, or the same location cluster) to avoid drawing completely irrelevant negatives that make no sense for a given user.  \n    •  KG-based KNS: leverage a knowledge graph to perform multi-hop walks from the user’s known positives, then sample negatives from nodes that share some high-order structure but differ in key aspects—this has been shown both to debias popularity skews and to inject richer semantics into the negative pool.  \n\nIn practice you often pick one of these fancy samplers or even combine them (a “mixed DNS” + “importance-reweighted” sampler, or a small adversarial generator followed by a distribution-based filter).  Any of these choices will tend to spread your negative mass more evenly across users, reduce over-exposure of blockbuster items, and close the gap in recommendation quality for less-popular users and items."
    },
    {
        "question": "What ethical concerns arise from using BPR Loss in the Recommender training process?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "BPR Loss",
            "Recommender"
        ],
        "id": 2011,
        "masked_question": "What ethical concerns arise from using [mask1] in the Recommender training process?",
        "masked_number": 1,
        "masked_elements": [
            "BPR Loss"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.07237v1_figure_1.png",
        "paperid": "2409.07237v1",
        "paper_path": "./MISS-QA/papers/2409.07237v1.json",
        "figure_id": "2409.07237v1_figure_1.png",
        "caption": "Figure 1. Illustration of the training stage of a personalized recommender system.",
        "qtype": "Others",
        "response": "First, let us be clear about what “​[mask1]​” is.  In the red‐boxed area of Fig. 1 the paper shows the BPR loss—the pairwise ranking loss that pushes the model to score a “positive” (observed) user–item pair higher than a “negative” (unobserved or sampled) one.  \n\nNow: what ethical worries come with training a recommender under a BPR‐style objective?  Thinking it through:  \n\n1.  **False negatives & misrepresentation of intent**  \n    –  BPR treats every item a user has never interacted with as a “negative.”  In reality the user may simply never have seen that item yet, or might have liked it if they had.  By punishing all unclicked items, the model may learn wrong conclusions about what users dislike.  \n\n2.  **Feedback loops and popularity bias**  \n    –  Once the model learns that certain popular items are “good” and certain unpopular ones are “bad,” it will tend to keep recommending the same handful of hits and to suppress long-tail or niche content.  Over time this can cement a “rich get richer” cycle, further starving smaller or minority‐taste items of exposure.  \n\n3.  **Filter bubbles and echo chambers**  \n    –  By aggressively pushing what the user appears to like and strongly downgrading everything else, the system can create an ever-narrowing view of the world.  Users stop seeing novel or challenging content and the system reinforces whatever viewpoint it has already “learned.”  \n\n4.  **Fairness and disparate impact**  \n    –  If certain user groups or types of items are under‐represented in the training logs, BPR will systematically score them lower.  That can lead to unfair treatment—some demographic groups or new creators may be effectively “written out” of the recommendation results.  \n\n5.  **Privacy and consent**  \n    –  The BPR loss depends on logging every single interaction (or non-interaction) and then mining it for training.  Users may not realize that the historical record of “what they didn’t click” is being treated as an active dislike signal.  Using those “missing” interactions as negatives without explicit consent raises privacy and transparency concerns.  \n\nIn short, although BPR‐style pairwise learning is very effective at driving up click or purchase metrics, it carries with it serious risks of mis-labeling user intent, amplifying popularity bias, conspiring in filter bubbles, undermining fairness and diversity, and encroaching on user privacy."
    },
    {
        "question": "What reasoning motivates employing a negative sampler that selects negative samples before recommender training?",
        "relevant_section_ids": [
            "1",
            "2.1",
            "3"
        ],
        "relevant_context": [
            "In alignment with other supervised tasks, providing appropriate supervised signals (both positive and negative feedback) is indispensable in the training phase of recommendation algorithms.",
            "Real-world recommendation systems frequently involve more than millions of users and items, rendering the integration of all corpus into the training process prohibitively expensive.",
            "Negative sampling is the critical and irreplaceable element in recommendation that could potentially improve the modeling of dynamic user preferences with their sparse interactions.",
            "Its crucial secret lies in its ability to select samples from each user’s vast collection of unobserved items, specifically tailored to enhance the model’s optimization within its current state.",
            "Negative sampling aims to select the appropriate instances from the unobserved item candidates as the negative samples to support the recommender optimization."
        ],
        "final_answer": "Because (1) most recommendation datasets lack explicit negative feedback, yet supervised training requires both positive and negative signals; (2) there are millions of unobserved items, making it computationally infeasible to treat all of them as negatives; and (3) by pre-selecting a small set of informative negative samples—especially hard negatives—the model can learn more discriminative user preferences, reduce bias, speed up convergence, and keep training costs manageable.",
        "relevant_elements": [
            "Negative Sampler",
            "Negative Samples",
            "Recommender"
        ],
        "id": 2012,
        "masked_question": "What reasoning motivates employing a [mask1] that selects negative samples before recommender training?",
        "masked_number": 1,
        "masked_elements": [
            "Negative Sampler"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.07237v1_figure_1.png",
        "paperid": "2409.07237v1",
        "paper_path": "./MISS-QA/papers/2409.07237v1.json",
        "figure_id": "2409.07237v1_figure_1.png",
        "caption": "Figure 1. Illustration of the training stage of a personalized recommender system.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought answer:\n\n1.  In implicit‐feedback recommendation there is no “ground‐truth” negative label – only a huge pool of unobserved items for each user.  \n2.  Trying to treat every unobserved item as a negative would (a) swamp the model with mostly meaningless data, (b) blow up the computation, and (c) bias learning toward popularity rather than true dislikes.  \n3.  Instead, one pre‐filters the unobserved set by running a small “sampler” that picks out a handful of informative negatives – for example, items the current model scores almost as highly as positives (hard negatives), or that are diverse enough to cover the space.  \n4.  These carefully chosen negatives then act as the negative supervision signal during training (e.g. in a BPR‐style loss), balancing the positives, cutting down runtime, and steering the gradient in the right direction.  \n\nAnswer to the question:  \nThe red‐boxed component is a “negative sampler.”  \nIt is introduced before the recommender to efficiently winnow the enormous set of unobserved items down to a small, high-value subset of negative examples. This both (1) provides the missing negative signals needed for unbiased learning and (2) dramatically reduces computational cost, while still exposing the model to the most informative (“hard”) negatives that speed up convergence and improve discrimination."
    },
    {
        "question": "What is the methodological motivation behind optimizing the recommender with BPR loss?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "BPR Loss",
            "Recommender"
        ],
        "id": 2013,
        "masked_question": "What is the methodological motivation behind optimizing the [mask1] with BPR loss?",
        "masked_number": 1,
        "masked_elements": [
            "Recommender"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.07237v1_figure_1.png",
        "paperid": "2409.07237v1",
        "paper_path": "./MISS-QA/papers/2409.07237v1.json",
        "figure_id": "2409.07237v1_figure_1.png",
        "caption": "Figure 1. Illustration of the training stage of a personalized recommender system.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed module in Figure 1 is simply the recommender’s scoring function – the part of the system that, given a user and an item, produces a real‐valued score F(u,i).  We train that scorer with a pairwise (“Bayesian Personalized Ranking” or BPR) objective because:\n\n 1.  In most recommender setups we only observe positive interactions (e.g. clicks, purchases) and have to manufacture negatives by sampling un-seen items.  BPR is tailor-made to that implicit-feedback setting.  \n 2.  Rather than predict absolute scores, BPR directly forces the model to rank observed positives higher than sampled negatives by minimizing  \n      ∑_{(u,p,n)} –log σ( F(u,p) – F(u,n) )  \n    over triples (user u, positive p, negative n).  \n 3.  This pairwise loss aligns the training objective with the ultimate goal (getting the positives to the top of the recommendation list) and has been shown to converge quickly and work well in large, sparse implicit datasets.  \n 4.  Finally, because it only requires comparing one positive against one negative at a time, it scales far better than trying to push positives above all un-seen items simultaneously."
    },
    {
        "question": "What motivates applying Local Hard Negative Sampling to each single-label classification subtask?",
        "relevant_section_ids": [
            "2.2",
            "4.3.1"
        ],
        "relevant_context": [
            "Thirdly, for each positive label, we select all of its negative siblings and negative descendant labels as the negative label set. Since the hierarchy of HTC is tree-like, classifiers on the same path will share similar hard negatives, which will drive them to have similar discriminative ability.",
            "Local Hard Negative Sampling. As mentioned in section 2.2, hard negative sampling adopts a strategy of sampling the nearest negative samples, which are challenging for the model. In HTC, given an input text and one of its positive labels, the closest labels are the sibling and descendant of the positive label as shown in Figure 3. Thus, we propose to sample the negative sibling and negative descendant labels as the hard negative label set, called Local Hard Negative Sampling. ... Formally, ... Since the positive labels are located within same sub-space, their hard negative labels are largely overlapped. Conducting LCL on the corresponding classifiers, their discriminative ability will be enhanced in similar direction and consequently share similar behavior."
        ],
        "final_answer": "Because in a tree‐structured taxonomy the most confusing (hard) negatives for any positive label are its sibling and descendant nodes, and classifiers along the same path share these hard negatives. By applying Local Hard Negative Sampling to each single‐label subtask, the model is driven to learn finer distinctions using the most challenging negatives, which enhances discriminative ability and enforces consistent behavior among related classifiers.",
        "relevant_elements": [
            "Local Hard Negative Sampling",
            "single-label classification"
        ],
        "id": 2014,
        "masked_question": "What motivates applying [mask1] to each [mask2] subtask?",
        "masked_number": 2,
        "masked_elements": [
            "Local Hard Negative Sampling",
            "single-label classification"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.05786v1_figure_2.png",
        "paperid": "2408.05786v1",
        "paper_path": "./MISS-QA/papers/2408.05786v1.json",
        "figure_id": "2408.05786v1_figure_2.png",
        "caption": "Figure 2: Illustration of HiLight. Given an input text, HiLight infers label probabilities by a text encoder and a multi-label classification head. With the inferred label probabilities and positive labels, HiLight conducts label space learning with MLC and HiLCL task. HiLCL is our proposed method and it divides the multi-label classification learning into multiple single-label classification learning. Then, HiLCL improves contrastive learning on each single-label classification learning with Local Hard Negative Sampling, which introduces negative labels from sibling and descendant label set of the positive label. Those negative labels outside the sibling and descendant label set are masked out during learning. HiLCL schedules learning with Hierarchical Learning strategy, which adopts a fine-to-coarse learning strategy to improve the discrimination of finest-grained labels.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition in three steps:\n\n1. Sub‐task division (“[mask2]”) breaks the overall multi‐label problem into a bunch of single‐label classification tasks—one task per positive label.  \n2. In a hierarchy, the “hardest” negatives for any given positive label are its siblings and its descendants (they live closest in the label‐space tree).  \n3. Because many of these hard negatives are shared along a path, if we apply Local Hard Negative Sampling (“[mask1]”) inside each of those single‐label subtasks, each classifier is forced to distinguish its positive label from exactly the labels that are most easily confused with it. This both sharpens each classifier’s decision boundary on the fine‐grained distinctions and ensures that classifiers along the same path all learn in a consistent direction."
    },
    {
        "question": "What drives using a fine-to-coarse Hierarchical Learning strategy for progressive label space division?",
        "relevant_section_ids": [
            "4.3.2"
        ],
        "relevant_context": [
            "Intuitively, HTC is a coarse-to-fine classification process and correspondingly the behavior of classifiers at same path is a coarse-to-fine label space division process.",
            "The scopes of label spaces along any path degrade gradually and eventually the label spaces at leaf level are the finest-grained division of the whole space.",
            "LCL adopts negative labels from lower levels and the space division learning of finer-grained labels will be affected due to suppression of softmax loss in Eq.9.",
            "In order to improve the discrimination of finest-grained labels, we propose a fine-to-coarse learning strategy, called Hierarchical Learning (HiLearn)."
        ],
        "final_answer": "Because HTC naturally performs a coarse-to-fine division of its label space and Local Contrastive Learning (LCL) with lower-level negatives can suppress finer-grained distinctions, a fine-to-coarse scheduling (Hierarchical Learning) is adopted to progressively introduce higher-level labels so as to enhance discrimination at the leaf (finest-grained) level before moving to coarser levels.",
        "relevant_elements": [
            "Hierarchical Learning",
            "fine-to-coarse"
        ],
        "id": 2015,
        "masked_question": "What drives using a fine-to-coarse [mask1] strategy for progressive label space division?",
        "masked_number": 1,
        "masked_elements": [
            "Hierarchical Learning"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.05786v1_figure_2.png",
        "paperid": "2408.05786v1",
        "paper_path": "./MISS-QA/papers/2408.05786v1.json",
        "figure_id": "2408.05786v1_figure_2.png",
        "caption": "Figure 2: Illustration of HiLight. Given an input text, HiLight infers label probabilities by a text encoder and a multi-label classification head. With the inferred label probabilities and positive labels, HiLight conducts label space learning with MLC and HiLCL task. HiLCL is our proposed method and it divides the multi-label classification learning into multiple single-label classification learning. Then, HiLCL improves contrastive learning on each single-label classification learning with Local Hard Negative Sampling, which introduces negative labels from sibling and descendant label set of the positive label. Those negative labels outside the sibling and descendant label set are masked out during learning. HiLCL schedules learning with Hierarchical Learning strategy, which adopts a fine-to-coarse learning strategy to improve the discrimination of finest-grained labels.",
        "qtype": "Design_Rationale",
        "response": "Let me walk through the reasoning:\n\n1.  In a hierarchical‐multi‐label problem the natural *inference* order is coarse→fine (you first pick a broad category, then drill down).  \n2.  However, when you do Local Contrastive Learning (LCL) you draw *hard negatives* from deeper (more specific) levels of the tree.  If you train in a coarse→fine order, those negative samples actually *suppress* the fine‐grained distinctions under the softmax/contrastive loss.  \n3.  To counteract that suppression—and in fact *boost* the model’s ability to separate the leaf‐level (most specific) classes—you invert the schedule.  You start by learning to discriminate the finest‐grained labels first, then gradually expand the target set to include their parents.  \n4.  That is what drives the fine‐to‐coarse strategy: *improving the discrimination of the most specific (leaf‐level) labels before bringing in the coarser levels.*  \n\nSo, the answer is:\n\nBecause sampling hard negatives from deeper levels during LCL weakens the model’s fine‐grained discrimination if you train coarse→fine, they reverse the order—training fine→coarse—to *improve the separation of the leaf‐level (finest‐grained) classes.*"
    },
    {
        "question": "How does Local Hard Negative Sampling identify and mask easy negatives during label space learning?",
        "relevant_section_ids": [
            "4.3.1"
        ],
        "relevant_context": [
            "Thus, we propose to sample the negative sibling and negative descendant labels as the hard negative label set, called Local Hard Negative Sampling.",
            "On the contrary, those negative labels outside the sibling and descendant set, share less characteristics with the positive label and easier to distinguish, which are treated as easy negative labels and masked out during learning as shown in Figure 3."
        ],
        "final_answer": "Local Hard Negative Sampling identifies ‘hard negatives’ as the sibling and descendant labels of a given positive label. All other labels (those outside the sibling and descendant sets) are considered easy negatives and are masked out during learning.",
        "relevant_elements": [
            "Local Hard Negative Sampling"
        ],
        "id": 2016,
        "masked_question": "How does [mask1] identify and mask easy negatives during label space learning?",
        "masked_number": 1,
        "masked_elements": [
            "Local Hard Negative Sampling"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.05786v1_figure_2.png",
        "paperid": "2408.05786v1",
        "paper_path": "./MISS-QA/papers/2408.05786v1.json",
        "figure_id": "2408.05786v1_figure_2.png",
        "caption": "Figure 2: Illustration of HiLight. Given an input text, HiLight infers label probabilities by a text encoder and a multi-label classification head. With the inferred label probabilities and positive labels, HiLight conducts label space learning with MLC and HiLCL task. HiLCL is our proposed method and it divides the multi-label classification learning into multiple single-label classification learning. Then, HiLCL improves contrastive learning on each single-label classification learning with Local Hard Negative Sampling, which introduces negative labels from sibling and descendant label set of the positive label. Those negative labels outside the sibling and descendant label set are masked out during learning. HiLCL schedules learning with Hierarchical Learning strategy, which adopts a fine-to-coarse learning strategy to improve the discrimination of finest-grained labels.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is nothing exotic – it is simply the “Local Hard Negative Sampling” step.  In practice what it does is:\n\n1.  For each positive label uᵢ in a training example, pull out its immediate “neighbors” in the hierarchy – i.e. all of uᵢ’s sibling nodes plus its descendants.  These are treated as **hard negatives** because they live close to uᵢ in the tree and share many features.  \n2.  All of the other negative labels (those not in the sibling ∪ descendant set) are deemed “easy negatives.”  \n3.  During the local contrastive learning pass, the easy negatives are simply masked out (ignored) and only the hard negatives (the sibling/descendant labels) remain in the contrastive loss.  \n\nBy doing so, the model focuses on the few truly confusable negatives and drops all of the far‐away (easy) negatives from the loss."
    },
    {
        "question": "How does Hierarchical Learning integrate with HiLCL Task to enforce fine-to-coarse label discrimination?",
        "relevant_section_ids": [
            "4.3.2",
            "4.3.3"
        ],
        "relevant_context": [
            "Hierarchical Learning is the scheduling strategy for LCL. Intuitively, HTC is a coarse-to-fine classification process and correspondingly the behavior of classifiers at same path is a coarse-to-fine label space division process. … we propose a fine-to-coarse learning strategy, called Hierarchical Learning (HiLearn). For each training sample (x, Y), HiLearn samples a subset of Y as the target set at each epoch t and enlarges the target set every β epoch by adding labels from higher levels. Formally, where d_i is the reverse depth (leaf depth = 0) and β is a scheduling parameter.",
            "Combining LCL and HiLearn, we propose Hierarchical Local Contrastive Learning task (HiLCL). HiLCL divides the multi-label classification learning into multiple single-label classification learning as shown in Figure 3. Then, HiLCL conducts LCL on each classifier (Eq.9) and schedules the learning with HiLearn (Eq.10), which adopts a fine-to-coarse strategy."
        ],
        "final_answer": "Within the HiLCL task, the Hierarchical Learning (HiLearn) mechanism schedules the Local Contrastive Learning (LCL) in a fine-to-coarse manner.  Specifically, at early training epochs HiLearn restricts contrastive learning to the finest-grained (leaf) labels, and then—every β epochs—adds parent labels to the target set.  By progressively enlarging the label set from leaves up to higher levels, HiLCL is guided to first distinguish fine-grained categories and then learn coarser distinctions, thereby enforcing fine-to-coarse label discrimination.",
        "relevant_elements": [
            "Hierarchical Learning",
            "HiLCL Task"
        ],
        "id": 2017,
        "masked_question": "How does [mask1] integrate with [mask2] to enforce fine-to-coarse label discrimination?",
        "masked_number": 2,
        "masked_elements": [
            "Hierarchical Learning",
            "HiLCL Task"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.05786v1_figure_2.png",
        "paperid": "2408.05786v1",
        "paper_path": "./MISS-QA/papers/2408.05786v1.json",
        "figure_id": "2408.05786v1_figure_2.png",
        "caption": "Figure 2: Illustration of HiLight. Given an input text, HiLight infers label probabilities by a text encoder and a multi-label classification head. With the inferred label probabilities and positive labels, HiLight conducts label space learning with MLC and HiLCL task. HiLCL is our proposed method and it divides the multi-label classification learning into multiple single-label classification learning. Then, HiLCL improves contrastive learning on each single-label classification learning with Local Hard Negative Sampling, which introduces negative labels from sibling and descendant label set of the positive label. Those negative labels outside the sibling and descendant label set are masked out during learning. HiLCL schedules learning with Hierarchical Learning strategy, which adopts a fine-to-coarse learning strategy to improve the discrimination of finest-grained labels.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be crystal-clear what each mask is pointing to:\n\n  • [mask1] is the “Hierarchical Learning” module (the red-boxed inset in Figure 3), i.e. the fine-to-coarse scheduling strategy that tells us, at each epoch, which subset of the hierarchy we should be learning over.  \n  • [mask2] is the “HiLCL Task” head (the blue box on the right), i.e. the set of single-label softmax classifiers whose loss (Eq. 9) is a standard softmax (contrastive) loss over a small, dynamically chosen band of labels.  \n\nHow they fit together to enforce fine-to-coarse discrimination:\n\n  1.  At its core, HiLCL (mask2) turns the multi-label job into a sequence of single-label softmax problems.  For each positive label ℓ, it pulls ℓ close and pushes away only its “hard negatives” (siblings + descendants) in that tiny softmax.  \n  2.  Hierarchical Learning (mask1) sits on top of that and *schedules* exactly *which* labels ℓ and which of their hard negatives appear in that softmax on each epoch.  \n  3.  Concretely, Hierarchical Learning starts out only with the *deepest* (leaf) labels in the softmax task.  You train the softmax-only on those leaf classes, forcing very fine-grained distinctions first.  \n  4.  Every few epochs, it *adds* the next level up in the tree into the softmax pool.  Now your HiLCL softmax (mask2) is discriminating between a slightly bigger—coarser—set of labels.  \n  5.  You keep repeating: fine labels first, then gradually parents, then grandparents, until you arrive at the root.  \n\nBy literally telling the HiLCL softmax head “begin with leaves, then include parents, then grandparents…,” the Hierarchical Learning scheduler (mask1) makes the softmax-based contrastive loss (mask2) enforce a guaranteed *fine-to-coarse* progression of discrimination."
    },
    {
        "question": "How does projection according to position x,y interact with adding perturbation to maintain object alignment?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Subsequently, it determines the designated x and y for each attack frame based on M_t (Line 4). Utilizing the derived x, y, and the models D and E, it computes the perturbed frame I'_t using Eqs. (1) and (2) (Line 5), which is then added to I_t.",
            "The loss determined for each object is used to identify specific perturbations (④), which take into account the objects’ coordinates and are then projected onto the input image (⑤), effectively integrating these perturbations into the original image (⑥)."
        ],
        "final_answer": "After computing a per‐object adversarial patch via back‐propagation on its feature, BankTweak uses the detected bounding‐box coordinates (x,y) to 'project' or place that patch back onto exactly the same region in the full‐frame image. This projection step aligns each perturbation with its corresponding object’s location, and only then is the patch added pixel‐wise to the original frame, ensuring the adversarial noise stays registered to the correct object.",
        "relevant_elements": [
            "Projection according to position x,y",
            "Adding perturbation"
        ],
        "id": 2018,
        "masked_question": "How does [mask1] interact with [mask2] to maintain object alignment?",
        "masked_number": 2,
        "masked_elements": [
            "Projection according to position x,y",
            "Adding perturbation"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.12727v1_figure_2.png",
        "paperid": "2408.12727v1",
        "paper_path": "./MISS-QA/papers/2408.12727v1.json",
        "figure_id": "2408.12727v1_figure_2.png",
        "caption": "Figure 2: Generating perturbed features by iterating ①–⑥ to induce ID switch between two objects in BankTweak: a focus on the feature extractor during the association phase.",
        "qtype": "Implementation_Details",
        "response": "First, let us identify the two masked regions in the figure:\n\n  – [mask2] (blue‐outlined box) is the entire “Detector D(·)” block (step ① in the diagram), i.e. the standard object detector that takes the current frame (with whatever tiny perturbations it already has) and outputs a set of bounding–boxes.  \n  – [mask1] (red‐outlined box) is the “Projection according to position x,y” block (step ⑤), i.e. the little routine that takes each object’s per–object perturbation patch and pastes (projects) it back into the full image, at exactly the coordinates the detector just gave us.\n\nHow do they “talk” to each other and thus keep everything lined up, iteration after iteration?  Here is the chain of reasoning:\n\n 1.  On every PGD iteration we start by running our (blue) Detector D(·) on the current frame  \n    – it returns a list of N bounding‐boxes, each box being “this object lives at pixel‐region (xᵢ,yᵢ,wᵢ,hᵢ).”  \n 2.  We crop and resize each of those N boxes, send them through the feature extractor, compute a per‐object gradient (④), take its sign, etc., and turn that into a little patch of adversarial noise for each object.  \n 3.  Now we have N tiny perturbation patches, but they are just floating in “patch‐space.”  To turn them back into a full‐frame perturbation we must know where to drop them.  That is exactly what the (red) Projection module does: it takes each patch and pastes it into the big image canvas at position (xᵢ,yᵢ), with the same width/height the detector told us.  \n 4.  The result of that pasting is a single Δ image (with all N patches in the right places).  We add Δ to the current frame, clip to [0,1], and feed it back into the detector in the next PGD iteration.\n\nBecause the red “projection” step always uses the very same box coordinates that the blue detector just produced, the adversarial noise for object A always lands exactly inside A’s bounding box, and the noise for B always lands inside B’s box.  That coordinate‐to‐coordinate mapping is what keeps “object A’s patch” glued to object A from one pass to the next, and likewise for object B.  In short:\n\n  • Detector D(·) (blue) tells us “object i lives at (xᵢ,yᵢ).”  \n  • Projection (red) uses those same (xᵢ,yᵢ) to paste object i’s perturbation patch back exactly there.  \n\nBecause of that tight “box‐in → gradient‐out → same box‐out” loop, our per–object adversarial tweaks never drift off; each patch is always aligned to the object that produced it."
    },
    {
        "question": "How does backpropagation αsgn(∇L(F*,F)) integrate with Feature extractor E(·) compared to gradient-based adversarial methods?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Feature extractor E(·)",
            "Backpropagation αsgn(∇L(F*,F))"
        ],
        "id": 2020,
        "masked_question": "How does backpropagation αsgn(∇L(F*,F)) integrate with [mask1] compared to gradient-based adversarial methods?",
        "masked_number": 1,
        "masked_elements": [
            "Feature extractor E(·)"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.12727v1_figure_2.png",
        "paperid": "2408.12727v1",
        "paper_path": "./MISS-QA/papers/2408.12727v1.json",
        "figure_id": "2408.12727v1_figure_2.png",
        "caption": "Figure 2: Generating perturbed features by iterating ①–⑥ to induce ID switch between two objects in BankTweak: a focus on the feature extractor during the association phase.",
        "qtype": "Literature_Background",
        "response": "The little red box in the figure is simply the feature‐extraction subroutine (step ②) together with its back-propagation path (step ③).  In other words,\n\n  [mask1]  ≡ “Feature extractor E(·)   →   Feature set F ∈ ℝ^{2×512}   ↔   Back-propagation α·sgn(∇₍F₎L(F*,F)))”\n\nWhen we say that BankTweak “injects” α sgn(∇L(F*,F)) into this red block what really happens is:\n\n  1.  We take each detected box, crop it out, and run it through the same CNN backbone E(·) that the tracker uses to produce an appearance feature vector F.  \n  2.  We compute a cosine‐distance–based loss L(F*, F) between that clean feature F and whatever target feature F* we are trying to implant in the tracker’s feature bank.  \n  3.  We back-propagate that loss right through the extraction network E(·) all the way back to the pixels of the cropped patch, take the sign of the resulting gradient, scale it by some small α, and write that tiny perturbation back into the original frame inside exactly the same box.\n\nContrast that with a vanilla FGSM or PGD attack on a classifier:\n\n  •  FGSM/PGD compute ∇ₓℓ(class,x) over the entire image  \n  •  They treat the network as a monolithic function from pixels to final logits, and then push pixels in the direction that will flip the classifier output.  \n\nBankTweak, by contrast, splits the detector ↦ feature extractor ↦ matcher pipeline into two stages and attacks only the second stage.  It defines ℓ on the intermediate 512-d appearance embeddings and back-props through E(·), *per object crop*, rather than back-propping a classification loss through the whole image all at once."
    },
    {
        "question": "How does Detector D(·) influence Adding perturbation relative to detection-driven adversarial attack strategies?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "For each input frame I_i, BankTweak performs the detection to obtain the object set O_i and then conducts feature extraction based on O_i to extract the feature set F_i (Line 3).",
            "Subsequently, it determines the designated x^t_i and x^t_j for each attack frame based on O_i (Line 4).",
            "The detector D is used for cropping the detected object from the input image after performing detection, and the perturbation is determined through the model E (Line 5)."
        ],
        "final_answer": "In BankTweak, Detector D(·) is not itself the attack target (as in detection‐driven adversarial attacks) but rather the mechanism used to locate each object. Its outputs—bounding boxes and object coordinates—define exactly where the pixel‐level perturbations are projected and added. In other words, D(·) supplies the spatial crops and (x, y) positions that guide the placement of the adversarial noise, rather than being fooled into false negatives or false positives.",
        "relevant_elements": [
            "Detector D(·)",
            "Adding perturbation"
        ],
        "id": 2021,
        "masked_question": "How does [mask1] influence [mask2] relative to detection-driven adversarial attack strategies?",
        "masked_number": 2,
        "masked_elements": [
            "Detector D(·)",
            "Adding perturbation"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.12727v1_figure_2.png",
        "paperid": "2408.12727v1",
        "paper_path": "./MISS-QA/papers/2408.12727v1.json",
        "figure_id": "2408.12727v1_figure_2.png",
        "caption": "Figure 2: Generating perturbed features by iterating ①–⑥ to induce ID switch between two objects in BankTweak: a focus on the feature extractor during the association phase.",
        "qtype": "Literature_Background",
        "response": "In the BankTweak diagram the little red box around “① Detector D(·)” is **not** something we are trying to fool – it simply gives us the object bounding‐boxes (x,y) that tell us where to stick our noise.  The blue box around “⑥ Adding perturbation” is the *act* of pasting those small, object‐wise patches back into the full image.  \n\nIn a **detection-driven** attack you would actually back-propagate through the detector and change its weights or outputs (i.e. you craft a perturbation whose gradient comes from the detector’s loss so that you make it miss or mis‐localize people).  **BankTweak** does exactly the opposite.  It uses D(·) only as a **location oracle** (no gradients through D), crops out each proposal, sends those crops into the *feature* network E(·), takes the gradients there, and then projects the resulting per‐object noise patches back into the image at exactly the (x,y) coordinates that D gave you.  \n\nIn short:\n– Mask 1 (Detector D) is just a **box‐generator**  \n– Mask 2 (Adding perturbation) is where you actually **paste** the object‐wise noise  \n– Unlike detection-driven attacks, we do **not** compute gradients through D; we only use D to tell us *where* to put the noise, and we get our gradients entirely out of the feature extractor."
    },
    {
        "question": "How does sketch attention branch adapt spatial self-attention mechanisms to inject sketch semantics into denoising?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Sketch attention. To interpret and inject sketch semantics into the latent space, we augment the DiT backbone with the sketch attention within each spatial self-attention module except for the first. The sketch attention incorporates a predefined binary matrix A to indicate correspondences between the latent code and the sketch sequence:",
            "Sketch attention is implemented as a parallel branch, and its outputs are added with a learnable scaling parameter λ, allowing adaptive weighting of injected sketch semantics."
        ],
        "final_answer": "Within each spatial self-attention module (except the very first), VIRES adds a parallel “sketch attention” branch. In that branch, queries come from the video latent features and keys/values come from the extracted sketch features, with a fixed binary matrix encoding which video tokens correspond to which sketch tokens. The sketch-attention output is then scaled by a learned parameter λ and summed into the original spatial self-attention output. In this way the network adaptively injects fine-grained sketch semantics into the denoising process.",
        "relevant_elements": [
            "sketch attention",
            "spatial self-attention module"
        ],
        "id": 2022,
        "masked_question": "How does [mask1] branch adapt spatial self-attention mechanisms to inject sketch semantics into denoising?",
        "masked_number": 1,
        "masked_elements": [
            "sketch attention"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.16199v2_figure_2.png",
        "paperid": "2411.16199v2",
        "paper_path": "./MISS-QA/papers/2411.16199v2.json",
        "figure_id": "2411.16199v2_figure_2.png",
        "caption": "Figure 2: The pipeline of VIRES model.\n(a) The input video clip and corresponding text descriptions are encoded using independent encoders.\nNoise is selectively added to the latent code according to the instance mask.\n(b) This noised latent code is fed into the denoising network, composed of staked transformer blocks with spatial and temporal self-attention modules, trained to reverse the diffusion process.\n(c) The Sequential ControlNet extracts structure layouts and injects them into the denoising network with the standardized self-scaling.\n(d) The sketch attention is implemented as a parallel branch within the spatial self-attention module, injecting structure semantics into subsequent transformer blocks.\n(e) A sketch-aware encoder additionally provides multi-level texture features during decoding, generating the repainted video clip from the denoised latent code.",
        "qtype": "Literature_Background",
        "response": "Here’s what the red‐boxed “sketch attention” branch is doing, step by step:\n\n1.  Parallel to the normal spatial self‐attention (SA) in each transformer block, we spawn a second, “sketch” attention head.  \n2.  We take the video‐latent features fᶻ and project them to Queries (Q), and separately take the sketch‐extracted features fˢ and project them to Keys (K) and Values (V).  \n3.  We compute the usual dot‐product affinity Q·Kᵀ, but then elementwise‐mask it with a fixed binary matrix A that encodes which video tokens should attend to which sketch tokens (Eq. 8 in the paper).  \n4.  After applying Softmax to get attention weights, we multiply by V to produce a “sketch‐informed” feature map.  \n5.  Finally, we multiply that map by a small learnable scalar λ and add it back into the original SA output (“⊕” in the figure).  \n\nIn this way, the transformer’s spatial self‐attention layers are explicitly augmented with fine‐grained sketch semantics: video tokens learn to attend to—and absorb—structural cues directly from the parallel sketch feature stream."
    },
    {
        "question": "How does standardized self-scaling after Sequential ControlNet refine sketch feature modulation compared to FiLM?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Feature modulation has proven effective in conditional image editing (e.g., AdaIN [9], FiLM [26], and SPADE [23]). Observing that the sketch has high-contrast transitions between black lines and the white background, we introduce the standardized self-scaling to adaptively capture sketch details, instead of performing simply addition.",
            "Specifically, we use sketch features f^s extracted by the Sequential ControlNet and standardize them to scale the features themselves, effectively highlighting the high-contrast regions: where μ and σ represent the function of mean and standard deviation, respectively. We then shift the feature domain from sketch to video by aligning their means: where f^z represents the video features.",
            "To reduce computational cost, standardized self-scaling is applied only once to the first transformer block of the DiT backbone."
        ],
        "final_answer": "Rather than using FiLM’s simple add-and-scale with learned parameters, standardized self-scaling normalizes the extracted sketch features (subtracting their mean and dividing by their standard deviation) and then uses those normalized values to scale themselves. This self-scaling step accentuates the sketch’s high-contrast strokes, and a subsequent mean alignment shifts the result into the video feature domain. By doing so, it adaptively highlights fine-grained sketch details in a contrast-sensitive way, instead of relying on a fixed additive modulation as in FiLM.",
        "relevant_elements": [
            "Sequential ControlNet",
            "standardized self-scaling"
        ],
        "id": 2023,
        "masked_question": "How does [mask1] after Sequential ControlNet refine sketch feature modulation compared to FiLM?",
        "masked_number": 1,
        "masked_elements": [
            "standardized self-scaling"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.16199v2_figure_2.png",
        "paperid": "2411.16199v2",
        "paper_path": "./MISS-QA/papers/2411.16199v2.json",
        "figure_id": "2411.16199v2_figure_2.png",
        "caption": "Figure 2: The pipeline of VIRES model.\n(a) The input video clip and corresponding text descriptions are encoded using independent encoders.\nNoise is selectively added to the latent code according to the instance mask.\n(b) This noised latent code is fed into the denoising network, composed of staked transformer blocks with spatial and temporal self-attention modules, trained to reverse the diffusion process.\n(c) The Sequential ControlNet extracts structure layouts and injects them into the denoising network with the standardized self-scaling.\n(d) The sketch attention is implemented as a parallel branch within the spatial self-attention module, injecting structure semantics into subsequent transformer blocks.\n(e) A sketch-aware encoder additionally provides multi-level texture features during decoding, generating the repainted video clip from the denoised latent code.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the “standardized self-scaling” module (Eq. 6) placed right after the Sequential ControlNet.  In FiLM you would take some external embedding (e.g. of the sketch) and learn a pair of affine parameters (γ, β) to scale and shift your features.  Here, by contrast, we derive our scaling and shifting directly from the sketch feature maps themselves:\n\n1.  We compute the per-channel mean μₛ and standard deviation σₛ of the extracted sketch features fˢ, and normalize them:  \n   f̂ˢ = (fˢ – μₛ) / σₛ  \n   This “standardizes” the sketch features so that the strongest high-contrast edges stand out.  \n\n2.  We then re-scale and re-shift these normalized features by the statistics of the current video features fᶻ:  \n   f_out = f̂ˢ ⊙ σᶻ + μᶻ.  \n\nBecause the scale and bias come from the sketch itself (rather than fixed, learned parameters as in FiLM), the module automatically adapts to whatever contrast or line-thickness the sketch presents.  In practice this sharper, content-aware modulation lets the network highlight the sketch’s boundaries much more faithfully than a blunt, learned affine transform."
    },
    {
        "question": "How does standardized self-scaling after Sequential ControlNet refine sketch feature modulation compared to FiLM?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Feature modulation has proven effective in conditional image editing (e.g., AdaIN [9], FiLM [26], and SPADE [23]). Observing that the sketch has high-contrast transitions between black lines and the white background, we introduce the standardized self-scaling to adaptively capture sketch details, instead of performing simply addition.",
            "Specifically, we use sketch features f^s extracted by the Sequential ControlNet and standardize them to scale the features themselves, effectively highlighting the high-contrast regions: where μ and σ represent the function of mean and standard deviation, respectively. We then shift the feature domain from sketch to video by aligning their means: where f^z represents the video features.",
            "To reduce computational cost, standardized self-scaling is applied only once to the first transformer block of the DiT backbone."
        ],
        "final_answer": "Rather than using FiLM’s simple add-and-scale with learned parameters, standardized self-scaling normalizes the extracted sketch features (subtracting their mean and dividing by their standard deviation) and then uses those normalized values to scale themselves. This self-scaling step accentuates the sketch’s high-contrast strokes, and a subsequent mean alignment shifts the result into the video feature domain. By doing so, it adaptively highlights fine-grained sketch details in a contrast-sensitive way, instead of relying on a fixed additive modulation as in FiLM.",
        "relevant_elements": [
            "Sequential ControlNet",
            "standardized self-scaling"
        ],
        "id": 2023,
        "masked_question": "How does [mask1] after Sequential ControlNet refine sketch feature modulation compared to FiLM?",
        "masked_number": 1,
        "masked_elements": [
            "standardized self-scaling"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.16199v2_figure_2.png",
        "paperid": "2411.16199v2",
        "paper_path": "./MISS-QA/papers/2411.16199v2.json",
        "figure_id": "2411.16199v2_figure_2.png",
        "caption": "Figure 2: The pipeline of VIRES model.\n(a) The input video clip and corresponding text descriptions are encoded using independent encoders.\nNoise is selectively added to the latent code according to the instance mask.\n(b) This noised latent code is fed into the denoising network, composed of staked transformer blocks with spatial and temporal self-attention modules, trained to reverse the diffusion process.\n(c) The Sequential ControlNet extracts structure layouts and injects them into the denoising network with the standardized self-scaling.\n(d) The sketch attention is implemented as a parallel branch within the spatial self-attention module, injecting structure semantics into subsequent transformer blocks.\n(e) A sketch-aware encoder additionally provides multi-level texture features during decoding, generating the repainted video clip from the denoised latent code.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in (c) is nothing other than the “standardized self-scaling” layer (Eq. 6) that is injected immediately after the Sequential ControlNet.  What this does that a vanilla FiLM layer does not is:\n\n 1.  Instead of learning a global γ, β per channel, it takes the raw sketch‐conditioning feature f s, computes its own per–channel mean μ(f s) and standard deviation σ(f s)), and then uses that to *normalize* and *re-scale* the sketch feature itself.  In other words, it does  \n     f̂ s  =  ( f s – μ(f s) )  /  σ(f s)  \n     f s′ =  f̂ s  ⊙  f s  \n  which has the effect of *automatically* boosting those channels (or spatial locations) in the sketch that have the highest contrast, instead of relying on a fixed affine map.  \n\n 2.  It then “shifts” the resulting map into the video‐feature domain by aligning its mean to that of the video features f z (Eq. 7), so that the sketch signal can be smoothly grafted onto the video latent code.\n\nBy contrast, a standard FiLM block would learn two static vectors (γ, β) and apply  \n    γ ⊙ f  +  β  \nto modulate the network’s activations.  Because FiLM’s γ, β are learned once and for all, FiLM can only inject a *fixed* pattern of scaling and shifting.  The standardized self-scaling, on the other hand, *derives* its scaling factors from the *sample* sketch statistics themselves, making it far more sensitive to the black-line/white-background contrasts that carry the structural cues in a sketch."
    },
    {
        "question": "How does sketch attention branch adapt spatial self-attention mechanisms to inject sketch semantics into denoising?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "Sketch attention. To interpret and inject sketch semantics into the latent space, we augment the DiT backbone with the sketch attention within each spatial self-attention module except for the first. The sketch attention incorporates a predefined binary matrix A to indicate correspondences between the latent code and the sketch sequence:",
            "Sketch attention is implemented as a parallel branch, and its outputs are added with a learnable scaling parameter λ, allowing adaptive weighting of injected sketch semantics."
        ],
        "final_answer": "Within each spatial self-attention module (except the very first), VIRES adds a parallel “sketch attention” branch. In that branch, queries come from the video latent features and keys/values come from the extracted sketch features, with a fixed binary matrix encoding which video tokens correspond to which sketch tokens. The sketch-attention output is then scaled by a learned parameter λ and summed into the original spatial self-attention output. In this way the network adaptively injects fine-grained sketch semantics into the denoising process.",
        "relevant_elements": [
            "sketch attention",
            "spatial self-attention module"
        ],
        "id": 2022,
        "masked_question": "How does [mask1] branch adapt spatial self-attention mechanisms to inject sketch semantics into denoising?",
        "masked_number": 1,
        "masked_elements": [
            "sketch attention"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.16199v2_figure_2.png",
        "paperid": "2411.16199v2",
        "paper_path": "./MISS-QA/papers/2411.16199v2.json",
        "figure_id": "2411.16199v2_figure_2.png",
        "caption": "Figure 2: The pipeline of VIRES model.\n(a) The input video clip and corresponding text descriptions are encoded using independent encoders.\nNoise is selectively added to the latent code according to the instance mask.\n(b) This noised latent code is fed into the denoising network, composed of staked transformer blocks with spatial and temporal self-attention modules, trained to reverse the diffusion process.\n(c) The Sequential ControlNet extracts structure layouts and injects them into the denoising network with the standardized self-scaling.\n(d) The sketch attention is implemented as a parallel branch within the spatial self-attention module, injecting structure semantics into subsequent transformer blocks.\n(e) A sketch-aware encoder additionally provides multi-level texture features during decoding, generating the repainted video clip from the denoised latent code.",
        "qtype": "Literature_Background",
        "response": "In each spatial‐self‐attention block of the DiT backbone (i.e. the “SA” modules in Fig. 2d), we simply fork off a second, parallel attention head – the red‐boxed “sketch attention” branch – whose sole job is to carry over structure information from the sketches into the latent video features.  Concretely:  \n\n 1.  We take the normal video‐latent feature map fᶻ and linearly project it to form the queries Q.  \n 2.  We take the sketch‐layout feature map fˢ (output of the Sequential ControlNet) and project it to form keys K and values V.  \n 3.  We then compute a masked cross‐attention  \n         Aₗ = Softmax( Q·Kᵀ  +  M )  \n     where M is a fixed, binary correspondence matrix that ties each video patch to its spatially aligned sketch patch.  \n 4.  The attended sketch semantics are then read out as Aₗ·V and summed back into the normal SA output, after multiplying by a small, learnable scale α.  \n\nBy doing this in every spatial‐self‐attention block (after the very first), the model is handed a continual, fine-grained injection of “this pixel (or patch) should look like that sketch line,” without disrupting the original self‐attention over the video latents."
    },
    {
        "question": "How does reverse distillation constrain latent vector divergence to generate diverse experts during training?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "After this, the training process, built upon the foundation of existing experts, introduces constraints with the objective of ensuring that the new interpretations sought by the neural network model in the parameter space are as distinct as possible from the existing interpretations. This is achieved by ensuring that the latent vector z, which is processed by the neural network right before its final layer, is as dissimilar as possible from the latent vectors z_pre of previous experts.",
            "To facilitate this process, we introduce an inverse reverse distillation loss below: where z represents a latent vector before being fed into the Fully Connected(FC) Layer layer, belonging to the current training process, while z_pre represents the latent vector of a previous expert. I(z, z_pre) indicates whether z and z_pre are dissimilar, cos(z, z_pre) is the cosine similarity between z and z_pre, m is the margin, a threshold beyond which the loss for dissimilar pairs starts increasing."
        ],
        "final_answer": "Reverse distillation adds a margin‐based hinge loss on the cosine similarity between the current expert’s latent vector and each previous expert’s latent vectors. By penalizing pairs whose similarity exceeds a predefined margin, it forces the new expert’s latent representation to stay sufficiently different from all earlier experts, thereby generating a diverse set of experts.",
        "relevant_elements": [
            "Reverse Distillation",
            "Training expert"
        ],
        "id": 2026,
        "masked_question": "How does [mask1] constrain latent vector divergence to generate diverse experts during training?",
        "masked_number": 1,
        "masked_elements": [
            "Reverse Distillation"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.08239v1_figure_2.png",
        "paperid": "2407.08239v1",
        "paper_path": "./MISS-QA/papers/2407.08239v1.json",
        "figure_id": "2407.08239v1_figure_2.png",
        "caption": "Figure 2: The workflow of SDF: On the left is our process of training diverse experts through reverse knowledge distillation. On the right is the process of mining the most informative samples in the target domain using diverse experts. Below is our process for generating labels for these most informative samples, which ultimately participate in the training.",
        "qtype": "Experimental_Results",
        "response": "[Mask1] is the “Reverse Distillation” (or “inverse reverse-distillation”) module.  What it does is very simple in principle:  whenever you train a new expert, you take its pre–FC hidden vector zᵗ and compare it (via a cosine‐similarity) to the corresponding hidden vectors zᵗ⁻ᵏ of all earlier experts.  You then impose a hinge-style penalty of the form  \n   Ldiv = ∑ₖ max(0, m – cos(zᵗ, zᵗ⁻ᵏ))  \nwhere m is a fixed margin.  Any latent pair whose cosine similarity is above m incurs a loss, so the network is forced to push its new hidden representations outside the “similarity ball” of every previous expert.  In this way the Reverse Distillation block directly constrains the latent vectors to diverge, ensuring each expert learns a genuinely different interpretation of the data."
    },
    {
        "question": "What risks arise from multi-perspective sampling by the Specialist RAG Drafter regarding document representativeness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Specialist RAG Drafter",
            "multi-perspective sampling"
        ],
        "id": 2030,
        "masked_question": "What risks arise from [mask1] by the Specialist RAG Drafter regarding document representativeness?",
        "masked_number": 1,
        "masked_elements": [
            "multi-perspective sampling"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.08223v1_figure_1.png",
        "paperid": "2407.08223v1",
        "paper_path": "./MISS-QA/papers/2407.08223v1.json",
        "figure_id": "2407.08223v1_figure_1.png",
        "caption": "Figure 1: Illustration of different RAG approaches. Given a knowledge-intensive query Q𝑄Qitalic_Q and retrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing input length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized instruction-tuning of the general-purpose language model (LM) to generate specific tags for self-reflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine document quality, focusing solely on contextual information without enhancing reasoning capabilities; (d) In contrast, our proposed Speculative RAG leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, specialized LM. Each draft is generated from a distinct subset of retrieved documents, providing diverse perspectives on the evidence while minimizing the number of input tokens per draft.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n\n1. In Fig. 1(d) the red box highlights the Specialist RAG Drafter, which (i) clusters the retrieved documents into topic‐based groups and then (ii) samples exactly one document from each cluster to form each mini‐subset used to draft an answer.  \n2. That one‐per‐cluster sampling strategy is what [mask1] refers to.  \n3. By design it cuts down on redundant facts and gives diverse “perspectives,” but it also means each draft only sees a thin slice of the full retrieval set.  \n4. If some clusters are small or if the clustering itself misses a topic, then important documents can simply never make it into any subset.  \n5. In other words, by sampling just one doc per cluster the drafter may produce drafts that are not truly representative of all the evidence, risking both selection bias (over- or under-weighting certain viewpoints) and the omission of key information.\n\nAnswer:\n\nBecause the specialist drafter only pulls in one document per topic cluster, it can inadvertently leave out important documents or over-emphasize fringe topics, leading to sampling bias and answer drafts that aren’t fully representative of the entire retrieved set."
    },
    {
        "question": "What is the motivation behind parallel drafting by the specialist RAG drafter using document subsets?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "We introduce a novel RAG framework that employs a smaller specialist RAG drafter to generate high-quality draft answers. Each draft is derived from a distinct subset of retrieved documents, offering diverse perspectives while reducing input token counts per draft.",
            "We cluster the retrieved documents by content similarity and sample one document from each cluster to form a subset, minimizing redundancy and maximizing diversity. These document subsets are then fed to multiple instances of the RAG module, which generate draft answers with corresponding rationales in parallel."
        ],
        "final_answer": "By partitioning the retrieved documents into diverse, non-redundant subsets and running multiple lightweight drafting processes in parallel, the specialist RAG drafter can cover different perspectives with smaller inputs—thereby reducing token counts, avoiding repeated information, and speeding up the overall RAG pipeline.",
        "relevant_elements": [
            "specialist RAG drafter",
            "document subsets"
        ],
        "id": 2032,
        "masked_question": "What is the motivation behind parallel drafting by the [mask1] using document subsets?",
        "masked_number": 1,
        "masked_elements": [
            "specialist RAG drafter"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.08223v1_figure_1.png",
        "paperid": "2407.08223v1",
        "paper_path": "./MISS-QA/papers/2407.08223v1.json",
        "figure_id": "2407.08223v1_figure_1.png",
        "caption": "Figure 1: Illustration of different RAG approaches. Given a knowledge-intensive query Q𝑄Qitalic_Q and retrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing input length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized instruction-tuning of the general-purpose language model (LM) to generate specific tags for self-reflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine document quality, focusing solely on contextual information without enhancing reasoning capabilities; (d) In contrast, our proposed Speculative RAG leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, specialized LM. Each draft is generated from a distinct subset of retrieved documents, providing diverse perspectives on the evidence while minimizing the number of input tokens per draft.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through the diagram and text step by step.\n\n1.  Image‐text alignment  \n    - The red‐boxed module in Figure 1(d) is the “Specialist RAG Drafter” (the smaller, specialist LM).  \n    - In the text it is described as a lightweight, instruction-tuned model that takes a handful of retrieved docs and spits out an answer draft plus a short rationale.\n\n2.  What the drafter actually does  \n    - First, all retrieved documents are clustered by content and then one document from each cluster is sampled to form a small “subset.”  \n    - Each subset is fed in parallel to its own copy of the Specialist RAG Drafter.  \n    - Each drafter produces a candidate answer (αᵢ) and rationale (βᵢ) grounded only in its tiny subset.\n\n3.  Why do this in parallel on subsets?  \n    a. Efficiency:  \n       – By only loading a few docs per draft, the specialist model keeps its input short, so each run is fast.  \n       – Running many small inferences in parallel on the small specialist LM is much quicker than doing one huge pass through a large generalist LM.  \n    b. Diversity & coverage:  \n       – Sampling across clusters ensures each draft sees a different “slice” of the evidence.  \n       – This maximizes the chance that at least one drafter will catch the right facts or perspective.  \n    c. Offloading heavy work:  \n       – The generalist LM never has to reread all retrieved docs. It simply scores the slimmed-down drafts (and their rationales).  \n       – This avoids long-context slowdowns and the “lost-in-the-middle” problem of overlong prompts.\n\n4.  Conclusion  \n    The core motivation is to use a small, fast specialist LM to generate multiple, diverse, well‐grounded answer drafts—each based on a small document subset—in parallel. This both speeds up end‐to‐end RAG inference and ensures broad evidence coverage, leaving the large generalist to do only a quick verification pass."
    },
    {
        "question": "What rationale supports using the generalist LM for draft verification without instruction-tuning?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "It is worth mentioning that L_G does not need to be instruction-tuned since we leverage its language modeling ability already learned during pre-training.",
            "Meanwhile, L_G can verify the drafts based on the informative rationale provided by L_D instead of processing tedious or possibly redundant retrieved documents."
        ],
        "final_answer": "The generalist LM requires no instruction-tuning because its pre-trained language modeling capability is sufficient to evaluate answer drafts against the provided rationales, avoiding the need to process all retrieved documents.",
        "relevant_elements": [
            "generalist LM",
            "draft verification"
        ],
        "id": 2033,
        "masked_question": "What rationale supports using the [mask1] for draft verification without instruction-tuning?",
        "masked_number": 1,
        "masked_elements": [
            "generalist LM"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.08223v1_figure_1.png",
        "paperid": "2407.08223v1",
        "paper_path": "./MISS-QA/papers/2407.08223v1.json",
        "figure_id": "2407.08223v1_figure_1.png",
        "caption": "Figure 1: Illustration of different RAG approaches. Given a knowledge-intensive query Q𝑄Qitalic_Q and retrieved documents, (a) Standard RAG incorporates all documents into the prompt, increasing input length and slowing inference; (b) Self-Reflective RAG (Asai et al., 2023) requires specialized instruction-tuning of the general-purpose language model (LM) to generate specific tags for self-reflection; (c) Corrective RAG (Yan et al., 2024) employs an external retrieval evaluator to refine document quality, focusing solely on contextual information without enhancing reasoning capabilities; (d) In contrast, our proposed Speculative RAG leverages a larger generalist LM to efficiently verify multiple RAG drafts produced in parallel by a smaller, specialized LM. Each draft is generated from a distinct subset of retrieved documents, providing diverse perspectives on the evidence while minimizing the number of input tokens per draft.",
        "qtype": "Design_Rationale",
        "response": "Chain of thought:\n\n1. In the red‐boxed area of panel (d) we see the familiar “robot” icon labeled as the Generalist LM.  \n2. The caption and text repeatedly emphasize that this Generalist LM is used off‐the‐shelf—“no need to instruction-tune the Generalist LM.”  \n3. Instead of feeding it all the raw documents (which would be long and possibly redundant), we feed it only each draft plus its concise, evidence‐grounded rationale.  \n4. Because the Generalist LM already has strong autoregressive language‐modeling abilities from pre-training, it can assign likelihoods (self-consistency scores) and answer simple self-reflection prompts (self-reflection scores) over those short draft-rationale pairs.  \n5. Hence, we can use the Generalist LM to verify and rank the drafts solely via these learned LM capacities—no additional instruction-tuning is required.\n\nAnswer:\n\nThe red‐boxed component is the off-the-shelf Generalist LM, and the rationale for using it to verify drafts without instruction-tuning is that we only ask it to score and reflect on the concise draft–rationale pairs—leveraging its pre-trained language-modeling abilities—rather than reprocess all the retrieved documents or require any specialized fine-tuning."
    },
    {
        "question": "How does optimized weight clustering integrate with CNN feature extraction to reduce storage and computation intensities?",
        "relevant_section_ids": [
            "2",
            "2.1"
        ],
        "relevant_context": [
            "As shown in Fig. 3 (a), similar weights are clustered into the same average value. Previous studies [7, 8] show that utilizing up to 16 unique weights per filter can achieve accuracy comparable to that of feature extraction processes without implementing weight clustering. This enables weights to be saved as 4-bit indices and indicates a specific pattern of the weight’s location in the filter. Also, as shown in Fig. 3 (b), it allows input pixels associated with the same weight to be accumulated together before multiplication. Furthermore, the clustering pattern is shared across filters for different channels so that the accumulated input pixels can be reused by the filters for many output channels.",
            "The activations associated with the same weight index (i.e., same cluster) are accumulated in the PEs. PEs are optimized for 3×3 convolution kernels. As in Fig. 4 (b), each PE contains four Register Files (RFs) that enhance its computational efficiency for convolution operations. Three of these RFs are allocated for accumulating input activations from three separate positions of the sliding convolution kernel ... The fourth RF is designated for executing multiplication operations with the actual weight values to produce the output pixels. As shown in Fig. 4 (c) timing diagram, this setup ensures that while accumulations for new inputs are underway in three RFs, the fourth can concurrently process multiplications for already accumulated inputs, optimizing the workflow within each PE and enabling more efficient handling of convolution tasks.",
            "Due to the proposed efficient feature extracting method, Fig. 5 (a) shows that weight clustering achieves 3.7 and 4.4 reduction in number of operations and parameters in VGG16, respectively."
        ],
        "final_answer": "Optimized weight clustering groups similar filter weights into a small set of average values (e.g. 16 clusters), so each weight is represented by a 4-bit index and a shared clustering pattern across channels. During CNN feature extraction, the PEs accumulate all input activations that share the same weight index before performing a single multiplication per cluster. By pipelining three RFs for accumulation and a fourth for multiplication, activations are reused across multiple filters and output channels. This integration of clustering, index-based storage, pattern sharing, and accumulation-first processing reduces both the number of stored weight parameters and the total convolution operations.",
        "relevant_elements": [
            "optimized weight clustering",
            "CNN feature extraction"
        ],
        "id": 2036,
        "masked_question": "How does [mask1] integrate with CNN feature extraction to reduce storage and computation intensities?",
        "masked_number": 1,
        "masked_elements": [
            "optimized weight clustering"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.10918v1_figure_1.png",
        "paperid": "2409.10918v1",
        "paper_path": "./MISS-QA/papers/2409.10918v1.json",
        "figure_id": "2409.10918v1_figure_1.png",
        "caption": "Figure 1: Overview of conventional Few-shot learning pipeline with multilayer perceptron (MLP) search and proposed FSL-HDnn pipeline.",
        "qtype": "Implementation_Details",
        "response": "Here’s what the red‐boxed “blob” in the bottom‐half of Fig. 1 is and how it plugs into the CNN to slash both storage and compute:\n\n 1. It is not a second neural net at all but a *weight‐clustering* unit that sits inside the CNN feature extractor.  In training (and at compile time) all of the filter’s real‐valued weights are run through a tiny clustering algorithm that replaces them with one of at most 16 “average” values (e.g. 0.8, –0.7, 0.0, …).  \n 2. At run‐time you no longer store 32- or 16-bit weights for every tap; you only store a 4-bit *cluster index* per position plus the 16 prototype values.  That drives the ~4× reduction in weight-memory.  \n 3. On the compute side, PEs accumulate incoming pixels that share the same cluster index into small registers, then do a single multiply per cluster prototype instead of one multiply per pixel.  By reusing these accumulated sums across many output channels (the same clustering pattern is shared across filters), you cut the total number of MACs by ~3.7×.  \n 4. The net effect is a dramatic drop in on-chip storage (fewer weight bits, no need for full activations everywhere) and in compute (far fewer multiplies and memory loads) without any appreciable loss in accuracy."
    },
    {
        "question": "How does the HV encoder implement cyclic random projection to encode input features into D-dimensional hypervectors?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "We address the overhead by adopting the low-complexity cyclic random projection (cRP) encoder described in Fig. 6(b), where weights in B are generated on the fly by a cyclic module rather than storing all elements explicitly in buffers.",
            "A block of size 256 is loaded into the cRP encoder for each cycle."
        ],
        "final_answer": "The HV encoder replaces the full stored F×D random projection matrix with a small cyclic generator that produces chunks of projection weights on-the-fly. Specifically, it repeatedly cycles through and loads blocks of 256 pseudo-random weights per cycle, using these to project the F-dimensional feature vector into a D-dimensional hypervector. This cyclic on-the-fly generation eliminates the need to buffer the entire base matrix, greatly reducing memory, energy, and area overhead.",
        "relevant_elements": [
            "HV encoder",
            "cyclic random projection"
        ],
        "id": 2037,
        "masked_question": "How does the [mask1] implement cyclic random projection to encode input features into D-dimensional hypervectors?",
        "masked_number": 1,
        "masked_elements": [
            "HV encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.10918v1_figure_1.png",
        "paperid": "2409.10918v1",
        "paper_path": "./MISS-QA/papers/2409.10918v1.json",
        "figure_id": "2409.10918v1_figure_1.png",
        "caption": "Figure 1: Overview of conventional Few-shot learning pipeline with multilayer perceptron (MLP) search and proposed FSL-HDnn pipeline.",
        "qtype": "Implementation_Details",
        "response": "Here is how the little “red‐boxed” module – the cyclic Random Projection (cRP) encoder – actually works to turn your F-dimensional CNN feature into a D-dimensional hypervector without ever storing the full F×D projection matrix:\n\n1.   Only a single, small block (say, 256 pseudo-random ±1 or bipolar weights) is ever resident on chip.  You can think of this block as one “row” of the full random-projection matrix.  \n2.   In each cycle the encoder:  \n     •  Streams that 256-word block out in parallel and takes the inner‐product with your F-length feature vector to produce 256 components of the final hypervector.  \n     •  Accumulates (or binarizes) those 256 results into positions i…i+255 of the D-length hypervector.  \n3.   At the end of the cycle, instead of fetching a new 256×F block from off-chip memory, the encoder simply *rotates* (“cycles”) its 256 weights by a fixed offset (e.g. one word) — exactly what you’d get if you had taken the next 256 columns of a giant pseudo-random matrix and slid them under your feature again.  \n4.   By repeating steps 2–3 exactly D/256 times you end up with all D hypervector components, each of which is a random linear projection of your F-dim feature, but you never paid the cost of storing or loading the full F×D matrix.  \n\nBecause the only state you keep on-chip is that one 256-word buffer plus a tiny rotating pointer, you slash memory (and energy and area) by orders of magnitude, yet still realize the full D-dimensional random projection needed for accurate HDC encoding."
    },
    {
        "question": "How does Semi-supervised Learning leverage Discriminator feedback to refine encoder representations for unlabeled emotion signals?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Semi-supervised Learning",
            "Discriminator"
        ],
        "id": 2038,
        "masked_question": "How does [mask1] leverage [mask2] feedback to refine encoder representations for unlabeled emotion signals?",
        "masked_number": 2,
        "masked_elements": [
            "Semi-supervised Learning",
            "Discriminator"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.03640v1_figure_1.png",
        "paperid": "2407.03640v1",
        "paper_path": "./MISS-QA/papers/2407.03640v1.json",
        "figure_id": "2407.03640v1_figure_1.png",
        "caption": "Figure 1: Schematic Diagram of Generation technology for Emotion Recognition. 444\n\nFacial image is sourced from https://zenodo.org/records/1188976.\nFeature extraction module is adapted from is from https://medium.com/clique-org/how-to-create-a-face-recognition-model-using-facenet-keras-fd65c0b092f1.\nDimensional emotion models can be found at [33].\nData augmentation schematic is from https://www.baeldung.com/cs/ml-gan-data-augmentation.\nSemi-supervised learning schematic is provided by [34].\nCross-domain image is from [35].\nSchematics of each of the four generative models are taken from https://www.compthree.com/blog/autoencoder/, https://www.javatpoint.com/generative-adversarial-network,\nhttps://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da,\nhttps://engineeringprompts.substack.com/p/frameworks-to-build-llm-applications",
        "qtype": "Implementation_Details",
        "response": "In the figure, the red‐boxed module is the “semi-supervised learning” block (mask₁) and the blue-boxed module is the emotion classification block (mask₂), which can be either a discrete‐label classifier or a continuous arousal–valence predictor.  What happens under the hood is this:\n\n 1.  Mask₁ (the semi-supervised generator/encoder) is trained on two kinds of data simultaneously: the small pool of fully labeled examples, and a much larger pool of unlabeled emotion signals.  \n 2.  For the labeled examples, the encoder’s latent representation is fed through the mask₂ classifier head, and the usual supervised emotion‐classification loss is computed (cross‐entropy for discrete labels, regression loss for dimensional labels).  \n 3.  For the unlabeled examples, the current classifier (mask₂) is used to produce pseudo-labels (or soft label distributions).  These pseudo-labels are then treated as “fake but plausible” emotion targets.  \n 4.  The same classification loss used on the real labels (from mask₂) is back-propagated through the classifier and in turn through the encoder.  In effect, mask₂ is “teaching” mask₁ how it would classify those unlabeled examples.  \n 5.  By continually generating pseudo-labels and applying the classification loss back into the encoder, the encoder learns to carve its latent space along emotion dimensions—even for signals that never carried a ground-truth tag.  \n\nIn short, semi-supervised learning “borrows” the feedback gradients from the emotion‐classification head (mask₂) to pull the encoder’s representations for unlabeled inputs into the same class- or valence/arousal-sensitive geometry that it has learned on the few labeled examples."
    },
    {
        "question": "How does GAN-based data augmentation synergize with semi-supervised learning to expand emotion representation space?",
        "relevant_section_ids": [
            "5.1",
            "5.3"
        ],
        "relevant_context": [
            "In recent years, generative models have emerged as a promising approach for data augmentation in SER [44, 139]. By leveraging the power of generative models, researchers can create realistic and diverse emotional speech samples, effectively expanding the training dataset.",
            "Zhao et al. [55] propose a semi-supervised GAN for SER, which is designed to capture underlying knowledge from both labeled and unlabeled data. In their approach, a generator creates synthetic audio descriptors from noise, while a discriminator is trained to distinguish between real and fake audio cues using both supervised and unsupervised loss functions. The discriminator not only classifies input samples as real or fake but also learns to identify the emotional class of real samples."
        ],
        "final_answer": "GAN-based data augmentation first enriches the emotion dataset by generating realistic, diverse samples (especially for under-represented classes). Those synthetic examples are then fed into a semi-supervised GAN framework alongside the limited labeled data and abundant unlabeled data. During training, the discriminator uses a supervised loss to learn true emotion labels on real data and an unsupervised (adversarial) loss to tell real versus generated samples apart. This joint adversarial+supervised training pulls the real and synthetic distributions closer in the latent space and forces the model to carve out a richer, more finely-grained emotion representation space than would be possible with labeled data alone.",
        "relevant_elements": [
            "Data Augmentation",
            "Semi-supervised Learning",
            "GAN"
        ],
        "id": 2040,
        "masked_question": "How does [mask1]-based data augmentation synergize with [mask2] to expand emotion representation space?",
        "masked_number": 2,
        "masked_elements": [
            "GAN",
            "Semi-supervised Learning"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.03640v1_figure_1.png",
        "paperid": "2407.03640v1",
        "paper_path": "./MISS-QA/papers/2407.03640v1.json",
        "figure_id": "2407.03640v1_figure_1.png",
        "caption": "Figure 1: Schematic Diagram of Generation technology for Emotion Recognition. 444\n\nFacial image is sourced from https://zenodo.org/records/1188976.\nFeature extraction module is adapted from is from https://medium.com/clique-org/how-to-create-a-face-recognition-model-using-facenet-keras-fd65c0b092f1.\nDimensional emotion models can be found at [33].\nData augmentation schematic is from https://www.baeldung.com/cs/ml-gan-data-augmentation.\nSemi-supervised learning schematic is provided by [34].\nCross-domain image is from [35].\nSchematics of each of the four generative models are taken from https://www.compthree.com/blog/autoencoder/, https://www.javatpoint.com/generative-adversarial-network,\nhttps://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da,\nhttps://engineeringprompts.substack.com/p/frameworks-to-build-llm-applications",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought  \n1. The red box ([mask1]) in the “Generative Models” panel encloses the GAN schematic, so [mask1] → “GAN-based data augmentation.”  \n2. The blue box ([mask2]) in the “Training Process” panel highlights the semi-supervised learning module, so [mask2] → “semi-supervised learning.”  \n3. GANs generate synthetic emotional samples (especially for under-represented classes), effectively filling gaps and smoothing the empirical distribution of emotions.  \n4. A semi-supervised learner can then ingest both the real labeled speech and these GAN-produced pseudo-examples (plus any available unlabeled data), using unsupervised losses to shape its latent space and supervised losses to anchor class boundaries.  \n5. In combination, the GAN injects diversity and rare‐class examples into the training pool, while the semi-supervised framework exploits that extra data to learn richer, more continuous emotion embeddings.  \n6. The result is an expanded emotion representation space that better covers subtle emotional variations and minority categories.  \n\nAnswer  \nGAN-based data augmentation and semi-supervised learning work hand in hand: the GAN floods the training set with diverse, synthetic emotion samples (balancing rare classes and introducing fine‐grained variations), and the semi-supervised learner then absorbs both real and generated data—using unlabeled and pseudo-labeled examples to sculpt a smoother, more comprehensive latent emotion manifold. This synergy broadens the learned emotion representation space, improves class coverage (especially for minority emotions), and yields more robust, generalizable SER models."
    },
    {
        "question": "How do diffusion model features integrate with cross-domain adaptation to mitigate distribution shifts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Diffusion Model",
            "Cross-domain"
        ],
        "id": 2041,
        "masked_question": "How do [mask1] features integrate with cross-domain adaptation to mitigate distribution shifts?",
        "masked_number": 1,
        "masked_elements": [
            "Diffusion Model"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.03640v1_figure_1.png",
        "paperid": "2407.03640v1",
        "paper_path": "./MISS-QA/papers/2407.03640v1.json",
        "figure_id": "2407.03640v1_figure_1.png",
        "caption": "Figure 1: Schematic Diagram of Generation technology for Emotion Recognition. 444\n\nFacial image is sourced from https://zenodo.org/records/1188976.\nFeature extraction module is adapted from is from https://medium.com/clique-org/how-to-create-a-face-recognition-model-using-facenet-keras-fd65c0b092f1.\nDimensional emotion models can be found at [33].\nData augmentation schematic is from https://www.baeldung.com/cs/ml-gan-data-augmentation.\nSemi-supervised learning schematic is provided by [34].\nCross-domain image is from [35].\nSchematics of each of the four generative models are taken from https://www.compthree.com/blog/autoencoder/, https://www.javatpoint.com/generative-adversarial-network,\nhttps://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da,\nhttps://engineeringprompts.substack.com/p/frameworks-to-build-llm-applications",
        "qtype": "Literature_Background",
        "response": "First, note that the red‐boxed block in the “Generative Models” panel is the Diffusion Model.  In other words, the question is asking:\n\n  “How do diffusion‐model features integrate with cross‐domain adaptation to mitigate distribution shifts?”\n\nHere is the step-by-step reasoning:\n\n1.  **What diffusion-model features look like.**  \n    A diffusion model gradually perturbs a clean example with noise (the forward process) and then learns to denoise it step by step (the reverse process).  At each noise-level \\(t\\) it produces an intermediate latent \\(x_t\\), and a learned network \\(\\epsilon_\\theta(x_t,t)\\) that predicts the added noise.  If you remove that noise you recover a cleaner estimate of the data.  In practice you can extract the sequence of hidden activations or the final denoised embedding as a rich, multi‐scale feature representation of the input.\n\n2.  **Why these features help with domain shifts.**  \n    Diffusion‐model embeddings tend to capture the “essence” of a sample—its global structure and fine details—while being robust to spurious, domain‐specific cues (for example, particular lighting, background clutter, recording device noise, etc.).  That robustness makes them a good starting point for building representations that do not overfit to one domain.\n\n3.  **How they’re plugged into a cross‐domain pipeline.**  \n    In a typical cross‐domain adaptation setup you have  \n      – a **source** domain \\(S\\) (where you have labels),  \n      – a **target** domain \\(T\\) (where you want to apply your model but possibly have few or no labels),  \n      – and a **feature extractor** \\(F(\\cdot)\\) whose goal is to produce domain-invariant features.  \n\n    With a diffusion model you simply let \\(F(x)\\) be the embedding (or denoised latent) that the diffusion network produces for any input \\(x\\).  You run both \\(x\\in S\\) and \\(x\\in T\\) through the diffusion encoder and collect those embeddings.\n\n4.  **Enforcing alignment on diffusion features.**  \n    Once you have diffusion-model features for source and target examples, you add any of the standard domain-adaptation losses on top of them:\n\n    •  *Adversarial domain discriminator:* train a small “domain‐classifier” \\(D\\) to tell apart \\(F(S)\\) vs. \\(F(T)\\), and simultaneously train \\(F\\) (the diffusion backbone) to confuse \\(D\\).  \n    •  *Moment matching or MMD:* directly minimize a distance (e.g. Maximum Mean Discrepancy) between the distributions of \\(F(S)\\) and \\(F(T)\\).  \n    •  *Cycle consistency or reconstruction:* generate synthetic target‐like samples from source embeddings (or vice versa) via the diffusion decoder and require that reconstructions match the true samples.\n\n5.  **Generating bridging samples.**  \n    A complementary strategy is to use the diffusion network’s generative power to synthesize “in‐between” samples that interpolate from source to target statistics.  By training the model on these mixed or target‐conditioned samples, you further densify the overlap between the two domains and close the gap.\n\n**In summary:**  \nYou simply take the denoised latents or hidden activations produced by the diffusion model as your unified feature space for both source and target.  You then impose any of the usual domain‐adaptation objectives (adversarial loss, distribution matching, synthetic bridging samples, etc.) on those diffusion features.  Because the diffusion features are inherently multi‐scale and robust to superficial domain artifacts, aligning them across \\(S\\) and \\(T\\) effectively mitigates distribution shifts and yields a model that generalizes more smoothly from the source to the target domain."
    },
    {
        "question": "How does depth-aware motion segmentation leverage OANet decoder for dynamic label disambiguation?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Inspired by ParticleSfM (Zhao et al., 2022 ###reference_b62###), we introduce a depth-aware trajectory feature encoder and a decoder like OANet (Zhang et al., 2019 ###reference_b61###) to disambiguate dynamic label prediction.",
            "Following OANet (Zhang et al., 2019 ###reference_b61###), the decoder first uses PointCN to obtain the local-global context features of the trajectory points, then uses softmax in the Diff Pool module to learn to cluster the input features, then performs spatial association on the clusters, and recovers each point through Diff Unpool contextual features. The features obtained by the Unpool layer are fed into several PointCN and followed by sigmoid activation, plus dynamic prediction of iterative transformer updates, to obtain the final dynamic mask."
        ],
        "final_answer": "Depth-aware motion segmentation constructs a rich trajectory feature by concatenating 2D and back-projected 3D coordinates plus motions, encodes it via an MLP and transformer, then hands these encoded features into an OANet-style decoder. The decoder applies PointCN to extract local‐global context, uses a softmax‐based Diff Pool to cluster features, performs spatial association, and recovers per-point contextual features via Diff Unpool. Finally, further PointCN layers with sigmoid activations, combined with the transformer’s dynamic logits, produce a refined dynamic mask that disambiguates moving from static points.",
        "relevant_elements": [
            "depth-aware motion segmentation",
            "OANet decoder"
        ],
        "id": 2042,
        "masked_question": "How does [mask1] leverage OANet decoder for dynamic label disambiguation?",
        "masked_number": 1,
        "masked_elements": [
            "depth-aware motion segmentation"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.13291v1_figure_2.png",
        "paperid": "2411.13291v1",
        "paper_path": "./MISS-QA/papers/2411.13291v1.json",
        "figure_id": "2411.13291v1_figure_2.png",
        "caption": "Figure 2: DATAP-SfM pipeline. Given monocular videos as input with consistent video depth (optional), DATAP can simultaneously estimate long-term point tracking with visible and dynamic characteristics. Incorporating with DATAP, we formalize a concise, elegant, and robust pipeline of structure from motion by performing global bundle adjustment for point tracking classified as static and visible.",
        "qtype": "Literature_Background",
        "response": "Here’s how the red-boxed module (“[mask1]”) uses an OANet-style decoder to untangle (“disambiguate”) which tracks are truly moving versus static:\n\n1.   Input into the red-box head  \n     - From the iterative transformer you already have per‐point  \n       • 2D image‐space trajectories (positions + 2D motion deltas)  \n       • 3D back-projected trajectories (positions + scene-flow deltas)  \n     - These four streams of information are concatenated into a single “trajectory feature” vector for each point.\n\n2.   Depth-aware encoding  \n     - A small MLP + transformer encoder ingests those concatenated features, mixing 2D motion, 3D motion and depth priors into a richer embedding per track.\n\n3.   OANet decoder stages  \n     a.  PointCN  \n        – First, a PointCN layer produces local‐and‐global context vectors for each trajectory.  \n     b.  DiffPool  \n        – A differentiable pooling layer clusters similar trajectories (e.g. those moving together) via a softmax over assignment scores.  \n     c.  Spatial association  \n        – Within each learned cluster, trajectories exchange information to reinforce coherent motion patterns.  \n     d.  DiffUnpool  \n        – Contextualized cluster‐level features are unpooled back onto each individual point, so every trajectory carries both its own local motion cue and the higher‐level motion group context.  \n     e.  Final PointCN + sigmoid  \n        – One more PointCN followed by a sigmoid gate yields a refined “dynamic‐vs‐static” score for each track.\n\n4.   Fusion with transformer’s own guess  \n     - The OANet-decoder’s dynamic scores are added to the last transformer update’s dynamic logits and renormalized, producing the final dynamic label.\n\nBy grouping trajectories into soft motion‐clusters (DiffPool) and then broadcasting cluster information back to each point (DiffUnpool), the OANet decoder injects powerful global‐and‐local context.  This clustering step is exactly what lets the network resolve the ambiguities that a plain per‐track transformer update could not, yielding crisp, disambiguated dynamic‐vs‐static labels."
    },
    {
        "question": "How does Global BA implement translation and rotation averaging akin to global SfM pipelines?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Specifically, trajectories marked as visible and static in point tracking are first extracted, and then translation averaging (Ozyesil & Singer, 2015 ###reference_b36###) and rotation averaging (Chatterjee & Govindu, 2013 ###reference_b8###) commonly used in global SfM pipelines are performed to obtain initial camera pose estimates."
        ],
        "final_answer": "Global BA first selects trajectories labeled as visible and static from DATAP’s point tracking. It then applies translation averaging (as in Ozyesil & Singer, 2015) and rotation averaging (as in Chatterjee & Govindu, 2013) — the standard steps in global SfM — to those static trajectories in order to compute initial camera poses before running the full bundle adjustment.",
        "relevant_elements": [
            "Global BA"
        ],
        "id": 2043,
        "masked_question": "How does [mask1] implement translation and rotation averaging akin to global SfM pipelines?",
        "masked_number": 1,
        "masked_elements": [
            "Global BA"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.13291v1_figure_2.png",
        "paperid": "2411.13291v1",
        "paper_path": "./MISS-QA/papers/2411.13291v1.json",
        "figure_id": "2411.13291v1_figure_2.png",
        "caption": "Figure 2: DATAP-SfM pipeline. Given monocular videos as input with consistent video depth (optional), DATAP can simultaneously estimate long-term point tracking with visible and dynamic characteristics. Incorporating with DATAP, we formalize a concise, elegant, and robust pipeline of structure from motion by performing global bundle adjustment for point tracking classified as static and visible.",
        "qtype": "Literature_Background",
        "response": "Here’s what the red‐boxed “Global BA” stage (i.e. your [mask1]) is doing, in SfM terms:\n\n1.  Extract only the tracks that are both visible and labeled “static.”\n2.  Build a view‐graph from those tracks, where each edge carries a relative rotation and a relative translation direction between two cameras.\n3.  Run a **rotation‐averaging** solver (e.g. Chatterjee & Govindu 2013) on the SO(3) part of that graph to get a globally consistent set of camera orientations.\n4.  Run a **translation‐averaging** solver (e.g. Ozyesil & Singer 2015) on the same graph—now with fixed rotations—to recover the global camera centers up to scale.\n5.  Finally, take those averaged orientations and centers plus the triangulated 3D points, and jointly refine everything with a **global nonlinear bundle adjustment**.\n\nThat sequence—rotation averaging → translation averaging → global BA—is exactly the recipe used in many “global” SfM pipelines."
    },
    {
        "question": "How does integrating consistent video depth into depth-aware motion segmentation refine dynamic mask prediction reliability?",
        "relevant_section_ids": [
            "3.1",
            "4.7"
        ],
        "relevant_context": [
            "Specifically, for each frame of the video, we use monocular depth estimation such as Midas (Birkl et al., 2023) or consistent video depth such as NVDS (Wang et al., 2023b) to obtain an initial depth estimate. Directly using 2D point tracking to predict dynamic labels will suffer from ambiguity.",
            "We normalize the relative depth of each frame to (0,1) and back-project it to 3D camera coordinates. For this reason, the trajectory of 2D point tracking can obtain sequential scene flow estimates.",
            "For the trajectory of the sliding window T, we concat the coordinates of the 2D trajectory, the coordinates of the 3D trajectory, the motion of the 2D trajectory, and the motion of the scene flow to form F features. These features are first fed into 2 layers of MLP and then fed into a transformer module to obtain the encoded features.",
            "Following OANet (Zhang et al., 2019), the decoder first uses PointCN ... and recovers each point through Diff Unpool contextual features. The features obtained by the Unpool layer are fed into several PointCN and followed by sigmoid activation, plus dynamic prediction of iterative transformer updates, to obtain the final dynamic mask.",
            "Dynamic-aware point tracking w/ video depth. Compared with monocular depth prior, the precision of motion segmentation can be further improved. As shown in Fig. 3, our method obtains more accurate motion segmentation, while ParticleSfM may misidentify static regions as dynamic."
        ],
        "final_answer": "By feeding in consistent video depth (e.g. NVDS) alongside 2D tracking, we normalize and back-project per-frame depth into 3D camera coordinates and extract scene flow. Concatenating 3D trajectory, 2D trajectory and their motions yields depth-aware trajectory features that go through a transformer encoder and OANet decoder. This richer, depth-grounded representation disambiguates static versus moving points, boosting motion-segmentation precision and producing a more reliable dynamic mask than using only monocular or 2D cues.",
        "relevant_elements": [
            "consistent video depth",
            "depth-aware motion segmentation"
        ],
        "id": 2044,
        "masked_question": "How does integrating [mask1] into [mask2] refine dynamic mask prediction reliability?",
        "masked_number": 2,
        "masked_elements": [
            "consistent video depth",
            "depth-aware motion segmentation"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.13291v1_figure_2.png",
        "paperid": "2411.13291v1",
        "paper_path": "./MISS-QA/papers/2411.13291v1.json",
        "figure_id": "2411.13291v1_figure_2.png",
        "caption": "Figure 2: DATAP-SfM pipeline. Given monocular videos as input with consistent video depth (optional), DATAP can simultaneously estimate long-term point tracking with visible and dynamic characteristics. Incorporating with DATAP, we formalize a concise, elegant, and robust pipeline of structure from motion by performing global bundle adjustment for point tracking classified as static and visible.",
        "qtype": "Experimental_Results",
        "response": "First, let us pin down the two masked regions in the figure:\n\n  1. [mask1] (red box)  \n     – “Consistent Video Depth (Optional)”  \n  2. [mask2] (blue box)  \n     – “DATAP Framework,” i.e. the transformer‐based tracking network  \n\nThe question then asks:  \n“How does integrating consistent video depth into the DATAP framework refine dynamic‐mask prediction reliability?”\n\nChain of thought:\n\n  • In the vanilla DATAP transformer we turn a track’s current 2D position, its appearance‐feature history, its 2D correlation volume, an initial “visible/not‐occluded” label and an initial “dynamic/static” label into a set of learned tokens.  After K transformer iterations we read out an updated position ∆x, an updated visibility mask v̂, and an updated dynamic mask d̂.  \n\n  • Empirically, while v̂ (visibility) is quite accurate right out of the transformer, the 2D‐only dynamic mask d̂ often remains ambiguous—points that undergo pure parallax can look “dynamic” to a network that only sees 2D motion.  \n\n  • To fix that, we feed in [mask1] – a per‐frame, per‐pixel “consistent” depth map (e.g. NVDS) – and back‐project those depths into the camera’s 3D coordinate system.  Now every tracked point has not only a 2D trajectory but a *3D* trajectory and an approximate scene flow attached.  \n\n  • We concatenate these depth‐aware trajectory features (2D positions, 2D displacements, 3D positions, 3D scene flow) and pass them through a small transformer encoder plus an OANet‐style decoder.  The decoder uses PointCN, DiffPool and DiffUnpool to cluster trajectories and recover point‐wise context.  \n\n  • Finally, we fuse the decoder’s output with the transformer’s original d̂ and run a sigmoid to produce the *final* dynamic mask.  \n\nBecause we have explicitly given the network 3D structure‐and‐motion cues from a *compatible* depth estimator, the model can now distinguish:\n\n  – Parallax motion induced by camera motion (static points)  \n  – True object motion in the scene (dynamic points)  \n\nThis 3D disambiguation dramatically cuts down on false positives in the dynamic mask and raises overall motion‐segmentation precision by a large margin (see ablations under “w/ depth” and “w/ video depth”)."
    },
    {
        "question": "How do iterative transformer updates and correlation features influence static point selection before global BA?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "To compute the correlation between the track features f_t and the image features F surrounding the current estimate of track position x_t, we adopt the dot products in RAFT (Teed & Deng, 2020b) to obtain the correlation volume. The correlation features C_t are obtained through bilinear interpolation of the stacked inner products...",
            "Iterative transformer updates. The transformer update U will be applied K times to progressively update the estimates of the tracks from an initial token G. With each iteration, we can obtain the delta of the position Δx_t and the feature Δf_t of the tracks: x_t←x_t+Δx_t, f_t←f_t+Δf_t, respectively. ... We find that such updates can better predict the visibility mask m_t^v, but the dynamic mask m_t^d cannot be accurately obtained.",
            "Specifically, trajectories marked as visible and static in point tracking are first extracted, and then translation averaging and rotation averaging ... are performed to obtain initial camera pose estimates. Then we apply global bundle adjustment over the selected point trajectories during the triangulation stage."
        ],
        "final_answer": "The dot–product-based correlation features feed into each transformer iteration, letting the network refine both point positions and appearance embeddings over K steps. Those iterative transformer updates yield progressively more accurate visibility predictions (m_t^v). Once the final visibility and dynamic labels are produced, all tracks that are classified as “visible” and “static” are pulled out as the static correspondences and passed into the global bundle adjustment.",
        "relevant_elements": [
            "iterative transformer updates",
            "correlation features",
            "global BA"
        ],
        "id": 2045,
        "masked_question": "How do [mask1] and correlation features influence static point selection before global BA?",
        "masked_number": 1,
        "masked_elements": [
            "iterative transformer updates"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.13291v1_figure_2.png",
        "paperid": "2411.13291v1",
        "paper_path": "./MISS-QA/papers/2411.13291v1.json",
        "figure_id": "2411.13291v1_figure_2.png",
        "caption": "Figure 2: DATAP-SfM pipeline. Given monocular videos as input with consistent video depth (optional), DATAP can simultaneously estimate long-term point tracking with visible and dynamic characteristics. Incorporating with DATAP, we formalize a concise, elegant, and robust pipeline of structure from motion by performing global bundle adjustment for point tracking classified as static and visible.",
        "qtype": "Experimental_Results",
        "response": "First, let us be very explicit about what “​[mask1]​” refers to in the diagram – it is simply the little red‐boxed module in the middle of the DATAP framework: the “iterative transformer updates” (i.e. the stack of K transformer update blocks that sit between the input tokens and the prediction heads for position, visibility and dynamics).\n\nNow, on to the question:\n\n“How do the iterative transformer updates (the red‐boxed module) and the correlation features influence which points are declared static and get passed on to the global bundle‐adjustment?”\n\nChain of thought:\n\n1.  **Correlation features**  \n    –  At each time step the tracker has a current 2D guess for each point.  \n    –  Around that guess it extracts a small window of “correlation” (RAFT-style dot-product between the point’s learned feature and the local image features).  \n    –  This correlation volume is a direct measure of how well the point “matches” in the new frame, and encodes both appearance and motion cues.  \n\n2.  **Input tokens to the transformer**  \n    –  For each track, we build a token that contains:  \n       •  The current 2D position & time embedding  \n       •  The current appearance feature of the point  \n       •  The newly computed correlation feature  \n       •  The last visibility and dynamic (moving/static) logits  \n    –  This rich token tells the transformer both “where we think the point is now” and “how good the match is,” plus “how visible/moving it was before.”  \n\n3.  **Iterative transformer updates**  \n    –  We run K rounds of standard Transformer blocks over all of these track tokens.  Each block uses self-attention to let every track share context, and cross-attention only if you have multiple views (but here mostly self-attention).  \n    –  In each round the transformer refines:  \n       •  the predicted 2D position (via a small regression head)  \n       •  the internal feature embedding of the point  \n       •  (only in the final round) the visibility and dynamic logits via an MLP + sigmoid.  \n\n4.  **From dynamic logits to static point selection**  \n    –  At the end of the last transformer iteration, we ask the network to output two binary masks for each point:  \n       1.  **visibility** (visible vs. occluded)  \n       2.  **dynamic** (moving vs. static relative to the camera)  \n    –  By construction the transformer has had the chance to look at both the motion (through the change in 2D position) and the matching‐quality (through the correlation feature) over all K updates.  Points whose correlation was inconsistent or whose motion was non-rigid will tend to get a “dynamic = 1 (moving)” label.  Points that track perfectly (high correlation, small residual motion consistent with the camera) will end up as “dynamic = 0 (static).”\n\n5.  **Feeding into global BA**  \n    –  Finally, the SfM stage only keeps those tracks that are both  \n       •  **visible** (visibility = 1)  \n       •  **static**  (dynamic = 0)  \n    –  These are the points that go into the global translation-averaging, rotation-averaging, and full bundle adjustment.  \n\nIn short, the RAFT-style correlation features give the transformer a local “matching confidence” signal, and the iterative transformer updates fuse that with positional and temporal context to arrive at a final dynamic mask.  Only points labelled static by that mask are kept for the global bundle‐adjustment."
    },
    {
        "question": "How does the interface map scenario prompts to control signals for the order-batch model?",
        "relevant_section_ids": [
            "2.2.1"
        ],
        "relevant_context": [
            "Fine-grained Signal Generation Interface. To enhance the controllability of MarS, we introduce a fine-grained signal generation interface. This interface can map vague descriptions in natural language or general configurations to fine-grained series of control signals.",
            "We provide an implementation that leverages LLM-based historical market record retrieval to generate precise control signals (e.g., price, volume) from vague descriptive prompts, such as DES_TEXT."
        ],
        "final_answer": "The interface takes a high-level, natural-language or general description of a target scenario (e.g., DES_TEXT) and, via an LLM-based retrieval of historical market records, converts it into a detailed time series of control signals (such as price and volume trajectories) that are fed into the order-batch model.",
        "relevant_elements": [
            "Interface",
            "order-batch model"
        ],
        "id": 2046,
        "masked_question": "How does the [mask1] map scenario prompts to control signals for the order-batch model?",
        "masked_number": 1,
        "masked_elements": [
            "Interface"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.07486v1_figure_1.png",
        "paperid": "2409.07486v1",
        "paper_path": "./MISS-QA/papers/2409.07486v1.json",
        "figure_id": "2409.07486v1_figure_1.png",
        "caption": "Figure 1: High-Level Overview of MarS.MarS is powered by a generative foundation model (LMM) trained on order-level historical financial market data. During real-time simulation, LMM dynamically generates order series in response to various conditions, including user-submitted interactive orders, vague target scenario descriptions, and current/recent market data. These generated order series, combined with user interactive orders, are matched in a simulated clearing house in real-time, producing fine-grained simulated market trajectories. The flexibility of LMM s order generation enables MarS to support various downstream applications, such as forecasting, detection/warning systems, analysis platforms, and agent training environments.",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed “Interface” in the lower–left of Fig. 1 is exactly the piece of the large market model that turns your free-form scenario text into the stream of numbers that the order-batch transformer can obey.  Internally it works roughly like this:\n\n  1.  You supply a DES_TEXT prompt (“volatility crush,” “price bump,” etc.).  \n  2.  The Interface invokes an LLM‐powered retriever over past market records to find historical episodes that match your description.  \n  3.  From those episodes it extracts the quantitative footprints you really care about – e.g. a time series of target prices, volumes (and if you wish, spreads or other signals) at each batch step.  \n  4.  Those extracted, fine-grained trajectories become the “control signal” input to the order-batch module, conditioning its next batch of synthetic orders so that, over time, your vague prompt is translated into a concrete, realistic market path."
    },
    {
        "question": "How does the ensemble model integrate order model and order-batch model outputs to guide order generation?",
        "relevant_section_ids": [
            "2.2.1",
            "2.3"
        ],
        "relevant_context": [
            "2.2.1: “Ensemble Model for Orders and Order Batches. The distinct advantages of order sequence modeling and order-batch sequence modeling necessitate their integration into a cohesive framework. The ensemble model we designed combines these two approaches, enabling improved market modeling and generation. It achieves this by balancing the fine-grained control of individual orders from the order model with the broader market dynamics captured by the order-batch model. This integration ensures that the generated market simulations are both detailed and contextually accurate, reflecting realistic market conditions.”",
            "2.3: “The trade-off between market impact and control signal is crucial for realistic simulation… ‘Shaping the Future Based on Realized Realities’. At each time step, order-batch model generates the next order-batch based on recent data from the simulated clearing house.… ‘Electing the Best from Every Possible Future’. At each time step, multiple predicted order-batches are generated. The best match to the fine-grained control signal is selected, enabling soft control of order-batch generation.… The order-level transformer, trained on historical orders, naturally learns market impact for subsequent order generation. Concurrently, the ensemble model influences order generation, aligning with the generated next order-batch.”"
        ],
        "final_answer": "The ensemble model fuses the two sub-models by first using the order-batch model to forecast candidate next batches, then selecting the batch that best matches the user’s fine-grained control signals, and finally conditioning the order-level model on that chosen batch. In this way it balances the batch model’s broad market‐level dynamics with the order model’s fine-grained market-impact learning to generate realistic, controllable order streams.",
        "relevant_elements": [
            "Ensemble Model",
            "order model",
            "order-batch model"
        ],
        "id": 2047,
        "masked_question": "How does the [mask1] integrate [mask2] and order-batch model outputs to guide order generation?",
        "masked_number": 2,
        "masked_elements": [
            "Ensemble Model",
            "order model"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.07486v1_figure_1.png",
        "paperid": "2409.07486v1",
        "paper_path": "./MISS-QA/papers/2409.07486v1.json",
        "figure_id": "2409.07486v1_figure_1.png",
        "caption": "Figure 1: High-Level Overview of MarS.MarS is powered by a generative foundation model (LMM) trained on order-level historical financial market data. During real-time simulation, LMM dynamically generates order series in response to various conditions, including user-submitted interactive orders, vague target scenario descriptions, and current/recent market data. These generated order series, combined with user interactive orders, are matched in a simulated clearing house in real-time, producing fine-grained simulated market trajectories. The flexibility of LMM s order generation enables MarS to support various downstream applications, such as forecasting, detection/warning systems, analysis platforms, and agent training environments.",
        "qtype": "Experimental_Results",
        "response": "Mask1 is the “Ensemble Model” block and Mask2 is the “Order Model” block.  In MarS’s Large Market Model they sit side by side: the Order Model (blue box) is a causal transformer that knows how single orders evolve (i.e. it captures fine-grained market impact order-by-order), and the Order-Batch Model is an auto-regressive transformer that forecasts coarse, interval-level behavior (batches of orders).  The Ensemble Model (red box) lives on top of them and does two things at each time step:\n\n 1.  It calls the Order-Batch Model repeatedly to propose a small set of possible next batches of orders, then picks the one whose aggregate price/volume profile best matches the user’s control signals (the “electing the best” principle).  \n 2.  It feeds that chosen batch back as a guiding context into the Order Model, whose causal transformer then generates the next individual order so that it both respects the micro-impact patterns it has learned and stays consistent with the coarse batch trajectory just selected.\n\nIn this way the ensemble fuses the batch-level forecast and the token-level order model into a single conditional probability for the next order, striking a balance between realistic market impact (from the Order Model) and higher-level controllable scenario structure (from the Order-Batch Model)."
    },
    {
        "question": "What risks does relying on Order-level Historical Market Data pose for market participant privacy and data bias?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Order-level Historical Market Data"
        ],
        "id": 2048,
        "masked_question": "What risks does relying on [mask1] pose for market participant privacy and data bias?",
        "masked_number": 1,
        "masked_elements": [
            "Order-level Historical Market Data"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.07486v1_figure_1.png",
        "paperid": "2409.07486v1",
        "paper_path": "./MISS-QA/papers/2409.07486v1.json",
        "figure_id": "2409.07486v1_figure_1.png",
        "caption": "Figure 1: High-Level Overview of MarS.MarS is powered by a generative foundation model (LMM) trained on order-level historical financial market data. During real-time simulation, LMM dynamically generates order series in response to various conditions, including user-submitted interactive orders, vague target scenario descriptions, and current/recent market data. These generated order series, combined with user interactive orders, are matched in a simulated clearing house in real-time, producing fine-grained simulated market trajectories. The flexibility of LMM s order generation enables MarS to support various downstream applications, such as forecasting, detection/warning systems, analysis platforms, and agent training environments.",
        "qtype": "Others",
        "response": "Here’s a step-by-step breakdown of the reasoning:\n\n1.  Identify [mask1]:  \n    In the red-boxed region of Fig. 1 we see the “Order-level Historical Market Data” cloud icon.  \n\n2.  The two risks of relying heavily on order-level historical market data are:  \n    •  **Participant privacy leakage**  \n       –  Order-by-order data can be so granular that it may reveal the trading fingerprints or even identities of large investors or high-frequency traders.  \n       –  Once an adversary gains access, they can reverse-engineer execution algorithms, front-run future trades, or deanonymize counterparties.  \n    •  **Historical data bias**  \n       –  The past record embeds the idiosyncrasies of a particular market period (e.g. bull vs. bear, regulatory regime, dominant trading strategies), so a simulator trained only on that slice will reproduce those biases.  \n       –  Important classes of behavior (dark-pool trades, newly emerged algorithmic patterns) may be under-represented or missing, leading the model to over-fit to the “majority” patterns and under-perform when the market regime shifts.  \n\n3.  Implications for MarS:  \n    –  **Privacy**: without proper aggregation or anonymization, participants could be re-identified from the synthetic order streams.  \n    –  **Bias**: downstream applications (forecasting, stress-testing, RL training) may systematically misestimate risk or impact if they inherit spurious correlations that were only artifacts of the original data.  \n\nIn short, resting MarS on raw order-level history risks both violating trader anonymity and baking in the quirks of one era’s trading into every future simulation."
    },
    {
        "question": "How might Simulated Clearing House latency affect fidelity of Generated Order Sequences during real-time interaction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Simulated Clearing House",
            "Generated Order Sequences"
        ],
        "id": 2049,
        "masked_question": "How might [mask1] latency affect fidelity of Generated Order Sequences during real-time interaction?",
        "masked_number": 1,
        "masked_elements": [
            "Simulated Clearing House"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.07486v1_figure_1.png",
        "paperid": "2409.07486v1",
        "paper_path": "./MISS-QA/papers/2409.07486v1.json",
        "figure_id": "2409.07486v1_figure_1.png",
        "caption": "Figure 1: High-Level Overview of MarS.MarS is powered by a generative foundation model (LMM) trained on order-level historical financial market data. During real-time simulation, LMM dynamically generates order series in response to various conditions, including user-submitted interactive orders, vague target scenario descriptions, and current/recent market data. These generated order series, combined with user interactive orders, are matched in a simulated clearing house in real-time, producing fine-grained simulated market trajectories. The flexibility of LMM s order generation enables MarS to support various downstream applications, such as forecasting, detection/warning systems, analysis platforms, and agent training environments.",
        "qtype": "Others",
        "response": "Chain‐of‐thought  \n1. In the red box (mask1) we see the tight interaction loop:  \n   – The LMM spits out a “Generated Order Sequence”  \n   – Those orders (together with any new “User‐Submitted Orders”) go into the Simulated Clearing House  \n   – The Clearing House matches them and feeds the updated LOB/market state back into the LMM for the next step in generation.  \n\n2. In real markets, orders arrive and execute in milliseconds. Our simulation must mirror that or else the model will be looking at an out-of-date market.  \n\n3. If the simulated clearing house introduces latency (i.e. it takes too long to process the generated and user orders and return an updated state), then:  \n   – The LMM will generate its next order on a stale snapshot of the book.  \n   – It will miss recent user interactions and the true unfolding of market impact.  \n   – As a result, the sequence drifts away from what a low-latency (realistic) market would have produced.  \n\n4. Therefore, higher mask1 latency ⇒ greater staleness in the LOB seen by the LMM ⇒ lower fidelity of the generated order stream.  \n\nAnswer  \nBecause the clearing‐house feedback is how the LMM “sees” the latest book state before generating each new order, any delay there means it’s working from stale information. As that latency grows, the model’s next‐order predictions no longer reflect the true, up-to-the-moment market and user inputs, and the synthetic order sequences lose realism (i.e. fidelity drops)."
    },
    {
        "question": "What limitations might arise from Orthogonal Attention (OA) under highly sparse or noisy triplane inputs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Orthogonal Attention (OA)"
        ],
        "id": 2050,
        "masked_question": "What limitations might arise from [mask1] under highly sparse or noisy triplane inputs?",
        "masked_number": 1,
        "masked_elements": [
            "Orthogonal Attention (OA)"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.07658v1_figure_2.png",
        "paperid": "2410.07658v1",
        "paper_path": "./MISS-QA/papers/2410.07658v1.json",
        "figure_id": "2410.07658v1_figure_2.png",
        "caption": "Figure 2: The overall framework of SeMv-3D. SeMv-3D consists of two components: 1) Triplane Prior Learner (TPL) that learns a triplane prior to capture consistent 3D visual details and 2) Semantic-aligned View Synthesizer (SVS) that enhances the alignment between the semantic with 3D content and enables single-step generation of arbitrary views. Here, Orthogonal Attention (OA) focuses on the orthogonal correspondences within the triplane, maintaining triplane consistency and extracting fine-grained features.",
        "qtype": "Others",
        "response": "Here is a step-by-step reasoning for why Orthogonal Attention (OA)—the module highlighted by [mask1]—can break down when the triplane inputs become very sparse or corrupted by noise:\n\n1.  OA’s design  \n    - At its core, OA enforces “hard” correspondences across the three latent planes by having every query at (x,y) in plane i look exactly at the keys along the x-line in plane j and the y-line in plane k.  \n    - In well-populated, clean triplanes this neatly stitches together front/top/side views, preserving fine detail and 3D structure.  \n\n2.  What happens under sparsity  \n    - If large regions of the triplane are empty (i.e. contain zeroed or near-zero features), many of the key–value pairs OA relies on simply aren’t present.  \n    - Queries end up attending to “blank” or uniform vectors, so the resulting features carry almost no useful cross-plane signal.  \n    - The net effect is that the orthogonal correspondence collapses—planes no longer reinforce each other, and multi-view consistency degrades (the network basically has no reliable cues to bind the three views).  \n\n3.  What happens under noise  \n    - If the triplane entries are corrupted with spurious activations (outliers, salt-and-pepper noise, quantization artifacts), OA will dutifully propagate those errors across all three planes.  \n    - A single noisy pixel in one plane can pollute the attending row/column in the other two, leading to “streak” artifacts and wildly incorrect feature‐fusion.  \n    - Rather than canceling out noise, the rigid cross-plane linking in OA ends up amplifying it.  \n\n4.  Practical consequences  \n    - Blurred or “floating” geometry in the rendered views.  \n    - Misalignment of textures or details (e.g. a stripe on the front view doesn’t line up on the side).  \n    - Slower or unstable training, since the attention weights can oscillate wildly if the key/value distributions are degenerate.  \n\nIn short, Orthogonal Attention is highly effective when the triplane priors are densely populated and reasonably clean—but under extreme sparsity or noise, its very mechanism of “hard” coordinate‐wise cross‐plane linking becomes its Achilles’ heel, leading to loss of spatial coherence and propagation of artifacts."
    },
    {
        "question": "How could Semantic-aligned View Synthesizer adapt to handle user bias in text prompts ethically?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Semantic-aligned View Synthesizer"
        ],
        "id": 2051,
        "masked_question": "How could [mask1] adapt to handle user bias in text prompts ethically?",
        "masked_number": 1,
        "masked_elements": [
            "Semantic-aligned View Synthesizer"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.07658v1_figure_2.png",
        "paperid": "2410.07658v1",
        "paper_path": "./MISS-QA/papers/2410.07658v1.json",
        "figure_id": "2410.07658v1_figure_2.png",
        "caption": "Figure 2: The overall framework of SeMv-3D. SeMv-3D consists of two components: 1) Triplane Prior Learner (TPL) that learns a triplane prior to capture consistent 3D visual details and 2) Semantic-aligned View Synthesizer (SVS) that enhances the alignment between the semantic with 3D content and enables single-step generation of arbitrary views. Here, Orthogonal Attention (OA) focuses on the orthogonal correspondences within the triplane, maintaining triplane consistency and extracting fine-grained features.",
        "qtype": "Others",
        "response": "First, let us be clear what “[mask1]” refers to.  In the figure it is the entire red‐boxed module on the right—the Semantic‐aligned View Synthesizer (SVS), and more specifically its Triplane Latents Transformation block (the stack of DINO→Transformer layers→Cross‐Attention + Orthogonal‐Attention→MLP).  This is the component that “deeply interacts” between the text embedding of the user’s prompt and the learned 3D triplane prior in order to synthesize novel views.  \n\nBecause SVS is where we pull in the raw text embedding and use cross‐attention to paint semantics onto the 3D latents, it is also the place where any biases in the user’s prompt are most likely to be amplified or encoded into the final 3D output.  To handle user bias in an ethical way, we can retrofit SVS with three new submodules and learning objectives:  \n\n1) Prompt Audit & Sanitization pre‐processor  \n   • Before the text ever reaches the SVS cross‐attention, run it through a lightweight bias‐detection model (e.g. an off‐the‐shelf fairness classifier or toxicity filter) that flags potentially prejudiced or stereotypical language (gender, race, age, etc.).  \n   • If the filter flags a term or phrase, either (a) prompt the user to rephrase, (b) neutralize that token (replace “male surgeon” → “surgeon”), or (c) record a log entry to track repeated flagged queries.  \n\n2) Debiasing Attention Head in SVS  \n   • Inside the Transformer stack of SVS insert a dedicated “debiasing attention” sublayer parallel to the existing cross‐attention.  \n   • That head is trained with an auxiliary “neutrality” loss which encourages it to project out components of the semantic key/query space known to encode protected attributes (e.g. using a small attribute classifier on the latent).  \n   • During inference, it down‐weights or “orthogonalizes” away any key/query activations that correlate with unintended stereotypes, before they mix into the visual latents.  \n\n3) Fairness‐Regularized Rendering Loss  \n   • When we render a batch of views in SVS, we already compare back to ground‐truth masks, depths, and RGB.  We now add a small fairness term in the loss:  \n       Ltotal = Lrgb + Lmask + Ldepth + λ_fair · Lfair  \n     where Lfair penalizes disparities in representation of different demographics or styles—for instance, if the prompt is “a doctor” we insist the model not systematically produce only male‐presenting characters.  \n   • We collect a tiny calibration set of prompts labeled for demographic balance (“a teacher,” “a firefighter,” etc.) and enforce that the distribution of rendered identities across the batch matches a uniform prior.  \n\nPutting it all together:  \n\n• At inference time the user’s raw text is audited and cleansed.  \n• The sanitized embedding then enters SVS’s transformer, where a debiasing head projects out stereotypical directions in the key–value space before the normal cross‐attention can “paint” semantics onto the triplanes.  \n• Finally, during training we explicitly regularize the synthesizer so that it does not collapse to a biased subspace of faces, genders, skin tones, or any other protected attribute.  \n\nThat three‐pronged strategy—prompt sanitization, debiasing within attention, and fairness‐aware loss—lets the Semantic‐aligned View Synthesizer adapt to user bias in text prompts in a transparent, controllable, and ethically responsible way."
    },
    {
        "question": "What motivates sequential application of Object Retention then Triplane Orthogonalization in the prior learner?",
        "relevant_section_ids": [
            "3.2.1",
            "3.2.2"
        ],
        "relevant_context": [
            "Current pretrained T2I models are able to produce images of high quality and great details. However, we only focus on the main object and need no other stuff like background. In the context of such diverse generative capabilities, directly fine-tuning would be severely impacted by irrelevant information, making it difficult to learn triplane effectively.",
            "After the Object Retention training, our model retains the strong capability to generate only the primary object. Next, to learn spatial orthogonal relationships within triplane priors, we introduce the Triplane Orthogonalization (TO) module."
        ],
        "final_answer": "The Object Retention step is first applied to strip away distracting background information and force the model to concentrate on the core object, ensuring that the learned triplane prior captures only relevant object details. Once the model reliably generates just the primary object, the Triplane Orthogonalization step is applied to inject and enforce spatial (orthogonal) relationships across the three planes, thereby improving multi-view consistency of the triplane representation.",
        "relevant_elements": [
            "Object Retention",
            "Triplane Orthogonalization"
        ],
        "id": 2052,
        "masked_question": "What motivates sequential application of [mask1] then [mask2] in the prior learner?",
        "masked_number": 2,
        "masked_elements": [
            "Object Retention",
            "Triplane Orthogonalization"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.07658v1_figure_2.png",
        "paperid": "2410.07658v1",
        "paper_path": "./MISS-QA/papers/2410.07658v1.json",
        "figure_id": "2410.07658v1_figure_2.png",
        "caption": "Figure 2: The overall framework of SeMv-3D. SeMv-3D consists of two components: 1) Triplane Prior Learner (TPL) that learns a triplane prior to capture consistent 3D visual details and 2) Semantic-aligned View Synthesizer (SVS) that enhances the alignment between the semantic with 3D content and enables single-step generation of arbitrary views. Here, Orthogonal Attention (OA) focuses on the orthogonal correspondences within the triplane, maintaining triplane consistency and extracting fine-grained features.",
        "qtype": "Design_Rationale",
        "response": "The key insight is that “object‐retention” and “triplane‐orthogonalization” are solving two very different problems, and if you try to do them both at once you either lose your object details or you never really learn to enforce true 3D spatial alignment.  So the authors break the prior‐learning into two steps:\n\n  1.  Mask 1 (the red box) is Object Retention.  Here they freeze almost all of Stable Diffusion, hook in a few new blocks, and train on background‐masked, single‐object images.  The sole job in this phase is to “teach” the network to ignore everything but the main object.  \n  2.  Mask 2 (the blue box) is Triplane Orthogonalization.  Now that the model knows how to render a clean object, they freeze all of that and attach orthogonal‐attention modules that learn to line up front, side and top views in a triplane representation.  \n\nBy doing Object Retention first they guarantee sharp, focused object features; by doing Orthogonalization second they guarantee those clean object features get stitched together into a truly 3D‐consistent triplane.  This staged (“first clean up the object, then impose 3D structure”) avoids the two objectives fighting each other and gives you both high‐fidelity object detail and fine‐grained multi‐view consistency."
    },
    {
        "question": "Why couple Cross Attn with Orthogonal Attention during triplane latents transformation?",
        "relevant_section_ids": [
            "3.3.1"
        ],
        "relevant_context": [
            "Taking this motivation into account, we propose to align the semantic information that describes different parts of the prompt with the low-level visual features in the triplane.",
            "Specifically, we first encode the pixel-space triplane P into latent space via E (Caron et al., 2021) to get triplane tokens T.",
            "To enhance the interaction of the learned 3D information and semantics, we then supplement triplane tokens with semantic information through cross-attention X and further align with the consistent 3D feature through our proposed orthogonal attention OA."
        ],
        "final_answer": "During triplane latents transformation, cross-attention is used to inject and align semantic information from the text prompt into the learned triplane features, while orthogonal attention is applied immediately afterward to enforce and preserve the correct spatial (multi-view) relationships within those augmented features. Coupling the two ensures that each semantic concept (e.g. “blonde hair”) is not only integrated into the latent representation but also remains coherent and consistent across all three orthogonal planes of the triplane.",
        "relevant_elements": [
            "Cross Attn",
            "Orthogonal Attention"
        ],
        "id": 2053,
        "masked_question": "Why couple [mask1] with Orthogonal Attention during triplane latents transformation?",
        "masked_number": 1,
        "masked_elements": [
            "Cross Attn"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.07658v1_figure_2.png",
        "paperid": "2410.07658v1",
        "paper_path": "./MISS-QA/papers/2410.07658v1.json",
        "figure_id": "2410.07658v1_figure_2.png",
        "caption": "Figure 2: The overall framework of SeMv-3D. SeMv-3D consists of two components: 1) Triplane Prior Learner (TPL) that learns a triplane prior to capture consistent 3D visual details and 2) Semantic-aligned View Synthesizer (SVS) that enhances the alignment between the semantic with 3D content and enables single-step generation of arbitrary views. Here, Orthogonal Attention (OA) focuses on the orthogonal correspondences within the triplane, maintaining triplane consistency and extracting fine-grained features.",
        "qtype": "Design_Rationale",
        "response": "Step 1: Locate the red‐boxed module in the diagram  \n– In the Triplane Latents Transformation block (top right), the red box surrounds the “Cross‐Attention” layer, which sits immediately before the Orthogonal Attention blocks.  \n\nStep 2: Read how the paper describes these two attentions working together  \n– Quoting Sect. 3.3.1:  \n  “To enhance the interaction of the learned 3D information and semantics, we then supplement triplane tokens with semantic information through cross-attention and further align with the consistent 3D feature through our proposed orthogonal attention.”  \n\nStep 3: Unpack why they need to be coupled  \n1. Cross-attention alone injects textual meaning (e.g. “blonde hair,” “suit,” “tie”) into the raw triplane tokens, but it does not care about 3D structure or multi-view coherence.  \n2. Orthogonal Attention alone enforces the correct spatial relationships across the three planes (front, top, side) but does not know which parts of the prompt belong to which region of the shape.  \n3. By chaining them—first cross-attending to the text, then orthogonally attending across planes—the model:  \n   a. Grounds each semantic concept in the appropriate latent features.  \n   b. At the same time preserves strict orthogonal (multi-view) consistency.  \n\nAnswer  \nThey couple the Cross-Attention ([mask1]) with Orthogonal Attention so that semantic cues from the text are first injected into the triplane latents and then these semantically enriched tokens are forced to respect the 3D orthogonal relationships.  This joint operation guarantees that (1) each piece of the prompt aligns with the correct triplane region, and (2) the resulting features remain spatially consistent across all three planes."
    },
    {
        "question": "What rationale motivated performing spatial calibration before depth estimation to align multimodal depth maps?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "As part of this work, we devised a dataset MAROON (Multimodal Aligned Radio and Optical frequency Object Reconstructions in the Near Field) (cf. Section 5  ###reference_###) that enables studying of different sensor modalities in direct comparison.",
            "Furthermore, the spatial calibration enables a direct comparison of all sensor reconstructions in a common metrical space."
        ],
        "final_answer": "They spatially calibrated all sensors first so that each modality’s depth map could be transformed into a shared metric coordinate frame, enabling the direct alignment and comparison of multimodal reconstructions.",
        "relevant_elements": [
            "Spatial Calibration",
            "Depth Estimation"
        ],
        "id": 2054,
        "masked_question": "What rationale motivated performing [mask1] before [mask2] to align multimodal depth maps?",
        "masked_number": 2,
        "masked_elements": [
            "Spatial Calibration",
            "Depth Estimation"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.00527v2_figure_1.png",
        "paperid": "2411.00527v2",
        "paper_path": "./MISS-QA/papers/2411.00527v2.json",
        "figure_id": "2411.00527v2_figure_1.png",
        "caption": "Figure 1. \nRecent developments for near-field imaging radars enabled the acquisition of high-resolution depth images, and the sensors are now increasingly gaining attention as complementary modalities to optical depth sensing. Direct comparisons from our MAROON dataset, however, highlight significant differences between radar and optical reconstructions. This work employs the collected multimodal data of four depth imagers, depicted on the left, to systematically characterize these fundamental differences together with sensor-specific findings in a joint evaluation framework.",
        "qtype": "Design_Rationale",
        "response": "They first solve for the rigid‐body transform between all the sensors (“spatial calibration,” the red–boxed step) so that every subsequent depth map lives in one common, metric co‐ordinate system.  Only once you know each sensor’s exact pose (its R∣t) can you take its raw measurements and turn them into depth images that will actually line up with those from the other modalities (“depth estimation,” the blue–boxed step).  If you tried to do depth estimation first, you’d end up with a bunch of range maps all locked into their own sensor frames – you wouldn’t have any way to bring them into register for a fair comparison or fusion."
    },
    {
        "question": "What motivates inserting joint alignment between reconstruction and depth deviation measurement for accurate deviation analysis?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Joint Alignment",
            "Depth Deviation Measurement"
        ],
        "id": 2055,
        "masked_question": "What motivates inserting [mask1] between reconstruction and [mask2] for accurate deviation analysis?",
        "masked_number": 2,
        "masked_elements": [
            "Joint Alignment",
            "Depth Deviation Measurement"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.00527v2_figure_1.png",
        "paperid": "2411.00527v2",
        "paper_path": "./MISS-QA/papers/2411.00527v2.json",
        "figure_id": "2411.00527v2_figure_1.png",
        "caption": "Figure 1. \nRecent developments for near-field imaging radars enabled the acquisition of high-resolution depth images, and the sensors are now increasingly gaining attention as complementary modalities to optical depth sensing. Direct comparisons from our MAROON dataset, however, highlight significant differences between radar and optical reconstructions. This work employs the collected multimodal data of four depth imagers, depicted on the left, to systematically characterize these fundamental differences together with sensor-specific findings in a joint evaluation framework.",
        "qtype": "Design_Rationale",
        "response": "They insert the joint‐alignment step between the raw reconstruction and the depth‐deviation computation simply because you cannot meaningfully compare two clouds (or two depth maps) until they live in the same metric frame.  In our pipeline (see Fig. 1, right), each sensor produces a 3D reconstruction in its own camera/radar coordinate system.  Before we can compute any Chamfer‐ or projective‐error to the “ground‐truth” MVS model, we first apply the estimated rigid transform [R|t] from our spatial calibration (the “Joint Alignment” box) and bring the GT and the sensor output into perfect register.  Only then does the “Depth Deviation Measurement” step give a true, millimetric error between corresponding points."
    },
    {
        "question": "How does spatial calibration optimize [R|t] to align diverse sensor coordinate systems?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Spatial Calibration"
        ],
        "id": 2056,
        "masked_question": "How does [mask1] optimize [R|t] to align diverse sensor coordinate systems?",
        "masked_number": 1,
        "masked_elements": [
            "Spatial Calibration"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.00527v2_figure_1.png",
        "paperid": "2411.00527v2",
        "paper_path": "./MISS-QA/papers/2411.00527v2.json",
        "figure_id": "2411.00527v2_figure_1.png",
        "caption": "Figure 1. \nRecent developments for near-field imaging radars enabled the acquisition of high-resolution depth images, and the sensors are now increasingly gaining attention as complementary modalities to optical depth sensing. Direct comparisons from our MAROON dataset, however, highlight significant differences between radar and optical reconstructions. This work employs the collected multimodal data of four depth imagers, depicted on the left, to systematically characterize these fundamental differences together with sensor-specific findings in a joint evaluation framework.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. Looking at the red‐boxed module in the middle of the pipeline, it is labeled “Spatial Calibration.”  This is the only block that produces an \\[R∣t\\] (rigid‐body rotation and translation).  \n2.  In the text (Sec. 5.1) they explain that they place four spherical markers (different materials so that both optical and RF see them), capture them with every depth sensor, and reconstruct their 3D point clouds.  \n3.  By automatically detecting the same four spheres in each sensor’s reconstruction, they obtain two sets of 3D points (one per sensor) that are known to correspond.  \n4.  They then solve for the rigid transform \\[R∣t\\] that best superimposes one set onto the other, typically by minimizing the sum of squared distances between corresponding sphere centers or surfaces (i.e. a Procrustes/SVD or ICP step over the four sphere correspondences).  \n5.  The result is a single 3×3 rotation and 3×1 translation that brings every sensor into the common metric frame.  \n\nAnswer  \nmask1 is the “Spatial Calibration” step.  It aligns all sensor coordinate systems by detecting four calibration spheres in each modality’s 3D reconstruction, pairing up their centers, and then solving a small rigid‐body least‐squares problem (e.g. Procrustes or ICP) to find the rotation R and translation t that minimizes the distance between matched sphere coordinates."
    },
    {
        "question": "How does depth estimation utilize optical segmentation masks for refining sensor depth data?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Optical Segmentation",
            "Depth Estimation"
        ],
        "id": 2057,
        "masked_question": "How does [mask1] utilize [mask2] masks for refining sensor depth data?",
        "masked_number": 2,
        "masked_elements": [
            "Depth Estimation",
            "Optical Segmentation"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.00527v2_figure_1.png",
        "paperid": "2411.00527v2",
        "paper_path": "./MISS-QA/papers/2411.00527v2.json",
        "figure_id": "2411.00527v2_figure_1.png",
        "caption": "Figure 1. \nRecent developments for near-field imaging radars enabled the acquisition of high-resolution depth images, and the sensors are now increasingly gaining attention as complementary modalities to optical depth sensing. Direct comparisons from our MAROON dataset, however, highlight significant differences between radar and optical reconstructions. This work employs the collected multimodal data of four depth imagers, depicted on the left, to systematically characterize these fundamental differences together with sensor-specific findings in a joint evaluation framework.",
        "qtype": "Implementation_Details",
        "response": "First, note that in the pipeline diagram the blue‐boxed step (“Optical Segmentation”) produces a per‐pixel binary mask of the object in the RGB view, and the red‐boxed step (“Depth Estimation”) produces a raw depth map from each sensor.  In our framework we simply multiply (or “apply”) the binary segmentation mask from the optical branch onto that sensor’s depth map, thus zeroing out any pixels that lie in the background or on the support structures.  The result is a “masked” depth map that contains only the object of interest, which we then use for all of our subsequent spatial calibration, joint alignment and quantitative error‐analysis steps."
    },
    {
        "question": "How is the plateau region of the loss surface identified to trigger freezing of LoRA parameters?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "As learning progresses, a decreasing loss indicates effective learning from current samples. Conversely, an increasing loss suggests a shift in data distribution, hindering effective learning.",
            "We assume that the model converges before the distribution shifts. Then between these phases, plateaus of the loss surface occurs, signaling that the model has reached a stable state by fitting well to the current data distribution (see Appendix C for more details).",
            "At these plateaus, it is best to consolidate the learned knowledge by freezing the current LoRA weights and initializing a pair of new, trainable LoRA parameters."
        ],
        "final_answer": "The plateau is detected by monitoring the training loss and identifying regions where the loss stops decreasing and remains roughly constant—indicating that the model has converged on the current data distribution. At that point (the loss plateau), the current LoRA parameters are frozen and a fresh pair of trainable LoRA parameters is initialized.",
        "relevant_elements": [
            "loss surface",
            "LoRA parameters"
        ],
        "id": 2058,
        "masked_question": "How is the plateau region of the [mask1] identified to trigger freezing of LoRA parameters?",
        "masked_number": 1,
        "masked_elements": [
            "loss surface"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.05663v1_figure_1.png",
        "paperid": "2411.05663v1",
        "paper_path": "./MISS-QA/papers/2411.05663v1.json",
        "figure_id": "2411.05663v1_figure_1.png",
        "caption": "Figure 1: The overview of Online-LoRA. As the data is continuously streamed (a), a new pair of trainable LoRA parameters (A4,B4subscript𝐴4subscript𝐵4A_{4},B_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT) is added (b) every time the loss surface encounters a plateau (c). Subsequently, the previous LoRA parameters (A1,B1;A2,B2;A3,B3subscript𝐴1subscript𝐵1subscript𝐴2subscript𝐵2subscript𝐴3subscript𝐵3A_{1},B_{1};A_{2},B_{2};A_{3},B_{3}italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) are frozen (the lock sign in (b)) and merged to the weights of the pre-trained ViT model.",
        "qtype": "Implementation_Details",
        "response": "They never peek at task‐IDs or any external signal – they simply watch the loosing curve itself.  In practice Online-LoRA monitors the streaming training loss and, after the big drop you get when the model first adapts to a new data regime, it looks for the point where that loss “levels off” into a long, flat valley.  In other words, once the gradient of the loss falls below a small threshold for a sustained window (i.e. the loss enters a plateau), the current LoRA A,B pair is frozen and merged into the backbone, and a fresh LoRA is born."
    },
    {
        "question": "How does loss plateau-triggered A4,B4 initialization align with dynamic architectural adaptation techniques?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "More precisely, we propose an extensible architecture that expands the model with additional LoRA parameters where the loss surface plateaus [3]. (Section 1)",
            "At these plateaus, it is best to consolidate the learned knowledge by freezing the current LoRA weights and initializing a pair of new, trainable LoRA parameters. (Section 3.2)"
        ],
        "final_answer": "By monitoring the loss surface and detecting plateaus, Online-LoRA dynamically grows its architecture: it freezes the current LoRA adapters and instantiates a new pair (A4, B4) exactly when the loss levels off. This mechanism mirrors other architecture-based continual-learning methods that add submodules on demand, but here the trigger comes from a data-driven signal (the loss plateau) rather than from known task boundaries.",
        "relevant_elements": [
            "loss surface plateaus",
            "A4",
            "B4"
        ],
        "id": 2060,
        "masked_question": "How does loss plateau-triggered [mask1],[mask2] initialization align with dynamic architectural adaptation techniques?",
        "masked_number": 2,
        "masked_elements": [
            "A4",
            "B4"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.05663v1_figure_1.png",
        "paperid": "2411.05663v1",
        "paper_path": "./MISS-QA/papers/2411.05663v1.json",
        "figure_id": "2411.05663v1_figure_1.png",
        "caption": "Figure 1: The overview of Online-LoRA. As the data is continuously streamed (a), a new pair of trainable LoRA parameters (A4,B4subscript𝐴4subscript𝐵4A_{4},B_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT) is added (b) every time the loss surface encounters a plateau (c). Subsequently, the previous LoRA parameters (A1,B1;A2,B2;A3,B3subscript𝐴1subscript𝐵1subscript𝐴2subscript𝐵2subscript𝐴3subscript𝐵3A_{1},B_{1};A_{2},B_{2};A_{3},B_{3}italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) are frozen (the lock sign in (b)) and merged to the weights of the pre-trained ViT model.",
        "qtype": "Literature_Background",
        "response": "Here the two masked items are simply the newly introduced LoRA adapter matrices:  \n- [mask1] (red box) is A₄, the new “input” projection  \n- [mask2] (blue box) is B₄, the new “output” projection  \n\nSo the question really reads:\n\n“How does loss-plateau-triggered A₄,B₄ initialization align with dynamic architectural adaptation techniques?”\n\nStep-by-step reasoning:\n\n1.  **What happens at a loss plateau?**  \n    Once the online model’s loss has settled (i.e. hit a plateau), we interpret that as an implicit signal that the current LoRA adapters have fully captured the present data distribution.\n\n2.  **What do we do?**  \n    We “freeze” the existing adapters (merge A₁–Aₙ and B₁–Bₙ into the backbone) and then instantiate a brand-new pair (A₄,B₄) that start training from that point on.\n\n3.  **Why is that like dynamic architecture adaptation?**  \n    Dynamic adaptation methods—think Progressive Neural Networks or “network growing” schemes—monitor some learning criterion (loss, gradient norms, etc.) and then *expand* the network only when it seems necessary (e.g. when current capacity is exhausted).  \n    - Here, the **loss plateau** plays exactly that role: it tells us “your current adapters aren’t learning anything new,” so you **grow** the model by adding (A₄,B₄).  \n    - Freezing and merging old adapters parallels the way some methods consolidate previous capacities (compress or fix old sub‐networks) to keep the overall size in check.\n\n4.  **Benefits of this alignment:**  \n    - **On-demand expansion:** we only add parameters when the loss suggests we need fresh capacity.  \n    - **Continual compression:** we merge (and thus remove from the active pool) old adapters, so the model doesn’t just keep ballooning.  \n    - **Implicit task detection:** unlike methods that need explicit task boundaries, the plateau criterion is entirely driven by training signals, which suits the task-free online setting.\n\nIn short, loss-plateau-triggered initialization of (A₄,B₄) is a lightweight, signal-driven way to grow the architecture exactly when it’s needed—mirroring the philosophy of dynamic architectural adaptation techniques in continual learning."
    },
    {
        "question": "What connection exists between merging frozen LoRA weights into pre-trained weights and weight consolidation regularization methods?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To prevent the accumulation of additional LoRA parameters, the frozen LoRA weights are merged into the pre-trained attention weights.",
            "Many studies have demonstrated the efficacy of weight regularization in reducing catastrophic forgetting [1, 43, 12]; this technique relies on estimating the importance of each parameter.",
            "After updating the importance weights, the model continues the learning process while penalizing changes to parameters that have been identified as important so far."
        ],
        "final_answer": "Merging frozen LoRA weights into the pre-trained weights is a form of hard weight consolidation—it permanently integrates previously learned LoRA adaptations into the base model. This mirrors the goal of weight-consolidation regularization methods (such as EWC), which preserve earlier knowledge by freezing or penalizing changes to parameters deemed important.",
        "relevant_elements": [
            "frozen LoRA weights",
            "pre-trained model weights"
        ],
        "id": 2061,
        "masked_question": "What connection exists between merging [mask1] into [mask2] and weight consolidation regularization methods?",
        "masked_number": 2,
        "masked_elements": [
            "frozen LoRA weights",
            "pre-trained model weights"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.05663v1_figure_1.png",
        "paperid": "2411.05663v1",
        "paper_path": "./MISS-QA/papers/2411.05663v1.json",
        "figure_id": "2411.05663v1_figure_1.png",
        "caption": "Figure 1: The overview of Online-LoRA. As the data is continuously streamed (a), a new pair of trainable LoRA parameters (A4,B4subscript𝐴4subscript𝐵4A_{4},B_{4}italic_A start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT) is added (b) every time the loss surface encounters a plateau (c). Subsequently, the previous LoRA parameters (A1,B1;A2,B2;A3,B3subscript𝐴1subscript𝐵1subscript𝐴2subscript𝐵2subscript𝐴3subscript𝐵3A_{1},B_{1};A_{2},B_{2};A_{3},B_{3}italic_A start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ; italic_A start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , italic_B start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT) are frozen (the lock sign in (b)) and merged to the weights of the pre-trained ViT model.",
        "qtype": "Literature_Background",
        "response": "First, let’s be concrete about what the two masks refer to:\n\n  • [mask₁] is the block of *frozen LoRA parameters* (A₁,B₁; A₂,B₂; A₃,B₃) highlighted in red.  \n  • [mask₂] is the *pre-trained ViT backbone* (the original Q/K/V weights) highlighted in blue.  \n\nWhat “merging [mask₁] into [mask₂]” really does is to take all the LoRA‐learned updates from the just-completed phase and *bake them permanently* into the backbone weights, then throw away (freeze) the LoRA adapters.  \n\nWhy is that analogous to weight-consolidation (EWC, MAS, etc.)?  In those classic methods you:\n\n  1. Learn a task and arrive at some θ⋆ (the MAP estimate under your prior).  \n  2. Approximate the posterior around θ⋆ with a Gaussian (via the Fisher or Hessian).  \n  3. Use that Gaussian as the *new prior* when learning the next task, i.e. you add a quadratic penalty ‖θ–θ⋆‖² weighted by parameter‐importance.  \n\nMerging the LoRA weights into the backbone is exactly the step “posterior → new prior, hard-commit.”  By absorbing the LoRA updates into the frozen ViT weights you:\n\n  – Fix the network at the old‐task solution (just like θ⋆).  \n  – Prevent future LoRA updates from drifting way off (the new LoRA starts from zero around this consolidated prior).  \n  – Implicitly enforce that the old task’s knowledge is preserved in the backbone rather than being overwritten.  \n\nSo in both cases you are *consolidating* what you’ve learned so far into a reference set of parameters and then regularizing (or in Online-LoRA’s case, structurally isolating) any further changes around that reference. It’s just that EWC/MAS do it via a soft quadratic penalty, whereas Online-LoRA does it by a *hard merge* of the low-rank updates into the base weights before moving on."
    },
    {
        "question": "How does Importance Score Estimation integrate Needle-in-a-Haystack methodology with contextual reasoning tests?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Wu et al. (2024) use the Needle-in-a-Haystack test … to estimate the importance score for each head. In these examples, a question that cannot be answered using the model’s parametric knowledge is paired with an answer (the “Needle”) inserted into a haystack at different positions. The model is required to retrieve the exact answer from the combined input.",
            "First, we construct retrieval-reasoning examples by adding explicit contextual reasoning steps to the retrieval examples from Wu et al. (2024), … We further modify the inserted needle into three parts: E_reasoning, ŷ_wrong, and ŷ_correct. The model must reason with E_reasoning to retrieve and generate the correct answer ŷ_correct, avoiding the wrong answer ŷ_wrong.",
            "Secondly, we refine the estimation method by focusing on the entire correct answer ŷ_correct (Correct Copy in Figure 1), since all tokens are relevant to the question. By considering the full correct answer, the importance score for each head no longer depends solely on the token with the highest attention score but is computed over the set of correct‐answer tokens."
        ],
        "final_answer": "The Importance Score Estimation builds on the Needle-in-a-Haystack retrieval test by taking its basic setup—injecting a ‘needle’ answer into a long context—and enriching it with explicit reasoning steps. Concretely, the authors create retrieval-reasoning examples by splitting the inserted needle into a reasoning prompt plus wrong and correct answers, forcing the model to perform context-based reasoning before retrieval. They then compute each head’s importance not just by exact-match attention on a single token, but by aggregating attention scores over all tokens of the full correct answer, thus capturing both retrieval and reasoning contributions.",
        "relevant_elements": [
            "Importance Score Estimation",
            "Needle-in-a-Haystack tests"
        ],
        "id": 2062,
        "masked_question": "How does [mask1] integrate [mask2] methodology with contextual reasoning tests?",
        "masked_number": 2,
        "masked_elements": [
            "Importance Score Estimation",
            "Needle-in-a-Haystack tests"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.19258v3_figure_1.png",
        "paperid": "2410.19258v3",
        "paper_path": "./MISS-QA/papers/2410.19258v3.json",
        "figure_id": "2410.19258v3_figure_1.png",
        "caption": "Figure 1: Our proposed head-level KV cache compression method consists of two steps: (1) Head-Level Importance Score Estimation (upper part): important heads that contribute to the\ncontextual reasoning ability are identified using Needle-in-a-Haystack tests. (2) Head-Level KV Cache Allocation (lower part): KV cache budgets for each head during the prefilling phase are allocated based on the importance score distribution identified in the first step.",
        "qtype": "Literature_Background",
        "response": "Here’s a step-by-step unpacking of the two highlighted regions and how they fit together:\n\n1.  Identify Mask 1 (the red box)  \n    -  Mask 1 surrounds our *new* Head-Level Importance Score Estimation pipeline (the “Upper part” in Figure 1).  \n    -  It shows, for a *particular* head, the full token stream (context → needle sentences → wrong-copy & correct-copy spans → context), plus the per-token attention bars and the final “sum” node that produces that head’s importance score.\n\n2.  Identify Mask 2 (the blue box)  \n    -  Mask 2 zooms in on the *example construction* for importance estimation—i.e. the “Retrieval-Reasoning Example” built off of Wu et al.’s Needle-in-a-Haystack test.  \n    -  It shows how we split the inserted “needle” into:  \n       a) a small *reasoning* step, and  \n       b) two candidate answers (one correct, one distractor),  \n    and then force the model to both retrieve and *reason* before it emits the right span.\n\n3.  What does the red box *do* with the blue box?  \n    -  Mask 1 (“Upper part”) *integrates* Mask 2’s methodology by:  \n       •  Taking Wu et al.’s pure retrieval-and-paste test and **injecting** explicit reasoning steps into it (that’s the three-piece needle in Mask 2).  \n       •  Replacing the old “single-token exact‐match” score with an **aggregate over all correct‐answer tokens**, weighted by their attention ranks (instead of just “did head attend to the one correct token?”).  \n       •  Summing that multi-token, reasoning-aware attention signal to produce a single importance score for each head.  \n\nIn short, the red-boxed module is our **new importance estimator**, and it plugs in the blue-boxed “needle in a haystack plus reasoning” examples from Wu et al., but then goes further by aggregating attention over the *entire* correct answer and the reasoning sub‐step. That is how we marry the original retrieval test (blue) with contextual‐reasoning checks to drive a richer, more robust head‐importance score (red)."
    },
    {
        "question": "How does Basic Budget allocation contrast with dynamic budget pooling from prior layer-level KV compression methods?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Each head h is initially assigned a fixed KV cache size B with an associated importance score. To allow dynamic allocation, we create a shared budget pool P by extracting a portion of the budget from each head, leaving the remainder as the basic budget.",
            "Previous works on KV cache compression during the prefill phase ... are limited to layer-level allocation, using either uniform or dynamic budgets per layer, but treating all heads within a layer equally."
        ],
        "final_answer": "In our scheme, every head first keeps a fixed per-head allocation—the “basic budget”—before any pooling takes place.  We then collect a small slice from each head into a shared dynamic pool P, which is redistributed according to head importance.  In contrast, prior layer-level methods never reserve a per-head basic budget: they form budgets at the layer level (either uniform or dynamically sized) and simply divide that entire layer budget equally or by coarse heuristics among all the heads in that layer.",
        "relevant_elements": [
            "Basic Budget",
            "Dynamic Budget"
        ],
        "id": 2063,
        "masked_question": "How does [mask1] allocation contrast with dynamic budget pooling from prior layer-level KV compression methods?",
        "masked_number": 1,
        "masked_elements": [
            "Basic Budget"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.19258v3_figure_1.png",
        "paperid": "2410.19258v3",
        "paper_path": "./MISS-QA/papers/2410.19258v3.json",
        "figure_id": "2410.19258v3_figure_1.png",
        "caption": "Figure 1: Our proposed head-level KV cache compression method consists of two steps: (1) Head-Level Importance Score Estimation (upper part): important heads that contribute to the\ncontextual reasoning ability are identified using Needle-in-a-Haystack tests. (2) Head-Level KV Cache Allocation (lower part): KV cache budgets for each head during the prefilling phase are allocated based on the importance score distribution identified in the first step.",
        "qtype": "Literature_Background",
        "response": "Here’s the reasoning in steps:\n\n1.  What is “\\[mask1\\]”?  \n    •  The red‐boxed region in the figure shows exactly how we assign KV‐cache slots to *each attention head*.  It lays out three things per head:  \n       –  A fixed “basic budget” (solid‐line boxes)  \n       –  A shared “dynamic budget” (dashed‐line boxes) that we carve off from every head at the outset  \n       –  A small “local window” (blue) of recent instruction tokens kept aside to guide selection.  \n\n2.  What did prior KV‐compression methods do?  \n    •  All of Xiao et al., Li et al., Cai et al. (and even Feng et al. when they add head info) still *start* by slicing budgets at the *layer* level.  They either:  \n    –  Give each layer a uniform or pre‐tuned cache size (and then split it equally among its heads), or  \n    –  Carve out a “dynamic pool” for the layer and then parcel it back out equally to every head in that layer.  \n\n3.  How does our head-level allocation contrast with that?  \n    •  Instead of treating the whole layer as one chunk, we:  \n    a.  Estimate an *importance score* for *each* head.  \n    b.  Give every head a small *basic* share.  \n    c.  Carve out the *dynamic pool* at the *head* granularity (not layer), then re-distribute that pool *across heads* in proportion to their individual importance scores.  \n\n4.  In short:  \n    •  Prior work pools & redistributes memory *per layer* (always splitting equally across its heads).  \n    •  Our [mask1] allocation is *head-fine*, carving and re-spreading budgets based on *head importance*, so the “rich” heads get more slots and the “poor” ones fewer."
    },
    {
        "question": "How does Importance Score Estimation integrate Needle-in-a-Haystack methodology with contextual reasoning tests?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "Wu et al. (2024) use the Needle-in-a-Haystack test … to estimate the importance score for each head. In these examples, a question that cannot be answered using the model’s parametric knowledge is paired with an answer (the “Needle”) inserted into a haystack at different positions. The model is required to retrieve the exact answer from the combined input.",
            "First, we construct retrieval-reasoning examples by adding explicit contextual reasoning steps to the retrieval examples from Wu et al. (2024), … We further modify the inserted needle into three parts: E_reasoning, ŷ_wrong, and ŷ_correct. The model must reason with E_reasoning to retrieve and generate the correct answer ŷ_correct, avoiding the wrong answer ŷ_wrong.",
            "Secondly, we refine the estimation method by focusing on the entire correct answer ŷ_correct (Correct Copy in Figure 1), since all tokens are relevant to the question. By considering the full correct answer, the importance score for each head no longer depends solely on the token with the highest attention score but is computed over the set of correct‐answer tokens."
        ],
        "final_answer": "The Importance Score Estimation builds on the Needle-in-a-Haystack retrieval test by taking its basic setup—injecting a ‘needle’ answer into a long context—and enriching it with explicit reasoning steps. Concretely, the authors create retrieval-reasoning examples by splitting the inserted needle into a reasoning prompt plus wrong and correct answers, forcing the model to perform context-based reasoning before retrieval. They then compute each head’s importance not just by exact-match attention on a single token, but by aggregating attention scores over all tokens of the full correct answer, thus capturing both retrieval and reasoning contributions.",
        "relevant_elements": [
            "Importance Score Estimation",
            "Needle-in-a-Haystack tests"
        ],
        "id": 2062,
        "masked_question": "How does [mask1] integrate [mask2] methodology with contextual reasoning tests?",
        "masked_number": 2,
        "masked_elements": [
            "Importance Score Estimation",
            "Needle-in-a-Haystack tests"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.19258v3_figure_1.png",
        "paperid": "2410.19258v3",
        "paper_path": "./MISS-QA/papers/2410.19258v3.json",
        "figure_id": "2410.19258v3_figure_1.png",
        "caption": "Figure 1: Our proposed head-level KV cache compression method consists of two steps: (1) Head-Level Importance Score Estimation (upper part): important heads that contribute to the\ncontextual reasoning ability are identified using Needle-in-a-Haystack tests. (2) Head-Level KV Cache Allocation (lower part): KV cache budgets for each head during the prefilling phase are allocated based on the importance score distribution identified in the first step.",
        "qtype": "Literature_Background",
        "response": "Here’s the high‐level “chain of thought” for how the red‐boxed component ([mask1]) actually folds in the old needle-in-a-haystack test ([mask2]) and turns it into a contextual‐reasoning probe:\n\n1.  **Start from Wu et al.’s Needle-in-a-Haystack**  \n    – In the original setup you plant a short “needle” answer in a long “haystack” context at various positions and measure which heads put the most attention mass on that exact answer token during generation.  \n    – That gives you a very sparse importance distribution (most heads score zero), and it tells you only about pure retrieval, not any multi‐step reasoning.\n\n2.  **Augment with a Reasoning Step**  \n    – We turn each pure retrieval example into a tiny “reasoning puzzle.”  \n    – The inserted content is no longer just one answer token.  It is now three pieces: (a) a little reasoning hint, (b) the correct answer, and (c) a plausible but wrong answer.  \n    – The model must use the hint to arrive at the correct answer and avoid the wrong one.  This forces any scoring procedure to look at “attention to reasoning tokens” as well as “attention to the final answer.”\n\n3.  **Red Box ([mask1]): Top-N Attention Aggregation**  \n    – Instead of only crediting the single highest‐attended token in each head, we look at each head’s **top N** attention scores **across all the correct‐answer tokens** (the red bars in the diagram).  \n    – We sum (or average) those N scores and then normalize by N.  That becomes each head’s *importance score*.\n\n4.  **Blue Box ([mask2]): Rest of the Attention**  \n    – The blue‐boxed bars are simply the “other” attention scores below the top N.  We explicitly ignore them in the final sum, so that each head’s score focuses on how strongly it attended the *correct reasoning‐answer span*, not on noise.\n\n5.  **Pulling It All Together**  \n    – By inserting a mini chain‐of‐thought into the needle test and then using a top-N‐sum (red box) over exactly the reasoning + answer tokens (ignoring the blue rest), we get a richer importance measure.  \n    – Heads that specialize in *both* retrieval **and** the contextual reasoning step now light up with high scores, instead of being invisible under the old, exact‐match-only metric.\n\nSo, in one sentence:\n\n   **Our new importance-score estimation (red box) embeds the original Needle-in-a-Haystack retrieval test (blue box) into tiny contextual-reasoning examples and then sums each head’s top-N attentions on the entire correct answer span—thereby fusing retrieval diagnostics with explicit reasoning probes.**"
    },
    {
        "question": "How does Head-Level Allocation’s dynamic budget pool improve cache distribution across attention heads?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Each head h is initially assigned a fixed KV cache size b_fix with an associated importance score s_h.",
            "To allow dynamic allocation, we create a shared budget pool B by extracting a portion of the budget from each head, leaving the remainder as the basic budget.",
            "The budget pool B is then distributed among the heads in proportion to their importance scores s_h.",
            "The retained KV cache for each head includes: (1) the basic budget (b_fix), (2) the dynamic budget b_dyn, proportional to its importance score, and (3) the last K instruct tokens."
        ],
        "final_answer": "Head-Level Allocation first carves out a small fraction of every head’s fixed cache to form a shared dynamic budget pool. It then redistributes that pool back to heads in proportion to their learnt importance scores. This gives larger caches to critical heads and smaller caches to less important ones, resulting in a more efficient, importance-weighted distribution of KV cache slots across all attention heads.",
        "relevant_elements": [
            "Head-Level Allocation",
            "Dynamic Budget"
        ],
        "id": 2065,
        "masked_question": "How does [mask1]’s dynamic budget pool improve cache distribution across attention heads?",
        "masked_number": 1,
        "masked_elements": [
            "Head-Level Allocation"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.19258v3_figure_1.png",
        "paperid": "2410.19258v3",
        "paper_path": "./MISS-QA/papers/2410.19258v3.json",
        "figure_id": "2410.19258v3_figure_1.png",
        "caption": "Figure 1: Our proposed head-level KV cache compression method consists of two steps: (1) Head-Level Importance Score Estimation (upper part): important heads that contribute to the\ncontextual reasoning ability are identified using Needle-in-a-Haystack tests. (2) Head-Level KV Cache Allocation (lower part): KV cache budgets for each head during the prefilling phase are allocated based on the importance score distribution identified in the first step.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1.  In the figure the entire red-boxed region is the “Head-Level KV Cache Allocation” module.  On the left you see a heatmap of per-head importance scores; on the right the little colored blocks show, for each head, three components of its cache budget:  \n   – a fixed “basic” budget (solid black outline)  \n   – a “dynamic” budget (dashed outline)  \n   – a small local window of the last K instruction tokens (light-blue)  \n\n2.  Mask1 therefore refers to that middle part of the allocation scheme where we “carve out” a shared pool B from all heads’ basic budgets and then reassign it.  \n\n3.  Concretely, each head i starts with a fixed quota b.  We extract a fraction α·b from every head to form the dynamic pool B.  We then redistribute B among the N·L heads in proportion to their importance scores S_i.  High-S heads get a larger slice of B, low-S heads get little or none.  \n\n4.  By doing this we:  \n   • recover unused capacity from unimportant heads  \n   • concentrate extra KV-cache capacity where it will do the most reasoning work  \n   • avoid over-allocating to “dead” heads that won’t use it  \n\n5.  In short, Mask1’s dynamic budget pool turns a one-size-fits-all quota into an adaptive, importance-weighted reallocation that boosts the cache size of critical heads at the expense of unimportant ones—leading to a much finer-grained, more efficient distribution of KV slots across attention heads."
    },
    {
        "question": "How does the Monitoring System's historical PRB input interface with the Predictor during model training phase?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Monitoring System",
            "Predictor"
        ],
        "id": 2066,
        "masked_question": "How does the [mask1]'s historical PRB input interface with the Predictor during model training phase?",
        "masked_number": 1,
        "masked_elements": [
            "Monitoring System"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.14377v1_figure_1.png",
        "paperid": "2407.14377v1",
        "paper_path": "./MISS-QA/papers/2407.14377v1.json",
        "figure_id": "2407.14377v1_figure_1.png",
        "caption": "Figure 1: O-RAN architecture with probabilistic forecast as rApp.",
        "qtype": "Experimental_Results",
        "response": "In our diagram the red‐boxed block is the Predictor inside the Analytic Engine of the non-real-time RIC (i.e. [mask1] = “Predictor”).  During the training phase the flow looks like this:\n\n1.  The O-DU continually exports its PRB-usage history up to the non-RT-RIC over the standard O1 management interface.  \n2.  That stream of time-stamped “History PRBs” lands first in the rApp’s Monitoring System module.  \n3.  The Monitoring System then hands the raw PRB time–series to the Analytic Engine, and specifically into the Predictor sub-component.  \n4.  Inside the Predictor the series is pre-processed, split into train/test folds, and fed into whichever probabilistic estimator you have selected (SFF, DeepAR, Transformer…).  The model then learns its parameters (via back-propagation or maximum likelihood) on the historical PRB values.  \n\nIn short, the Predictor never reaches out on its own to the O-DU; rather the PRB history is pushed in over O1 → Monitoring System → Predictor, and it is that sequence of values that is used to train the forecast model."
    },
    {
        "question": "How does the Decision Engine schedule actuator triggers based on predicted PRB ranges?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Decision Engine",
            "Actuator"
        ],
        "id": 2067,
        "masked_question": "How does the [mask1] schedule actuator triggers based on predicted PRB ranges?",
        "masked_number": 1,
        "masked_elements": [
            "Decision Engine"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.14377v1_figure_1.png",
        "paperid": "2407.14377v1",
        "paper_path": "./MISS-QA/papers/2407.14377v1.json",
        "figure_id": "2407.14377v1_figure_1.png",
        "caption": "Figure 1: O-RAN architecture with probabilistic forecast as rApp.",
        "qtype": "Experimental_Results",
        "response": "In our picture the little red box is really pointing at the decision‐making core of the rApp (often simply called the “Decision Engine”), so the question really is:\n\n  “How does the Decision Engine schedule actuator triggers based on the predicted PRB ranges?”\n\nStep-by-step the answer is as follows:\n\n 1.  At every forecasting interval the Analytic Engine spits out not a single “best guess” but a full probability distribution (e.g. the 1st–99th percentiles) of how many PRBs will be needed in the next time slot.\n\n 2.  The Decision Engine sits on top of that distribution and, under the guidance of the operator’s high‐level SLA or risk‐policy (for instance “we always want to provision up to the 90th percentile forecast” or “we only grow allocation if the 75th percentile exceeds current resources by more than 5 PRBs”), it picks a single deterministic PRB count for the next window.\n\n 3.  As soon as that quantile‐selection logic fires, the Decision Engine packages that chosen PRB count into an O1‐interface command and hands it off to the Actuator.\n\n 4.  The Actuator then “schedules” (really, immediately sends) that O1 message to the O-DU at the start of the next allocation interval.\n\nPut another way, the Decision Engine continuously watches the full forecast range, applies a simple rule over the percentiles (e.g. “use the 95 % quantile,” “use the median plus a safety‐margin,” or “only change if there’s more than 10 % risk of running out of PRBs”), and whenever that rule says “yes, we need more (or fewer) PRBs,” it triggers the Actuator to fire off an O1 reconfiguration at the very next control‐plane slot."
    },
    {
        "question": "How could latency constraints in Non-Real Time RIC affect Decision Engine efficiency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Non-Real Time RIC",
            "Decision Engine"
        ],
        "id": 2068,
        "masked_question": "How could latency constraints in [mask1] affect [mask2] efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "Non-Real Time RIC",
            "Decision Engine"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.14377v1_figure_1.png",
        "paperid": "2407.14377v1",
        "paper_path": "./MISS-QA/papers/2407.14377v1.json",
        "figure_id": "2407.14377v1_figure_1.png",
        "caption": "Figure 1: O-RAN architecture with probabilistic forecast as rApp.",
        "qtype": "Others",
        "response": "In our diagram, the red‐boxed cluster is the set of “near-real-time” forwarding and control plane functions (the split CU-CP/CU-UP and the UPF/MEC) that sit under the Near-RT-RIC, and the blue‐boxed block is our non-real-time resource-provisioning rApp.  In plain English the question is:\n\n   “What happens to the efficiency of the non-RT rApp if stringent latency limits are imposed on the CU/UPF/MEC?”\n\nStep-by-step reasoning:\n\n1.  The rApp lives in the Non-RT RIC, where it collects coarse‐grained data, fits a probabilistic forecast, and decides how many PRBs to hand out for the next seconds‐to‐minutes interval.\n\n2.  Once it has made its decision it pushes that allocation down to the RAN side via the O1/O2 interfaces—the packets ultimately land in the CU-UP (the user-plane path) or in the UPF/MEC (the edge-compute/Core‐UPF).\n\n3.  If those target elements (CU-UP, CU-CP or the UPF/MEC) must honor very tight latency budgets (for example sub‐10 ms scheduling deadlines in the CU-UP or sub‐1 ms path through the MEC), two things can happen:\n\n    •  Enforcement Delays:  The non-RT rApp may not be allowed enough time to run a heavyweight probabilistic‐forecast workflow before its control commands are “too late” for the CU/UPF’s scheduling window.  As a result it must simplify or shorten its computations (sacrifice model complexity), which directly reduces its forecasting accuracy and thus its resource-allocation efficiency.  \n   \n    •  Feedback Staleness:  Even if it does finish its calculations on time, the enforced low latency in the CU/UPF means that status updates (e.g. current PRB usage, queue lengths, any MEC‐hosted analytics) can only be sent back at coarse granularity—so the rApp sees stale or low-resolution telemetry, again degrading its ability to make good future‐looking decisions.\n\n4.  In short, very tight latency requirements on the red-boxed functions force the blue-boxed rApp either to (a) cut down on its forecasting work so that it can push decisions faster or (b) live with delayed, coarse feedback.  In both cases the end result is sub-optimal PRB provisioning—higher error rates in demand prediction, over/under-allocation of spectrum, wasted CPU cycles, and ultimately poorer QoS."
    },
    {
        "question": "What ethical risks does exposing tenant PRB histories to the Analytic Engine via Monitoring System pose?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Monitoring System",
            "Analytic Engine"
        ],
        "id": 2069,
        "masked_question": "What ethical risks does exposing tenant PRB histories to the [mask1] via [mask2] pose?",
        "masked_number": 2,
        "masked_elements": [
            "Analytic Engine",
            "Monitoring System"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.14377v1_figure_1.png",
        "paperid": "2407.14377v1",
        "paper_path": "./MISS-QA/papers/2407.14377v1.json",
        "figure_id": "2407.14377v1_figure_1.png",
        "caption": "Figure 1: O-RAN architecture with probabilistic forecast as rApp.",
        "qtype": "Others",
        "response": "Here is a step‐by‐step reasoning through the question:\n\n1.  Identify mask₁ and mask₂ in the figure.  \n    -  The red box (mask₁) encloses the RIC components that actually push PRB‐allocation decisions down into the O-DU.  In the annotated diagram it is labeled “Resource Provisioning APP.”  \n    -  The blue box (mask₂) encloses the block inside the Non–Real‐Time RIC that does the machine-learning work – in particular the Predictor and its surrounding data‐preparation logic.  In the text this is called the “Analytic Engine.”  \n\n2.  What does “tenant PRB history” mean?  \n    -  Every tenant (e.g. a virtual operator or slice) periodically reports how many Physical Resource Blocks (PRBs) it has used over time.  That fine-grained time series (hours or seconds of usage) is exactly what the Monitoring System feeds into the Analytic Engine for probabilistic forecasting.  \n\n3.  Why is giving the Resource Provisioning APP (via its Analytic Engine) access to that history ethically risky?  \n    a.  Privacy of end users.  A tenant’s PRB usage profile is a direct proxy for how many subscribers, IoT devices or machines they have active, when they are busiest, where they are located, and even what kinds of applications they are running.  Exposing that history to a third‐party analytics app can allow that app (or an attacker who compromises it) to reconstruct user movement patterns or infer sensitive behavior (e.g. medical‐IoT data, customer footfall in retail outlets, etc.).  \n    b.  Tenant confidentiality and competitive intelligence.  If the rApp is provided by a neutral or third‐party vendor, it may learn exactly when one tenant’s network is most heavily loaded, how it grows over time, and can thus reverse‐engineer business volumes, staffing levels, special events, or planned marketing campaigns.  That is insider information that competitors or malicious actors could exploit.  \n    c.  Risk of re-identification & de-anonymization.  Even “anonymized” PRB logs, when correlated with other side data (location databases, public events calendars, etc.), can be re-identified back to individual users or enterprises.  \n    d.  Fairness & bias in resource allocation.  A forecasting app that knows the full history of one slice could systematically privilege or penalize it (or another slice) by over- or under-provisioning resources in future—leading to discriminatory QoS that may breach neutrality rules or SLAs.  \n    e.  Regulatory and legal exposure.  In many jurisdictions the sharing of usage data without explicit end‐user consent runs counter to privacy regulations (GDPR, CCPA, etc.).  \n\n4.  Summary answer  \n    Exposing tenant PRB histories to the Resource Provisioning APP via the Analytic Engine risks leaking extremely sensitive subscriber-level and business-level information, invites profiling or de-anonymization attacks, undermines slice‐level confidentiality, opens the door to unfair or biased resource allocations, and can breach data‐protection laws."
    },
    {
        "question": "What are limitations of first-stage calibration when handling incomplete modalities in both queries and references?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "We now explain the Any2Any framework in detail. ... We convert all the entries in the matrices to their corresponding conformal probabilities, as described Eq. 3, where σ in Eq. 7 is Γ in Eq. 3.",
            "Note that similarity scores in each feature space are calibrated differently, thus we have M^{2} prediction bands in total, denoted as Γ_{mn} for each cross-modal feature space of modalities (m,n). They map the similarity scores in the space to the probabilities of correct retrieval. We define the corresponding error rate as α_{mn}, which can be obtained by Eq. 6.",
            "After the first stage calibration, the distributions both range between [0,1], thus in the same scale. We still need a mechanism to enable direct comparison between the conformal matrices C in Eq. 9.",
            "The first stage of the conformal prediction is done. We ground the similarity scores from each feature space to conformal probabilities—the lower bounds of the probabilities of correct retrieval."
        ],
        "final_answer": "First‐stage calibration can only normalize each individual cross‐modal similarity score into a probability on [0,1]. When queries or references are missing entire modalities, that simply leaves empty entries (marked “×”) in the similarity (now probability) matrices. By itself, the first stage cannot fuse or compare these partial probability matrices across modality combinations – it only operates “cell‐by‐cell,” so you still end up with matrices of varying sparsity that cannot be directly ranked or compared without a second‐stage aggregation step.",
        "relevant_elements": [
            "first-stage calibration",
            "incomplete modalities"
        ],
        "id": 2070,
        "masked_question": "What are limitations of [mask1] when handling incomplete modalities in both queries and references?",
        "masked_number": 1,
        "masked_elements": [
            "first-stage calibration"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.10513v2_figure_1.png",
        "paperid": "2411.10513v2",
        "paper_path": "./MISS-QA/papers/2411.10513v2.json",
        "figure_id": "2411.10513v2_figure_1.png",
        "caption": "Figure 1: The Any2Any retrieval framework retrieves multimodal data with varying incomplete modalities across instances.\nWe employ a two-stage calibration process using conformal prediction to facilitate comparisons between query and reference instances, each has different incomplete modalities.\nAny2Any supports any number and combination of modalities, enabling it to adapt to any multimodal retrieval dataset.\nThis illustrative figure uses data from the KITTI dataset [9] and captions generated by LLaVA [24].",
        "qtype": "Others",
        "response": "Here’s the step‐by‐step reasoning to arrive at what the red‐boxed component (“1st stage calibration”) cannot do when either the query or the database item (or both) are missing modalities:\n\n1.  In the toy example on the left, we start with a raw similarity matrix  \n       – rows = query modalities (image, LiDAR, text)  \n       – cols = reference modalities (image, LiDAR, text)  \n   Some entries are simply “×” because that modality is missing in that instance.\n\n2.  The “1st stage calibration” arrow (the thing in the red box) takes each *individual* similarity score (e.g. “0.6” between query‐image and ref‐image, “0.3” between query‐LiDAR and ref‐text, etc.) and remaps it via a conformal calibration curve into a probability in [0,1].  \n    – After this stage every *present* cell in each matrix is on the same [0,1] probability scale.\n\n3.  **What it still CANNOT do** (i.e. its limitations):  \n    a.  It does *not* fill in or impute the missing (“×”) entries.  Matrices that correspond to different combinations of missing modalities still have different shapes/patterns of observed cells.  \n    b.  It does *not* produce a single summary score or a directly comparable scalar for the *whole* query–reference pair.  You still have a little probability *matrix*, not a single probability.  \n    c.  Hence, you still cannot tell, “Is Query A vs. Ref 1 overall more likely than Query B vs. Ref 2?” until you do a second aggregation step.  \n\n4.  That is precisely why the authors introduce the **2nd stage calibration**, which (a) aggregates all the non-missing calibrated entries in a single pool (e.g. by taking their mean) and then (b) re-calibrates that aggregate to produce one bona-fide retrieval probability.  \n\nIn short, the 1st stage calibration *only* brings every individual cross-modal score into the [0,1] band but leaves you with:\n  – A sparse matrix whose sparsity pattern depends on which modalities are missing, and  \n  – No single scalar score over all modalities.  \n\nNeither of those is sufficient for comparing arbitrary incomplete query–reference pairs, so you need the 2nd stage to handle incomplete modalities properly and get a final, comparable retrieval probability."
    },
    {
        "question": "What motivates separating first stage calibration and second stage calibration in this retrieval framework?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "To address the incomparable issue, we use a two-stage conformal prediction to ground the similarity matrices.",
            "The first stage grounds individual similarity scores in each cross-modal feature space to conformal probabilities.",
            "The second stage fuses these conformal probabilities from a pair of query and retrieval instances to a scalar, enabling comparison across similarity matrices.",
            "After the first stage calibration, the distributions both range between [0,1], thus in the same scale. We still need a mechanism to enable direct comparison between the conformal matrices. We ground M to a statistically meaningful scalar by conformal prediction, which is the second stage.",
            "To sum up, the first stage grounds the similarity scores to probabilities in the same scale, enabling the arithmetic operation of M later used in the second stage. The second stage is necessary since we need a mechanism to fuse all entries in the conformal matrix to a single probability to enable comparisons of scalars, not matrices."
        ],
        "final_answer": "Because raw cross-modal similarity scores lie in different, incomparable ranges, the first stage calibration maps each modality’s score to a probability so that all are on the same [0,1] scale. Once we have a conformal‐probability matrix for each instance pair, we still need a single summary score to compare different pairs. The second stage therefore fuses the matrix entries into one scalar and recalibrates it, yielding a unified retrieval confidence. This two‐stage design ensures both comparability across modalities and a meaningful aggregated retrieval probability.",
        "relevant_elements": [
            "first stage calibration",
            "second stage calibration"
        ],
        "id": 2072,
        "masked_question": "What motivates separating [mask1] and second stage calibration in this retrieval framework?",
        "masked_number": 1,
        "masked_elements": [
            "first stage calibration"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.10513v2_figure_1.png",
        "paperid": "2411.10513v2",
        "paper_path": "./MISS-QA/papers/2411.10513v2.json",
        "figure_id": "2411.10513v2_figure_1.png",
        "caption": "Figure 1: The Any2Any retrieval framework retrieves multimodal data with varying incomplete modalities across instances.\nWe employ a two-stage calibration process using conformal prediction to facilitate comparisons between query and reference instances, each has different incomplete modalities.\nAny2Any supports any number and combination of modalities, enabling it to adapt to any multimodal retrieval dataset.\nThis illustrative figure uses data from the KITTI dataset [9] and captions generated by LLaVA [24].",
        "qtype": "Design_Rationale",
        "response": "The reason we actually split the conformal‐prediction step into two is that we have two fundamentally different “hard” problems to solve:\n\n 1.  Stage 1 (the red‐boxed “first stage calibration”) takes each raw similarity score—be it image‐text, LiDAR‐text, LiDAR‐image or pure unimodal—and independently re‐scales it into a bona fide probability (a “conformal probability of correct retrieval”).  That step is what lets us take wildly different similarity ranges (e.g. CLIP scores vs. LiDAR‐based scores) and put them all on the same [0,1] footing so that, say, 0.7 means the same thing regardless of modality.\n\n 2.  Stage 2 must then take the entire matrix of these per‐entry probabilities (one row and one column for every present modality pair) and collapse it into a single number so that we can directly compare “query–reference A” vs. “query–reference B.”  We do that by choosing a simple aggregation (mean, max, etc.) and then running one more conformal calibration on that scalar so that our final “which is the safer bet?” decision is again statistically valid under our chosen mis-coverage level.\n\nIf you tried to do it all in one go, you’d either be (a) unable to compare across modalities or (b) left with a multi‐dimensional object you couldn’t turn into a single retrieval score with a meaningful error guarantee. Hence the two stages—first normalize each similarity channel, then fuse and re-calibrate."
    },
    {
        "question": "Why transform cross-modal similarity scores into conformal probabilities prior to fusion?",
        "relevant_section_ids": [
            "1",
            "5.2"
        ],
        "relevant_context": [
            "However, as we later examine, the pair-wise cross-modal similarities are not directly comparable because the encoder outputs have different ranges. It thus employs a two-stage calibration process. The first stage utilizes conformal prediction to standardize and align similarity scores to probabilities of correct retrieval.",
            "Fig. 2 shows the incomparability of similarity scores from different modality encoders ... The two distributions of similarity scores lie in completely different ranges before calibration. For example, a score of 0.6 represents the 80-th percentile in the LiDAR modality but the 60-th percentile in the text modality. After the first stage calibration, the distributions both range between 0 and 1, thus in the same scale."
        ],
        "final_answer": "Because raw similarity scores from different modalities reside in different, incomparable ranges, converting them to conformal probabilities standardizes and aligns all scores onto a common, meaningful scale (probabilities of correct retrieval), which makes it possible to fairly fuse and compare them across modalities.",
        "relevant_elements": [
            "cross-modal similarity",
            "conformal probability"
        ],
        "id": 2073,
        "masked_question": "Why transform [mask1] scores into [mask2] prior to fusion?",
        "masked_number": 2,
        "masked_elements": [
            "cross-modal similarity",
            "conformal probability"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.10513v2_figure_1.png",
        "paperid": "2411.10513v2",
        "paper_path": "./MISS-QA/papers/2411.10513v2.json",
        "figure_id": "2411.10513v2_figure_1.png",
        "caption": "Figure 1: The Any2Any retrieval framework retrieves multimodal data with varying incomplete modalities across instances.\nWe employ a two-stage calibration process using conformal prediction to facilitate comparisons between query and reference instances, each has different incomplete modalities.\nAny2Any supports any number and combination of modalities, enabling it to adapt to any multimodal retrieval dataset.\nThis illustrative figure uses data from the KITTI dataset [9] and captions generated by LLaVA [24].",
        "qtype": "Design_Rationale",
        "response": "Because the raw cross‐modal similarity scores (the red–boxed matrix) all live in different ranges and are not directly comparable across feature spaces, we first “ground” them into conformal retrieval probabilities (the blue–boxed matrix).  Conformal calibration takes each encoder’s similarity output and maps it into the same [0,1] probability scale (stage 1) and then into an overall retrieval probability (stage 2).  That common probabilistic scale is what makes arithmetic fusion (e.g. taking the mean over modalities) meaningful and allows us to compare heterogeneous modality pairs fairly."
    },
    {
        "question": "What is the rationale for using dual-length patch shuffling in the DHP module for bias mitigation?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Therefore, inspired by ShuffleNet [40], we propose the DHP module, which tries to fully explore the discriminative information with more diverse coverage from the features themselves and alleviate the impact of feature bias through special feature shuffling and grouping operations.",
            "After shuffling and grouping, the dual-length hybrid patch embedding features cover several random patch embeddings from different body parts of the human body, and have dense and sparse coverage respectively, which endow the local features with the ability to recognize global information. In addition, the original feature and the local features are concatenated as the final feature representation to balance the feature bias of the original features caused by objective factors such as pedestrian posture, occlusion, and shooting Angle."
        ],
        "final_answer": "By randomly shuffling patch embeddings and then grouping them into two different lengths, the DHP module produces local features with both dense and sparse coverage over diverse body regions. This dual-length, hybrid coverage lets the model learn more discriminative cues from varied spatial locations, thereby mitigating biases caused by pose changes, occlusion, and viewpoint variation without relying on external models.",
        "relevant_elements": [
            "Dual-length Hybrid Patch (DHP) module"
        ],
        "id": 2074,
        "masked_question": "What is the rationale for using [mask1] in the DHP module for bias mitigation?",
        "masked_number": 1,
        "masked_elements": [
            "Dual-length Hybrid Patch (DHP) module"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.00330v1_figure_2.png",
        "paperid": "2411.00330v1",
        "paper_path": "./MISS-QA/papers/2411.00330v1.json",
        "figure_id": "2411.00330v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the proposed MIPL approach. It consists of the backbone, the CIS module, the BGA module, and the DHP module. ’SCHP’ is a semantic analysis module to obtain the human semantic information. ’Norm.’ and ’Trans.’ denote the normalization and transpose operations, respectively.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed “gadget” in the DHP module is nothing mystical – it is simply the piece of the network that takes your normal CLIP‐style feature map, shuffles all of its patch embeddings at random (leaving the [CLS] token in place), and then carves that shuffled map into several smaller “dual‐length” sub‐patches (some dense, some sparse).  By reassembling these random, multi-scale snippets alongside your original global feature, the network is forced to look for identity cues all over the body – not just in whatever pose or viewpoint happened to dominate in training.  In practice this very simple shuffle-and-split trick produces a set of complementary local descriptors that cover different body parts and scales, and when you concatenate them back onto your global token they substantially reduce the usual bias toward a single pose, occlusion pattern or camera angle."
    },
    {
        "question": "What drives sequential CIS prompt learning before image encoder fine-tuning in the two-stage training?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "III-A Clothing Information Stripping (CIS): “in the first training stage, A set of learnable prompt words are introduced, which are an identity-dependent text prompt (“A photo of a    person.”) and a clothes-dependent text prompt (“A photo of a    clothes.”)… Then we use the text encoder and image encoder with frozen parameters to obtain the corresponding text features  and image features  (the encoder is pre-trained by CLIP [25]). A contrastive learning loss function is used to constrain the alignment between text features and image features. In this way, a unique prompt is learned for different identities and clothes separately, providing precise guidance for the decoupling of clothing information from the original image.”",
            "III-D Loss Function – The first training stage: “In the first stage, we freeze the parameters of the image encoder and text encoder. And optimize the identity-dependent text prompt  and cloth-dependent text prompt  by contrastive learning… The contrastive learning loss for the first stage is defined by…which includes the image-text contrastive loss  and the text-image contrastive loss .”"
        ],
        "final_answer": "The need to learn identity-dependent and clothing-dependent text prompts via image–text contrastive learning (i.e., using contrastive losses to align frozen image and text encoder features) drives the sequential CIS prompt learning before any fine-tuning of the image encoder.",
        "relevant_elements": [
            "Clothing Information Stripping (CIS) module",
            "Image Encoder"
        ],
        "id": 2075,
        "masked_question": "What drives sequential [mask1] prompt learning before [mask2] fine-tuning in the two-stage training?",
        "masked_number": 2,
        "masked_elements": [
            "Clothing Information Stripping (CIS) module",
            "Image Encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.00330v1_figure_2.png",
        "paperid": "2411.00330v1",
        "paper_path": "./MISS-QA/papers/2411.00330v1.json",
        "figure_id": "2411.00330v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the proposed MIPL approach. It consists of the backbone, the CIS module, the BGA module, and the DHP module. ’SCHP’ is a semantic analysis module to obtain the human semantic information. ’Norm.’ and ’Trans.’ denote the normalization and transpose operations, respectively.",
        "qtype": "Design_Rationale",
        "response": "The very short answer is: nothing mystical is hiding under the masks – it is simply the two‐stage CLIP-style prompt-tuning recipe that compels you to learn your text prompts first (red box) under a frozen encoder with a contrastive loss, and only once those “identity” and “clothing” prompts are stable do you unlock (unfreeze) the visual backbone (blue box) and fine-tune it with your ReID losses (cross-entropy, triplet, clothing‐stripping, bio-guided, etc.).  \n\nChain of thought:  \n1.  In stage 1 (the red‐outlined CIS module), both the CLIP image and text encoders are frozen and you only optimize the learnable text tokens (one set for identity, one set for clothing).  \n2.  This prompt learning is driven entirely by an image–text contrastive loss (and its text–image counterpart), so you get very clean, semantically meaningful prompts before touching the vision model.  \n3.  In stage 2 (once the prompts are fixed), you freeze the text side and then unfreeze the blue‐boxed image encoder.  Now you train the vision model with the standard ReID objectives (cross-entropy + triplet) plus the bespoke clothing-stripping and bio-guided losses.  \n4.  By separating the two steps – contrastive prompt learning first, vision fine-tuning second – the model avoids overwhelming the prompt vectors and the vision backbone at the same time and guarantees better decoupling of clothing vs. identity information."
    },
    {
        "question": "How does the CIS module optimise learnable prompt tokens to align clothing image and text embeddings?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "Specifically, in the first training stage, a set of learnable prompt words are introduced, which are an identity-dependent text prompt (\"A photo of a [X] person.\") and a clothes-dependent text prompt (\"A photo of a [X] clothes.\"). … Then we use the text encoder and image encoder with frozen parameters to obtain the corresponding text features T_id, T_clo and image features F_img_ori, F_img_clo … A contrastive learning loss function is used to constrain the alignment between text features and image features.",
            "The first training stage. In the first stage, we freeze the parameters of the image encoder and text encoder, and optimize the identity-dependent text prompt P_id and cloth-dependent text prompt P_clo by contrastive learning … The contrastive learning loss for the first stage is defined by L_stage1 = L_i2t + L_t2i, where i and t denote image-to-text and text-to-image contrastive terms."
        ],
        "final_answer": "In CIS’s first stage, the module introduces learnable clothing prompts and keeps both the CLIP-based image and text encoders frozen. It then jointly optimises the prompt tokens by applying a bi-directional contrastive loss (image-to-text and text-to-image), thereby aligning the learned text embeddings of the clothing prompts with the corresponding frozen clothing image embeddings.",
        "relevant_elements": [
            "CIS module",
            "clothing prompt",
            "Text Encoder"
        ],
        "id": 2076,
        "masked_question": "How does the [mask1] optimise learnable prompt tokens to align clothing image and text embeddings?",
        "masked_number": 1,
        "masked_elements": [
            "CIS module"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.00330v1_figure_2.png",
        "paperid": "2411.00330v1",
        "paper_path": "./MISS-QA/papers/2411.00330v1.json",
        "figure_id": "2411.00330v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the proposed MIPL approach. It consists of the backbone, the CIS module, the BGA module, and the DHP module. ’SCHP’ is a semantic analysis module to obtain the human semantic information. ’Norm.’ and ’Trans.’ denote the normalization and transpose operations, respectively.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed sub‐network in Figure 2 is exactly the Clothing Information Stripping (CIS) module, and in particular its first training stage.  In that stage the authors do the following to “teach” the text prompt tokens to line up with clothing‐only image features:\n\n 1.  They freeze both the image and text encoders (these are standard CLIP encoders pre‐trained on large image–text pairs).  \n 2.  They replace the usual manually written template (“A photo of a T-shirt,” say) with a small set of _learnable_ token vectors \\[X\\]i, concatenated into a prompt “A photo of a \\[X\\]1 \\[X\\]2 … \\[X\\]M clothes.”  \n 3.  They run the clothing patch (obtained via SCHP human parsing) through the frozen image encoder to get a clothing‐only image embedding Fimg_clo.  \n 4.  They run the prompt “A photo of a \\[X\\]1… clothes.” through the frozen text encoder to get a text embedding Ftext_clo.  \n 5.  They impose a CLIP‐style _contrastive loss_ (both image→text and text→image directions) so that Fimg_clo and Ftext_clo have high cosine similarity for correct pairs and low similarity for mismatches.  \n 6.  By back‐propagating _only_ into the prompt token vectors \\[X\\]i (all encoder weights remain frozen), the model “learns” exactly which token embeddings cause the clothing image features and the clothing‐describing text features to align.  \n\nIn short, CIS (Stage 1) turns on only the prompt tokens, freezes the rest, and uses an image–text contrastive objective to pull the learned “clothing prompt” embeddings into the same space as the corresponding clothing‐image embeddings."
    },
    {
        "question": "How does the BGA module use normalization and transpose operations to compute biological attention weights?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The attention enhancement operation is performed through F_ori_clone^img and F_bio^img to explicitly emphasize the information of the biological key regions, and the information enhancement features F_enh^img for the model to learn the prompts are obtained.",
            "The operation is defined by [formula] where N represents the normalization operation, T represents the transpose operation, ⨂ represents matrix multiplication, and M indicates the biological key information mask."
        ],
        "final_answer": "Within the BGA module, the cloned original image feature and the biological feature are each first normalized (N). Then the normalized original feature is matrix-multiplied (⨂) with the transpose (T) of the normalized biological feature to produce a raw attention map. This attention map is then masked by the biological key-area mask M to yield the biological attention weights, which are finally used to re-weight and enhance the original feature into F_enh^img.",
        "relevant_elements": [
            "BGA module",
            "Norm.",
            "Trans."
        ],
        "id": 2077,
        "masked_question": "How does the [mask1] use normalization and transpose operations to compute biological attention weights?",
        "masked_number": 1,
        "masked_elements": [
            "BGA module"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.00330v1_figure_2.png",
        "paperid": "2411.00330v1",
        "paper_path": "./MISS-QA/papers/2411.00330v1.json",
        "figure_id": "2411.00330v1_figure_2.png",
        "caption": "Figure 2: Pipeline of the proposed MIPL approach. It consists of the backbone, the CIS module, the BGA module, and the DHP module. ’SCHP’ is a semantic analysis module to obtain the human semantic information. ’Norm.’ and ’Trans.’ denote the normalization and transpose operations, respectively.",
        "qtype": "Implementation_Details",
        "response": "In the figure, the red‐boxed block is the Bio–Guided Attention (BGA) module, and the missing “mask1” line is simply the little equation that shows how BGA computes its attention map.  Concretely, it does the following:\n\n1.  Take the original feature map F ori (a sequence of token embeddings) and the biological‐key feature map F bio.  \n2.  Apply a per‐token normalization (“Norm.”) to each, so that every token is unit length.  \n3.  Transpose (“Trans.”) the normalized original feature sequence so that its token dimension is on the opposite axis.  \n4.  Perform a matrix multiply:  \n       Attention =  Norm(F bio)   ⊗   [Norm(F ori)]ᵀ  \n   This gives a score for how strongly each biological token attends to each original token.  \n5.  Finally, you multiply that attention map back into F ori (and add it back) to produce the enhanced feature F enh.\n\nBecause both inputs were L₂–normalized, their dot–product produces cosine‐similarity scores, and the transpose is just the usual trick to line up the dimensions for a token‐to‐token similarity matrix."
    },
    {
        "question": "How does local control of complex amplitude modulate incoming wavefronts at each (x,y) coordinate?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "local control of complex amplitude"
        ],
        "id": 2078,
        "masked_question": "How does [mask1] modulate incoming wavefronts at each (x,y) coordinate?",
        "masked_number": 1,
        "masked_elements": [
            "local control of complex amplitude"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.09133v1_figure_1.png",
        "paperid": "2411.09133v1",
        "paper_path": "./MISS-QA/papers/2411.09133v1.json",
        "figure_id": "2411.09133v1_figure_1.png",
        "caption": "Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed panel in Fig. 1 b is labeled “local control of complex amplitude.”  In other words, the metasurface is acting as a spatially varying complex‐valued mask.  Concretely, at each point (x,y) on the aperture the local meta‐atom is designed to impose a prescribed complex transmission (or reflection) coefficient t(x,y), so that if the incident field is Ein(x,y) then the outgoing field is  \n\n    Eout(x,y) = t(x,y) · Ein(x,y) .  \n\nBecause t(x,y) can be engineered to have both an amplitude and a phase, the metasurface “pixels” each sculpt the incoming wavefront in exactly the way you would with a pixelated complex filter—modulating amplitude and retarding phase locally across the (x,y) plane."
    },
    {
        "question": "How are sparsity and complexity priors integrated into the least-square reconstruction algorithm?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "least-square",
            "priors"
        ],
        "id": 2079,
        "masked_question": "How are sparsity and complexity [mask1] integrated into the least-square reconstruction algorithm?",
        "masked_number": 1,
        "masked_elements": [
            "priors"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.09133v1_figure_1.png",
        "paperid": "2411.09133v1",
        "paper_path": "./MISS-QA/papers/2411.09133v1.json",
        "figure_id": "2411.09133v1_figure_1.png",
        "caption": "Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field.",
        "qtype": "Implementation_Details",
        "response": "In the simplest “vanilla” least‐squares reconstruction one would solve  \n minₓ ‖A x – y‖₂²  \nfor your unknown image or scene parameters x.  What the red‐boxed inset in Fig. 1(c) is calling out is that you don’t stop there: you tack on extra penalty (or constraint) terms that encode your prior beliefs about x.  In practice that means you solve a penalized regression of the form  \n minₓ ‖A x – y‖₂² + λ₁‖x‖₁ + λ₂‖x‖₂².  \n\n– The ‖x‖₁ term is the ​sparsity​ prior (it forces x into the “diamond,” i.e. an ℓ₁ ball in your DOF‐space).  \n– The ‖x‖₂² term is the low‐​complexity​ (Tikhonov) prior (it keeps x within the “round,” i.e. an ℓ₂ ball).  \n\nBy tuning the weights λ₁ (sparsity) and λ₂ (complexity) you trade off data fidelity against how sparse or how “simple” you believe the true solution should be."
    },
    {
        "question": "How does spectral dispersion inform MLE for reconstructing frequency degrees of freedom?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "spectral dispersion",
            "MLE",
            "frequency"
        ],
        "id": 2080,
        "masked_question": "How does [mask1] inform [mask2] for reconstructing frequency degrees of freedom?",
        "masked_number": 2,
        "masked_elements": [
            "spectral dispersion",
            "MLE"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.09133v1_figure_1.png",
        "paperid": "2411.09133v1",
        "paper_path": "./MISS-QA/papers/2411.09133v1.json",
        "figure_id": "2411.09133v1_figure_1.png",
        "caption": "Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field.",
        "qtype": "Literature_Background",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  Identify the two highlighted blocks in Fig. 1:  \n    – The red‐boxed physics block is “spectral dispersion.”  \n    – The blue‐boxed algorithms block is “MLE” (maximum‐likelihood estimation).\n\n2.  What does spectral dispersion do?  \n    A metasurface with engineered spectral dispersion routes each wavelength to a different spatial channel on the sensor (i.e. it gives each frequency its own point‐spread function or look–up signature).\n\n3.  What does MLE do?  \n    MLE builds a statistical forward model of how the incident spectrum produces the observed detector counts (including the noise model) and then finds the spectrum that maximizes the likelihood of having produced the measurements.\n\n4.  How are they linked?  \n    – The spectral‐dispersion stage defines exactly how each frequency maps to the sensor (this is your measurement matrix or set of PSFs for each λ).  \n    – MLE uses that known, wavelength‐dependent mapping inside its likelihood function. By comparing the actual detector readout to the predictions generated by each candidate spectrum (via the dispersion‐defined forward model), it picks the spectrum that most probably gave rise to the data.\n\n5.  Conclusion:  \n    Spectral dispersion provides the wavelength‐to‐sensor mapping (the forward model), and MLE inverts that mapping—under a chosen noise model—to recover the underlying frequency content that best explains the measured image."
    },
    {
        "question": "How does non-locality support neural networks in momentum reconstruction?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "non-locality",
            "neural networks",
            "momentum"
        ],
        "id": 2081,
        "masked_question": "How does [mask1] support [mask2] in momentum reconstruction?",
        "masked_number": 2,
        "masked_elements": [
            "non-locality",
            "neural networks"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.09133v1_figure_1.png",
        "paperid": "2411.09133v1",
        "paper_path": "./MISS-QA/papers/2411.09133v1.json",
        "figure_id": "2411.09133v1_figure_1.png",
        "caption": "Figure 1: Computational imaging with metaoptics: degrees of freedom, physics, and algorithms. a. The general goal of a computational imaging device is to reconstruct various degrees of freedom of an incident light field, for instance its polarization, frequency, momentum, and complex amplitude distribution. Advanced degrees of freedom (e.g., density matrix of the quantum state of light) may also be of interest. b. Light manipulation is realized by leveraging physical properties of metaoptical devices, such as their ability to locally control the complex amplitude of an incoming wavefront, engineered spectral dispersion, non-locality (spatial dispersion), active control of physical properties (e.g., complex transmission), and nonlinear optical properties. c. Once imaged by a detector, the signal may be reconstructed using various reconstruction and estimation methods, such as least-square error minimization (which may include priors on the reconstructed degrees of freedom, such as high sparsity or low complexity). Black-box methods, such as fully connected neural networks, may also be utilized to classify detected signals and images. Other parameter estimation methods, such as maximum likelihood estimation (MLE) may also be used to estimate the degrees of freedom of the incident light field.",
        "qtype": "Literature_Background",
        "response": "In our framework (Fig. 1b–c), the “red‐boxed” physics knob is non-locality – i.e. spatial dispersion in the metasurface – and the “blue-boxed” reconstruction engine is a neural network.  What non-locality buys you is a highly angle-sensitive, spatially coupled optical response: different incident k-vectors (momenta) get mapped onto very different intensity–phase patterns across the sensor.  Those patterns are in effect a high‐dimensional, optically preconditioned feature map of the incoming momentum distribution.  A neural network can then be trained to invert that mixing and read out the original momenta with high fidelity.\n\nIn other words,\n\n 1. Non-locality (spatial dispersion) in the metasurface introduces strong, momentum-dependent coupling between different points on the aperture.  \n 2. That coupling scrambles each incoming k-vector into a distinct two-dimensional intensity pattern.  \n 3. The neural network uses those patterns as a richly informative feature set, learns the inverse map, and thus reconstructs the incident momentum distribution much more robustly than if the metasurface were purely local.  \n\nBy “pre-encoding” momentum into a high-dimensional, spatially interleaved signal, the non-local metasurface makes the neural-network inversion problem both well conditioned and data-efficient."
    },
    {
        "question": "How might in-context learning strategies shape Prompt Template Design for Code Translation scenarios?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "The half-bottom part of Figure 3 shows the prompt templates for the Code translation task in RMCBench. We also refer to EgoAlpha’s prompt-in context-learning (EgoAlpha, 2024) to design the prompts.",
            "Specifically, {malicious code} is the original and complete malicious code we have collected, {original language} is the programming language of the original code, and {target language} is the target language to be translated into."
        ],
        "final_answer": "The authors leverage in-context learning principles (as in EgoAlpha) to structure their code-translation templates. Each prompt pairs a concrete code example ({malicious code}) with explicit natural-language instructions ({original language} → {target language}). By embedding the code snippet and specifying source/target languages directly in the template, the model receives the necessary context and demonstration cues to perform the translation task.",
        "relevant_elements": [
            "Prompt Template Design",
            "Code Translation Prompts"
        ],
        "id": 2082,
        "masked_question": "How might in-context learning strategies shape [mask1] for Code Translation scenarios?",
        "masked_number": 1,
        "masked_elements": [
            "Prompt Template Design"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.15154v1_figure_2.png",
        "paperid": "2409.15154v1",
        "paper_path": "./MISS-QA/papers/2409.15154v1.json",
        "figure_id": "2409.15154v1_figure_2.png",
        "caption": "Figure 2. Overview of RMCBench Construction Process. T2C-L represents text-to-code level, CC represents code completion, CT represents code translation.",
        "qtype": "Literature_Background",
        "response": "First, let’s pin down what the red‐boxed region is in Figure 2.  That box surrounds the very first step of the whole pipeline: “Prompt Template Design.”  In other words, whenever RMCBench builds a new task—whether it’s text-to-code or code-to-code—the authors start by sketching out a high-level template that says:\n\n  •  Here goes your natural-language instruction,  \n  •  here go any few‐shot examples or special tags,  \n  •  here goes the piece of code (or code with holes),  \n  •  here’s the format we expect your answer in.\n\nSo [mask1] = “Prompt Template Design.”\n\n---\n\nNow:   “How might in-context learning strategies shape Prompt Template Design for Code Translation scenarios?”\n\nChain-of-thought:\n\n1.  **Role of in-context learning (ICL):**  In-context learning is the practice of putting worked examples or special cues directly into the LLM’s prompt so that it picks up the pattern “on the fly” without further fine-tuning.\n\n2.  **Code-translation setting:**  In RMCBench, a code-translation prompt looks roughly like:\n\n      ```\n      // Translate the following malicious Python function into JavaScript\n      {malicious code in Python}\n      ```\n\n3.  **Injecting few-shot examples:**  Under an ICL strategy, you would prepend 1–k exemplar pairs of already-translated snippets:\n\n      ```\n      // EXAMPLE 1\n      // Python input:\n      def encrypt(data): …  \n      // JavaScript output:\n      function encrypt(data) { … }\n\n      // EXAMPLE 2\n      // Python input:\n      def send_request(url): …  \n      // JavaScript output:\n      async function send_request(url) { … }\n\n      // NOW YOU:\n      // Python input:\n      {new malicious code}\n      // JavaScript output:\n      ```\n\n   By doing this, you give the LLM a clear template of “here’s how we frame the task, here’s how we present input and output.”  \n\n4.  **Choosing the right number and diversity of shots:**  ICL teaches us that 1–2 examples often help, but more may introduce noise or exceed token limits.  You would experiment with different shot counts, selecting examples that cover the kinds of language constructs and malicious patterns you care about (loops, network calls, file I/O, etc.).\n\n5.  **Formatting decisions:**  ICL strategies also drive choices like\n   – using explicit markers (`// EXAMPLE`, `——`),  \n   – aligning indentation,  \n   – naming your functions consistently,  \n   – inserting inline comments to emphasize “semantic equivalence,”  \n   – or even adding a mini‐chain-of-thought hint (“First, map Python’s `open()` to JS `fs.open()`, then port the loop logic…”).\n\n6.  **Placeholders and instruction phrasing:**  You may decide to use a tag such as `<SOURCE>` and `<TARGET>` so that the model knows exactly where the untranslated code goes and where the translated code should appear.  The natural-language instruction (“Please perform a line-by-line translation preserving side-effects and error handling”) can be tuned via ICL as well, by showing good versus bad translations in the examples.\n\n---\n\nPutting it all together:\n\nIn-context learning strategies will directly dictate how you craft your “Prompt Template Design” for code translation.  Instead of a bare template, you wrap it in a few‐shot shell, carefully selecting and formatting demonstration pairs, choosing how many examples to include, deciding on placeholder tokens, and sprinkling in mini-chain-of-thought hints.  All of those ICL decisions become baked into your prompt template right at the “Prompt Template Design” stage."
    },
    {
        "question": "How do jailbreak attack methodologies inform Data Collection for Level 3 prompts?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Level 3 (T2C-L3) prompts are built based on Level 2 prompts, which consist of two components: a jailbreak template and the original prompt from Level 2.",
            "To build the Level 3 prompt, we need to connect the Level 2 prompts with the jailbreak templates. jailbreakchat.com (Albert, 2023) is a famous website that collects jailbreak templates, and many studies (Liu et al., 2023a; Wei et al., 2024; Puttaparthi et al., 2023; Deng et al., 2023) related to jailbreaks have used the data from it. Note that the website is no longer accessible as of June 2024. Thus, we used all the available jailbreak templates (a total of 78) by the time.",
            "Many jailbreak prompts from jailbreakChat.com are designed for ChatGPT and often begin with \"Hi, ChatGPT…\". To ensure consistency when testing other LLMs, we need to modify these jailbreak templates. For instance, when testing Llama2, we change the original salutation words to “Hi, Llama…” This adaptation is important, as our preliminary experiment finds that if we call Llama “ChatGPT”, Llama will prioritize correcting its identity instead of asking its actual task.",
            "We construct a complete Level 3 prompt by integrating jailbreak templates with Level 2 prompts. Given the extensive possibility of 7,956 (102*78) combinations, to maintain a balance in quantity relative to the other two levels of prompts, we randomly select 100 Level 3 prompts from the 7,956 combinations for further empirical study."
        ],
        "final_answer": "Jailbreak attack methodologies inform Level 3 data collection by providing a set of real-world prompt-injection templates—specifically, 78 jailbreak templates sourced from jailbreakChat.com—that are known to bypass LLM safety constraints. These templates are adapted (e.g. renaming “Hi, ChatGPT…” to “Hi, Llama…”) to target each model, then systematically combined with Level 2 natural-language prompts. From the resulting 7,956 possible combinations, 100 are randomly sampled to form the final Level 3 prompt dataset.",
        "relevant_elements": [
            "Data Collection",
            "Level 3 Prompts"
        ],
        "id": 2083,
        "masked_question": "How do jailbreak attack methodologies inform [mask1] for Level 3 prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Data Collection"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.15154v1_figure_2.png",
        "paperid": "2409.15154v1",
        "paper_path": "./MISS-QA/papers/2409.15154v1.json",
        "figure_id": "2409.15154v1_figure_2.png",
        "caption": "Figure 2. Overview of RMCBench Construction Process. T2C-L represents text-to-code level, CC represents code completion, CT represents code translation.",
        "qtype": "Literature_Background",
        "response": "Mask1 refers to the “Data Collection” box (the red‐outlined module showing the GitHub and Jailbreak Chat logos).  Concretely, all of our Level 3 prompts are built by harvesting real‐world jailbreak attack templates (78 in total) from jailbreakchat.com (and related GitHub repos), adapting them (e.g. changing “Hi, ChatGPT…” to “Hi, Llama…”), and then pairing each one with a Level 2 malicious‐functionality prompt.  Those methodology‐derived templates feed directly into the Data Collection stage, which in turn supplies the raw material for constructing the Level 3 jailbreak prompts."
    },
    {
        "question": "How does Prompt Template Design adapt to integrate Level 3 Prompts for enhanced adversarial testing?",
        "relevant_section_ids": [
            "3.2",
            "3.2.2"
        ],
        "relevant_context": [
            "Level 3 (T2C-L3) prompts are built based on Level 2 prompts, which consist of two components: a jailbreak template and the original prompt from Level 2.",
            "To build the Level 3 prompt, we need to connect the Level 2 prompts with the jailbreak templates. jailbreakchat.com is a famous website that collects jailbreak templates, and many studies related to jailbreaks have used the data from it. Note that the website is no longer accessible as of June 2024. Thus, we used all the available jailbreak templates (a total of 78) by the time.",
            "Many jailbreak prompts from jailbreakChat.com are designed for ChatGPT and often begin with \"Hi, ChatGPT…\". To ensure consistency when testing other LLMs, we need to modify these jailbreak templates. For instance, when testing Llama2, we change the original salutation words to “Hi, Llama…”",
            "We construct a complete Level 3 prompt by integrating jailbreak templates with Level 2 prompts. Given the extensive possibility of 7,956 (102*78) combinations, to maintain a balance in quantity relative to the other two levels of prompts, we randomly select 100 Level 3 prompts from the 7,956 combinations for further empirical study."
        ],
        "final_answer": "During Prompt Template Design, the authors extend their Level 2 text-to-code templates by prefixing each with a jailbreak template. They collect 78 jailbreak instructions from jailbreakchat.com, adapt the opening salutation for each target model (e.g., changing “Hi, ChatGPT…” to “Hi, Llama…”), and then merge each modified jailbreak template with the corresponding Level 2 prompt. From the resulting 7,956 possible combinations (102 Level 2 prompts × 78 jailbreak templates), they randomly sample 100 to serve as adversarial Level 3 prompts.",
        "relevant_elements": [
            "Prompt Template Design",
            "Level 3 Prompts"
        ],
        "id": 2084,
        "masked_question": "How does [mask1] adapt to integrate [mask2] for enhanced adversarial testing?",
        "masked_number": 2,
        "masked_elements": [
            "Prompt Template Design",
            "Level 3 Prompts"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.15154v1_figure_2.png",
        "paperid": "2409.15154v1",
        "paper_path": "./MISS-QA/papers/2409.15154v1.json",
        "figure_id": "2409.15154v1_figure_2.png",
        "caption": "Figure 2. Overview of RMCBench Construction Process. T2C-L represents text-to-code level, CC represents code completion, CT represents code translation.",
        "qtype": "Experimental_Results",
        "response": "Here’s the rough chain of thought:\n\n1. In the figure, the red‐boxed item is the “Prompt Template Design” step in the overall workflow.  \n2. The blue‐boxed item is “Level 3 Prompts,” which (per § 3.2.2) are adversarial, jailbreak‐style prompts built on top of the Level 2 text‐to‐code prompts.  \n3. To create Level 3, the authors take each Level 2 prompt and prepend a jailbreak template harvested from jailbreakChat.com.  \n4. Because most jailbreaks address “ChatGPT,” during the Prompt Template Design pass they systematically edit those templates (e.g. changing “Hi, ChatGPT…” to “Hi, Llama2…” when targeting Llama) so that the jailbreaker will actually obey the malicious instruction rather than correct its own identity.  \n5. Thus the Prompt Template Design stage “adapts” by merging (1) a cleaned‐up, model‐specific jailbreak preamble with (2) the original Level 2 prompt to yield a true Level 3 adversarial prompt.  \n\nAnswer:  \nPrompt Template Design is extended to incorporate Level 3 by taking each Level 2 text‐to‐code prompt and wrapping it in a customized jailbreak template—updating salutations (e.g. from “ChatGPT” to the target LLM’s name) and stitching the two together—so that the resulting Level 3 prompts mount a stronger adversarial (jailbreak) attack."
    },
    {
        "question": "How does Data Collection ensure Code Completion Prompts capture genuine malicious code diversity?",
        "relevant_section_ids": [
            "3.3",
            "3.3.1"
        ],
        "relevant_context": [
            "Constructing a code completion prompt requires malicious code. In Section 3.2.1, we have collected raw data of malicious code from Github.",
            "Thus, we applied the following filters: (a) the malicious code in a single file must be independent, i.e., its malicious functional components do not rely on third-party libraries or files; (b) only the source code files are retained, and executable files and assembly files (such as files with .bin and .exe extensions) are not excluded. Through filtering, we obtained a total of 91 samples of malicious code.",
            "Inspiring by previous works, we hollowed out sections from the collected malicious code samples according to the following rules: (a) For code with multiple functions, we randomly remove one complete function. (b) For single-function code that is divided into multiple parts by empty lines, we randomly remove one part. (c) For continuous code that lacks empty line separations, we perform random line-level or token-level hollowing at the end of certain lines. Then, the hollowed-out parts are replaced with a “<FILL_HERE>” placeholder to indicate where completion is needed. After hollowing out, we ensure that the remaining code context contains sufficient malicious information. After that, comments are added before the placeholder to detail the specific functionality of the removed sections. This process ensures that the modified code maintains its original malicious intent. The average number of lines of code in the hollowed-out part is 3.8, with a maximum value of 17.",
            "To make our prompts more diversity, we utilized the approach outlined in CoderEval to design another prompt method. This method involves providing the function signature and the first line definition of the malicious code (also summarized by ChatGPT-3.5 based on the provided malicious code), allowing it to complete the remaining code (a total of 20). Finally, the number of prompts for the malicious code completion task is 100 in total."
        ],
        "final_answer": "The Data Collection phase for Code Completion Prompts begins by harvesting real malicious code from GitHub (392 repositories) and filtering it down to 91 self-contained source files so that each sample captures a distinct, standalone malicious functionality. It then applies three randomized ‘hollowing’ strategies—removing whole functions, contiguous code blocks, or random lines/tokens—and replaces them with “<FILL_HERE>” placeholders (while preserving contextual comments) to maintain the code’s original malicious intent. Finally, it adds a secondary prompt style based on just the function signature and first line (via the CoderEval method), yielding 100 diverse completion tasks that together span genuine malicious behaviors across different structures and languages.",
        "relevant_elements": [
            "Data Collection",
            "Code Completion Prompts"
        ],
        "id": 2085,
        "masked_question": "How does [mask1] ensure Code Completion Prompts capture genuine malicious code diversity?",
        "masked_number": 1,
        "masked_elements": [
            "Data Collection"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.15154v1_figure_2.png",
        "paperid": "2409.15154v1",
        "paper_path": "./MISS-QA/papers/2409.15154v1.json",
        "figure_id": "2409.15154v1_figure_2.png",
        "caption": "Figure 2. Overview of RMCBench Construction Process. T2C-L represents text-to-code level, CC represents code completion, CT represents code translation.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought:\n\n1. In our pipeline (Figure 2) the red-boxed icon under “Data Collection” is GitHub.  It is GitHub that supplies all of the raw malicious code used to build the Code-Completion (CC) prompts.\n\n2. To make sure those CC prompts really reflect the wild diversity of malware, we don’t just grab one or two toy examples.  We:\n   • Search GitHub for any repo tagged “malware” or “malicious code” with ≥ 200 stars → 392 repos  \n   • Manually weed out non-malicious projects, dedupe overlapping functionality, and rephrase summaries for clarity  \n   • Filter for fully independent source files (no hidden external dependencies), discarding binaries, compiled artifacts, etc.  \n   • End up with 91 distinct source‐file samples that span 9 programming languages and 11 malware categories  \n   • From each of those genuine code samples we “hollow out” random functions, code blocks or lines, insert the <FILL_HERE> placeholder, and prepend a short comment describing what the missing section does.\n\n3. By pulling straight from hundreds of real‐world malicious repos, by enforcing broad language coverage and malware categories, and by hollowing out at different granularities, the GitHub‐sourced data guarantees that our CC prompts capture the true variety of malicious code “in the wild.”  \n\nAnswer:\n\nmask1 (the GitHub data‐collection step) ensures genuine malicious-code diversity for the Code Completion prompts by mining hundreds of real-world malware repositories, filtering for independent source files across multiple languages and malware types, and then hollowing out varied code snippets so that each prompt reflects a different real malicious coding pattern."
    },
    {
        "question": "How does the generator use sequence outputs to produce perturbations preserving classification utility while removing bias?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To guarantee the semantic and fairness properties of generated perturbations, we generate them based on the semantic-rich sequence representation T.",
            "As shown in Figure 2, for each input text x, the generator G takes T as input to generate a fairness-aware perturbation mask δ of the same dimension with z.",
            "The fair classification representation z′ is then obtained by superimposing δ into z, i.e., z′ = z + δ.",
            "The second part is for the utility-preserving purpose, which is also viewed as the optimization objective of Fθ. Thus, it can be formalized as L_util = E_{(x,y)∼P} [CE(Fθ(z′), y)] where a small hyperparameter controls regularization.",
            "Hence, the final loss function L_G of G is calculated by L_G = L_fair + β L_util, balancing the adversarial fairness goal with classification utility."
        ],
        "final_answer": "The generator G takes the semantic-rich sequence representation T (the full token-level outputs of the BPLM) as its input and produces a perturbation mask δ of the same dimension as the pooled [CLS] vector z. This mask δ is designed so that when added to z (i.e. z′ = z + δ), it removes information predictive of the sensitive attribute (minimizing L_fair) while simultaneously incurring minimal damage to downstream task performance (minimizing the classification loss L_util). The joint loss L_G = L_fair + β L_util therefore ensures the perturbations are both bias-removing and utility-preserving.",
        "relevant_elements": [
            "Generator",
            "Sequence Outputs"
        ],
        "id": 2086,
        "masked_question": "How does the [mask1] use [mask2] to produce perturbations preserving classification utility while removing bias?",
        "masked_number": 2,
        "masked_elements": [
            "Generator",
            "Sequence Outputs"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.08189v1_figure_2.png",
        "paperid": "2407.08189v1",
        "paper_path": "./MISS-QA/papers/2407.08189v1.json",
        "figure_id": "2407.08189v1_figure_2.png",
        "caption": "Figure 2. The framework of fairBERTs.",
        "qtype": "Experimental_Results",
        "response": "In the picture, the red‐boxed module is the generator “G” and the blue‐boxed feature is the full sequence of token embeddings coming out of BERT (which we have been calling  in the text).  The way G uses those sequence outputs to produce a debiasing perturbation that “breaks” sensitive information but keeps task accuracy is as follows:\n\n 1.   Input to G  G takes as input the entire sequence representation  \n     –  = [h₁, h₂, …, hₙ], the final hidden state of each token.  \n     – This “semantic‐rich” representation still contains both class‐relevant and sensitive bits of information.  \n\n 2.   Mask generation  A very small neural network (a few‐layer MLP with LeakyReLU units) transforms the token‐level sequence embeddings  into a single perturbation vector m whose dimensionality matches the pooled [CLS] embedding  \n     –  m = G(  )  \n\n 3.   Perturb + pool  That mask m is then added to the original pooled [CLS] vector h_cls:  \n          h_fair = h_cls + m  \n\n 4.   Adversarial & utility losses  G is trained with two competing objectives:  \n     a)  Adversarial fairness loss L_adv:  G tries to fool a discriminator D that is trying to predict the sensitive label z from h_fair.  By maximizing D’s error, G “erases” sensitive cues.  \n     b)  Classification preservation loss L_cls:  G is simultaneously penalized if the downstream classifier F_θ makes worse predictions on the true label y; this keeps the perturbed representation still good for the original task.  \n\n 5.   Balance and update  A single combined loss  \n           \n     is used, with a hyper‐parameter λ to trade off “how strongly do we want to hide z?” vs. “how much do we insist on keeping y‐accuracy?”  \n     G is updated to minimize this loss (i.e. to both confuse D and not degrade F_θ).  \n\nBy drawing its input from the full sequence  rather than just the pooled vector, G can create a perturbation that (1) reaches into the rich semantic contexts and (2) surgically removes the sensitive dimensions without collapsing the entire class‐relevant signal.  That is why the generator uses the blue‐boxed sequence outputs to build a mask that preserves classification utility while stripping away bias."
    },
    {
        "question": "How does feedback from the discriminator refine generator’s perturbations to obscure sensitive attribute signals?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The first discriminator D tries to distinguish sensitive attribute s from the perturbed latent representation z'.",
            "In contrast to D, the generator G aims to make it hard for D to predict s, while also ensuring that the generated perturbations would not destroy the semantic and classification utility of the original representation.",
            "Therefore, there are also two parts in the optimization objective of G. The first part is for the fairness purpose, which can be defined as the negative of D’s cross-entropy loss on predicting s from the perturbed representation."
        ],
        "final_answer": "The generator receives gradient feedback from the discriminator’s classification loss on the perturbed representations. Whenever the discriminator successfully predicts the sensitive attribute, its loss gradient tells the generator how to adjust the perturbation mask so that future perturbations better obscure any sensitive‐attribute cues, thus gradually refining δ to fool the discriminator and remove sensitive signals.",
        "relevant_elements": [
            "Generator",
            "Discriminator"
        ],
        "id": 2087,
        "masked_question": "How does feedback from the [mask1] refine [mask2]’s perturbations to obscure sensitive attribute signals?",
        "masked_number": 2,
        "masked_elements": [
            "Discriminator",
            "Generator"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.08189v1_figure_2.png",
        "paperid": "2407.08189v1",
        "paper_path": "./MISS-QA/papers/2407.08189v1.json",
        "figure_id": "2407.08189v1_figure_2.png",
        "caption": "Figure 2. The framework of fairBERTs.",
        "qtype": "Experimental_Results",
        "response": "Here’s how it works step by step:\n\n1. Identify the two modules in question  \n   - [mask1] is the red‐boxed Discriminator D that tries to predict the sensitive attribute s from the perturbed representation z′.  \n   - [mask2] is the blue‐boxed Generator G that produces the perturbation Δ to add into the original latent z.\n\n2. The adversarial loop  \n   - During training D is trained to get better at guessing s from z′.  \n   - Whenever D succeeds, it incurs a low adversarial loss Ladv. Whenever it fails, Ladv is high.\n\n3. Feeding back into G  \n   - G’s adversarial objective is to maximize D’s loss Ladv (i.e. to make D perform as poorly as possible).  \n   - We take the gradient of D’s loss w.r.t. the generated mask Δ and back‐propagate it into G.  \n   - In effect, G “listens” to which bits of Δ still carry signal that D can exploit, and adjusts its weights to push those bits toward zero or to obscure them.\n\n4. Iterative refinement  \n   - Over many mini‐max steps D grows more sensitive to any remnant of the sensitive feature in z′, and G in turn refines Δ to remove exactly those residual cues.  \n   - The result is a learned perturbation that effectively “erases” the sensitive signal from the BERT embedding, while a separate utility loss keeps z′ still predictive of the target label.\n\nIn short, the discriminator’s classification loss on the perturbed representations provides a gradient signal that trains the generator to sculpt its mask so as to hide any remaining traces of the sensitive attribute."
    },
    {
        "question": "What limitations does Generator encounter in generating semantic & fairness-aware perturbations?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Generator",
            "semantic & fairness-aware perturbation"
        ],
        "id": 2088,
        "masked_question": "What limitations does [mask1] encounter in generating [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Generator",
            "semantic & fairness-aware perturbation"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.08189v1_figure_2.png",
        "paperid": "2407.08189v1",
        "paper_path": "./MISS-QA/papers/2407.08189v1.json",
        "figure_id": "2407.08189v1_figure_2.png",
        "caption": "Figure 2. The framework of fairBERTs.",
        "qtype": "Others",
        "response": "Here’s the reasoning in two steps:\n\n1.  Identify the two boxed components in the figure  \n    -  [mask1] (red box) is the little “G” block – the generator in the adversarial debiasing GAN.  \n    -  [mask2] (blue box) is the “semantic & fairness-aware perturbation” that G produces and then adds back into BERT’s hidden states.\n\n2.  What problems does G run into when it tries to produce those perturbations?  \n    -  **Simplicity vs. expressiveness.**  In order to keep training stable and fast, the paper deliberately makes G a very shallow, lightweight network.  That means it simply doesn’t have the capacity to learn very rich, high-dimensional perturbations.  \n    -  **Fairness–utility trade-off.**  G’s loss is a tug-of-war: it must both fool the discriminator into “not seeing” the protected attribute (fairness) and at the same time leave the downstream classifier’s accuracy untouched (utility).  In practice that leads to a delicate balancing act and can force G to compromise on one side or the other.  \n    -  **Transferability.**  Vanilla adversarial generators typically over-fit the one discriminator they were trained against and do not generalize well.  Without further work, the perturbations G learns will often not transfer cleanly to other models or tasks.  \n\nIn short, the generator is intentionally kept simple (limiting its capacity), is caught in a fundamental fairness-accuracy trade-off, and its learned perturbations tend to be brittle when moved to new models."
    },
    {
        "question": "What alternative strategies could supplement adversarial Discriminator to ensure robustness of semantic & fairness-aware perturbations?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, to assist in learning a better generator G, we propose counterfactual adversarial training by borrowing from conventional adversarial training schemes. In contrast to conventional adversarial training, our method aims to flip the sensitive labels by substituting tokens associated with identity groups (e.g., replacing “male” with “female”) without changing task-relevant class labels, which will help D and G locate sensitive information better while not impacting the task-specific model performance."
        ],
        "final_answer": "Augment adversarial training with counterfactual adversarial examples: flip sensitive labels by substituting identity-related tokens (e.g. “male”→“female”) so the discriminator and generator learn to locate and erase protected information more robustly without harming task performance.",
        "relevant_elements": [
            "Discriminator",
            "semantic & fairness-aware perturbation"
        ],
        "id": 2089,
        "masked_question": "What alternative strategies could supplement adversarial [mask1] to ensure robustness of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Discriminator",
            "semantic & fairness-aware perturbation"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.08189v1_figure_2.png",
        "paperid": "2407.08189v1",
        "paper_path": "./MISS-QA/papers/2407.08189v1.json",
        "figure_id": "2407.08189v1_figure_2.png",
        "caption": "Figure 2. The framework of fairBERTs.",
        "qtype": "Others",
        "response": "First, let’s align the two masked regions to their names in the paper:\n\n  • Mask₁ (red box) is the adversarial discriminator, D, that is trained to predict the sensitive attribute s from the (perturbed) pooled representation z′.  \n  • Mask₂ (blue box) is the generator, G, which produces the “semantic‐ & fairness‐aware perturbation” that is added to the original hidden state h to yield the debiased representation z′.\n\nSo the question becomes:\n\n  “What alternative strategies could supplement adversarial D (the discriminator) to ensure robustness of G (the semantic‐ & fairness‐aware perturbations)?”\n\nHere is a list of plausible, complementary techniques you could layer on top of or alongside your GAN‐style debiasing game to make those generated perturbations more stable, semantically faithful, and truly fair:\n\n  1. Reconstruction (autoencoder) losses  \n     – Train a small decoder R so that R(z′) ≃ R(h).  This forces G not only to confuse D but also to preserve the information in h that matters for your downstream task.  \n\n  2. Mutual‐Information Minimization  \n     – Add a term that explicitly penalizes I(z′; s), e.g. via an MINE‐style estimator or variational bound, rather than relying purely on the discriminator’s cross‐entropy.  \n\n  3. Orthogonal‐Projection or Subspace Removal  \n     – Identify the principal components or subspace in h most correlated with s (e.g. via PCA on embeddings labeled by s) and explicitly project them out of z′ at each step.  \n\n  4. Contrastive (Invariance) Objectives  \n     – For each example, generate two perturbations z′₁ and z′₂ with different random seeds or counterfactual token swaps, then pull them together in representation space while pushing away examples with different y.  This encourages G to learn perturbations that are invariant to s but discriminative for y.  \n\n  5. Spectral Normalization / Gradient Penalty  \n     – Stabilize the adversarial game by regularizing D (and/or G) with a Lipschitz constraint (Wasserstein‐GP, spectral norm).  This keeps gradients well‐behaved and avoids mode collapse in the perturbations.  \n\n  6. Auxiliary Task / Multi‐Task Learning  \n     – Add a small side‐classifier that must reconstruct task‐relevant attributes (e.g. topic, sentiment) from z′.  G then sees an additional loss term that prevents it from throwing away everything but fairness.  \n\n  7. Mixup / Data Augmentation  \n     – Instead of only perturbing the latent space, also do token‐level or embedding‐level mixup between examples of different sensitive groups.  This widens the support of your perturbation distribution and makes G more robust.  \n\n  8. Cycle‐Consistency  \n     – Learn a reverse generator G′ such that G′(z′) ≃ h.  Enforce ‖G′(G(h)) − h‖₂ to be small.  This cycle‐consistency penalty ensures G is not arbitrarily destroying the representational structure of h.\n\nAny one (or combination) of these techniques can be “dropped in” alongside your adversarial discriminator to give G stronger geometric, probabilistic or semantic constraints.  In practice you’ll often see a reconstruction term plus a gradient‐penalty on the discriminator and a small mutual‐information or orthogonality penalty on the generator to strike the best trade‐off between fairness, semantic fidelity and stability."
    },
    {
        "question": "How might sensitive user attributes be inferred from Pre-trained Graph Layer through User Side Metapath Expansion?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pre-trained Graph Layer",
            "User Side Metapath Expansion"
        ],
        "id": 2090,
        "masked_question": "How might sensitive user attributes be inferred from [mask1] through [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Pre-trained Graph Layer",
            "User Side Metapath Expansion"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.00056v1_figure_2.png",
        "paperid": "2407.00056v1",
        "paper_path": "./MISS-QA/papers/2407.00056v1.json",
        "figure_id": "2407.00056v1_figure_2.png",
        "caption": "Figure 2. The overall framework of MMBee, consists of two stages: (i) the offline Graph-guided Interest Expansion (GIE) stage conducts the behavior expansion based on the target user and author; (ii) the online GTR prediction stage aggregates the real-time multi-modal content and expanded behavior for end-to-end training.",
        "qtype": "Others",
        "response": "Here is one concrete way an attacker (or even the platform itself) could go from the red‐boxed component (“Pre-trained Graph Layer θ”) all the way to inferring sensitive attributes by exploiting the blue‐boxed offline metapath expansions:\n\n1.  **Step 1 – Obtain the user’s graph embedding**  \n    - At serving time the online model uses the Pre-trained Graph Layer θ to look up a low-dimensional embedding   \n      h₍u₎ = θ(u)   \n      for the target user.  This vector encodes “who they have gifted to” (U→A edges) and “which authors are similar” (A→A edges), but in a form that is _not_ human-readable.  \n\n2.  **Step 2 – Pull out the expanded neighbors from offline storage**  \n    - In the blue box the system has pre–materialized all k-hop metapath walks from each user, e.g.  \n        • U–A–U (other users who have donated to the same authors),  \n        • U–A–A (similar authors to those the user has donated to),  \n        • A–A–U (users who donated to authors similar to the target author), etc.  \n    - These neighbor lists (and their embeddings) sit in a fast key–value store so you can fetch, say, the top 50 “U–A–A” and “U–A–U” peers of your target.\n\n3.  **Step 3 – Form a rich feature vector**  \n    - Aggregate the user’s own graph embedding h₍u₎ with statistics over the metapath neighbors—e.g. mean-pool or attention-pool their embeddings.  \n    - You now have a single feature vector that mixes:  \n        • the user’s direct gifting history, and  \n        • the 2- and 3-hop patterns of “who else gave to whom” and “which authors cluster together.”\n\n4.  **Step 4 – Train (or apply) a sensitive-attribute classifier**  \n    - Collect a small labeled set of users whose sensitive attribute (say, age bracket, gender, political leaning, health status, etc.) is known.  \n    - Train a lightweight classifier (e.g. logistic regression or a two-layer MLP) to predict that attribute from the feature vector you built in Step 3.  \n\n5.  **Step 5 – Apply to new users**  \n    - At inference time, you simply:  \n        a) read out θ(u),  \n        b) fetch the same metapath-expanded neighbors,  \n        c) aggregate,  \n        d) run the classifier,  \n    – and you get a prediction of the user’s sensitive attribute.  \n\nIn short, the pre-trained graph layer (red box) gives you a privacy-sensitive embedding of “who gifts to whom,” and the offline metapath expansion (blue box) populates high-order neighbourhoods around each user.  Combined, these two steps produce a feature set rich enough that a small downstream model can reliably infer attributes that users never explicitly disclosed."
    },
    {
        "question": "What motivates decoupling Graph-guided Interest Expansion from online MFQ Module?",
        "relevant_section_ids": [
            "1",
            "5.4"
        ],
        "relevant_context": [
            "Finally, to meet the low latency requirements of the online serving system, we propose a decoupled graph offline training and online inference strategy.",
            "However, this method can impose significant computational overhead on the KGNN server and result in great time delays when walking on the entire graph."
        ],
        "final_answer": "The need to meet strict low-latency requirements in online serving motivates decoupling the Graph-guided Interest Expansion from the MFQ module, so that expensive graph computations can be done offline and only fast, pre-computed expansions are accessed at inference time.",
        "relevant_elements": [
            "Graph-guided Interest Expansion",
            "MFQ Module"
        ],
        "id": 2092,
        "masked_question": "What motivates decoupling [mask1] from online MFQ Module?",
        "masked_number": 1,
        "masked_elements": [
            "Graph-guided Interest Expansion"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.00056v1_figure_2.png",
        "paperid": "2407.00056v1",
        "paper_path": "./MISS-QA/papers/2407.00056v1.json",
        "figure_id": "2407.00056v1_figure_2.png",
        "caption": "Figure 2. The overall framework of MMBee, consists of two stages: (i) the offline Graph-guided Interest Expansion (GIE) stage conducts the behavior expansion based on the target user and author; (ii) the online GTR prediction stage aggregates the real-time multi-modal content and expanded behavior for end-to-end training.",
        "qtype": "Design_Rationale",
        "response": "Mask1 is in fact the entire “Graph‐guided Interest Expansion” component (the red‐boxed block at the bottom of Figure 1).  We deliberately pull that whole graph‐expansion machinery (GraphCL pretraining plus multi‐hop metapath neighbor searches) out of the online MFQ module and run it offline because:\n\n 1. Graph contrastive learning and multi‐hop metapath walks on a billion-node U2A/A2A graph are far too expensive to do in the hot path of a low-latency serving system.  \n 2. If we tried to issue live KGNN requests or random‐walks at query time, the extra round‐trips and computation would blow out our tail latencies.  \n 3. By decoupling and precomputing all node embeddings and metapath‐expanded neighbor lists once per day (and storing them in a key–value/“Graph Behavior Offline Storage”), the online MFQ module can simply fetch ready-to-use vectors at inference time.  \n\nIn short, it’s the need to meet strict online latency and scale requirements that motivates moving the heavy graph tasks offline, decoupling them from the real‐time MFQ pipeline."
    },
    {
        "question": "What drives User Side Metapath Expansion for enriching sparse user behavior sequences?",
        "relevant_section_ids": [
            "5.3"
        ],
        "relevant_context": [
            "When analyzing the node number distribution of the constructed A2U graph, we observe that the average outdegree of user nodes is 0.32.",
            "It becomes difficult for widely used behavior-based models like SIM to study meaningful representations and explore potential gifting preferences.",
            "To address these challenges, we expand the behavior sequence of the target user and author using various pre-defined metapaths (Fan et al., 2019)."
        ],
        "final_answer": "User Side Metapath Expansion is driven by the extreme sparsity of users’ donation histories (average outdegree of 0.32 in the User-to-Author graph), which makes it hard for behavior-based models like SIM to learn meaningful user representations and motivates the expansion of user behavior sequences via metapaths.",
        "relevant_elements": [
            "User Side Metapath Expansion"
        ],
        "id": 2093,
        "masked_question": "What drives [mask1] for enriching sparse user behavior sequences?",
        "masked_number": 1,
        "masked_elements": [
            "User Side Metapath Expansion"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.00056v1_figure_2.png",
        "paperid": "2407.00056v1",
        "paper_path": "./MISS-QA/papers/2407.00056v1.json",
        "figure_id": "2407.00056v1_figure_2.png",
        "caption": "Figure 2. The overall framework of MMBee, consists of two stages: (i) the offline Graph-guided Interest Expansion (GIE) stage conducts the behavior expansion based on the target user and author; (ii) the online GTR prediction stage aggregates the real-time multi-modal content and expanded behavior for end-to-end training.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module is exactly the “User-Side Metapath Expansion” – i.e. the metapath-guided neighbor expansion on the U2A/A2A graphs (part of our Graph-guided Interest Expansion). By walking the user→author and author→author metapaths we retrieve extra “neighbors” for each target user, which in turn enriches their otherwise very sparse behavior sequence."
    },
    {
        "question": "What motivates supporting both structured and unstructured historical events input?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To give formal definition of the problem, we separate it into two sub-tasks given the different representations of historical information."
        ],
        "final_answer": "The framework supports both structured and unstructured historical‐events input because temporal‐event data can naturally come in two distinct formats—graph-based (structured) and text-based (unstructured). By separating the problem into these two sub-tasks, the model can uniformly handle both representations of historical information.",
        "relevant_elements": [
            "structured",
            "unstructured"
        ],
        "id": 2094,
        "masked_question": "What motivates supporting both [mask1] and [mask2] historical events input?",
        "masked_number": 2,
        "masked_elements": [
            "structured",
            "unstructured"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.04388v1_figure_2.png",
        "paperid": "2408.04388v1",
        "paper_path": "./MISS-QA/papers/2408.04388v1.json",
        "figure_id": "2408.04388v1_figure_2.png",
        "caption": "Figure 2. The schematic overview of MM-Forecast.\nBy consuming historical events in either format of unstructured or structured input (left), our image function identification module (middle) recognizes the image functions as verbal descriptions, which are then feed into LLM-based forecasting model (right).\nOur framework is versatile to handle both structured and unstructured events, meanwhile, it is compatible to popular LLM components for event forecasting, i.e., ICL and RAG.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought  \n1. In Fig. 2 (left), the red‐boxed block is the “Structured” input: event graphs (quintuples) plus their images. The blue‐boxed block is the “Unstructured” input: textual sub‐events (summaries of news articles) plus their images.  \n2. In Sect. 3.1 the authors explicitly define two flavours of temporal forecasting – one that takes a graph of timestamped quadruples/quintuples as history (structured), and one that takes a sequence of text snippets as history (unstructured).  \n3. Real‐world news events often exist in both forms: databases of relations (who‐did‐what‐to‐whom) and narrative articles. Each has its own strengths – graphs give precise relations and context (complex event labels), while text captures richer, nuanced descriptions.  \n4. By building MM-Forecast so that it can accept either format, they make their method far more generally applicable. On one hand they can leverage graph-based retrieval or in‐context learning; on the other they can exploit text retrieval and prompt‐based LLM reasoning.  \n5. Hence, the motivation for supporting both is to cover the full spectrum of historical event representations encountered in practice, and to let downstream ICL or RAG modules tap into whichever format (or combination) is available.  \n\nAnswer  \nThey support both structured (graph‐based) and unstructured (text‐based) historical inputs so that the forecasting framework can flexibly handle whichever form the past event data takes in practice – exploiting the precise relational context of event graphs when available, or the richer narrative detail of text otherwise – and thereby broaden applicability across settings and retrieval/ICL methods."
    },
    {
        "question": "What advantages does passing verbalized image functions through the Image Function Identification module bring to ICL and RAG?",
        "relevant_section_ids": [
            "3.2",
            "3.3.1",
            "3.3.2"
        ],
        "relevant_context": [
            "We propose an Image Function Identification module to recognize these functions as verbal descriptions using MLLMs, and subsequently incorporate these function descriptions into LLM-based forecasting models. (Section 3.2)",
            "Finally, with the highlighting and complementary functions of the images, the input historical event graph is , where  and  denotes the key events,  represents the remaining events, and  corresponds to the complementary events, respectively. (Section 3.3.1)",
            "Similarly, through the function of images, the retrieval process also contains key events and complementary events. (Section 3.3.2)"
        ],
        "final_answer": "By verbalizing image functions (highlighting vs. complementary) and feeding them into the forecasting pipeline, both ICL and RAG can explicitly distinguish and include “key” events (those underscored by the images) and “complementary” events (those enriched by the images). In ICL this produces a structured prompt of key, related, and complementary sub-events, focusing the model on the most salient historical signals. In RAG this guides retrieval to pull in both highlighted and supplementary events, helping to filter out noise and improve relevance and forecasting accuracy.",
        "relevant_elements": [
            "Image Function Identification",
            "ICL",
            "RAG"
        ],
        "id": 2095,
        "masked_question": "What advantages does passing verbalized image functions through the [mask1] module bring to ICL and RAG?",
        "masked_number": 1,
        "masked_elements": [
            "Image Function Identification"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.04388v1_figure_2.png",
        "paperid": "2408.04388v1",
        "paper_path": "./MISS-QA/papers/2408.04388v1.json",
        "figure_id": "2408.04388v1_figure_2.png",
        "caption": "Figure 2. The schematic overview of MM-Forecast.\nBy consuming historical events in either format of unstructured or structured input (left), our image function identification module (middle) recognizes the image functions as verbal descriptions, which are then feed into LLM-based forecasting model (right).\nOur framework is versatile to handle both structured and unstructured events, meanwhile, it is compatible to popular LLM components for event forecasting, i.e., ICL and RAG.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.  What is in the red box?  \n    – The red box in Figure 2 is the “Image Function Identification” module.  Its job is to take each image associated with a news sub‐event or graph event and classify its role as one of:  \n      •  Highlighting (the image underlines a key sub‐event)  \n      •  Complementary (the image adds side information not fully spelled out in the text)  \n      •  Irrelevant (to be dropped)  \n    – It then verbalizes that function in plain language (e.g. “This picture highlights the president’s handshake with the prime minister,” or “This photo shows protesters in the background, supplementing the text’s description of the rally.”).\n\n2.  How do those verbalized functions get used by ICL?  \n    – In the in‐context learning (ICL) setup you feed the LLM a sequence of past events plus a few demonstrations.  By explicitly tagging some events as “key” (highlighted by images) and others as “complementary,” you give the model a much clearer signal about which pieces of the history it should pay most attention to when predicting the next relation or object.  \n    – In practice you split the history into three buckets—key events, ordinary events, complementary events—and roll that into your prompt.  That both reduces noise (irrelevant events get pushed aside) and sharpens the model’s focus on the truly predictive facts.\n\n3.  And how does it help RAG?  \n    – A retrieval‐augmented generation system has two stages: first you retrieve a handful of relevant historical records, then you condition the generator on them.  By filtering and ranking using the image‐function labels you:  \n      •  Prioritize “key” events in your retrieval index  \n      •  Bring in “complementary” items only when they’re truly needed to flesh out the context  \n      •  Discard or downweight events that neither the text nor the images deemed important  \n    – The result is less noisy retrieval and more tightly focused input to the generator, which again improves its forecasting accuracy.\n\n4.  In short, what’s the advantage of piping those verbalized image functions through that module?  \n    – It turns raw pixels into crisp, explicit cues about which historical facts matter most and which only augment the story.  Both ICL and RAG then:  \n      •  See fewer irrelevant events  \n      •  Zero in on exactly the “highlighted” facts that the images back up  \n      •  Still have access to useful supplemental details via “complementary” labels  \n    – That sharper triage of the past history consistently boosts forecasting performance."
    },
    {
        "question": "How does Image Function Identification module leverage MLLM outputs to categorize images into highlighting or complementary functions?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "We propose an Image Function Identification module to recognize these functions as verbal descriptions using MLLMs, and subsequently incorporate these function descriptions into LLM-based forecasting models.",
            "To determine which sub-event is a key event, we leverage the MLLMs to analyze the images and sub-events from multiple aspects, including main objects, celebrities, activities, environment, and labeled items.",
            "In cases where the function of associated image is complementary, the visual content contains information that supplements and extends what is covered in the news text. To more effectively extract the relevant supplementary information, we consider the following aspects: 1) identify the main subject of the image as the central point, 2) directly relate the extracted information to the news event in the article, 3) prioritize the most newsworthy visual elements, 4) ensure all information comes directly from the provided news article without fabrication, and 5) aim for a concise summary using clear language."
        ],
        "final_answer": "The Image Function Identification module feeds each image together with its associated sub-events into a Multimodal LLM (MLLM), and uses the MLLM’s zero-shot multimodal reasoning to output a verbal description of the image’s role. If the MLLM’s analysis—drawing on cues such as main objects, celebrities, activities, environment, and labeled items—matches and emphasizes one of the text sub-events, the image is tagged as “highlighting.” If instead the MLLM extracts supplementary details that extend beyond the text (by identifying the image’s main subject, relating it to the article event, prioritizing newsworthy elements, and summarizing concisely), the image is tagged as “complementary.”",
        "relevant_elements": [
            "Image Function Identification",
            "MLLM"
        ],
        "id": 2096,
        "masked_question": "How does [mask1] module leverage MLLM outputs to categorize images into highlighting or complementary functions?",
        "masked_number": 1,
        "masked_elements": [
            "Image Function Identification"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.04388v1_figure_2.png",
        "paperid": "2408.04388v1",
        "paper_path": "./MISS-QA/papers/2408.04388v1.json",
        "figure_id": "2408.04388v1_figure_2.png",
        "caption": "Figure 2. The schematic overview of MM-Forecast.\nBy consuming historical events in either format of unstructured or structured input (left), our image function identification module (middle) recognizes the image functions as verbal descriptions, which are then feed into LLM-based forecasting model (right).\nOur framework is versatile to handle both structured and unstructured events, meanwhile, it is compatible to popular LLM components for event forecasting, i.e., ICL and RAG.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:  \n1. The red-boxed component in Fig. 2 is the “Image Function Identification” module (this is what [mask1] refers to).  \n2. According to §3.2, this module feeds each associated image together with its candidate text sub-events into a multimodal LLM (MLLM).  \n3. The MLLM is prompted to “verbalize” the role that the image plays vis-à-vis the text—namely, does it directly reinforce (“highlight”) one of the text’s key sub-events, or does it supply extra facts that go beyond the text (“complement”).  \n4. To decide “highlighting,” the MLLM inspects visual cues (main objects, people/celebrities, activities, setting, labeled items) and matches them back to specific text sub-events.  \n5. To decide “complementary,” the MLLM pulls out novel but relevant details (central subject, relation to the news story, most newsworthy elements), ensures fidelity to the article, and summarizes them concisely.  \n6. The verbal outputs—marked up as red for highlighting and blue for complementary—are then parsed into two sets of sub-events (key vs. supplementary) that downstream forecasting models ingest.  \n\nAnswer:  \nThe Image Function Identification module sends each image plus its candidate text sub-events into an MLLM, asking it to generate a short verbal account of whether the visual content directly underpins a key sub-event (“highlighting”) or instead adds new, supporting details (“complementary”). By analyzing visual attributes (objects, people, activities, environment) and cross-referencing them with the text, the MLLM’s descriptions are parsed into two buckets—highlighting versus complementary—which are then fed as separate event groups into the LLM forecasting stage."
    },
    {
        "question": "How does the Mid-Level policy modulate Oscillator amplitude and frequency via skill vector inputs?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "The mid-level reinforcement learning control policy can combine with the CPG module to form many coordinated motor skills.",
            "To achieve this, we use the parameterized neural network π_μ as the mid-level policy, and output μ and ω to adjust the internal amplitude and frequency of the oscillation, i.e. μ, ω, with a control frequency of 16.67 Hz, according to the higher skill vector z and the robot’s proprioception s (including 18 joint angles of the legs, rotational quaternions, angular velocities and linear accelerations information measured by the internal measurement unit (IMU), as well as the morphological parameters and maximum oscillation frequency of the CPG module)."
        ],
        "final_answer": "The mid-level policy is a parameterized neural network that takes as input a skill vector z and the robot’s proprioceptive state s, and outputs two modulation signals μ and ω. These signals directly adjust the oscillator’s internal amplitude (μ) and frequency (ω) at a control rate of 16.67 Hz.",
        "relevant_elements": [
            "Mid-Level",
            "Oscillator"
        ],
        "id": 2098,
        "masked_question": "How does the [mask1] policy modulate [mask2] amplitude and frequency via skill vector inputs?",
        "masked_number": 2,
        "masked_elements": [
            "Mid-Level",
            "Oscillator"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.03525v1_figure_1.png",
        "paperid": "2408.03525v1",
        "paper_path": "./MISS-QA/papers/2408.03525v1.json",
        "figure_id": "2408.03525v1_figure_1.png",
        "caption": "Figure 1: Overview of the central nervous system and hierarchical learning control framework.\na, Mammalian central nervous system structure, the figure contains the cerebral cortex partition, the spinal cord s internal structure, and the double-layer structure of CPGs neural circuits.\nb, Schematic diagram of the proposed hierarchical control framework.\nThe gray nodes in a and the gray box in b represent the sensing mechanism in the nervous system and control framework, respectively, and are responsible for the acquisition of sensing signals. In the nervous system, S1 and the visual cortex are mainly responsible. In the control frame, it is provided by sensor measurement. The green nodes and boxes in a and b represent the high-level institutions in the nervous system and control framework, respectively, responsible for observing the environment and making decisions. In the nervous system, most cortical regions are responsible for this function. In the control framework, this part is realized by the deep reinforcement learning neural network policy. The yellow nodes and boxes in a and b represent the mid-level institutions responsible for coordinating the limbs and generating various motion patterns. In the nervous system, the cerebellum and primary motor cortex are responsible. In the control framework, this part adopts an unsupervised reinforcement learning algorithm and skill-driven neural network. The purple nodes and boxes in a and b represent the low-level institutions that are responsible for the generation and execution of motion signals. In the nervous system, the brain stem and spinal cord are responsible. In the control framework, it is realized by the CPG module, which contains an oscillator and a desired pose solver to provide the desired joint position and uses the built-in PID feedback loop of the robot to control 18 motors. The solid line in a connects different nerve regions, representing the information flow relationship, and the thin purple solid line on the right represents the ascending and descending spinal nerves. Dotted lines indicate descending pathway feedback of the CPGs. The solid line in b represents the action relationship between the sensor and the control signal, and the black dotted line connects the specific analysis of each module.\nc, Four different indoor obstacle terrain crossing tasks.\nd, Various new obstacle terrain crossing tasks have never been learned.",
        "qtype": "Implementation_Details",
        "response": "Here’s what’s going on, step by step:\n\n1.  Mask 1 is the mid-level policy (π_mid) – the little red box in Fig. 1b – and Mask 2 is the CPG’s oscillator (the RG block in the blue box), whose two key internal parameters are its amplitude a and angular frequency ω.\n\n2.  At each mid-level timestep (16.7 Hz) π_mid takes as input  \n    •  the high-level “skill” vector s (drawn from the unit circle)  \n    •  the robot’s proprioceptive state x (joint angles, IMU, CPG morphological parameters, etc.).\n\n3.  The policy network π_mid then spits out a small adjustment vector  \n       δm  =  (δa₁,…,δa₆, δω₁,…,δω₆)  \n    one pair (δaᵢ, δωᵢ) for each of the six leg-oscillators.\n\n4.  These offsets are added to the oscillator’s default (“natural”) amplitude and frequency – themselves simple linear functions of the skill scalar sᵢ – so that  \n       aᵢ ← a_nat(sᵢ) + δaᵢ  \n       ωᵢ ← ω_nat(sᵢ) + δωᵢ  \n\n5.  The modified (aᵢ, ωᵢ) go straight into the Hopf differential equations of the RG.  By nudging a and ω in real time, π_mid sculpts entirely different limit‐cycle rhythms (i.e. gaits) out of the same low‐level oscillator.\n\nIn short: the mid-level policy “reads” a desired skill direction s and the robot’s sensors, then “writes” tiny amplitude and frequency shifts δa, δω into each Hopf oscillator, thereby steering the CPG’s a/ω and generating the corresponding gait."
    },
    {
        "question": "How does the Desired poses solver use Oscillator mixed-phase outputs to generate motor joint trajectories?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "After the oscillator generates φ_i, we calculate the desired pose of the end of each leg and then obtain the position under Cartesian Coordinates of the end of the leg, then convert it into the desired motor angles through the inverse kinematics, to generate the control signal of the motors.",
            "After obtaining the position of leg end, we calculate the desired angles of θ1, θ2 and θ3 joints of each leg through the inverse kinematics model. (See Supplementary Section 11 for the calculation process). Through the PID controller inside the robot, the motors can be controlled to run to the specified angles."
        ],
        "final_answer": "The Desired poses solver (PF layer) takes each leg’s mixed‐phase output φ_i from the oscillator, plugs it into a parametric foot‐trajectory formula to compute the foot’s Cartesian position, and then applies inverse kinematics to that position to derive the three joint angles (θ1, θ2, θ3). These desired angles are sent to the motors’ PID controllers, producing the motor joint trajectories.",
        "relevant_elements": [
            "Desired poses solver",
            "Oscillator"
        ],
        "id": 2099,
        "masked_question": "How does the [mask1] use [mask2] mixed-phase outputs to generate motor joint trajectories?",
        "masked_number": 2,
        "masked_elements": [
            "Desired poses solver",
            "Oscillator"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.03525v1_figure_1.png",
        "paperid": "2408.03525v1",
        "paper_path": "./MISS-QA/papers/2408.03525v1.json",
        "figure_id": "2408.03525v1_figure_1.png",
        "caption": "Figure 1: Overview of the central nervous system and hierarchical learning control framework.\na, Mammalian central nervous system structure, the figure contains the cerebral cortex partition, the spinal cord s internal structure, and the double-layer structure of CPGs neural circuits.\nb, Schematic diagram of the proposed hierarchical control framework.\nThe gray nodes in a and the gray box in b represent the sensing mechanism in the nervous system and control framework, respectively, and are responsible for the acquisition of sensing signals. In the nervous system, S1 and the visual cortex are mainly responsible. In the control frame, it is provided by sensor measurement. The green nodes and boxes in a and b represent the high-level institutions in the nervous system and control framework, respectively, responsible for observing the environment and making decisions. In the nervous system, most cortical regions are responsible for this function. In the control framework, this part is realized by the deep reinforcement learning neural network policy. The yellow nodes and boxes in a and b represent the mid-level institutions responsible for coordinating the limbs and generating various motion patterns. In the nervous system, the cerebellum and primary motor cortex are responsible. In the control framework, this part adopts an unsupervised reinforcement learning algorithm and skill-driven neural network. The purple nodes and boxes in a and b represent the low-level institutions that are responsible for the generation and execution of motion signals. In the nervous system, the brain stem and spinal cord are responsible. In the control framework, it is realized by the CPG module, which contains an oscillator and a desired pose solver to provide the desired joint position and uses the built-in PID feedback loop of the robot to control 18 motors. The solid line in a connects different nerve regions, representing the information flow relationship, and the thin purple solid line on the right represents the ascending and descending spinal nerves. Dotted lines indicate descending pathway feedback of the CPGs. The solid line in b represents the action relationship between the sensor and the control signal, and the black dotted line connects the specific analysis of each module.\nc, Four different indoor obstacle terrain crossing tasks.\nd, Various new obstacle terrain crossing tasks have never been learned.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step of what happens once the blue-boxed Oscillator (the RG) spits out its mixed-phase signals φᵢ:\n\n1.  The red-boxed Desired-Pose Solver (the PF layer) takes each leg’s phase φᵢ as its lone input.  \n2.  It plugs φᵢ into a small set of pre‐defined sinusoidal trajectory formulas (one for the x–y “step” profile, one for the z “lift/drop” profile).  These formulas are parameterized by the robot’s morphological constants (step length S, link lengths, body height, maximum clearance and penetration, etc.).  \n3.  The solver thus computes a full 3D Cartesian target pᵢ = (xᵢ, yᵢ, zᵢ) for that foot at the current time.  \n4.  An analytic inverse-kinematics module then turns pᵢ into the three joint angles (coxa, femur, tibia) for leg i.  \n5.  Finally, those desired joint angles are fed to the onboard PID loops, which drive the actual motors.  \n\nIn this way the red-boxed PF block “listens” to the blue-boxed RG phases and turns them into smooth, physically realizable joint‐angle trajectories."
    },
    {
        "question": "How does the hierarchical interaction between the high-level and mid-level policies reflect established hierarchical reinforcement learning methodologies?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Another advantage is that the skill space can be conveniently used as the abstract action space of the high-level policy.",
            "Use the learned skills to control the robot movement, we can get the environmental reward. Due to the time abstraction of the hierarchical structure, the action execution frequency (1.67Hz) of the high-level policy is only 1/10 of that of the middle level, which saves computational resources and improves efficiency."
        ],
        "final_answer": "The framework implements a standard hierarchical‐RL design: during pre‐training the mid‐level policy learns a set of parameterized motor skills (encoded by skill vectors) which form its action space. The high‐level policy then treats these learned skills as abstract actions, issuing one skill vector every 10 low‐level steps. This use of an abstract action space (skills) combined with reduced execution frequency (time abstraction) directly mirrors the options or feudal hierarchy approach in hierarchical reinforcement learning.",
        "relevant_elements": [
            "High-Level",
            "Mid-Level"
        ],
        "id": 2100,
        "masked_question": "How does the hierarchical interaction between the [mask1] and [mask2] policies reflect established hierarchical reinforcement learning methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "High-Level",
            "Mid-Level"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.03525v1_figure_1.png",
        "paperid": "2408.03525v1",
        "paper_path": "./MISS-QA/papers/2408.03525v1.json",
        "figure_id": "2408.03525v1_figure_1.png",
        "caption": "Figure 1: Overview of the central nervous system and hierarchical learning control framework.\na, Mammalian central nervous system structure, the figure contains the cerebral cortex partition, the spinal cord s internal structure, and the double-layer structure of CPGs neural circuits.\nb, Schematic diagram of the proposed hierarchical control framework.\nThe gray nodes in a and the gray box in b represent the sensing mechanism in the nervous system and control framework, respectively, and are responsible for the acquisition of sensing signals. In the nervous system, S1 and the visual cortex are mainly responsible. In the control frame, it is provided by sensor measurement. The green nodes and boxes in a and b represent the high-level institutions in the nervous system and control framework, respectively, responsible for observing the environment and making decisions. In the nervous system, most cortical regions are responsible for this function. In the control framework, this part is realized by the deep reinforcement learning neural network policy. The yellow nodes and boxes in a and b represent the mid-level institutions responsible for coordinating the limbs and generating various motion patterns. In the nervous system, the cerebellum and primary motor cortex are responsible. In the control framework, this part adopts an unsupervised reinforcement learning algorithm and skill-driven neural network. The purple nodes and boxes in a and b represent the low-level institutions that are responsible for the generation and execution of motion signals. In the nervous system, the brain stem and spinal cord are responsible. In the control framework, it is realized by the CPG module, which contains an oscillator and a desired pose solver to provide the desired joint position and uses the built-in PID feedback loop of the robot to control 18 motors. The solid line in a connects different nerve regions, representing the information flow relationship, and the thin purple solid line on the right represents the ascending and descending spinal nerves. Dotted lines indicate descending pathway feedback of the CPGs. The solid line in b represents the action relationship between the sensor and the control signal, and the black dotted line connects the specific analysis of each module.\nc, Four different indoor obstacle terrain crossing tasks.\nd, Various new obstacle terrain crossing tasks have never been learned.",
        "qtype": "Literature_Background",
        "response": "First, note that in the figure the red‐boxed module is the “high‐level” policy and the blue‐boxed module is the “mid‐level” (skill) policy.  In our system the high‐level policy πᴴ runs at a low rate (∼1.7 Hz), observes the robot’s proprioceptive and environmental state, and emits a “skill vector” z as its action.  The mid‐level policy πᴹ runs at a higher rate (∼16.7 Hz), conditions on z plus proprioception, and outputs adjustments to the CPG parameters that generate the actual joint commands.\n\nWhy is this just hierarchical RL in disguise?  Because:\n\n 1.  Temporal abstraction.  The high‐level only chooses a new skill every K low‐level steps—exactly as in the Options framework or Feudal RL, where a manager issues subgoals that persist for a fixed number of steps.\n 2.  Parameterized “options.”  The high‐level’s action space is not raw torques but a low‐dimensional skill vector.  That vector serves as the input to a pre‐trained skill policy (an option) which then executes a motor primitive.\n 3.  Two‐stage training.  We first unsupervisedly pre‐train πᴹ to discover a wide repertoire of locomotion primitives (akin to “option discovery”).  We then freeze πᴹ and train πᴴ on high‐level tasks, just as in many HRL pipelines.\n 4.  Manager–worker separation.  πᴴ handles goal‐setting and long‐horizon planning; πᴹ handles fine‐grained control.  This decoupling of planning and control is the hallmark of hierarchical approaches such as h‐DQN, Option‐Critic, and Feudal Networks.\n\nIn short, our red‐to‐blue interaction is a classic manager‐worker / options structure: the high‐level policy decides “which skill” to run and when, and the mid‐level policy implements that skill at finer time scales. This exactly mirrors established hierarchical RL methodologies."
    },
    {
        "question": "How does the oscillator design within the low-level CPG module build upon classical central pattern generator models?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "To generate the basic motion rhythm signal, we use the Hopf oscillation differential equations[20,49] to implement the RG layer of CPGs.",
            "Due to the effect of the coupling term, the left front leg (LF), the left hind leg (LH) and the right middle leg (RM) of the robot are a group. Their φ_i is the same, while the other three legs are another group, and their φ_i lags π radians. This setting makes the six legs form a tripod gait.",
            "On this basis, the mid-level controller can adjust the φ_i of each leg to directly change the amplitude A and adjustable phase φ of the oscillator, then adjust the mixed phase ψ to make the CPG module produce different gaits.",
            "μ and ω are used to calculate the internal natural amplitude and frequency, where μ=μ_raw and ω=ω_raw, they map μ_raw,ω_raw∈[0,1] to A_μ,Ω_ω. μ is a linear mapping, which maps the μ_raw between 0 and 1 to A_μ. ω is a fixed value in Hz, which can ensure that A_μ is always positive, thus ensuring that the independent tripod gait phase ψ is not affected by any external factors, and can always produce periodic tripod gait signals.",
            "This is different from previous work [20,21,22]. These methods add the external feedback signal γ and the coupling term directly and take them as the differential of a single phase. When the feedback signal is boundary value (such as 0), the only coupling term cannot make the phase oscillate periodically, which makes the oscillator invalid."
        ],
        "final_answer": "The low-level oscillator is built on classical CPGs by using a network of coupled Hopf oscillators (the RG layer) rather than simple phase oscillators.  Each Hopf oscillator has an adjustable amplitude and phase, and they are tied together by fixed coupling weights and biases so that the six legs naturally split into two tripod groups with a π phase offset.  Control inputs (μ, ω) are linearly mapped to the oscillator’s natural amplitude and frequency to guarantee positive amplitude and robust, self-sustained oscillations even in the face of boundary feedback.  This design extends classical CPG models by embedding a stable internal phase representation and explicit amplitude/frequency modulation, ensuring continuous periodic rhythms where earlier direct-coupling schemes could fail.",
        "relevant_elements": [
            "Low-Level",
            "CPG module"
        ],
        "id": 2101,
        "masked_question": "How does the oscillator design within the [mask1] [mask2] build upon classical central pattern generator models?",
        "masked_number": 2,
        "masked_elements": [
            "Low-Level",
            "CPG module"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.03525v1_figure_1.png",
        "paperid": "2408.03525v1",
        "paper_path": "./MISS-QA/papers/2408.03525v1.json",
        "figure_id": "2408.03525v1_figure_1.png",
        "caption": "Figure 1: Overview of the central nervous system and hierarchical learning control framework.\na, Mammalian central nervous system structure, the figure contains the cerebral cortex partition, the spinal cord s internal structure, and the double-layer structure of CPGs neural circuits.\nb, Schematic diagram of the proposed hierarchical control framework.\nThe gray nodes in a and the gray box in b represent the sensing mechanism in the nervous system and control framework, respectively, and are responsible for the acquisition of sensing signals. In the nervous system, S1 and the visual cortex are mainly responsible. In the control frame, it is provided by sensor measurement. The green nodes and boxes in a and b represent the high-level institutions in the nervous system and control framework, respectively, responsible for observing the environment and making decisions. In the nervous system, most cortical regions are responsible for this function. In the control framework, this part is realized by the deep reinforcement learning neural network policy. The yellow nodes and boxes in a and b represent the mid-level institutions responsible for coordinating the limbs and generating various motion patterns. In the nervous system, the cerebellum and primary motor cortex are responsible. In the control framework, this part adopts an unsupervised reinforcement learning algorithm and skill-driven neural network. The purple nodes and boxes in a and b represent the low-level institutions that are responsible for the generation and execution of motion signals. In the nervous system, the brain stem and spinal cord are responsible. In the control framework, it is realized by the CPG module, which contains an oscillator and a desired pose solver to provide the desired joint position and uses the built-in PID feedback loop of the robot to control 18 motors. The solid line in a connects different nerve regions, representing the information flow relationship, and the thin purple solid line on the right represents the ascending and descending spinal nerves. Dotted lines indicate descending pathway feedback of the CPGs. The solid line in b represents the action relationship between the sensor and the control signal, and the black dotted line connects the specific analysis of each module.\nc, Four different indoor obstacle terrain crossing tasks.\nd, Various new obstacle terrain crossing tasks have never been learned.",
        "qtype": "Literature_Background",
        "response": "First, let’s be clear about the two masked terms: in Fig. 1b the entire little red‐boxed block at the bottom is the “low-level” module, and the little blue-boxed sub-block inside it is the “oscillator (RG).”  So the question really asks:\n\n  “How does the oscillator design within the low-level oscillator build upon classical central pattern generator models?”\n\nStep-by-step answer (chain of thought)  \n1.  Classical biological CPGs are often modeled as simple phase oscillators (e.g. Kuramoto models) or half-center leaky‐integrator pairs, with coupling terms that act directly on phase derivatives.  In those schemes, if you ever drive the feedback term to an extreme (for instance zero) there is nothing in the equations to pull the phase back into a limit cycle, and the rhythm can collapse.  \n2.  In our “low-level” block we instead implement a network of Hopf oscillators—one per leg—whose states obey a pair of first‐order differential equations for amplitude and phase:  \n     •   Each oscillator has its own amplitude state \\(r\\) that is driven back to a prescribed “natural” amplitude by a convergence term (\\(\\alpha\\,(r^* - r)\\)).  \n     •   We keep an explicit phase variable \\(\\phi\\), again with its own “rest” frequency \\(\\omega^*\\) that is guaranteed always positive by a simple linear mapping from a normalized driving input.  \n3.  The six leg oscillators are then coupled by addition of fixed weights \\(W_{ij}\\) and biases \\(b_i\\) so that three legs form one tripod and the other three legs form the other tripod, with a hardwired \\(\\pi\\)-radian phase lag between the two sets.  That coupling is purely additive (not multiplicative or shoved into the phase derivative), so it never kills the limit cycle even if the external feedback momentarily vanishes.  \n4.  Finally, we expose the two Hopf parameters—amplitude gain and phase bias—to the mid-level controller, so that higher layers can continuously modulate both the size of each step (by tweaking the amplitude state) and the foot-fall timing (by nudging the phase bias), again without ever risking a collapse of the oscillation.  \n\nIn short, the “low-level oscillator” is just a Hopf‐type CPG rather than a bare‐bones Kuramoto or half‐center integrator.  By giving it its own amplitude dynamics, by mapping its intrinsic frequency to stay strictly positive, and by using simple additive coupling biases, we guarantee a robust limit-cycle tripod gait that a higher‐level controller can safely modulate on the fly."
    },
    {
        "question": "How does the combination of Levenshtein distance calculation and BERT-based semantic similarity calculation enhance cloning detection?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "We employed Levenshtein distance and BERT-based semantic similarity to detect plagiarism or app cloning in LLM app descriptions and instructions. Levenshtein distance identified exact or near-exact matches by measuring minimal edits, while the BERT model captured deeper semantic similarities, even with different wording.",
            "By analyzing both these components, we effectively detected cloning attempts, revealing instances of content replication ranging from direct copying to subtle paraphrasing, and highlighting the prevalence of cloning in the LLM app ecosystem.",
            "When the text’s meaning remained consistent but the wording varied, the BERT-based approach was more effective than character-based methods. For example, the Levenshtein method misses the similarity between App1 and App2 due to minor text variations, while the BERT model effectively captures the semantic consistency across all three apps, demonstrating its advantage in detecting deeper similarities."
        ],
        "final_answer": "By combining Levenshtein distance—which catches exact or near-exact matches via minimal character edits—with BERT-based semantic similarity—which identifies cases where wording differs but meaning is the same—the approach uncovers both direct copies and subtle paraphrases, yielding a more comprehensive detection of cloning.",
        "relevant_elements": [
            "Levenshtein distance calculation",
            "BERT-based semantic similarity calculation"
        ],
        "id": 2105,
        "masked_question": "How does the combination of [mask1] and [mask2] enhance cloning detection?",
        "masked_number": 2,
        "masked_elements": [
            "Levenshtein distance calculation",
            "BERT-based semantic similarity calculation"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.07518v1_figure_2.png",
        "paperid": "2411.07518v1",
        "paper_path": "./MISS-QA/papers/2411.07518v1.json",
        "figure_id": "2411.07518v1_figure_2.png",
        "caption": "Figure 2: Our approach to identifying squatting and cloning LLM apps.",
        "qtype": "Experimental_Results",
        "response": "Here’s the high-level reasoning on why pairing those two measures makes for a far more robust cloning detector:\n\n1. The red-boxed component is a Levenshtein-distance check (with a high similarity threshold, e.g. ≥0.95).  \n   - Strength: It very reliably flags cases where one app’s instructions have only tiny edit distances (insertions/deletions/substitutions) from another.  \n   - Limitation: If someone paraphrases or reorders large chunks, the edit distance jumps and you miss it.\n\n2. The blue-boxed component is a BERT-based semantic similarity check (again with a high threshold).  \n   - Strength: Because it reasons over embeddings, it will catch two instruction texts that say the same thing in different words.  \n   - Limitation: It isn’t as precise for catching one-or-two character tweaks—very near copies can look “too” similar and sometimes be under- or over-scored.\n\nBy running both in tandem you get:\n• Character-level coverage (via Levenshtein) so you catch almost-verbatim clones with slight tweaks.  \n• Meaning-level coverage (via BERT) so you catch paraphrases and reorganized text.  \nTogether, they drastically reduce both false negatives (missed clones) and false positives (innocuous similarity), giving you a far more complete picture of who’s really copying whom."
    },
    {
        "question": "How does conversion to SNN support partial parameters fine-tuning alongside unsupervised loss during online adaptation?",
        "relevant_section_ids": [
            "2.2",
            "2.6"
        ],
        "relevant_context": [
            "Section 2.2: “By explicitly counting the range of activation values in the -th layer to determine the maximum activation , … we can get the mapping between ANN and SNN to convert the weights of an ANN with ReLU activation to an SNN with IF neurons.”",
            "Section 2.6: “Given a source SNN model … we update the model parameters on test data in an online streaming manner. … Upon receiving a batch of input test data, the model produces predictions on this batch and, at the meantime, updates its parameters based on the unsupervised instantaneous entropy losses … In the online adaptation phase, only a small portion of the model parameters are updated. In the experiments, we only update the normalization layers, which is sufficient for achieving satisfactory performance in adapting to corruptions … Besides, in the online adaptation phase, the adaptive activation scaling scheme proposed in Section 2.3 is adopted to adaptively adjust the firing rate distribution, which introduces additional clip parameters to be learned in the adaptation phase. Overall, the parameters to be updated online include the parameters in normalization layers and the clip parameters.”"
        ],
        "final_answer": "After converting a pre‐trained ANN into an SNN via layer‐wise weight mapping (Section 2.2), all original weights are carried over into the spiking network and frozen at test time. During online adaptation (Section 2.6), only a small subset of parameters—namely the affine scale/shift parameters of each normalization layer and the per‐layer clip parameters introduced by the adaptive activation scaling scheme—are unfrozen. An unsupervised instantaneous entropy loss is computed at each time step, and its surrogate‐gradient signal is used to fine‐tune just these partial parameters on the incoming test batches, while the rest of the SNN remains fixed.",
        "relevant_elements": [
            "Conversion to SNN",
            "Partial parameters fine-tuning",
            "Unsupervised loss"
        ],
        "id": 2106,
        "masked_question": "How does [mask1] support partial parameters fine-tuning alongside unsupervised loss during online adaptation?",
        "masked_number": 1,
        "masked_elements": [
            "Conversion to SNN"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.02146v1_figure_1.png",
        "paperid": "2409.02146v1",
        "paper_path": "./MISS-QA/papers/2409.02146v1.json",
        "figure_id": "2409.02146v1_figure_1.png",
        "caption": "Figure 1: The framework of proposed SNN adaptation pipeline.\nA pre-trained source model may degrade significantly on test data in wild-world scenarios, e.g., diverse weather conditions.\nThe proposed method adapts a source SNN model on the test data in an online manner based on unsupervised loss, which can significantly improve the SNN model s performance in such scenarios.",
        "qtype": "Experimental_Results",
        "response": "The mystery “red‐boxed” region is nothing more exotic than the _converted_ SNN itself, instrumented so that only a very small subset of its parameters ever “see” gradients at test time.  In practice what happens is:\n\n  1.  At test time we take the frozen, converted SNN (all of whose synaptic weights have been cloned from the ANN via the usual ANN-to-SNN mapping) and we _unlock_ only two things:  \n     – the _affine_ scale and shift in each batch–norm (or other normalization) layer,  \n     – plus the extra per-layer clip (activation-scaling) parameters introduced for adaptive firing-rate control.  \n  2.  We then run the network on a mini‐batch of unlabeled test patches, accumulate an _instantaneous_ entropy loss at each time step (rather than a big, unrolled BPTT loss), and use the surrogate gradient machinery to send that unsupervised loss signal _only_ back into the tiny set of unlocked parameters.  \n  3.  Everything else—the vast majority of synaptic weights—remains locked solid (“frozen”), so the whole rerun costs almost no extra memory or compute.  \n\nBy boxing this structure in red, the paper is simply pointing out that at test time you do a light-touch, _partial_ fine-tuning (just norm-affines and clipping thresholds) driven by an unsupervised entropy criterion, rather than re-training or unrolling the full SNN."
    },
    {
        "question": "How do gradient enabled layers contribute to firing dynamics in spiking layers over multiple time steps?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Gradient enabled layer",
            "Spiking layer"
        ],
        "id": 2107,
        "masked_question": "How do [mask1] contribute to firing dynamics in [mask2] over multiple time steps?",
        "masked_number": 2,
        "masked_elements": [
            "Gradient enabled layer",
            "Spiking layer"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.02146v1_figure_1.png",
        "paperid": "2409.02146v1",
        "paper_path": "./MISS-QA/papers/2409.02146v1.json",
        "figure_id": "2409.02146v1_figure_1.png",
        "caption": "Figure 1: The framework of proposed SNN adaptation pipeline.\nA pre-trained source model may degrade significantly on test data in wild-world scenarios, e.g., diverse weather conditions.\nThe proposed method adapts a source SNN model on the test data in an online manner based on unsupervised loss, which can significantly improve the SNN model s performance in such scenarios.",
        "qtype": "Experimental_Results",
        "response": "Here is what the two highlighted regions mean and how the one (mask 1) affects the other (mask 2) over time:\n\n• mask 1 (“Partial parameters fine-tuning” in the red box)  \n  In our online scheme we do **not** re-learn the full SNN—only a small subset of parameters:  \n  1. The affine scale‐and‐shift of each Batch-Norm layer  \n  2. The per‐layer “clip” (activation‐scaling) coefficients introduced for adaptive firing-rate control  \n\n  These are the only weights that receive gradient updates from the unsupervised entropy loss.\n\n• mask 2 (the spiking layers in the blue box)  \n  These are the leaky-integrate-and-fire (IF) neuron layers whose membrane potentials  and resulting spike outputs  evolve over T timesteps according to  \n    Vₗ[t] ← Vₗ[t–1] + Wₗ·Sₗ₋₁[t] – Sₗ[t–1]·θₗ  \n    Sₗ[t] = H(Vₗ[t] – θₗ)  \n\nHow mask 1 drives the firing dynamics in mask 2 over multiple time steps:  \nBy tweaking only the BN‐affine parameters and the per‐layer clip thresholds, we subtly change each layer’s effective gain and operating range.  Over the course of T timesteps of spike‐by‐spike integration:\n\n  1. A small change in the BN scale/shift moves the baseline membrane potential up or down.  \n  2. A change in the clip parameter modifies how large incoming currents can grow before reset.  \n\nTogether these two tiny adjustments alter how quickly Vₗ accumulates to threshold and therefore how often  becomes 1 (i.e. a spike) at each time step.  In practice this “partial fine-tuning” steers the layer-wise firing-rate distributions to remain uniform (rather than collapsing low or saturating high) under domain shift, thus shaping the temporal spike trains in the SNN (mask 2) adaptively over all T time steps."
    },
    {
        "question": "What limitations arise from partial parameters fine-tuning using unsupervised loss in spiking layers?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "partial parameters fine-tuning",
            "unsupervised loss",
            "spiking layer"
        ],
        "id": 2108,
        "masked_question": "What limitations arise from [mask1] using unsupervised loss in spiking layers?",
        "masked_number": 1,
        "masked_elements": [
            "partial parameters fine-tuning"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.02146v1_figure_1.png",
        "paperid": "2409.02146v1",
        "paper_path": "./MISS-QA/papers/2409.02146v1.json",
        "figure_id": "2409.02146v1_figure_1.png",
        "caption": "Figure 1: The framework of proposed SNN adaptation pipeline.\nA pre-trained source model may degrade significantly on test data in wild-world scenarios, e.g., diverse weather conditions.\nThe proposed method adapts a source SNN model on the test data in an online manner based on unsupervised loss, which can significantly improve the SNN model s performance in such scenarios.",
        "qtype": "Others",
        "response": "When you simply plug an unsupervised entropy‐loss into the spiking layers and do straight BPTT–style updates (i.e. “directly updating the SNN model in this way”), two things immediately go wrong:\n\n  1. Accuracy collapses.  On shifted test inputs the spike counts in each layer tend to fall towards zero, clustering all neurons into a very low firing-rate regime.  With only a handful of time‐steps this non-uniform, low‐rate code loses almost all of its quantization fidelity and the network’s output accuracy plunges – in our experiments it never comes close to what comparable ANN‐based test‐time‐adaptation schemes achieve.  \n  2. Memory blows up.  A textbook BPTT through a multi‐step spiking network requires you to store the entire time‐unfolded graph (all membrane potentials and traces) so the cost, both in memory and compute, grows linearly with the number of time‐steps.  That simply isn’t tenable for a few dozen—or even a few—time‐steps on a resource‐constrained edge device.\n\nIn short, “naïve” unsupervised fine‐tuning of the converted SNN leads to (1) very poor domain‐shift accuracy because of collapsing spike‐rate distributions and (2) prohibitive memory/computation costs if you try to do full BPTT."
    },
    {
        "question": "Could alternative ANN-SNN conversion reduce frozen layer constraints in online adaptation pipeline?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "conversion to SNN",
            "frozen layer"
        ],
        "id": 2109,
        "masked_question": "Could alternative ANN-SNN conversion reduce [mask1] constraints in online adaptation pipeline?",
        "masked_number": 1,
        "masked_elements": [
            "frozen layer"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.02146v1_figure_1.png",
        "paperid": "2409.02146v1",
        "paper_path": "./MISS-QA/papers/2409.02146v1.json",
        "figure_id": "2409.02146v1_figure_1.png",
        "caption": "Figure 1: The framework of proposed SNN adaptation pipeline.\nA pre-trained source model may degrade significantly on test data in wild-world scenarios, e.g., diverse weather conditions.\nThe proposed method adapts a source SNN model on the test data in an online manner based on unsupervised loss, which can significantly improve the SNN model s performance in such scenarios.",
        "qtype": "Others",
        "response": "Here’s how I read the question and why the answer is “Yes.”\n\n1.  What is [mask1]?  \n    In the bottom half of Fig. 1 you see an SNN into which only a handful of layers have been “unlocked” for gradient updates (the red‐boxed region).  The vast majority of the network – all of the blue spiking‐layers and most of the gray ReLU‐layers – remains frozen, and at test‐time we only update a few parameters (essentially the affine‐BN scales and the new clipping thresholds).  In the paper they even call this “partial parameters fine-tuning.”\n\n2.  Why do they have to freeze everything else?  \n    It all stems from the way they convert a pre-trained ANN into an SNN.  They use a classic weight-and-threshold matching procedure (equations 6→9 in the text) to guarantee that, at large time-steps, the average spiking rate reproduces the original ReLU activations.  Unfortunately that conversion procedure by its very nature fixes most of the weights and thresholds.  If you tried to backpropagate through all of those converted layers you’d violate the original mapping assumptions (you’d break the “I guarantee that my firing rate = your ReLU activation” property), and you’d force a costly full-BPTT over time.\n\n3.  So could a different ➔ANN→SNN conversion loosen those “freeze almost everything” constraints?  \n    Absolutely.  Over the last year or two there have been several conversion schemes whose goal is precisely to preserve gradient‐through‐time and thereby allow **all** layers to remain trainable:\n\n    •   **SpikeNorm / Threshold‐balance conversions** (e.g. Wu et al, Bu et al) calibrate layer‐wise thresholds with small batches of data so that you never need a hard “max‐activation” pre-compute and you can keep thresholds as learnable parameters.  \n    •   **Quantization‐aware** or **training-aware** conversions insert small surrogate‐gradient hooks right at conversion time, so that every weight ends up in a fully differentiable subgraph.  \n    •   **Hybrid conversion + fine-tune** schemes – you do a lightweight conversion, then immediately do a brief surrogate-gradient fine-tuning on a small “hold-out” of representative (or even unlabeled) samples.  By the time test samples arrive you already have a fully trainable SNN.\n\n    In each of these you no longer have to say, “OK, I’ll freeze 95 percent of the network and only update BN scales + threshold clips.”  Instead, you carry over the full ANN gradient graph (with surrogate functions in place of the Heaviside) and you can update every layer online with that same low-overhead “forward‐in-time” credit‐assignment algorithm from Bellec et al.\n\n4.  Is there a downside?  \n    The trade-off is that those richer conversion methods usually involve either  \n    – a small extra calibration or finetuning pass up front (before you ever hit the satellite); or  \n    – a heavier conversion routine that bakes in surrogate centers so that each layer remains fully differentiable.  \n\n    In practice though, if your goal is truly plug-and-play online adaptation, swapping in one of these newer conversion recipes will let you “unlock” far more of the network at test time, thereby freeing you from the red-boxed “partial parameter fine-tuning” constraint.\n\n— So the bottom line is: yes.  by adopting a more gradient‐friendly ANN→SNN conversion (spike-norm, quant-aware or hybrid conversion schemes), you can preserve end-to-end differentiability and no longer have to lock down (freeze) almost the entire network during online adaptation."
    },
    {
        "question": "What cumulative error risks stem from cascading DaYu stage1 and DaYu stage2 autoregressive predictions?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Additionally, to reduce the accumulation error, we adopt staged fine-tuning approach, where the model is fine-tuned and saved within fixed prediction leading time. These sub-models are then combined in a cascading manner to achieve good predict capability.",
            "The final predictions from DaYu take the form of autoregressive predictions, using the initial true observed cloud image as the starting for the autoregressive prediction. Each single-time-step prediction output is used as the input for the next time step in the autoregressive process. This cycle continues until the output for the last time step is produced, which is the predicted cloud image 12 hours later."
        ],
        "final_answer": "By cascading the two autoregressive sub-models (stage1 for 0–6h and stage2 for 6–12h), any forecasting error made by stage1 is fed forward into stage2. As each half-hour forecast in the chain uses the previous prediction as input, small errors accumulate step by step. Over multiple steps this compounding — called accumulation error — can lead to progressively larger deviations from the true future state, especially at longer lead times.",
        "relevant_elements": [
            "DaYu stage1",
            "DaYu stage2"
        ],
        "id": 2110,
        "masked_question": "What cumulative error risks stem from cascading [mask1] and DaYu stage2 autoregressive predictions?",
        "masked_number": 1,
        "masked_elements": [
            "DaYu stage1"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.10144v1_figure_1.png",
        "paperid": "2411.10144v1",
        "paper_path": "./MISS-QA/papers/2411.10144v1.json",
        "figure_id": "2411.10144v1_figure_1.png",
        "caption": "Figure 1: Overview of the DaYu Architecture. DaYu cascades the parameters of Stage 1 and Stage 2 models. Taking continuous cloud images from two moments x−1superscript𝑥1x^{-1}italic_x start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, x0superscript𝑥0x^{0}italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT as inputs, DaYu Stage 1 autoregressively forecasts 12 cloud images for the 0-6 hour period, and then uses x11superscript𝑥11x^{11}italic_x start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT, x12superscript𝑥12x^{12}italic_x start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT as inputs. Stage 2 forecasts 12 cloud images for the 6-12 hour period as the same pattern. Red-bordered boxes indicate the initial inputs for the stage models. Spatio-Temporal Encoder extracts feature embeddings from inputs. Transformer layers are then used to learn the global relationships of high-level abstract features. Finally, Spatio-Temporal Decoder generates the predicted cloud image for the next moment. In this figure, i𝑖iitalic_i ranges from 0 to 23.",
        "qtype": "Others",
        "response": "Here’s the reasoning in detail:\n\n1.  Identifying [mask1]  \n    In the figure, the red‐outlined block on the left (“DaYu stage1”) is exactly what [mask1] is calling out.  During inference, DaYu Stage 1 takes the two true observations at t = –½ h and t = 0 h and then autoregressively produces twelve half‐hour forecasts out to t = 6 h.\n\n2.  How Stage 2 is driven by Stage 1  \n    Immediately after Stage 1 has produced its 12th half‐hour forecast (the t = 6 h field), Stage 2 “plugs in” the last two Stage 1 outputs (the t = 5.5 h and t = 6 h fields) as if they were true observations.  It then runs its own twelve‐step autoregressive loop to go from 6 h out to 12 h.\n\n3.  Where the cumulative error comes from  \n    •  Any small bias or noise Stage 1 injected into its forecasts now becomes the *input* for Stage 2.  \n    •  Stage 2, being autoregressive, treats those imperfect inputs as ground truth, so its very first forecast already “inherits” Stage 1’s errors.  \n    •  Those inherited errors then feed into the next half‐hour prediction, and so on, so that errors cascade through all subsequent steps.  \n\n4.  The practical consequences  \n    –  Systematic biases get amplified (e.g. too-bright or too-dark cloud tops become even more off in the 12 h forecast)  \n    –  Random noise grows, leading to blurrier, less physically realistic structures  \n    –  Forecast skill degrades rapidly with lead time, since neither stage ever “sees” the real atmosphere after t = 0 h  \n\nIn short, by cascading the Stage 1 outputs into Stage 2’s autoregressive loop, any error—bias or random—introduced early on is continually compounded, leading to increasing drift and loss of forecast fidelity as you push out toward the 12 h horizon."
    },
    {
        "question": "How could alternative embedding methods reduce computational overhead of the Patch Embedding & Merge block?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "Patch Embedding & Merge"
        ],
        "id": 2111,
        "masked_question": "How could alternative embedding methods reduce computational overhead of the [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Patch Embedding & Merge"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.10144v1_figure_1.png",
        "paperid": "2411.10144v1",
        "paper_path": "./MISS-QA/papers/2411.10144v1.json",
        "figure_id": "2411.10144v1_figure_1.png",
        "caption": "Figure 1: Overview of the DaYu Architecture. DaYu cascades the parameters of Stage 1 and Stage 2 models. Taking continuous cloud images from two moments x−1superscript𝑥1x^{-1}italic_x start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, x0superscript𝑥0x^{0}italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT as inputs, DaYu Stage 1 autoregressively forecasts 12 cloud images for the 0-6 hour period, and then uses x11superscript𝑥11x^{11}italic_x start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT, x12superscript𝑥12x^{12}italic_x start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT as inputs. Stage 2 forecasts 12 cloud images for the 6-12 hour period as the same pattern. Red-bordered boxes indicate the initial inputs for the stage models. Spatio-Temporal Encoder extracts feature embeddings from inputs. Transformer layers are then used to learn the global relationships of high-level abstract features. Finally, Spatio-Temporal Decoder generates the predicted cloud image for the next moment. In this figure, i𝑖iitalic_i ranges from 0 to 23.",
        "qtype": "Others",
        "response": "First, let’s be precise about what “[mask1]” actually is.  In the red‐outlines on the left of Fig. 1 you see the “Patch Embedding & Merge” block: the very first thing we do to the two consecutive full‐disk brightness‐temperature frames.  Concretely, the paper says\n\n  •  We start with a tensor of size  \n     (2 time steps) × (8 channels) × (2401 × 2401 pixels).  \n  •  We run a small conv‐based patch embedding (4×4 kernel, stride 4)  \n     to collapse the two times and eight channels into a single “token”  \n     map of size 600 × 600 (per-frame).\n\nEven after that we still have a 600×600 feature map, so the authors need two more 3×3 down-sampling residual conv layers before the Transformer can even touch it.\n\nThe question is:  \n  “How could alternative embedding methods reduce the computational overhead of the [mask1]?”\n\nIn other words: what are cheaper ways to do “patch embedding and merge” so that 2401×2401 → 600×600 (and fuse two time-steps/channels) uses far less compute and memory?  Below is the step-by-step reasoning and a few concrete suggestions:\n\n1.  Identify the bottleneck  \n    –  Naively embedding an N×N image with C channels by a single 4×4 stride-4 convolution over 2401×2401 costs on the order of  \n       FLOPs ≃ (2401/4)² × 4² × C × D,  \n       where D is the output dimension.  \n    –  That alone is millions of multiply-adds, even before you do the two down-sampling residual blocks.\n\n2.  Think “separable” or “factorized” convs  \n    A.  Depthwise + pointwise conv  \n       –  Instead of a full 4×4×C→D conv, do a depthwise 4×4 (one filter per input channel) followed by a 1×1 pointwise conv.  \n       –  This cuts the cost from O(4²×C×D) to O(4²×C + C×D).  \n    B.  Grouped conv  \n       –  Split the C channels into G groups, convolve each group separately, then concatenate.  \n       –  This reduces the heavy C×D mat-mul into G smaller (C/G × D/G) blocks.\n\n3.  Patchify + linear projection (ViT style)  \n    –  Instead of a learned 4×4 conv, manually slice the image into 4×4 non-overlapping patches, flatten each patch of size (4×4×C) into a vector, then apply one D-dimensional linear projection.  \n    –  Complexity per patch: (4×4×C)×D.  \n    –  You still do (2401/4)² patches, but matrix multiplications are extremely well optimized, and you avoid the 4×4 strided sliding window altogether.\n\n4.  Lightweight downsampling—pooling + 1×1 conv  \n    –  First do a simple 4×4 average- or max-pool (cheap) to go from 2401→600, then a single 1×1 conv to change channel dimension/merge time steps.  \n    –  Pooling costs virtually nothing compared to a big conv, and 1×1 conv only scales with number of channels.\n\n5.  Random or fixed feature maps (Fourier embeddings, hashed embeddings)  \n    –  You can project patches into a higher-dimensional space via random Fourier features or a fixed, non-learned mapping, then fine-tune only a small linear layer.  \n    –  This pushes most of the “heavy lifting” outside the training loop into random projections that do not require backprop or large convolutions.\n\n6.  Temporal/channel fusion first, spatial resize later  \n    –  Fuse the two time steps and eight channels by a tiny MLP per pixel (1×1 conv)—cost ∼O(C_in×C_mid)×(2401²).  \n    –  Then apply a global, non-learned downsampling (e.g. bilinear interpolation or pooling) from 2401→600.  \n    –  You defer any large‐kernel convolution until after you’ve already shrunk the H×W by 16×.\n\nAll of the above share the same basic theme: factor the big 4×4 strided convolution into smaller, cheaper operations plus a small projection.  In practice one can\n\n  •  replace “Conv4×4, stride 4” by  \n     –  “DepthwiseConv4×4, stride 4 + PointwiseConv1×1,” or  \n     –  “AveragePool(4×4, stride 4) + Conv1×1,” or  \n     –  “Patchify then Linear,”\n\nand cut both the number of parameters and the multiply-adds by a factor of 3–10 (or more), without substantially harming the quality of the fused time‐step/channel embedding.\n\nIn short, the “Patch Embedding & Merge” block can be dramatically lightened by using separable or factorized convolutions, pooling-plus-pointwise projections, or explicit patch-flattening + linear layers, all of which shrink both memory footprint and FLOPs compared to a single monolithic 4×4×C→D conv."
    },
    {
        "question": "What motivates cascading DaYu stage1 and stage2 autoreg instead of a single direct forecast model?",
        "relevant_section_ids": [
            "2.1.1",
            "3.2.2"
        ],
        "relevant_context": [
            "However, based on experience and similar meteorological forecasting efforts, it is challenging to directly learn the mapping from current brightness temperature observations to the brightness temperature states 12 hours into the future, which would result in significant errors. Therefore, DaYu aims to learn a mapping to predict the data for the next time step, and then uses this next time step’s data as input to generate multi-step predictions in an autoregressive manner.",
            "Additionally, to reduce the accumulation error, we adopt staged fine-tuning approach, where the model is fine-tuned and saved within fixed prediction leading time. These sub-models are then combined in a cascading manner to achieve good predict capability."
        ],
        "final_answer": "Because directly forecasting 12 hours ahead in one shot leads to large errors, DaYu splits the task into two autoregressive sub-models (0–6 h and 6–12 h). This staged, cascading approach reduces error accumulation and yields more accurate predictions than a single direct forecast model.",
        "relevant_elements": [
            "DaYu stage1",
            "DaYu stage2",
            "autoreg"
        ],
        "id": 2112,
        "masked_question": "What motivates cascading [mask1] and stage2 autoreg instead of a single direct forecast model?",
        "masked_number": 1,
        "masked_elements": [
            "DaYu stage1"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.10144v1_figure_1.png",
        "paperid": "2411.10144v1",
        "paper_path": "./MISS-QA/papers/2411.10144v1.json",
        "figure_id": "2411.10144v1_figure_1.png",
        "caption": "Figure 1: Overview of the DaYu Architecture. DaYu cascades the parameters of Stage 1 and Stage 2 models. Taking continuous cloud images from two moments x−1superscript𝑥1x^{-1}italic_x start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, x0superscript𝑥0x^{0}italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT as inputs, DaYu Stage 1 autoregressively forecasts 12 cloud images for the 0-6 hour period, and then uses x11superscript𝑥11x^{11}italic_x start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT, x12superscript𝑥12x^{12}italic_x start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT as inputs. Stage 2 forecasts 12 cloud images for the 6-12 hour period as the same pattern. Red-bordered boxes indicate the initial inputs for the stage models. Spatio-Temporal Encoder extracts feature embeddings from inputs. Transformer layers are then used to learn the global relationships of high-level abstract features. Finally, Spatio-Temporal Decoder generates the predicted cloud image for the next moment. In this figure, i𝑖iitalic_i ranges from 0 to 23.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning in steps:\n\n1. The red‐boxed “mask1” in the figure is simply the Stage 1 sub-model (the “DaYu stage1” block) that takes X⁻¹ and X⁰ and autoregressively predicts the 0–6 h sequence.  \n2. From the text (Sect. 2.1.1 and 3.2) we know the authors tried, and found it very hard, to learn a single mapping   \n      (X⁻¹,X⁰) → X¹² directly (i.e. a 12 h one-shot forecast).  \n3. In practice a one-shot, long-range forecast:  \n   • suffers from huge training instability,  \n   • accumulates large errors,  \n   • requires an enormous model capacity to capture all long-term spatio-temporal dependencies at once.  \n4. By contrast, breaking the 12 h horizon into two sub-intervals (0–6 h and 6–12 h), each handled by its own autoregressive finely tuned sub-model, gives you:  \n   • shorter horizons in each stage (so smaller error growth per stage),  \n   • the ability to pre-train/fine-tune each stage on its own lead-time (removing the burden of a single gigantic model),  \n   • reduced computational and memory cost in training each sub-model,  \n   • overall better long-range stability when you cascade the two.  \n5. Hence, cascading Stage 1 and Stage 2 autoregressively is motivated by the need to tame error accumulation and make long-range (12 h) forecasting both trainable and accurate, something a single direct model could not achieve."
    },
    {
        "question": "Why implement skip connections linking Residual Convolution Layer and Transformer Layer features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Skip Connection",
            "Residual Convolution Layer",
            "Transformer Layer"
        ],
        "id": 2113,
        "masked_question": "Why implement [mask1] linking Residual Convolution Layer and Transformer Layer features?",
        "masked_number": 1,
        "masked_elements": [
            "Skip Connection"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.10144v1_figure_1.png",
        "paperid": "2411.10144v1",
        "paper_path": "./MISS-QA/papers/2411.10144v1.json",
        "figure_id": "2411.10144v1_figure_1.png",
        "caption": "Figure 1: Overview of the DaYu Architecture. DaYu cascades the parameters of Stage 1 and Stage 2 models. Taking continuous cloud images from two moments x−1superscript𝑥1x^{-1}italic_x start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT, x0superscript𝑥0x^{0}italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT as inputs, DaYu Stage 1 autoregressively forecasts 12 cloud images for the 0-6 hour period, and then uses x11superscript𝑥11x^{11}italic_x start_POSTSUPERSCRIPT 11 end_POSTSUPERSCRIPT, x12superscript𝑥12x^{12}italic_x start_POSTSUPERSCRIPT 12 end_POSTSUPERSCRIPT as inputs. Stage 2 forecasts 12 cloud images for the 6-12 hour period as the same pattern. Red-bordered boxes indicate the initial inputs for the stage models. Spatio-Temporal Encoder extracts feature embeddings from inputs. Transformer layers are then used to learn the global relationships of high-level abstract features. Finally, Spatio-Temporal Decoder generates the predicted cloud image for the next moment. In this figure, i𝑖iitalic_i ranges from 0 to 23.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed “mystery” in Figure 1 is nothing more exotic than a set of U-Net-style skip‐connections running from each convolutional block in the encoder over into its matching block in the decoder.  In practice they serve three purposes:\n\n  1.  Preserve fine, high-resolution spatial detail that would otherwise be washed out by the successive down-sampling and global attention in the middle (Transformer) stacks.  \n  2.  Give the decoder direct access to the encoder’s low-level feature maps so that local texture and edge information can be fused back in alongside the higher-level, globally contextualized representations.  \n  3.  Improve gradient flow through the entire network (from decoder back to early encoder layers), making the whole thing much easier to train.  \n\nWithout those skip links the model would have to re-learn all of its local structure from the heavily compressed Transformer output, which typically leads to blurrier, less accurate reconstructions."
    },
    {
        "question": "What motivates combining inter-modality and inter-cancer contrastive learning objectives?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "In the first stage, we aim to inject multimodal knowledge into the slide aggregator by contrastive learning, including inter-modality contrastive learning (following CLIP [43]), and inter-cancer contrastive learning.",
            "To alleviate the heterogeneity of various cancer types, we utilized inherent cancer labels available in TCGA for the inter-cancer pretraining objective."
        ],
        "final_answer": "They combine inter-modality contrastive learning to inject and align information across WSIs, reports, and gene profiles with inter-cancer contrastive learning to reduce variability across different cancer types, yielding a slide aggregator that is both multimodally informed and robust to cancer heterogeneity.",
        "relevant_elements": [
            "inter-modality contrastive learning",
            "inter-cancer contrastive learning"
        ],
        "id": 2114,
        "masked_question": "What motivates combining [mask1] and [mask2] objectives?",
        "masked_number": 2,
        "masked_elements": [
            "inter-modality contrastive learning",
            "inter-cancer contrastive learning"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.15362v2_figure_2.png",
        "paperid": "2407.15362v2",
        "paper_path": "./MISS-QA/papers/2407.15362v2.json",
        "figure_id": "2407.15362v2_figure_2.png",
        "caption": "Figure 2: The Overview of mSTAR Pipeline. mSTAR is a whole-slide pretraining paradigm comprising two-stage pretraining. a, Stage 1 aims to inject multimodal knowledge into a slide aggregator by slide-level contrastive learning among WSIs, pathology reports and gene expression data. b, Stage 2 aims to seamlessly propagate multimodal knowledge learned at the slide level into the patch extractor by Self-Taught training, which leverages the slide aggregator pretrained in Stage 1 as “Teacher” and enforces patch extractor to be “Student”.",
        "qtype": "Design_Rationale",
        "response": "First, let’s identify the two masked objectives from the figure and text:\n\n- [mask1] is the red-boxed “inter-modality contrastive learning” objective – i.e. pulling together the [CLS] representations of matching WSI⇄report, WSI⇄gene and report⇄gene pairs.  \n- [mask2] is the blue-boxed “inter-cancer contrastive learning” objective – i.e. a triplet loss that brings samples of the same cancer type closer and pushes different types apart.\n\nWhy do we use both at once?  \n\nChain of thought:  \n1. Our primary goal in stage 1 is to teach the slide‐aggregator to absorb as much complementary information as possible from the three modalities (whole‐slide images, text reports, and gene‐expression profiles).  Inter-modality contrastive learning ([mask1]) forces the network to align and fuse those disparate views into a shared embedding space.  \n2. But TCGA covers 32 very different cancer types, each with its own morphology, report style, and molecular signature.  If you only did inter-modality alignment, the model might learn to correlate modalities within each sample but still conflate samples from, say, lung adenocarcinoma and breast carcinoma.  \n3. To guard against that—and to regularize the embedding space so that it respects the known cancer‐type labels—we add the inter-cancer triplet loss ([mask2]).  That loss explicitly clusters same-type cases and separates different-type cases.  \n4. Together, the two objectives ensure that (a) all three modalities talk to each other, and (b) the global layout of the embedding space also reflects cancer‐type structure, yielding a slide aggregator that (i) is multimodally informed and (ii) is robust to the heterogeneity across tumor types.\n\nIn short, we combine inter-modality contrastive learning and inter-cancer contrastive learning so that the slide aggregator both fuses information across WSIs, reports, and genes and simultaneously respects (and leverages) the inherent grouping by cancer type."
    },
    {
        "question": "What is the purpose of using EMA updates in the patch extractor Self-Taught training?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Additionally, to avoid the catastrophic forgetting problem, a siamese structure is employed for the patch extractor consisting of two identical branches, where the parameters of one branch are updated using gradient descent, while the parameters of the other branch are updated using an Exponential Moving Average (EMA) of the parameters from the previous branch, without any gradient updates."
        ],
        "final_answer": "The EMA updates are used to prevent catastrophic forgetting by maintaining a secondary ‘teacher’ branch of the patch extractor whose parameters evolve more smoothly via exponential moving averaging.",
        "relevant_elements": [
            "EMA",
            "patch extractor",
            "Self-Taught training"
        ],
        "id": 2115,
        "masked_question": "What is the purpose of using [mask1] updates in the [mask2] during Self-Taught training?",
        "masked_number": 2,
        "masked_elements": [
            "EMA",
            "patch extractor"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.15362v2_figure_2.png",
        "paperid": "2407.15362v2",
        "paper_path": "./MISS-QA/papers/2407.15362v2.json",
        "figure_id": "2407.15362v2_figure_2.png",
        "caption": "Figure 2: The Overview of mSTAR Pipeline. mSTAR is a whole-slide pretraining paradigm comprising two-stage pretraining. a, Stage 1 aims to inject multimodal knowledge into a slide aggregator by slide-level contrastive learning among WSIs, pathology reports and gene expression data. b, Stage 2 aims to seamlessly propagate multimodal knowledge learned at the slide level into the patch extractor by Self-Taught training, which leverages the slide aggregator pretrained in Stage 1 as “Teacher” and enforces patch extractor to be “Student”.",
        "qtype": "Design_Rationale",
        "response": "In Stage 2’s self-taught training we actually instantiate the patch extractor as a little Siamese network, with two copies of the same ViT.  One copy is your “student” (its weights receive ordinary gradient‐based updates) and the other copy is your slowly-evolving “target” network – its weights are never directly back-propagated, but are updated instead by an Exponential Moving Average (EMA) of the student’s weights (this is the blue-boxed “stop-gradient” branch in the figure).  \n\nThe reason for using EMA updates there is twofold:  \n 1. It produces a much more stable set of “teacher” embeddings for each patch, since that network only drifts gradually over training rather than lurching around with every SGD step.  \n 2. By matching the student’s output to these slowly-evolving targets, you get a strong self-supervised learning signal without collapse or catastrophic forgetting.  \n\nIn short, the EMA-updated branch serves as a stable teacher, which the gradient-updated student then learns to imitate."
    },
    {
        "question": "How does Self-Taught training leverage EMA updates to prevent catastrophic forgetting in the patch extractor?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Additionally, to avoid the catastrophic forgetting problem, a siamese structure is employed for the patch extractor consisting of two identical branches, where the parameters of one branch are updated using gradient descent, while the parameters of the other branch are updated using an Exponential Moving Average (EMA) of the parameters from the previous branch, without any gradient updates.",
            "Afterward, we enforced a similarity constraint between the patch features extracted by the branch with gradient updates and those embedded by the branch with EMA updates."
        ],
        "final_answer": "Self-Taught training uses a siamese setup for the patch extractor: one branch is trained normally with gradient descent, while the other branch keeps an EMA-updated copy of its parameters (no gradients). By enforcing the student branch’s features to match those of the EMA branch, the model retains previously learned multimodal knowledge and thus avoids catastrophic forgetting.",
        "relevant_elements": [
            "Self-Taught training",
            "EMA",
            "patch extractor"
        ],
        "id": 2116,
        "masked_question": "How does [mask1] leverage [mask2] updates to prevent catastrophic forgetting in the patch extractor?",
        "masked_number": 2,
        "masked_elements": [
            "Self-Taught training",
            "EMA"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.15362v2_figure_2.png",
        "paperid": "2407.15362v2",
        "paper_path": "./MISS-QA/papers/2407.15362v2.json",
        "figure_id": "2407.15362v2_figure_2.png",
        "caption": "Figure 2: The Overview of mSTAR Pipeline. mSTAR is a whole-slide pretraining paradigm comprising two-stage pretraining. a, Stage 1 aims to inject multimodal knowledge into a slide aggregator by slide-level contrastive learning among WSIs, pathology reports and gene expression data. b, Stage 2 aims to seamlessly propagate multimodal knowledge learned at the slide level into the patch extractor by Self-Taught training, which leverages the slide aggregator pretrained in Stage 1 as “Teacher” and enforces patch extractor to be “Student”.",
        "qtype": "Implementation_Details",
        "response": "In the Stage-2 “self-taught” schematic the red-boxed module is the *patch extractor* being trained (i.e. the “Student” network) and the blue-boxed element is the *EMA (exponential moving average) update* that keeps a slow-moving copy of that same extractor (i.e. its own “Teacher” branch).  In practice you form a Siamese pair:\n\n 1. one branch of the extractor is updated by ordinary back-prop (the red branch)  \n 2. the second branch is never directly back-propped, but is instead updated by EMA of the red branch’s weights (the blue branch)  \n\nAt each training step you enforce two consistency losses:  \n – a regression loss that pulls the red branch’s patch features toward the re-embedded features produced by the frozen slide-aggregator “Teacher”  \n – a similarity loss that constrains the red branch’s outputs to stay close to the blue (EMA) branch’s outputs  \n\nBecause the blue branch is a running average of past student weights, it serves as a slowly changing anchor.  That “anchor” prevents the red branch from drifting too far and thus guards against catastrophic forgetting of the multimodal knowledge injected in Stage 1."
    },
    {
        "question": "How does inter-cancer contrastive learning mine hard positives and negatives from concatenated [CLS] embeddings for triplet loss?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "To alleviate the heterogeneity of various cancer types, we utilized inherent cancer labels available in TCGA for the inter-cancer pretraining objective. Specifically, [CLS] tokens of available modalities (regardless of whether they involved two or three modalities) would be concatenated into a single anchor representation $\\boldsymbol{h}^\\text{anchor}$. Furthermore, positive and negative samples were obtained within the mini-batch, and they were from the same cancer and different cancers, respectively. Similarly, they were constructed in the same way by concatenating the [CLS] tokens from available modalities, leading to $\\boldsymbol{h}^\\text{pos}$ and $\\boldsymbol{h}^\\text{neg}$ for positive and negative samples, respectively.",
            "Subsequently, we enforced a triplet loss $\\mathcal{L}_\\text{triplet}$ for them to bring the samples of the same cancer closer than that of the negative sample:\n$$\n\\mathcal{L}_\\text{triplet} = \\max(0,\\, d(\\boldsymbol{h}^\\text{anchor},\\,\\boldsymbol{h}^{\\text{far-pos}}) - d(\\boldsymbol{h}^\\text{anchor},\\,\\boldsymbol{h}^{\\text{near-neg}}) + m),\n$$\nwhere $d(\\cdot,\\cdot)$ is the $L_2$ distance and $m$ is the margin. Here $\\boldsymbol{h}^{\\text{far-pos}}$ and $\\boldsymbol{h}^{\\text{near-neg}}$ represent the farthest positive sample (hard positive) and the nearest negative sample (hard negative) within a mini-batch, respectively, following the hard sample mining technique [46]."
        ],
        "final_answer": "In inter-cancer contrastive learning, the [CLS] embeddings from all available modalities for each sample are first concatenated to form an anchor representation. Within each mini-batch, other samples of the same cancer (‘positives’) and of different cancers (‘negatives’) are also concatenated into representations in the same way. The hardest positive is chosen as the one farthest (maximum L₂ distance) from the anchor among the same-cancer samples, and the hardest negative is the one closest (minimum L₂ distance) among the different-cancer samples. A triplet loss is then applied to pull the anchor closer to that hard positive than to the hard negative by at least a fixed margin.",
        "relevant_elements": [
            "inter-cancer contrastive learning",
            "triplet loss"
        ],
        "id": 2117,
        "masked_question": "How does [mask1] mine hard positives and negatives from concatenated [CLS] embeddings for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "inter-cancer contrastive learning",
            "triplet loss"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.15362v2_figure_2.png",
        "paperid": "2407.15362v2",
        "paper_path": "./MISS-QA/papers/2407.15362v2.json",
        "figure_id": "2407.15362v2_figure_2.png",
        "caption": "Figure 2: The Overview of mSTAR Pipeline. mSTAR is a whole-slide pretraining paradigm comprising two-stage pretraining. a, Stage 1 aims to inject multimodal knowledge into a slide aggregator by slide-level contrastive learning among WSIs, pathology reports and gene expression data. b, Stage 2 aims to seamlessly propagate multimodal knowledge learned at the slide level into the patch extractor by Self-Taught training, which leverages the slide aggregator pretrained in Stage 1 as “Teacher” and enforces patch extractor to be “Student”.",
        "qtype": "Implementation_Details",
        "response": "Mask‐1 is the inter‐cancer contrastive objective; Mask‐2 is the triplet‐loss term.  Concretely, during inter‐cancer pretraining we take each patient’s [CLS] vectors from whichever modalities are present (WSI, report, gene), concatenate them into a single “anchor” embedding, and do the same for every other patient in the batch.  We then use the true cancer‐type labels to split those batch mates into positives (same cancer) and negatives (different cancer) and, by hard-sample mining, pick\n\n • the farthest positive (i.e. the same‐type sample whose embedding is most distant from the anchor)  \n • the nearest negative (i.e. the different‐type sample whose embedding is closest to the anchor)\n\nThese two “hard” examples are then fed into a standard triplet loss (with margin 0.3) so that the anchor is pulled closer to its hardest positive than to its hardest negative."
    },
    {
        "question": "How does the state tracker parse doctor utterances to assign action types?",
        "relevant_section_ids": [
            "3.2.1"
        ],
        "relevant_context": [
            "As previously discussed, the state tracker categorizes doctors’ actions to guide subsequent behaviors from three perspective.",
            "(1) The state tracker first determines whether the action is Specific or Broad. If the doctor’s language is clear and specific, the patient-actor can respond to the doctor’s action. On the contrary, if the language is vague and broad, the patient-actor should require the doctor to ask a more specific question.",
            "(2) Following the real-world scenarios, we classify the specific actions into two main categories: Basic Information Inquiry and Medical Examination Recommendations. This classification determines whether the patient agent directly responds to questions or undergoes the relevant examinations.",
            "(3) Finally, we categorize actions based on whether they retrieve relevant patient information into Relevant and Irrelevant types. If the information or the suggested examination/test results requested by the doctor are present in the patient’s information, the action is considered Relevant; otherwise, the action is Irrelevant."
        ],
        "final_answer": "The state tracker splits each doctor utterance along three dimensions: first it checks if the language is clear and specific (Specific) or vague and general (Broad); if specific, it then classifies it as either a Basic Information Inquiry or a Medical Examination Recommendation; finally, it marks the request as Relevant or Irrelevant depending on whether the requested data exist in the patient’s record. These three binary decisions produce eight possible action types.",
        "relevant_elements": [
            "State Tracker",
            "Action Type"
        ],
        "id": 2118,
        "masked_question": "How does the [mask1] parse doctor utterances to assign action types?",
        "masked_number": 1,
        "masked_elements": [
            "State Tracker"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.08693v1_figure_1.png",
        "paperid": "2408.08693v1",
        "paper_path": "./MISS-QA/papers/2408.08693v1.json",
        "figure_id": "2408.08693v1_figure_1.png",
        "caption": "Figure 1: Overview of the Med-PMC evaluation framework. The whole framework can be divided into three parts, including a) Multi-modal consultation, b) Patient Simulator, and c) Evaluation.",
        "qtype": "Implementation_Details",
        "response": "First, the red‐boxed module is the “State Tracker.”  Whenever the doctor MLLM emits an utterance, the State Tracker runs a three‐step classification over that text:\n\n 1. Specificity check (Broad vs. Specific)  \n  – “Broad” if the question is vague (“Anything else I should know?”)  \n  – “Specific” if it clearly targets some piece of patient data or an exam.\n\n 2. Content type (Basic Information Inquiry vs. Medical Examination Recommendation)  \n  – If it’s a request for history, symptoms, demographics, etc., it’s an Information Inquiry.  \n  – If it’s ordering or suggesting a lab test, scan, or other exam, it’s an Examination Recommendation.\n\n 3. Relevance (Relevant vs. Irrelevant)  \n  – “Relevant” if the requested data/test is actually present in the patient profile.  \n  – “Irrelevant” if it isn’t.\n\nBy composing those three binary decisions (Specific/Broad × Inquiry/Exam × Relevant/Irrelevant), the tracker assigns one of eight possible “action types” to the doctor’s turn."
    },
    {
        "question": "How does the technician agent coordinate with information extractor to retrieve and format examination results?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Technician Agent",
            "Information Extractor"
        ],
        "id": 2119,
        "masked_question": "How does the technician agent coordinate with [mask1] to retrieve and format examination results?",
        "masked_number": 1,
        "masked_elements": [
            "Information Extractor"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.08693v1_figure_1.png",
        "paperid": "2408.08693v1",
        "paper_path": "./MISS-QA/papers/2408.08693v1.json",
        "figure_id": "2408.08693v1_figure_1.png",
        "caption": "Figure 1: Overview of the Med-PMC evaluation framework. The whole framework can be divided into three parts, including a) Multi-modal consultation, b) Patient Simulator, and c) Evaluation.",
        "qtype": "Implementation_Details",
        "response": "The red–boxed module in the figure is the “Information Extractor.”  Whenever the doctor issues an examination request (as detected by the State Tracker), the Technician Agent:\n\n1. calls on the Information Extractor to pull out only the fields that correspond to that exam (for example, ALT, AST, GGT and bilirubin for a liver panel, or the radiologist’s narrative for a CT),  \n2. receives back a small, structured bundle of exactly those values and findings,  \n3. and then wraps them into a two‐stage response—a terse “here are the raw results you asked for” followed by a human-readable, clinician-ready summary with units, normal ranges, and any key imaging observations.\n\nIn short, the Technician Agent “knocks” on the Information Extractor for the right bits of the report and then packages them up neatly for the doctor."
    },
    {
        "question": "How does Patient Simulator merge state tracking and personalized personas like existing role-playing simulation methods?",
        "relevant_section_ids": [
            "3.2.1",
            "3.2.3"
        ],
        "relevant_context": [
            "As previously discussed, the state tracker categorizes doctors’ actions to guide subsequent behaviors from three perspectives…we have defined a total of eight types of doctor actions to enable the patient-actor agent to respond more reliably.",
            "After obtaining the response information from the response generator, the Personalized Actor will rewrite the response, imbuing the patient agent with specific tones and personalities to simulate the diversity in how different patients express themselves in clinical settings. Specifically, we have set up 10 personas by combining two genders with five distinct professions."
        ],
        "final_answer": "Patient Simulator combines a state-tracking module that classifies the doctor’s latest action into one of eight types with a personalized actor module that rewrites the factual response in one of ten distinct patient personas (two genders × five professions). In this way, it merges precise state management with individualized role-playing characteristics in each simulated interaction.",
        "relevant_elements": [
            "Patient Simulator",
            "State Tracker",
            "Personalized Actor"
        ],
        "id": 2120,
        "masked_question": "How does [mask1] merge state tracking and personalized personas like existing role-playing simulation methods?",
        "masked_number": 1,
        "masked_elements": [
            "Patient Simulator"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.08693v1_figure_1.png",
        "paperid": "2408.08693v1",
        "paper_path": "./MISS-QA/papers/2408.08693v1.json",
        "figure_id": "2408.08693v1_figure_1.png",
        "caption": "Figure 1: Overview of the Med-PMC evaluation framework. The whole framework can be divided into three parts, including a) Multi-modal consultation, b) Patient Simulator, and c) Evaluation.",
        "qtype": "Literature_Background",
        "response": "Here’s the reasoning step by step:\n\n1.  In the red‐boxed region of Figure 1 (panel b) you see the “Patient Simulator,” which in the text is referred to as the patient‐actor agent.  \n2.  That agent is explicitly made of three components:\n    - A **state tracker** that classifies each doctor query along three axes (specific vs. broad, basic‐info vs. exam request, relevant vs. irrelevant),  \n    - A **response generator** that pulls the correct fact or triggers the technician agent for exam results,  \n    - A **personalized actor** that rewrites those factual responses in one of ten patient personas (two genders × five professions).  \n3.  By chaining state tracking (so the simulator “knows” exactly what kind of query it’s replying to and whether the information exists) with a persona module (so the answer sounds like a coherent, lived-in patient character), it effectively combines the strengths of structured state‐machine dialogue systems and free‐form role‐playing simulations.  \n\nAnswer:  \nThe red‐boxed “Patient Simulator” (i.e. the patient‐actor agent) merges state tracking and personalized personas by first using a state tracker to tag and route each doctor action, then generating the correct factual reply, and finally passing that reply through a persona module that re‐scripts it in the voice of one of ten preset patient profiles—thus uniting rigorous turn‐by‐turn state management with character-driven variation just like a role-playing simulation."
    },
    {
        "question": "How does LLM-based Evaluation complement automatic evaluation akin to prior multimodal clinical assessment frameworks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LLM-based Evaluation",
            "Automatic Evaluation"
        ],
        "id": 2121,
        "masked_question": "How does [mask1] complement automatic evaluation akin to prior multimodal clinical assessment frameworks?",
        "masked_number": 1,
        "masked_elements": [
            "LLM-based Evaluation"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.08693v1_figure_1.png",
        "paperid": "2408.08693v1",
        "paper_path": "./MISS-QA/papers/2408.08693v1.json",
        "figure_id": "2408.08693v1_figure_1.png",
        "caption": "Figure 1: Overview of the Med-PMC evaluation framework. The whole framework can be divided into three parts, including a) Multi-modal consultation, b) Patient Simulator, and c) Evaluation.",
        "qtype": "Literature_Background",
        "response": "In our framework “automatic evaluation” (e.g. ROUGE-based recall of chief complaint, lab findings, diagnosis, treatment plans) can tell you how much “correct” content the model parrots back, but it knows nothing about *how* the model arrived at those answers or whether its reasoning makes clinical sense. The component highlighted in the red box—LLM-based evaluation—fills exactly that gap. By feeding a second, expert-tuned LLM (in our case GPT-4o) a series of short, structured prompts (scoring inquiry, examination planning, multi-modal interpretation, diagnostic logic and treatment rationale on a 1–5 scale), we get:\n\n• A fine-grained, semantically aware score on each key clinical competency  \n• A check on step-by-step reasoning, not just surface overlap  \n• Sensitivity to multimodal understanding (e.g. “Did the model actually *interpret* the CT image, or just re‐state findings?”)  \n\nIn short, LLM-based evaluation complements simple overlap metrics by providing a human-like, clinically informed appraisal of the model’s *decision-making process* and *multimodal reasoning*, rather than just its final word choices."
    },
    {
        "question": "How does Single-turn QA guide the Generate→Evidence→Optimize pipeline to enhance Multi-turn Dialogue authenticity?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Based on the extracted data in section III-A, we select a subset of the highest upvoted answers for each question to generate multi-turn dialogue data using KimiChat.",
            "First, we use an appropriate prompt to guide the KimiChat to construct multi-turn dialogue data between pre-defined roles according to the selected pairs. The original pairs serve as prior knowledge to guide KimiChat in generating multi-turn conversations, thereby enhancing the fidelity of the generated data to real-world conversations.",
            "Second, we employ an additional prompt to assess whether the answers in the multi-turn dialogue dataset are derived from the original context. If the majority of responses in the multi-turn dialogue can be supported by evidence extracted from the original text data, it is considered to better reflect the conversational flow of realistic counselors.\nConversely, if most responses are generated predominantly by the model without effectively leveraging the given context, such data entries require further processing. This can be achieved by using an additional prompt to enhance the integration of factual evidence and incorporate content-relevant information from the original context without compromising fluency.",
            "Third, to enhance the quality of the multi-turn dialogue data further, we utilize a prompt to revise the data in terms of empathy, supportiveness, guidance, and safety."
        ],
        "final_answer": "The single-turn QA pairs collected in Section III-A are used as the grounding ‘prior knowledge’ for each stage of the pipeline: in the Generate step they are fed as prompts so KimiChat produces a multi-turn exchange that stays true to real user questions and professional answers; in the Evidence step the same QA pairs serve as the reference to check whether model responses are actually drawn from the original content; and in the Optimize step they guide the refinement prompts that inject missing factual details and reshape the dialogue to be more empathic, supportive, and realistic—together ensuring that the final multi-turn dialogues authentically reflect genuine counseling conversations.",
        "relevant_elements": [
            "Single-turn QA",
            "Generate→Evidence→Optimize",
            "Multi-turn Dialogue"
        ],
        "id": 2122,
        "masked_question": "How does [mask1] guide the Generate→Evidence→Optimize pipeline to enhance Multi-turn Dialogue authenticity?",
        "masked_number": 1,
        "masked_elements": [
            "Single-turn QA"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.05721v2_figure_1.png",
        "paperid": "2407.05721v2",
        "paper_path": "./MISS-QA/papers/2407.05721v2.json",
        "figure_id": "2407.05721v2_figure_1.png",
        "caption": "Figure 1: Overview of dataset preparation.",
        "qtype": "Literature_Background",
        "response": "Mask1 refers to the “Single‐turn QA” block in the middle of Figure 1.  In our three-step Generate→Evidence→Optimize pipeline it plays exactly the role of our “prior knowledge” anchor:\n\n1.  Generate (using KimiChat)  \n    – We prompt the model with the original high‐quality question–answer pairs from the Single‐turn QA pool so that every new turn is rooted in a genuine user concern and its professional reply.  \n\n2.  Evidence  \n    – We then run a second prompt that flags any generated utterances that stray from or invent facts not present in those seed QA pairs.  Any turn that cannot be traced back to the Single-turn QA is flagged for revision.  \n\n3.  Optimize  \n    – Finally, we send the vetted, evidence-backed dialogue through a style‐tuning prompt (and manual proofread) that infuses the conversation with counselor-like empathy, guidance and safety.  \n\nBy using the Single-turn QA examples as both the content and the factual touchstone for generation, we ensure each new turn stays authentic to a real counseling exchange, while still sounding like a natural, multi-turn conversation."
    },
    {
        "question": "How does Qwen1.5-72B leverage psychological knowledge content to generate multiple-choice and short-answer Knowledge QA pairs?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA from them.",
            "Specifically, we segment books into text spans using a predefined fixed length, identifying the nearest sentence or paragraph as segmentation indicators. These text spans serve as the fundamental units for subsequent QA generation through the utilization of LLMs.",
            "First, the LLM generates questions and their corresponding answers. These question-answer pairs are then input into two LLM-based student modules, one utilizing retrieval-augmented generation (RAG) and the other without RAG, to produce two new sets of answers.",
            "Subsequently, a teacher module, also based on an LLM, evaluates and selects the best answer from those generated by the student modules.",
            "Furthermore, to ensure the quality and accuracy of the generated QA pairs, a manual validation process is implemented, wherein human evaluators assess and eliminate low-quality data.",
            "In addition, we extract after-school exercises from several books and convert them to QA format, with the corresponding answer analyses.",
            "Finally, we obtain 10K knowledge QA data."
        ],
        "final_answer": "Qwen1.5-72B first ingests raw psychological knowledge—by crawling psychology books and splitting them into manageable text spans. It then prompts the model to generate question–answer pairs from each span. To refine and verify those pairs, it runs two “student” LLM modules (one with retrieval-augmented generation, one without) to produce alternative answers, and a “teacher” LLM module to pick the best answer. Human annotators then manually validate and remove low-quality items. Finally, after-school exercises from the books are also converted into QA form with answer analyses, yielding a high-quality set of multiple-choice and short-answer knowledge QA pairs.",
        "relevant_elements": [
            "Qwen1.5-72B",
            "psychological knowledge content",
            "Knowledge QA"
        ],
        "id": 2123,
        "masked_question": "How does [mask1] leverage [mask2] to generate multiple-choice and short-answer Knowledge QA pairs?",
        "masked_number": 2,
        "masked_elements": [
            "Qwen1.5-72B",
            "psychological knowledge content"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.05721v2_figure_1.png",
        "paperid": "2407.05721v2",
        "paper_path": "./MISS-QA/papers/2407.05721v2.json",
        "figure_id": "2407.05721v2_figure_1.png",
        "caption": "Figure 1: Overview of dataset preparation.",
        "qtype": "Literature_Background",
        "response": "Mask1 is the LLM “Qwen1.5-72B,” and Mask2 is the “psychological knowledge content” (i.e. the text spans extracted from psychology books).  In the pipeline the raw knowledge spans are first chunked into manageable passages and then fed as context into Qwen1.5-72B.  The model is prompted to generate question–answer pairs from that context, producing both multiple‐choice questions and short‐answer questions.  Those initial outputs are then refined (via student/teacher reranking) and lightly human-checked to yield the final 10 K knowledge-QA items."
    },
    {
        "question": "How does web crawler preprocessing influence the variety of single-turn QA topics?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "After these data cleaning procedures, we obtain over 155k pairs for single-turn QA.",
            "We divide the processed data into 9 major topics and 127 subtopics, as illustrated by the distribution of major topics depicted in Fig. 2.",
            "It is observed that the number of reports related to emotional issues and regulation, as well as interpersonal relationships and socializing, is higher than those in other categories, each comprising more than 20% of the total. Following these topics, family and marriage, and personal growth and development are the next most significant areas, each accounting for more than 10% of the reports."
        ],
        "final_answer": "By cleaning and filtering the raw crawled data (removing short, low-engagement and irrelevant entries), the preprocessing yields a high-quality set of over 155k single-turn QA pairs that cover a broad spectrum: they span 9 major topics and 127 subtopics, with emotional regulation and interpersonal relationships each comprising over 20% of entries, and family/marriage and personal growth each over 10%.",
        "relevant_elements": [
            "Web crawlers",
            "Single-turn QA"
        ],
        "id": 2124,
        "masked_question": "How does [mask1] preprocessing influence the variety of [mask2] topics?",
        "masked_number": 2,
        "masked_elements": [
            "Web crawlers",
            "Single-turn QA"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.05721v2_figure_1.png",
        "paperid": "2407.05721v2",
        "paper_path": "./MISS-QA/papers/2407.05721v2.json",
        "figure_id": "2407.05721v2_figure_1.png",
        "caption": "Figure 1: Overview of dataset preparation.",
        "qtype": "Experimental_Results",
        "response": "Here’s the reasoning step by step:\n\n1.  Diagram alignment  \n    - The red box (`[mask1]`) highlights “Web crawlers” under the Data-collection stage.  \n    - The blue box (`[mask2]`) highlights “Single-turn QA,” i.e. the first slice of the final dataset.\n\n2.  What “Web crawlers” does  \n    - It automatically scrapes question–answer pairs from a variety of online and published sources (Zhihu, Yixinli, psychology books, papers, etc.).  \n    - This gives us a very large, heterogeneous pool of raw QA data spanning many aspects of psychology.\n\n3.  How that influences topic variety in the Single-turn QA  \n    - Because we crawled from multiple communities and from books/papers, the initial raw set naturally covers dozens of sub-domains.  \n    - After cleaning (removing ads, very short threads, low-liked answers, non-professional respondents), we still retain breadth—but drop noise.  \n    - In the end, our Single-turn QA splits into 9 major psychological topics and 127 subtopics.  \n    - Moreover, it preserves where real demand is highest (emotional regulation and social/interpersonal issues each > 20%, then family/marriage and personal growth each > 10%).\n\nAnswer  \n“By using web crawlers to harvest QA from multiple online communities and published texts, we start with a very broad raw set; after filtering out noise (ads, very short posts, low-rated or non-professional answers), the resulting Single-turn QA spans 9 major topic areas and 127 subtopics—most heavily weighted toward emotion regulation and social/interpersonal concerns but still covering the full spectrum of everyday counseling needs.”"
    },
    {
        "question": "How does the evidence judgment step enhance fidelity before the optimization step in multi-turn dialogue generation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Second, we employ an additional prompt to assess whether the answers in the multi-turn dialogue dataset are derived from the original context. If the majority of responses in the multi-turn dialogue can be supported by evidence extracted from the original text data, it is considered to better reflect the conversational flow of realistic counselors. Conversely, if most responses are generated predominantly by the model without effectively leveraging the given context, such data entries require further processing. This can be achieved by using an additional prompt to enhance the integration of factual evidence and incorporate content-relevant information from the original context without compromising fluency."
        ],
        "final_answer": "The evidence judgment step checks each dialogue response against the original text, keeping only those turns that can be supported by extracted evidence and flagging the rest for re-prompting. By doing so, it forces the generation process to ground its replies in real source content, improving the factual fidelity of the resulting multi-turn conversations before the subsequent empathy and style optimization.",
        "relevant_elements": [
            "Evidence",
            "Optimize",
            "Multi-turn Dialogue"
        ],
        "id": 2125,
        "masked_question": "How does the [mask1] judgment step enhance fidelity before the optimization step in multi-turn dialogue generation?",
        "masked_number": 1,
        "masked_elements": [
            "Evidence"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.05721v2_figure_1.png",
        "paperid": "2407.05721v2",
        "paper_path": "./MISS-QA/papers/2407.05721v2.json",
        "figure_id": "2407.05721v2_figure_1.png",
        "caption": "Figure 1: Overview of dataset preparation.",
        "qtype": "Experimental_Results",
        "response": "Mask1 in the diagram is the “evidence‐support judgment” step that sits between the initial generation of a multi‐turn dialogue and its final optimization.  Here’s how it works and why it boosts fidelity:\n\n1. Generation (Step 1)  \n   – We prompt KimiChat with the original single‐turn QA pair (question + highly up-voted answer) to produce a simulated multi‐turn exchange (seeker ↔ counselor).  \n\n2. Evidence‐Support Judgment (Mask1)  \n   – We run a second LLM prompt that inspects each counselor turn and asks: “Can this response be directly grounded in the source text?”  \n   – If most utterances can be traced back to factual snippets or logical inferences from the original answer, we label the example as well grounded.  \n   – If not—i.e. the model has hallucinated or gone off–topic—we mark that dialogue for revision, either by re‐prompting for tighter context‐conditioning or by injecting explicit evidence.  \n\n3. Optimization (Step 3)  \n   – Only after passing the evidence check do we move on to polishing the dialogue (adding empathy, safety checks, etc.).  \n\nBy explicitly weeding out or re-working any counselor turns that aren’t “evidence-backed,” this middle step makes sure the resulting conversation stays faithful to the real‐world counseling material.  In other words, it verifies and enforces that each response is traceable to the original QA, preventing the model from drifting into generic or hallucinated advice before we even start the final optimization pass."
    },
    {
        "question": "How does global sparse sampling shape video feature sequences v and v′ to improve temporal coverage?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Existing video–music retrieval usually takes one continuous fixed‐duration (FD) clip from the original media to represent the whole sequence, e.g. cutting 30 s around the center of both video and music as in [7]. Those methods ignore the rest parts of video and music, so that the retrieved music may only be partially related to the video. To extract features of the entire video and the whole music, the global sparse (GS) sampling [34] is applied. For video v, it is split evenly into T_v clips and the video feature sequence V is obtained where V ∈ R^{T_v × d} (d is the feature dimension).",
            "To extract the temporal information from the frame‐level video and music feature sequences, V and M are fed into two sequence encoders (biLSTM, transformer encoder, etc.), respectively. After encoding, the encoded video feature V′ and music feature M′ are obtained, where d′ is the fixed hidden dimension of the sequence encoders for both video and music modalities."
        ],
        "final_answer": "Global sparse sampling first divides each video evenly into T_v non‐overlapping clips and extracts a pretrained feature for each clip, producing a fixed‐length raw feature sequence V = [v₁, v₂, …, v_T_v] that covers the entire video. This sequence V is then fed into a temporal sequence encoder (e.g. biLSTM or transformer) which outputs an encoded sequence V′ = [v₁′, v₂′, …, v_T_v′], thereby preserving and modeling temporal information across the whole video rather than from a single continuous segment.",
        "relevant_elements": [
            "Global Sparse Sampling",
            "v",
            "v′"
        ],
        "id": 2126,
        "masked_question": "How does global sparse sampling shape video feature sequences [mask1] and v′ to improve temporal coverage?",
        "masked_number": 1,
        "masked_elements": [
            "v"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.19415v1_figure_1.png",
        "paperid": "2407.19415v1",
        "paper_path": "./MISS-QA/papers/2407.19415v1.json",
        "figure_id": "2407.19415v1_figure_1.png",
        "caption": "Figure 1: The structure of II-CLVM. The global sparse (GS) sampling method is applied for each video and music to extract the pretrained feature sequences visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ajsubscript𝑎𝑗a_{j}italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The encoded features vi′superscriptsubscript𝑣𝑖′v_{i}^{\\prime}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and mj′superscriptsubscript𝑚𝑗′m_{j}^{\\prime}italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are then obtained by video and music encoders, respectively. Then, the inter-modal similarity matrix S𝑆Sitalic_S and the intra-modal similarity matrix Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are calculated. The inter-modal loss is calculated from the matrix S𝑆Sitalic_S, and the intra-modal losses for video and music modalities are calculated by Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, respectively.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought:  \n1. In the diagram the red box surrounds the sequence of green bars labeled v₁, v₂, …, v_{T_v} that are fed into the video encoder, and the corresponding blue bars v₁′, v₂′, …, v_{T_v}′ that come out.  \n2. In the text (Sec. III-A) these green bars are denoted as the raw feature sequence V = {v_i} and the blue bars as the encoded sequence V′ = {v_i′}.  \n3. “Global sparse sampling” refers to splitting the entire video into T_v equal‐duration clips and extracting a feature from each, rather than taking one fixed‐center snippet.  \n4. This produces a fixed‐length sequence V of clip‐level features, and after the sequence encoder a fixed‐length sequence V′, both covering the entire video timeline.  \n5. By construction V and V′ thus carry information from every part of the original video, improving temporal coverage and avoiding the blind spots of single‐clip sampling.  \n\nAnswer:  \nThe mask [mask1] is V, the raw video feature sequence. Global sparse sampling splits a video into T_v evenly spaced clips and extracts features v₁…v_{T_v} to form V, then runs V through the video encoder to get V′ = {v_i′}. Because the clips are sampled sparsely but across the whole duration, both V and V′ cover the entire temporal span of the video, thereby improving temporal coverage."
    },
    {
        "question": "How does intra-modal loss between Sv and Sv′ influence encoder’s preservation of video feature relationships?",
        "relevant_section_ids": [
            "3.3.2"
        ],
        "relevant_context": [
            "For the video modality, two intra-modal similarity matrices Sv and Sv′ are calculated as shown in Fig 1. In a mini-batch, Sv and Sv′ describe the similarity of different video features before and after the encoder, respectively.",
            "To achieve the invariance of feature distribution before and after encoding, Sv and Sv′ should be similar."
        ],
        "final_answer": "By penalizing differences between the pre-encoder similarity matrix Sv and the post-encoder similarity matrix Sv′, the intra-modal loss ensures that the pairwise relationships among video features are preserved through the encoding process.",
        "relevant_elements": [
            "Sv",
            "Sv′",
            "Intra-modal loss"
        ],
        "id": 2127,
        "masked_question": "How does [mask1] between [mask2] and Sv′ influence encoder’s preservation of video feature relationships?",
        "masked_number": 2,
        "masked_elements": [
            "Intra-modal loss",
            "Sv"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.19415v1_figure_1.png",
        "paperid": "2407.19415v1",
        "paper_path": "./MISS-QA/papers/2407.19415v1.json",
        "figure_id": "2407.19415v1_figure_1.png",
        "caption": "Figure 1: The structure of II-CLVM. The global sparse (GS) sampling method is applied for each video and music to extract the pretrained feature sequences visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ajsubscript𝑎𝑗a_{j}italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The encoded features vi′superscriptsubscript𝑣𝑖′v_{i}^{\\prime}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and mj′superscriptsubscript𝑚𝑗′m_{j}^{\\prime}italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are then obtained by video and music encoders, respectively. Then, the inter-modal similarity matrix S𝑆Sitalic_S and the intra-modal similarity matrix Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are calculated. The inter-modal loss is calculated from the matrix S𝑆Sitalic_S, and the intra-modal losses for video and music modalities are calculated by Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, respectively.",
        "qtype": "Experimental_Results",
        "response": "Here’s the answer, filling in the two masks:\n\n – [mask1] is the video “intra‐modal loss” L intra,v  \n – [mask2] is the pre‐encoder video similarity matrix S_v  \n\nHow it works (chain of thought):\n\n1. S_v (the green N×N matrix in the blue box) captures all pairwise cosine similarities among the raw (pre-encoder) video feature vectors in the batch.  \n2. After the bi-LSTM/Transformer encoder, you get S_{v′} (the blue matrix to its right), which captures those same pairwise similarities among the encoded video features.  \n3. L intra,v is defined as a (row- and column-wise) distance between S_v and S_{v′}.  By minimizing L intra,v during training, any changes in the inter-frame relationships introduced by the encoder are explicitly penalized.  \n4. In effect, this “lock‐step” alignment forces the encoder to preserve the original similarity (i.e. relative distances) of the video feature vectors, rather than arbitrarily warping or collapsing them just to satisfy the cross-modal (video–music) loss.  \n\nSo in short: by inserting L intra,v between S_v and S_{v′}, the model is regularized to keep the encoded video features’ pairwise similarities as close as possible to those before encoding, thus preserving the video’s internal structure."
    },
    {
        "question": "What are the limitations of global sparse sampling for capturing diverse video-music content relationships?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Global Sparse Sampling"
        ],
        "id": 2128,
        "masked_question": "What are the limitations of [mask1] for capturing diverse video-music content relationships?",
        "masked_number": 1,
        "masked_elements": [
            "Global Sparse Sampling"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.19415v1_figure_1.png",
        "paperid": "2407.19415v1",
        "paper_path": "./MISS-QA/papers/2407.19415v1.json",
        "figure_id": "2407.19415v1_figure_1.png",
        "caption": "Figure 1: The structure of II-CLVM. The global sparse (GS) sampling method is applied for each video and music to extract the pretrained feature sequences visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ajsubscript𝑎𝑗a_{j}italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The encoded features vi′superscriptsubscript𝑣𝑖′v_{i}^{\\prime}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and mj′superscriptsubscript𝑚𝑗′m_{j}^{\\prime}italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are then obtained by video and music encoders, respectively. Then, the inter-modal similarity matrix S𝑆Sitalic_S and the intra-modal similarity matrix Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are calculated. The inter-modal loss is calculated from the matrix S𝑆Sitalic_S, and the intra-modal losses for video and music modalities are calculated by Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, respectively.",
        "qtype": "Others",
        "response": "Here’s how I read the question and arrive at the answer:\n\n1.  The red box in Fig. 1 encloses the standard self-supervised cross-modal retrieval “backbone” – namely (a) the global‐sparse sampling of a single fixed video–music pair, (b) separate video and audio encoders, and (c) the usual inter-modal contrastive loss that pulls one true pair together and pushes every other clip in the batch apart.\n\n2.  Almost all prior video–music methods use exactly that set-up.  They assume each video has exactly one matching soundtrack (and vice versa) and they treat every other video–music pair in the mini‐batch as a hard negative.\n\n3.  In reality, however, a given track can fit many different videos and a video can work with many different tracks.  By forcing every non-paired sample apart, the standard inter-modal loss\n   – treats truly compatible but unpaired samples as negatives (i.e. false negatives),\n   – blows apart semantically similar clips,\n   – overfits to the one‐to‐one pairing in the training set,\n   – and therefore cannot learn the richer, many-to-many affinities that exist in real video–music relationships.\n\nThose are precisely the limitations of the red-boxed “one-to-one inter-modal contrastive” approach."
    },
    {
        "question": "How could biased pretrained feature extractors affect retrieval fairness across diverse video-music samples?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Video Feature Extractor",
            "Music Feature Extractor"
        ],
        "id": 2129,
        "masked_question": "How could biased pretrained [mask1] affect retrieval fairness across diverse video-music samples?",
        "masked_number": 1,
        "masked_elements": [
            "Video Feature Extractor"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.19415v1_figure_1.png",
        "paperid": "2407.19415v1",
        "paper_path": "./MISS-QA/papers/2407.19415v1.json",
        "figure_id": "2407.19415v1_figure_1.png",
        "caption": "Figure 1: The structure of II-CLVM. The global sparse (GS) sampling method is applied for each video and music to extract the pretrained feature sequences visubscript𝑣𝑖v_{i}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and ajsubscript𝑎𝑗a_{j}italic_a start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT. The encoded features vi′superscriptsubscript𝑣𝑖′v_{i}^{\\prime}italic_v start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT and mj′superscriptsubscript𝑚𝑗′m_{j}^{\\prime}italic_m start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT are then obtained by video and music encoders, respectively. Then, the inter-modal similarity matrix S𝑆Sitalic_S and the intra-modal similarity matrix Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT, Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT, Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT are calculated. The inter-modal loss is calculated from the matrix S𝑆Sitalic_S, and the intra-modal losses for video and music modalities are calculated by Svsubscript𝑆𝑣S_{v}italic_S start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and Sv′subscript𝑆superscript𝑣′S_{v^{\\prime}}italic_S start_POSTSUBSCRIPT italic_v start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, Smsubscript𝑆𝑚S_{m}italic_S start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT and Sm′subscript𝑆superscript𝑚′S_{m^{\\prime}}italic_S start_POSTSUBSCRIPT italic_m start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT, respectively.",
        "qtype": "Others",
        "response": "First, let us identify exactly what is in that red‐boxed region (i.e. what [mask1] must be).  If you look at Fig. 1, the red box encloses two things on each side:\n\n  1. On the left, the “Video Feature Extractor” together with the “Video Encoder.”  \n  2. On the right, the “Music Feature Extractor” together with the “Audio Encoder.”  \n\nBoth of those feature‐processing pipelines are initialized from large, frozen, pretrained models––so we can safely infer that “[mask1]” refers to those pretrained feature extractors (and encoders).  \n\nNow, why would bias in those pretrained feature extractors harm fairness when we later do cross–modal retrieval?  Here is the step‐by‐step reasoning:\n\n  1.  **Representation matters.**  The entire retrieval system measures cosine similarity in the embedding space created by those pretrained extractors / encoders.  Whatever information they choose to emphasize (and whatever they ignore) will directly shape which videos and music clips end up “close together.”  \n  2.  **Pretraining data bias.**  If the feature extractors were trained on a dataset that over‐represents, say, Western pop music and under‐represents South Asian classical or Afrobeat, then clips from those under‐represented genres will tend to get poorer, less distinctive embeddings.  The same goes for video: if the vision backbone saw mostly urban scenes and few rural or ethnically diverse settings, it will not carve out a fair subspace for under‐represented video content.  \n  3.  **Similarity distortions.**  Because retrieval is based solely on those embeddings, any systematic tilt in them will translate immediately into systematic under-ranking of certain styles.  For example, a video of an indigenous dance might never look “close” in embedding space to its ideal music match if both the video and audio backbones have never learned strong representations for that genre.  \n  4.  **Unfair recall and user experience.**  In practice this means that users producing or consuming under-represented content will see worse matches, lower recall@K, and fewer fitting recommendations.  The model will appear “unfair” because it simply echoes whatever blind spots the pretraining corpus had.  \n\nIn short, biased pretrained feature extractors produce embedding spaces that privilege the majority modes seen during pretraining.  Since the inter–intra loss and the retrieval machinery never revisit raw audio or video, they cannot correct those upstream biases.  The result is that certain video/music pairs—especially those from under-represented genres or cultural backgrounds—will systematically score lower and be less likely to be retrieved, harming fairness across the dataset."
    },
    {
        "question": "What are the limitations of multi-modal parallel graph autoencoder with high-resolution image features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "multi-modal parallel graph autoencoder",
            "image features"
        ],
        "id": 2130,
        "masked_question": "What are the limitations of [mask1] with high-resolution image features?",
        "masked_number": 1,
        "masked_elements": [
            "multi-modal parallel graph autoencoder"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.02534v1_figure_1.png",
        "paperid": "2411.02534v1",
        "paper_path": "./MISS-QA/papers/2411.02534v1.json",
        "figure_id": "2411.02534v1_figure_1.png",
        "caption": "Figure 1: The overall structure of the proposed model, stMMC is plotted here, where trapezoids represent the GCN layer, and rectangles represent extracted features. Dashed lines with double arrowheads represent that both GCNs share the same weight. stMMC takes two data modalities and passes them through the multi-modal parallel graph autoencoder (MPGA), where each modality is regulated by a contrastive learning mechanism. The detailed process of contrastive learning is shown in Figure 2. The MPGA reconstructs a refined gene expression data, which is then used for spatial clustering.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed module is the “multi-modal parallel graph autoencoder” (MPGA) in Figure 1 – i.e. the pair of GCN encoders (one for gene‐expression, one for image) plus the simple weighted sum of their latent codes and a decoder that only ever reconstructs gene expression.  What that means in practice is:\n\n 1.  There is no decoder or reconstruction loss on the image branch at all.  All of the supervision comes from (a) reconstructing gene‐expression and (b) the contrastive‐learning term.  As soon as you throw in very rich, high‐resolution image features, the image encoder has no direct incentive to preserve any of that fine‐grained detail – it can collapse everything down to the parts of the embedding that help gene‐expression reconstruction or satisfy the contrastive loss.\n\n 2.  Because the fusion happens by simply taking  \n       Z  = α⋅Z_G  + (1–α)⋅Z_I  \n    and then decoding that to gene expression, any bits of the image code Z_I that don’t immediately help the decoder will tend to be “thrown away.”  You never actually reconstruct the image modality, so the network learns to ignore the very high-resolution texture or morphological cues if they aren’t directly useful for predicting gene counts.\n\n 3.  In practical terms this means that, although you feed the network incredibly detailed (e.g. 200×200‐pixel) patch features, the MPGA has no built-in mechanism to force it to “pay attention” to the fine structure.  All of that information can be discarded in the pooling/fusion step without penalty.\n\n 4.  Finally, running two full GCN stacks on very large feature vectors (e.g. a 2 × 512 or 2 × 1 024 encoder) can become a memory‐ and compute‐bottleneck if you try to push the image patch dimensions—or the embedding dimension—far beyond what you used for the 96‐pixel DLPFC example.\n\nIn short: as soon as the image branch is only ever judged by its ability to help reconstruct gene expression (and by an agnostic contrastive objective), there is no direct supervisory signal that forces it to faithfully encode or reconstruct the high‐resolution morphology.  The result is that fine‐grained histology detail can easily be “left on the cutting‐room floor.”"
    },
    {
        "question": "How might reliance on image features within contrastive learning mechanism bias clustering assignments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "contrastive learning mechanism",
            "image features"
        ],
        "id": 2131,
        "masked_question": "How might reliance on [mask1] within [mask2] bias clustering assignments?",
        "masked_number": 2,
        "masked_elements": [
            "image features",
            "contrastive learning mechanism"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.02534v1_figure_1.png",
        "paperid": "2411.02534v1",
        "paper_path": "./MISS-QA/papers/2411.02534v1.json",
        "figure_id": "2411.02534v1_figure_1.png",
        "caption": "Figure 1: The overall structure of the proposed model, stMMC is plotted here, where trapezoids represent the GCN layer, and rectangles represent extracted features. Dashed lines with double arrowheads represent that both GCNs share the same weight. stMMC takes two data modalities and passes them through the multi-modal parallel graph autoencoder (MPGA), where each modality is regulated by a contrastive learning mechanism. The detailed process of contrastive learning is shown in Figure 2. The MPGA reconstructs a refined gene expression data, which is then used for spatial clustering.",
        "qtype": "Others",
        "response": "Here’s a step‐by‐step rationale:\n\n1.  Identify the masks  \n    -  [mask1] is the “Image Features” (the red‐boxed patches extracted from the histology)  \n    -  [mask2] is the “Contrastive Learning Mechanism” (the blue‐boxed module that pulls together real vs. corrupted embeddings)  \n\n2.  What the contrastive module does  \n    -  It takes GCN‐derived embeddings from the real graph and compares them to embeddings from a corrupted (node‐shuffled) graph.  \n    -  By design, it encourages each spot’s embedding to be close to its local community on the real graph and far from its “fake” counterpart.  \n\n3.  What happens if the image features dominate that loss  \n    -  The model will over‐emphasize morphological similarity (texture, staining intensity, edges) when deciding which spots are “positively paired.”  \n    -  Even subtle histology artifacts—uneven staining, folds, scanning noise—may drive spots to be treated as highly similar, simply because they look alike under the microscope.  \n\n4.  How that skews clustering  \n    -  Clusters may end up grouping spots that share the same color gradients or micro-architecture, rather than true transcriptional programs.  \n    -  Biologically distinct regions that happen to look similar in H&E (or vice versa) can be forced into the same cluster or split apart.  \n\n5.  Conclusion  \n    Over‐reliance on the histology‐derived image features within the contrastive learning module thus risks producing clusters that reflect tissue morphology or staining quirks more than genuine gene‐expression domains."
    },
    {
        "question": "What benefits arise from sharing GCN layer weights across modalities in the multi-modal parallel graph autoencoder?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "GCN layer",
            "multi-modal parallel graph autoencoder"
        ],
        "id": 2132,
        "masked_question": "What benefits arise from sharing [mask1] weights across modalities in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "GCN layer",
            "multi-modal parallel graph autoencoder"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.02534v1_figure_1.png",
        "paperid": "2411.02534v1",
        "paper_path": "./MISS-QA/papers/2411.02534v1.json",
        "figure_id": "2411.02534v1_figure_1.png",
        "caption": "Figure 1: The overall structure of the proposed model, stMMC is plotted here, where trapezoids represent the GCN layer, and rectangles represent extracted features. Dashed lines with double arrowheads represent that both GCNs share the same weight. stMMC takes two data modalities and passes them through the multi-modal parallel graph autoencoder (MPGA), where each modality is regulated by a contrastive learning mechanism. The detailed process of contrastive learning is shown in Figure 2. The MPGA reconstructs a refined gene expression data, which is then used for spatial clustering.",
        "qtype": "Design_Rationale",
        "response": "First, let’s decode the two masks:\n\n1.  [mask1] is the set of learnable GCN weights (the grey trapezoids in the red box).  \n2.  [mask2] is the Multi‐modal Parallel Graph Autoencoder (MPGA, the entire module outlined in blue).\n\nIn the MPGA we have two GCN‐based autoencoders running in parallel—one over the gene‐expression graph and one over the image‐feature graph—but, crucially, both use exactly the same GCN weight matrices (the dashed double‐arrows in Figure 1).  Sharing those weights across the two branches brings two big wins:\n\n1. Parameter efficiency & regularization  \n   • By forcing the same weight matrices to encode both modalities, you dramatically cut the total number of free parameters.  \n   • Fewer parameters makes the model harder to overfit, so you get better generalization on new slices.  \n\n2. Built‐in alignment of latent spaces  \n   • A shared encoder ensures that gene‐expression embeddings and image‐patch embeddings live in a common feature space.  \n   • When you later fuse (“α‐weighted sum”) those two embeddings, they’re already directly comparable, which makes the downstream reconstruction and clustering more stable and coherent.  \n\nIn short, weight sharing in the MPGA reduces model complexity, regularizes training, and—most importantly—brings the two modalities into alignment so that their fused representation is both compact and semantically consistent."
    },
    {
        "question": "What is the rationale behind generating corrupted graphs for each modality in the contrastive learning mechanism?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Inspired by the Deep Graph Infomax approach[32], a corrupted graph is generated for each modality by shuffling nodes while maintaining the same graph topology, denoted as G*_G and G*_I for gene expression data and image feature data, respectively.",
            "The key idea of the implemented contrastive learning mechanism is that the local community representation of spot i, g_i, should be close to the original learned embedding of the same spot, z_i^(l), in the latent space, but far away from the corrupted learned embedding of the same spot, z_i^*(l)."
        ],
        "final_answer": "Corrupted graphs are generated to create negative samples in the contrastive learning framework: by shuffling node attributes (while preserving topology), the model is trained to pull the true spot embeddings closer to their local community representations (positive pairs) and push the corrupted embeddings away (negative pairs), thereby improving the quality of the learned representations.",
        "relevant_elements": [
            "corrupted graph",
            "contrastive learning mechanism"
        ],
        "id": 2133,
        "masked_question": "What is the rationale behind generating [mask1] for each modality in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "corrupted graph",
            "contrastive learning mechanism"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.02534v1_figure_1.png",
        "paperid": "2411.02534v1",
        "paper_path": "./MISS-QA/papers/2411.02534v1.json",
        "figure_id": "2411.02534v1_figure_1.png",
        "caption": "Figure 1: The overall structure of the proposed model, stMMC is plotted here, where trapezoids represent the GCN layer, and rectangles represent extracted features. Dashed lines with double arrowheads represent that both GCNs share the same weight. stMMC takes two data modalities and passes them through the multi-modal parallel graph autoencoder (MPGA), where each modality is regulated by a contrastive learning mechanism. The detailed process of contrastive learning is shown in Figure 2. The MPGA reconstructs a refined gene expression data, which is then used for spatial clustering.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step‐by‐step:\n\n1.  In the blue‐boxed module of Figure 1 (“Contrastive Learning Mechanism for Image,” and by analogy the same for genes) the authors want to set up a contrastive loss: pull “good” embeddings together and push “bad” ones apart.  \n2.  To get “bad” embeddings, they take the original graph for that modality and simply shuffle (or reassign) the node features while keeping the edge structure fixed.  This is what is drawn in the red box (the corrupted graph \\(G_I^*\\) or \\(G_G^*\\)).  \n3.  When they run their GAE on this corrupted version, they get corrupted embeddings \\(Z_I^{*(\\ell)}\\) (or \\(Z_G^{*(\\ell)}\\)).  These act as negative samples.  \n4.  In the contrastive loss they then force the true embedding of each spot to be close to its “community representation” computed on the original graph, but far from its embedding under the corrupted graph.  \n5.  By doing this for each modality, the model is encouraged to learn representations that truly reflect the local community structure in the data, rather than trivial or noisy features.  \n\nIn short, the corrupted graph is generated so that its embeddings can serve as negative examples in the contrastive loss, which in turn drives the learned node representations to capture meaningful, modality‐specific structure."
    },
    {
        "question": "What motivates mapping purity gap through tanh function to derive re-initialization strength?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, the re-initialization strength α_m for modality m is calculated based on purity gap: α_m = f(β · Δ_m) where β is the hyper-parameter to further control the re-initialization strength. Then we can have α_m ∈ [α_min, α_max].",
            "The use of function f aims to map the final re-initialization strength to a value between α_min and α_max, while ensuring a monotonically increasing property when β · Δ_m. These properties make the re-initialization strength α_m proportional to the purity gap Δ_m."
        ],
        "final_answer": "Mapping the purity gap through the tanh function ensures the re-initialization strength is bounded within a preset interval [α_min, α_max] and grows monotonically with the purity gap, thereby making the strength directly proportional to the diagnosed gap.",
        "relevant_elements": [
            "Clustering",
            "tanh"
        ],
        "id": 2134,
        "masked_question": "What motivates mapping purity gap through [mask1] function to derive re-initialization strength?",
        "masked_number": 1,
        "masked_elements": [
            "tanh"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.09705v1_figure_2.png",
        "paperid": "2407.09705v1",
        "paper_path": "./MISS-QA/papers/2407.09705v1.json",
        "figure_id": "2407.09705v1_figure_2.png",
        "caption": "Figure 2: Illustration of multimodal framework and the proposed Diagnosing & Re-learning method.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step by step:\n\n1.  After diagnosing each modality’s learning state by computing the purity gap  \n      δ = P_train – P_val,  \n    we want to turn that gap into a re-initialization strength α.\n\n2.  Requirements on α:  \n    •  It must lie between 0 and 1 (so we never over- or under-mix).  \n    •  It should increase smoothly as the purity gap δ grows.  \n    •  We’d like a simple way to control its sensitivity via a hyper-parameter.\n\n3.  A classic choice that meets all three requirements is a squashing function like tanh.  By feeding δ (possibly scaled by a λ) into tanh, we get  \n      α = tanh(λ ⋅ δ)  \n    which is automatically bounded in (0, 1), monotonically increases with δ, and can be made steeper or shallower by adjusting λ.\n\n4.  In other words, mapping the raw purity gap through tanh gives us a normalized, smoothly increasing α that remains in the desired [0, 1] interval—exactly what we need to control how strongly each modality’s encoder parameters are re-initialized."
    },
    {
        "question": "How does the tanh transformation map purity gap to re-initialization strength alpha?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Specifically, the re-initialization strength αᵢ for modality i is calculated based on purity gap: \n\n    αᵢ = f(ΔPᵢ)\n\n    where λ is the hyper-parameter to further control the re-initialization strength.  Then we can have αᵐᵢₙ and αᵐᵃˣ.  The use of function f aims to map the final re-initialization strength to a value between αᵐᵢₙ and αᵐᵃˣ, while ensuring a monotonically increasing property when ΔPᵢ.  These properties make the re-initialization strength α proportional to the purity gap.",
            "…We choose f to be a scaled tanh so that larger purity gaps yield larger α, but α always stays within [αᵐᵢₙ,αᵐᵃˣ]."
        ],
        "final_answer": "They pass the purity gap ΔPᵢ through a tanh-based mapping f(·), then linearly rescale the tanh output into [αᵐᵢₙ, αᵐᵃˣ].  Concretely,\n\n  αᵢ = αᵐᵢₙ + (αᵐᵃˣ - αᵐᵢₙ) · tanh(λ · ΔPᵢ)\n\nso that when ΔPᵢ grows, tanh(λ·ΔPᵢ) grows, and αᵢ increases monotonically but never falls outside [αᵐᵢₙ, αᵐᵃˣ].",
        "relevant_elements": [
            "tanh",
            "re-initialization strength alpha"
        ],
        "id": 2135,
        "masked_question": "How does the [mask1] transformation map purity gap to [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "tanh",
            "re-initialization strength alpha"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.09705v1_figure_2.png",
        "paperid": "2407.09705v1",
        "paper_path": "./MISS-QA/papers/2407.09705v1.json",
        "figure_id": "2407.09705v1_figure_2.png",
        "caption": "Figure 2: Illustration of multimodal framework and the proposed Diagnosing & Re-learning method.",
        "qtype": "Implementation_Details",
        "response": "They simply take the purity‐gap ΔP=Pᴅ–Pᴠ and pass it through a tanh nonlinearity to produce the re-initialization strength α. In other words,  \nα = tanh(ΔP)  \nso the red-boxed tanh transforms the gap into the blue-boxed α."
    },
    {
        "question": "How are current and initialized parameters combined to yield re-initialized parameters?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Then, the encoder parameters of modality i are re-initialized by:\n\n    \\theta_i^{(t)} \\leftarrow (1 - \\alpha_i) \\theta_i^{(t)} + \\alpha_i \\theta_i^{0},\n\nwhere \\theta_i^{(t)} is the current parameter and \\theta_i^{0} is the initialized parameter."
        ],
        "final_answer": "The re-initialized parameters are computed as a convex combination of the current and initialized parameters: θ_new = (1 − α)·θ_current + α·θ_initialized.",
        "relevant_elements": [
            "current parameters",
            "initialized parameters",
            "re-initialized parameters"
        ],
        "id": 2136,
        "masked_question": "How are [mask1] and initialized parameters combined to yield re-initialized parameters?",
        "masked_number": 1,
        "masked_elements": [
            "current parameters"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.09705v1_figure_2.png",
        "paperid": "2407.09705v1",
        "paper_path": "./MISS-QA/papers/2407.09705v1.json",
        "figure_id": "2407.09705v1_figure_2.png",
        "caption": "Figure 2: Illustration of multimodal framework and the proposed Diagnosing & Re-learning method.",
        "qtype": "Implementation_Details",
        "response": "They simply take a convex‐combination of the two parameter vectors.  In practice if θᶜ is the current encoder weights and θ⁰ the “fresh” (initial) weights, then the re-initialized weights θʳⁱ are  \n  \n  θʳⁱ = (1−α)·θᶜ  +  α·θ⁰  \n  \nwhere α∈[0,1] is the strength of the re-initialization."
    },
    {
        "question": "How does Distributed Prioritized Replay Buffer compute and update trajectory priorities under asynchronous collection?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "To improve sample efficiency, we employ Distributed Prioritized Experience Replay (DPER). For each trajectory τ, we compute the priority P(τ) as:\n\n    P(τ) = w₁ · (1/|τ| ∑ₜ |δₜ|) + w₂ · (1/|τ| ∑ₜ ρₜ) + w₃ · (1/|τ| ∑ₜ H(π(·|sₜ)))\n\nwhere δₜ is the temporal-difference (TD) error at time t, ρₜ is the importance sampling ratio between the target policy and behavior policy, H(π(·|sₜ)) is the policy entropy at state sₜ, and w₁,w₂,w₃ are weights balancing these terms (selected via grid search).",
            "Trajectories with higher priorities are replayed more frequently, focusing learning on the most informative experiences.",
            "Priorities are periodically updated based on the latest policy, recalculating them to focus learning on the most informative experiences, ensuring continual adaptation to evolving behavior policies."
        ],
        "final_answer": "Under asynchronous collection, each trajectory’s priority P(τ) is computed as a weighted sum of three per-trajectory averages—the absolute TD errors, the importance sampling ratios, and the policy entropies—with weights chosen by grid-search. The replay buffer then samples trajectories proportionally to these priorities, and as the central learner’s policy changes, it periodically re-computes and updates all trajectory priorities using the latest policy to keep the buffer focused on the most informative experiences.",
        "relevant_elements": [
            "Distributed Prioritized Replay Buffer"
        ],
        "id": 2137,
        "masked_question": "How does [mask1] compute and update trajectory priorities under asynchronous collection?",
        "masked_number": 1,
        "masked_elements": [
            "Distributed Prioritized Replay Buffer"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.14803v3_figure_1.png",
        "paperid": "2410.14803v3",
        "paper_path": "./MISS-QA/papers/2410.14803v3.json",
        "figure_id": "2410.14803v3_figure_1.png",
        "caption": "Figure 1: Overview of On-device LLM control with DistRL.",
        "qtype": "Implementation_Details",
        "response": "Mask1 in the red box is the “Distributed Prioritized Replay Buffer” (DPER).  Under asynchronous collection it does the following:\n\n1.  As each worker pushes back a whole trajectory, the buffer computes three per‐trajectory statistics (all averaged over the trajectory):  \n   •  The absolute TD–error  \n   •  The importance–sampling ratio  \n   •  The policy entropy  \n\n2.  It then combines them into a single priority score via  \n   pᵢ = w₁·⎯|δ|  +  w₂·⎯ρ  +  w₃·⎯H(π)  \n   where the over–bars denote trajectory means and w₁,w₂,w₃ are tunable weights.\n\n3.  These priority scores are used to build a distributed sum–tree (or equivalent) so that high–priority trajectories are replayed more often.\n\n4.  Because the behavior policy and critic are constantly changing, the buffer periodically “refreshes” the priorities of stored trajectories by recomputing their ⎯|δ|, ⎯ρ and ⎯H under the latest networks.  This ensures that even old trajectories get re‐weighted when the policy shifts."
    },
    {
        "question": "How does Host Learner integrate Auto Evaluators' expert labels into multi-GPU policy updates with A-RIDE?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Host Learner",
            "Auto Evaluators",
            "A-RIDE"
        ],
        "id": 2138,
        "masked_question": "How does [mask1] integrate [mask2]' expert labels into multi-GPU policy updates with A-RIDE?",
        "masked_number": 2,
        "masked_elements": [
            "Host Learner",
            "Auto Evaluators"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.14803v3_figure_1.png",
        "paperid": "2410.14803v3",
        "paper_path": "./MISS-QA/papers/2410.14803v3.json",
        "figure_id": "2410.14803v3_figure_1.png",
        "caption": "Figure 1: Overview of On-device LLM control with DistRL.",
        "qtype": "Implementation_Details",
        "response": "Here is a concise, step-by-step reasoning and answer:\n\n1.  Identify the two masked components in the diagram:  \n    –  Mask1 (red box) is the “Host Learner: Centralized Training” module.  \n    –  Mask2 (blue box) is the “Auto Evaluators” block, which produces automated, “expert” labels on completed trajectories (i.e. success/failure or partial credit).\n\n2.  What the Auto Evaluators do (Mask2)  \n    –  Each worker (on-device agent or emulator) runs an interaction and sends the raw trajectory (screenshots, actions, etc.) back to the host.  \n    –  The Auto Evaluators inspect each trajectory, compare screenshots against the original task instruction, and emit an “expert label” (e.g. a binary success signal or a graded score).\n\n3.  How the Host Learner (Mask1) consumes these labels  \n    –  As trajectories stream in, the Host Learner attaches the expert labels from the Auto Evaluators as the on-line reward signal.  \n    –  These labeled trajectories enter the Distributed Prioritized Experience Replay (DPER), where the TD-error and importance weights (which incorporate the expert labels) are used to compute a priority score for each trajectory.\n\n4.  How A-RIDE uses the expert-labeled data across multiple GPUs  \n    –  The multi-GPU Host Learner pulls mini-batches of high-priority, expert-labeled trajectories from the replay buffer.  \n    –  Within each GPU, A-RIDE applies its off-policy “Retrace” corrections and advantage-based policy-gradient update, using the expert label as the true reward.  \n    –  Gradients from all GPUs are synchronized (e.g. via all-reduce) before updating the central policy weights.  \n\n5.  Outcome  \n    –  By feeding Auto Evaluators’ labels into DPER and into the A-RIDE loss (reward shaping, TD-error, advantage estimates), the Host Learner ensures that the most informative, expert-validated experiences drive the multi-GPU policy updates.  \n\nIn short, the Host Learner centralizes the Auto Evaluators’ expert-provided success/failure labels, uses them to prioritize and shape reward signals in its Distributed Prioritized Experience Replay, and then runs A-RIDE’s off-policy, Retrace-based gradient updates in parallel over multiple GPUs, synchronizing the gradients to update a single, shared control policy."
    },
    {
        "question": "How does Distributed Prioritized Replay Buffer compute and update trajectory priorities under asynchronous collection?",
        "relevant_section_ids": [
            "5.2"
        ],
        "relevant_context": [
            "To improve sample efficiency, we employ Distributed Prioritized Experience Replay (DPER). For each trajectory τ, we compute the priority P(τ) as:\n\n    P(τ) = w₁ · (1/|τ| ∑ₜ |δₜ|) + w₂ · (1/|τ| ∑ₜ ρₜ) + w₃ · (1/|τ| ∑ₜ H(π(·|sₜ)))\n\nwhere δₜ is the temporal-difference (TD) error at time t, ρₜ is the importance sampling ratio between the target policy and behavior policy, H(π(·|sₜ)) is the policy entropy at state sₜ, and w₁,w₂,w₃ are weights balancing these terms (selected via grid search).",
            "Trajectories with higher priorities are replayed more frequently, focusing learning on the most informative experiences.",
            "Priorities are periodically updated based on the latest policy, recalculating them to focus learning on the most informative experiences, ensuring continual adaptation to evolving behavior policies."
        ],
        "final_answer": "Under asynchronous collection, each trajectory’s priority P(τ) is computed as a weighted sum of three per-trajectory averages—the absolute TD errors, the importance sampling ratios, and the policy entropies—with weights chosen by grid-search. The replay buffer then samples trajectories proportionally to these priorities, and as the central learner’s policy changes, it periodically re-computes and updates all trajectory priorities using the latest policy to keep the buffer focused on the most informative experiences.",
        "relevant_elements": [
            "Distributed Prioritized Replay Buffer"
        ],
        "id": 2137,
        "masked_question": "How does [mask1] compute and update trajectory priorities under asynchronous collection?",
        "masked_number": 1,
        "masked_elements": [
            "Distributed Prioritized Replay Buffer"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.14803v3_figure_1.png",
        "paperid": "2410.14803v3",
        "paper_path": "./MISS-QA/papers/2410.14803v3.json",
        "figure_id": "2410.14803v3_figure_1.png",
        "caption": "Figure 1: Overview of On-device LLM control with DistRL.",
        "qtype": "Implementation_Details",
        "response": "Here is what the red‐boxed module (“[mask1]”) is doing, in plain English, under fully asynchronous data collection:\n\n1. What “[mask1]” is  \n   It is a *Distributed Prioritized Replay Buffer* (DPER).  Its job is to take in complete trajectories from many workers (emulators or devices), compute a single scalar priority for each trajectory, store them, and then hand them back out to the learner in priority order.\n\n2. How it *computes* a trajectory’s priority  \n   For each trajectory τ it observes, it computes three statistics, all averaged over the steps in τ:  \n   •  The mean absolute TD–error,  \\(\\overline{|\\delta_t|}\\).  \n   •  The mean importance–sampling ratio,  \\(\\overline{\\rho_t}=\\overline{\\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)}}\\).  \n   •  The mean policy entropy,  \\(\\overline{\\mathcal{H}(\\pi(\\cdot|s_t))}\\).  \n\n   These three terms are then combined in a weighted sum:  \n   \\[\n     P(τ)\\;=\\;w_1\\;\\overline{|\\delta_t|}\n           \\;+\\;w_2\\;\\overline{\\rho_t}\n           \\;+\\;w_3\\;\\overline{\\mathcal{H}(\\pi(\\cdot|s_t))}\\,,\n   \\]\n   where \\(w_1,w_2,w_3\\) are hyper‐parameters chosen (e.g. by grid–search) to trade off focusing on high–TD error, off–policy corrections, and exploration.\n\n3. How it *updates* priorities under asynchronous collection  \n   – **Insert on the fly.**  As each worker finishes a trajectory (completely independently of all other workers), it pushes the trajectory into DPER.  DPER immediately computes \\(P(τ)\\) via the formula above and places τ in the distributed buffer with that priority.  \n   – **Sample proportionally.**  When the central learner is ready for a minibatch of trajectories, it draws them in proportion to their stored \\(P(τ)\\), so that “more informative” or “more novel” trajectories get replayed more often.  \n   – **Periodic refresh.**  Because the policy (and hence TD–errors, IS ratios and entropies) drifts over time, DPER periodically *recomputes* each trajectory’s \\(P(τ)\\) using the *current* network parameters.  This refresh can happen on a fixed schedule (e.g.\\ every N gradient steps) or when a trajectory is revisited, ensuring that stale priorities are corrected to reflect the latest policy.\n\n4. Why this works in an *asynchronous* setting  \n   Because each of the K workers can push data at arbitrary times, and the learner can request samples at arbitrary times, DPER decouples generation from consumption.  It never blocks:  \n   – Workers just fire-and-forget trajectories into the buffer.  \n   – The learner just pulls the highest–priority items when it’s ready.  \n   – Priority updates happen in the background, so there’s no need to synchronize all workers before a policy update.  \n\nTaken together, this design allows DistRL to scale to dozens or hundreds of devices, always focusing on the trajectories that the latest policy is finding most surprising (large TD error), off–policy (large IS ratio), or informative for exploration (high entropy)."
    },
    {
        "question": "How does stop-gradient on cache store representations affect recommendation loss optimization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Cache Store",
            "Recommendation Loss"
        ],
        "id": 2141,
        "masked_question": "How does stop-gradient on [mask1] representations affect recommendation loss optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Cache Store"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.11739v1_figure_1.png",
        "paperid": "2411.11739v1",
        "paper_path": "./MISS-QA/papers/2411.11739v1.json",
        "figure_id": "2411.11739v1_figure_1.png",
        "caption": "Figure 1. (a) Representation Unmatching: multi-modal features are obtained through down-streaming task-agnostic self-supervised tasks like image-text matching. (b) Representation Unlearning: generated multi-modal representation is always only served as additional fixed static input feature for recommendation model.",
        "qtype": "Literature_Background",
        "response": "The little red box in the figure is simply the “cache‐store’’ that holds your pre–computed multi–modal LLM features (image, text, audio embedding) and then hands them off to the downstream recommender.  As soon as you wrap those cached features in a stop–gradient, you have effectively told the system:\n\n  1.   “These multi–modal vectors are constants – do not back‐propagate through them.”\n  2.   Any gradient signal from your final recommendation loss will only update the recommendation‐side weights (ID embeddings, MLPs, etc.), but *never* the contents of the cache.\n\nIn practical terms that means your recommendation objective can never “pull” on or reshape those LLM‐derived embeddings.  They remain frozen, and only the downstream model parameters change during training."
    },
    {
        "question": "How does stop-gradient on cache store representations affect recommendation loss optimization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Cache Store",
            "Recommendation Loss"
        ],
        "id": 2141,
        "masked_question": "How does stop-gradient on [mask1] representations affect recommendation loss optimization?",
        "masked_number": 1,
        "masked_elements": [
            "Cache Store"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.11739v1_figure_1.png",
        "paperid": "2411.11739v1",
        "paper_path": "./MISS-QA/papers/2411.11739v1.json",
        "figure_id": "2411.11739v1_figure_1.png",
        "caption": "Figure 1. (a) Representation Unmatching: multi-modal features are obtained through down-streaming task-agnostic self-supervised tasks like image-text matching. (b) Representation Unlearning: generated multi-modal representation is always only served as additional fixed static input feature for recommendation model.",
        "qtype": "Literature_Background",
        "response": "By wrapping the cached multi‐modal vectors in a stop‐gradient, you literally “clip” the back‐propagation there – none of the recommendation‐loss gradient ever flows back into the cache or the underlying MLLM.  In other words, during rec-model training those [mask1] features are treated as fixed constants, so the rec‐loss only updates the downstream ID‐embeddings and the recommendation network’s weights, and never alters the cached multi‐modal representations."
    },
    {
        "question": "How are multi-modal LLM outputs fused with ID-based embeddings before recommendation loss computation?",
        "relevant_section_ids": [
            "2.4"
        ],
        "relevant_context": [
            "Item-side feature: Straightforwardly, we utilize the VQ code and RQ code as item ID feature, and then assign corresponding embedding spaces for these codes to lookup end-to-end learnable embeddings.",
            "User-side feature: To describe users’ interests, we employ the quantitative codes of latest user’s positive interacted items’ sequence as a part of model input.",
            "Target item-aware feature: Instead of learning our code representation directly, we also apply the target item quantitative code to search several item sequences as target item-aware cross features.",
            "The above features modeling methods are basically the same as some common works in the industry, and overall learning processes formed are as follows: where the h is the selected embedding of item i, ItemCodeRep means the item-side code features, UserCodeRep is the user-side code feature, the CrossCodeRep denote the target item-aware multi-modal feature, the OtherFeaRep denote the additional other user/item features’ representations, the MoE is a multi-task prediction module, and the L_rec is the training objective of our ranking model."
        ],
        "final_answer": "Before computing the recommendation loss, the frozen multi-modal LLM outputs are first quantized into discrete VQ and RQ codes.  These codes are then used as item-side, user-side, and cross (target-aware) features by looking up learnable embeddings for each code.  Finally, these multi-modal code embeddings are concatenated with the standard ID-based embeddings (and any other feature embeddings) and passed through a shared MoE prediction head, whose output is trained with the recommendation loss.",
        "relevant_elements": [
            "Multi-Modal LLM",
            "ID-based Embedding",
            "Recommendation Loss"
        ],
        "id": 2144,
        "masked_question": "How are multi-modal LLM outputs fused with [mask1] before recommendation loss computation?",
        "masked_number": 1,
        "masked_elements": [
            "ID-based Embedding"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.11739v1_figure_1.png",
        "paperid": "2411.11739v1",
        "paper_path": "./MISS-QA/papers/2411.11739v1.json",
        "figure_id": "2411.11739v1_figure_1.png",
        "caption": "Figure 1. (a) Representation Unmatching: multi-modal features are obtained through down-streaming task-agnostic self-supervised tasks like image-text matching. (b) Representation Unlearning: generated multi-modal representation is always only served as additional fixed static input feature for recommendation model.",
        "qtype": "Experimental_Results",
        "response": "First, let’s ground what the red‐boxed “​[mask1]​” is.  If you look at the right half of the diagram you’ll see two streams feeding into the “Recommendation Model.”  On the left comes the frozen, cached multi–modal LLM outputs (visual/text/audio), and on the right (in the red box) is simply the standard, learnable ID–based embedding lookup.  \n\nSo the question really is:\n\n  “How do we fuse the LLM‐derived multi–modal vectors with the ordinary ID‐based embeddings before we compute our recommendation loss?”\n\nHere is the step-by-step:\n\n1.  At serving time each item has  \n    •  A small fixed-length multi–modal code vector, produced by the MLLM and stored in our cache (the purple/orange/green bars).  \n    •  A classic ID-embedding vector (the grey/blue/yellow bars in the red box).  \n\n2.  We pull both of those into the ranking network simultaneously.  \n    –  The multi–modal codes are pulled through a “stop‐gradient” so that the MLLM stays frozen.  \n    –  The ID‐embeddings remain fully trainable by the downstream loss.  \n\n3.  Inside the recommendation model we simply concatenate (or “stack”) the two vectors into one long feature vector  \n      [multi–modal ∥ ID‐embedding]  \n\n4.  That joint vector is passed into our MoE / feed-forward heads and we apply our usual recommendation loss (click-through, long-view, purchase, etc.).  \n\nBecause the multi–modal side is stop-gradient, only the ID branch (and the final prediction head) actually gets updated by the rec-loss, but at run-time they are fused by straightforward concatenation before computing the loss."
    },
    {
        "question": "How does freezing the diffusion UNet affect mask generator adaptation during fine-tuning?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Text-to-Image Diffusion UNet",
            "Mask Generator"
        ],
        "id": 2146,
        "masked_question": "How does freezing the [mask1] affect [mask2] adaptation during fine-tuning?",
        "masked_number": 2,
        "masked_elements": [
            "Text-to-Image Diffusion UNet",
            "Mask Generator"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.14326v1_figure_1.png",
        "paperid": "2407.14326v1",
        "paper_path": "./MISS-QA/papers/2407.14326v1.json",
        "figure_id": "2407.14326v1_figure_1.png",
        "caption": "Figure 1: \nThe overview of our framework, adapted from ODISE.\nFeatures extracted by the text-to-image diffusion model are passed to a mask generator, which outputs binary mask predictions and mask embeddings for individual objects detected in the image. These mask embeddings are then combined with category embeddings from the text encoder via a dot product to supervise the classification task. Additionally, an implicit captioner encodes the image to provide a conditioning signal for the diffusion process.",
        "qtype": "Experimental_Results",
        "response": "Here’s the step‐by‐step reasoning:\n\n1.  Identify the two modules in question from the figure and text:\n    -  [mask1] (red box) is the frozen text‐to‐image diffusion U-Net (either Stable Diffusion or MAM-E).  In the diagram it even has a “* Module frozen” tag.\n    -  [mask2] (blue box) is the Mask Generator (Mask2Former), which sits on top of the diffusion features and is the only part we actually fine-tune for segmentation.\n\n2.  What “freezing” means in practice:\n    -  When you freeze a module, you do not back‐propagate any gradients into its weights.  Its parameters stay exactly as they were in the pre-trained diffusion model.\n    -  All of the learnable capacity during fine-tuning must therefore reside in the unfrozen parts—in this case, the Mask2Former (and the small MLP/classification head).\n\n3.  Consequences for Mask Generator adaptation:\n    -  The Mask Generator now has to learn to interpret the fixed feature maps coming out of the diffusion U-Net.  \n    -  It cannot ask the U-Net to reshape or re-encode certain aspects of the image—whatever representation the U-Net spits out is set in stone.\n    -  As a result, during fine-tuning, all of the model’s ability to bridge the domain gap (mammograms vs. the original diffusion domain) must be absorbed by Mask2Former’s parameters.\n\n4.  Practical trade‐offs:\n    -  + You save a lot of compute and reduce the risk of “forgetting” the diffusion model’s general representations.\n    -  – You may cap your ultimate performance if those frozen features omit information that would have been useful for segmentation in this new medical domain.\n\nAnswer:\nBy freezing the text-to-image diffusion U-Net ([mask1]), no gradient updates ever flow back into it.  This forces the Mask2Former module ([mask2]) to do all of the adaptation: it must learn to turn the fixed, pre-trained diffusion features into accurate mammogram masks.  In other words, fine-tuning is wholly concentrated in the mask generator (and classification head), while the diffusion backbone remains unchanged."
    },
    {
        "question": "What ethical concerns arise from freezing the Text-to-Image Diffusion UNet during training on mammography images?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Text-to-Image Diffusion UNet"
        ],
        "id": 2147,
        "masked_question": "What ethical concerns arise from freezing the [mask1] during training on mammography images?",
        "masked_number": 1,
        "masked_elements": [
            "Text-to-Image Diffusion UNet"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.14326v1_figure_1.png",
        "paperid": "2407.14326v1",
        "paper_path": "./MISS-QA/papers/2407.14326v1.json",
        "figure_id": "2407.14326v1_figure_1.png",
        "caption": "Figure 1: \nThe overview of our framework, adapted from ODISE.\nFeatures extracted by the text-to-image diffusion model are passed to a mask generator, which outputs binary mask predictions and mask embeddings for individual objects detected in the image. These mask embeddings are then combined with category embeddings from the text encoder via a dot product to supervise the classification task. Additionally, an implicit captioner encodes the image to provide a conditioning signal for the diffusion process.",
        "qtype": "Others",
        "response": "Chain-of-thought reasoning:\n\n1.  In the figure, the red‐boxed module (“mask1”) is the frozen Text-to-Image Diffusion UNet (the Stable-Diffusion/MAM-E feature extractor).  \n2.  “Freezing” here means that during our mammogram panoptic‐segmentation training we never update its weights—no fine-tuning on the new mammography data.  \n3.  That choice has several downstream implications:  \n    •  Any biases or artefacts already baked into those pretrained UNet weights (from the 55 k healthy mammograms or any other data the model saw) stay fixed. You can’t correct or re‐weigh them for different patient populations (e.g. different ethnic groups, breast densities, imaging devices).  \n    •  Because it’s a large, closed‐weight diffusion model, it may encode subtle patient‐specific or site‐specific features that risk leaking private information. Freezing prevents you from applying differential‐privacy methods or deliberate de‐biasing.  \n    •  If the frozen UNet simply under-represents certain lesion types or fails in particular subgroups, you cannot adapt it at training time, so the segmentation may systematically underperform on, say, very young or very old patients, or on imaging from different clinics.  \n    •  A frozen black-box raises transparency and accountability concerns: it’s harder to trace an error back into a line of fine-tuned weights, and regulators typically insist on validated, fully auditable models in clinical use.  \n\nTaken together, freezing the diffusion UNet without ever fine-tuning it can perpetuate hidden biases, leak sensitive information, undermine fairness across patient subpopulations, impair model safety, and make clinical‐regulatory approval more difficult."
    },
    {
        "question": "What potential limitations result from dot product fusion of mask embeddings and category embeddings?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "mask embeddings",
            "category embeddings"
        ],
        "id": 2148,
        "masked_question": "What potential limitations result from dot product fusion of [mask1] and [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "mask embeddings",
            "category embeddings"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.14326v1_figure_1.png",
        "paperid": "2407.14326v1",
        "paper_path": "./MISS-QA/papers/2407.14326v1.json",
        "figure_id": "2407.14326v1_figure_1.png",
        "caption": "Figure 1: \nThe overview of our framework, adapted from ODISE.\nFeatures extracted by the text-to-image diffusion model are passed to a mask generator, which outputs binary mask predictions and mask embeddings for individual objects detected in the image. These mask embeddings are then combined with category embeddings from the text encoder via a dot product to supervise the classification task. Additionally, an implicit captioner encodes the image to provide a conditioning signal for the diffusion process.",
        "qtype": "Others",
        "response": "Here is a step-by-step reasoning of what’s going on and where the bottleneck arises:\n\n1.  In the Mask2Former‐style decoder we get, for each detected region i, a fixed-length “mask embedding” zᵢ (this is what you’ve boxed in red).  \n2.  In the parallel “text” branch we encode each class name cₖ (e.g. benign, malignant) into a vector T(cₖ) (the blue box).  \n3.  To decide which class each mask belongs to, we simply take the dot-product ⟨zᵢ, T(cₖ)⟩ for all class vectors, apply a softmax over k, and call the largest score our prediction.  \n\nWhy is such a simple fusion potentially limiting?  \n\n•  **Only a linear similarity test**  \n   A dot‐product can only measure (a scaled form of) cosine- or Euclidean‐similarity between zᵢ and T(cₖ).  It cannot model any richer, non-linear interaction between the shape features in zᵢ and the semantic dimensions of T(cₖ).  \n\n•  **Requires very tight embedding alignment**  \n   For the dot‐product to actually discriminate one class from another, zᵢ and T(cₖ) must live in the _same_ vector space and be _well aligned_.  If either encoder drifts (e.g. mask embeddings trained on mammogram visuals versus text embeddings pre-trained on general medical text) you get poor scores.  \n\n•  **Sensitive to vector norms**  \n   Without explicit normalization (e.g. turning both vectors into unit‐length), masks whose embeddings have larger norm will systematically get higher scores, even if they aren’t the best semantic match.  \n\n•  **Can’t capture context or multi‐label structure**  \n   If two classes are very semantically close (e.g. “probably benign” vs. “benign with atypia”) a simple dot‐product may not have the capacity to tease them apart.  And if one mask really _did_ represent multiple subcategories, there is no mechanism here to split its score across them.  \n\nIn short, by fusing the mask vectors and the class vectors _only_ via a dot‐product, you force the network into a very shallow, similarity-search style classifier.  Any nuance beyond “how close is zᵢ to T(cₖ)?” is simply thrown away."
    },
    {
        "question": "What limitations arise from the Swin-Transformer Block’s window-based self-attention under high-noise medical images?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Swin-Transformer Block"
        ],
        "id": 2149,
        "masked_question": "What limitations arise from the [mask1]’s window-based self-attention under high-noise medical images?",
        "masked_number": 1,
        "masked_elements": [
            "Swin-Transformer Block"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.00347v2_figure_2.png",
        "paperid": "2408.00347v2",
        "paper_path": "./MISS-QA/papers/2408.00347v2.json",
        "figure_id": "2408.00347v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed diffusion transformer segmentation(DTS) model.",
        "qtype": "Others",
        "response": "The red‐boxed module in your figure is nothing more than the Swin transformer’s window‐based self‐attention (W-MSA) (and its shifted‐window variant, SW-MSA).  What we found, however, is that under very noisy CT/MR inputs this purely local attention scheme starts to break down:\n\n 1.  Limited Receptive Field  \n    •  Each W-MSA block only “sees” a small patch (e.g. 7×7 or 8×8) at a time.  \n    •  If that little patch is overwhelmed by noise, the block has no way of peeking outside its window to find a cleaner, more stable signal.  \n\n 2.  Poor Cross-Window Propagation  \n    •  The shifted‐window trick (SW-MSA) helps—just barely—by letting adjacent windows talk to one another, but it still requires stacking many layers before genuine long-range correlations emerge.  \n    •  In a high‐noise regime the information simply cannot propagate far enough fast enough to suppress the spurious activations caused by noise.  \n\n 3.  Unreliable Attention Weights  \n    •  When local windows are uniformly noisy the dot-product attention scores become essentially random, so the module ends up attending to noise rather than structure.  \n\nPut bluntly, a solely windowed self-attention backbone struggles to (a) filter out locally dominant noise and (b) capture the global organ shape priors that are critical for robust medical segmentation."
    },
    {
        "question": "How could Patch Merging’s downsampling compromise fine-detail retention in diffusion encoder hierarchies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Patch Merging",
            "Diffusion Encoder"
        ],
        "id": 2150,
        "masked_question": "How could [mask1]’s downsampling compromise fine-detail retention in [mask2] hierarchies?",
        "masked_number": 2,
        "masked_elements": [
            "Patch Merging",
            "Diffusion Encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.00347v2_figure_2.png",
        "paperid": "2408.00347v2",
        "paper_path": "./MISS-QA/papers/2408.00347v2.json",
        "figure_id": "2408.00347v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed diffusion transformer segmentation(DTS) model.",
        "qtype": "Others",
        "response": "First, let’s identify exactly what we’re talking about:\n\n1.  [mask1] (red‐boxed) in the inset is the “Patch Merging” operation inside each Swin-Transformer stage.  \n2.  [mask2] (blue‐boxed) are the successive feature‐map stages F₀→F₁→F₂→F₃ of the diffusion encoder (i.e. the multi-scale hierarchy produced by the Swin encoder).\n\nNow, why would the Patch Merging downsampling hurt fine‐detail retention in those encoder hierarchies?  Here’s the step‐by-step reasoning:\n\n•  In Swin-Transformer, “Patch Merging” takes each 2×2 block of neighboring tokens, concatenates them (so channel-wise you go from C→4C) and then projects back down, but spatially you halve both width and height.  \n•  Every time you halve spatial resolution, you lose any structure that lay entirely inside those 2×2 patches—essentially you throw away high-frequency variation (tiny edges, thin boundaries, small lesions).  \n•  Because the diffusion encoder is built as a stack of these Swin stages (F₀ at full resolution, F₁ at ½, F₂ at ¼, F₃ at 1/8), each Patch Merging step compounds the resolution loss.  \n•  By the time you reach the deeper levels (F₂, F₃), much of the sub‐patch detail—critical for precise organ boundaries—has been smoothed or aliased away.  \n\nIn short, the very mechanism that gives Swin its hierarchical, efficient encoding (Patch Merging) also shrinks the spatial grid and discards fine‐scale texture and edge information with each downsampling. That makes it harder for the subsequent decoder (and especially the reverse‐boundary‐attention modules) to recover crisp, small‐structure details from those coarsened feature maps."
    },
    {
        "question": "What motivates integrating time embedding and linear projection before Swin Transformer blocks in the diffusion encoder?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "time embedding",
            "linear projection",
            "Swin Transformer block"
        ],
        "id": 2151,
        "masked_question": "What motivates integrating [mask1] and [mask2] before Swin Transformer blocks in the diffusion encoder?",
        "masked_number": 2,
        "masked_elements": [
            "time embedding",
            "linear projection"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.00347v2_figure_2.png",
        "paperid": "2408.00347v2",
        "paper_path": "./MISS-QA/papers/2408.00347v2.json",
        "figure_id": "2408.00347v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed diffusion transformer segmentation(DTS) model.",
        "qtype": "Design_Rationale",
        "response": "First, let us be very clear about what the two masked boxes actually are:\n\n  – The red‐outlined box is the “time embedding.”  \n  – The blue‐outlined box is simply a small “linear projection” that sits on top of your patch embeddings.\n\nWhy do we stitch those in *before* we hand anything to the Swin blocks?  Here is the step–by–step reasoning:\n\n1.  In a diffusion model you *must* condition your denoiser on the current noise level (i.e. the time-step t).  Without that, the network has no idea “how much noise I’m supposed to be removing right now?”  \n2.  The standard way to inject that conditioning into a transformer-style backbone is to create a learned time embedding ϕ(t) and then add or concatenate it into your early feature representation.  That’s exactly what the red box is doing.  \n3.  The Swin Transformer expects its input to live in a particular embedding dimension (the size of its token vectors).  Our raw image-patch features (and any concatenated conditional inputs) may not have the correct channel count.  Hence we slap on a tiny linear layer (the blue box) to “project” whatever we’ve got into the Swin’s working dimension.  \n4.  By first linearly projecting into the right dimension *and* simultaneously folding in the time embedding, every Swin block that follows now (a) knows the current diffusion step and (b) is working in the exact vector space it was designed for.  \n\nIn short:  \n– The red box is there to give the transformer awareness of *which diffusion time‐step* it is denoising.  \n– The blue box is there to *shape* your raw patch features into the correct embedding size for the Swin blocks.  \n\nPulling both of those operations up front is exactly what lets the transformer encoder then proceed, at every layer, with the proper dimensionality and with full knowledge of the denoising schedule."
    },
    {
        "question": "What motivates channeling global feature map through reverse boundary attention to enhance diffusion decoder outputs?",
        "relevant_section_ids": [
            "4"
        ],
        "relevant_context": [
            "Complex anatomy and the inherent ambiguity in defining boundaries of adjacent organs are factors that hinder accurate segmentation of organ boundaries in medical images.",
            "Considering that these factors are likely to result in false positives or missing details in the initial segmentation, our approach includes selectively dropping or reducing the prediction weights of overlooked regions.",
            "The Reverse Boundary Attention method aims to improve the prediction of segmentation models by gradually capturing and specifying areas that may have been initially ambiguous."
        ],
        "final_answer": "Because complex anatomy and ambiguous organ boundaries in medical images often lead to false positives or missed details in the coarse decoder output, the model channels the global feature map through reverse boundary attention in order to selectively focus on and refine those subtle, ambiguous edge regions—progressively improving boundary accuracy in the final segmentation.",
        "relevant_elements": [
            "global feature map",
            "reverse boundary attention",
            "diffusion decoder"
        ],
        "id": 2152,
        "masked_question": "What motivates channeling [mask1] through reverse boundary attention to enhance diffusion decoder outputs?",
        "masked_number": 1,
        "masked_elements": [
            "global feature map"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.00347v2_figure_2.png",
        "paperid": "2408.00347v2",
        "paper_path": "./MISS-QA/papers/2408.00347v2.json",
        "figure_id": "2408.00347v2_figure_2.png",
        "caption": "Figure 2: Overview of our proposed diffusion transformer segmentation(DTS) model.",
        "qtype": "Design_Rationale",
        "response": "First, we have to resolve what “[mask1]” actually is in the network diagram – it’s the “global feature map” produced at the output of the diffusion decoder (that grey‐scale map in the red box).  In the figure this map is upsampled and then sent into the stack of RBA (reverse‐boundary‐attention) modules alongside the multi‐scale features F₀…F₃.  \n\nWhy do we bother feeding that global feature map back through reverse‐boundary attention?  Here’s the step‐by‐step reasoning:\n\n1.   The diffusion decoder produces a *coarse* global segmentation estimate – it’s good at the rough shape but often blurs or misses fine boundary details (especially in medical images where organ borders can be soft or ambiguous).\n\n2.   Reverse‐boundary‐attention (RBA) is specifically designed to “erase” or *reverse* the parts of that coarse prediction that it was already confident about and force the network to *re-focus* on the uncertain, boundary, or “overlooked” regions.\n\n3.   By multiplying the decoder’s global feature map (i.e. its current segmentation guess) with a reversed attention mask, RBA effectively “drops out” the high‐confidence areas and highlights exactly the pixels where the model was least sure – which are typically the organ boundaries or small structures.\n\n4.   Those highlighted error‐prone regions are then fused back into each deeper feature level (F₀…F₃) so that the decoder can go back and refine *just* the boundaries and fine details it got wrong the first time.\n\n5.   The net result is a progressive sharpening of borders and recovery of small anatomical structures that would otherwise be missed or merged in the global map.\n\nIn short: **we channel the global feature map through reverse‐boundary attention so that the network “hunts down” its own weak spots (the fuzzy or missing edges in its coarse mask) and then re-injects that corrective signal into the diffusion pipeline to produce crisper, more accurate organ boundaries.**"
    },
    {
        "question": "Why adopt a text-only Query Text instead of image-based support set for estimating query keypoints in CAPE?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Employing the support information—support images and corresponding keypoint annotations—comes with inherent drawbacks.",
            "Since this method aligns support and query images that differ in many aspects aside from belonging to the same category, inadequate generalization during training can cause the model’s performance to vary depending on the quality of the support data, even with the same query image.",
            "Additionally, because keypoint information in the support is based on human annotations, it is inconvenient to update annotations whenever keypoints are modified.",
            "Even if the method [22] that uses an image with text as input seems to overcome those limitations, a structural dependency utilizing skeletal representations still exists, suggesting that a new approach that is simple yet yields reliable outputs is necessary."
        ],
        "final_answer": "Because relying on support images and their human-annotated keypoints leads to performance fluctuations when support quality varies, and because annotations are cumbersome to update, the paper proposes using text-only query instructions. This removes dependence on visual supports and skeletal structures, simplifying the input while leveraging an LLM’s ability to reason about detailed textual descriptions of keypoints.",
        "relevant_elements": [
            "Query Text",
            "support set",
            "query keypoints"
        ],
        "id": 2154,
        "masked_question": "Why adopt a text-only [mask1] instead of image-based support set for estimating query keypoints in CAPE?",
        "masked_number": 1,
        "masked_elements": [
            "Query Text"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.06869v1_figure_1.png",
        "paperid": "2411.06869v1",
        "paper_path": "./MISS-QA/papers/2411.06869v1.json",
        "figure_id": "2411.06869v1_figure_1.png",
        "caption": "Figure 1: Architectural difference from conventional CAPE methods. Previous methods (top) are support-dependent approaches requiring support images and keypoint annotations, but ours (bottom) is one of the support-free approaches which do not need any additional images and annotations, but just text description of the keypoints",
        "qtype": "Design_Rationale",
        "response": "Here is a step-by-step reasoning tracing how the paper motivates replacing the traditional image-based support set by a text‐only prompt (i.e. the “USER: Where is the {keypoint}…” line highlighted in red):\n\n1.   In conventional CAPE you always need at least one “support” image of the same category plus its hand‐drawn keypoints.  \n2.   That design ties the quality of your query prediction to the particular support images you happen to pick (poses, occlusions, illumination, etc.), and it forces you to re‐annotate every time you change or add keypoints.  \n3.   Earlier work even tried to replace raw images with text‐augmented images or skeleton templates, but there still remained a structural dependence on visual supports.  \n4.   Meanwhile, multimodal LLMs have shown they can follow quite detailed text instructions to produce correct spatial outputs (boxes, masks, etc.).  \n5.   If you simply give the model a text prompt—“Where is the {keypoint} of this object in this image?”—along with the names/descriptions of your keypoints, you can exploit the LLM’s powerful language reasoning and grounding without ever fetching another image or annotation.  \n6.   This “support‐free” approach\n     •  removes the burden of collecting and curating support images  \n     •  avoids mismatches between query and support  \n     •  makes it trivial to add, remove, or rename keypoints (just change the text)  \n     •  reduces overfitting to your training categories  \n     •  in practice even outperforms 5-shot support‐based methods in 1-shot mode.  \n\nSo, the red­boxed “[mask1]” is simply the text prompt (the “USER: Where is the {keypoint} …” query), and it is adopted because it liberates the model from relying on extra images and hand‐drawn annotations, leverages the LLM’s native text‐understanding strengths, and yields a simpler, more flexible, and better‐generalizing CAPE system."
    },
    {
        "question": "How does the Matching module output feed into the Proposal Generator to initialize keypoint proposals?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Matching",
            "Proposal Generator"
        ],
        "id": 2155,
        "masked_question": "How does the [mask1] module output feed into the [mask2] to initialize keypoint proposals?",
        "masked_number": 2,
        "masked_elements": [
            "Matching",
            "Proposal Generator"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.06869v1_figure_1.png",
        "paperid": "2411.06869v1",
        "paper_path": "./MISS-QA/papers/2411.06869v1.json",
        "figure_id": "2411.06869v1_figure_1.png",
        "caption": "Figure 1: Architectural difference from conventional CAPE methods. Previous methods (top) are support-dependent approaches requiring support images and keypoint annotations, but ours (bottom) is one of the support-free approaches which do not need any additional images and annotations, but just text description of the keypoints",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought:\n\n1. In the red box of the support-dependent diagram, the “Matching” module takes as input  \n   – a support image plus its annotated keypoints,  \n   – and the query image.  \n   Internally it computes cross‐image correspondences (e.g. feature correlations or cross‐attention maps) between each support keypoint and every location in the query.\n\n2. Those correspondence maps are exactly what the Matching module emits – essentially a set of similarity‐or response‐maps that mark where in the query each support keypoint is most likely to lie.\n\n3. In the next block (blue box), the “Proposal Generator” ingests those similarity/response maps as its input features. It then interprets the peaks in those maps (or applies a small regression head on them) to spit out a set of coarse (x,y) keypoint proposals on the query image.\n\n4. Those initial proposals are then sent on to the final Refinement stage for fine adjustment.  \n\nAnswer:  \nThe red-boxed Matching module produces a set of similarity (correspondence) maps between support keypoints and the query image. These maps are passed directly into the blue-boxed Proposal Generator, which reads their peak responses (or applies a small regression head) to spawn the initial coarse keypoint proposals on the query image."
    },
    {
        "question": "How does the Decoder integrate encoded prompts and label inputs to generate synthetic chest X-ray data?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Decoder",
            "RoentGen Model"
        ],
        "id": 2157,
        "masked_question": "How does the [mask1] integrate encoded prompts and label inputs to generate synthetic chest X-ray data with the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Decoder",
            "RoentGen Model"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.19436v1_figure_2.png",
        "paperid": "2409.19436v1",
        "paper_path": "./MISS-QA/papers/2409.19436v1.json",
        "figure_id": "2409.19436v1_figure_2.png",
        "caption": "Figure 2: \nOverview of the proposed SDICE index. We input the real and synthetic dataset to the contrastive pretrained encoder to obtain similarity score distributions. The F-ratio between the two distributions after exponential normalization can be used to assess the diversity of the synthetic dataset.",
        "qtype": "Implementation_Details",
        "response": "Here’s what’s going on in that left-hand “Synthetic Data Generation” panel:\n\n1.  The red box highlights the **decoder** portion of the RoentGen model.  This is essentially a latent-diffusion “generator head” (often implemented as a U-Net) that takes as input the joint embedding produced by the text encoder (the “prompts,” which themselves include the CheXpert labels) and then runs the standard diffusion denoising steps—using cross-attention layers to fuse in the prompt/label information at every stage.\n\n2.  The blue box is simply the **synthetic chest X-ray** that comes out of that decoder.  In practice the decoder starts from a noise tensor and, conditioned on the encoded prompt + labels, iteratively denoises it until it yields a realistic looking CXR image.\n\nSo, in plain language: the RoentGen **decoder** (red box) pulls in the text embeddings (which were augmented with CheXpert label information) via cross-attention, and then uses a latent diffusion process to turn noise into a **synthetic CXR** (blue box)."
    },
    {
        "question": "How does SDICE index γ leverage exponential normalization beyond traditional softmax-based scaling of distances?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "SDICE index γ",
            "exponential normalization",
            "distribution distance d"
        ],
        "id": 2160,
        "masked_question": "How does [mask1] leverage exponential normalization beyond traditional softmax-based scaling of distances?",
        "masked_number": 1,
        "masked_elements": [
            "SDICE index γ"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.19436v1_figure_2.png",
        "paperid": "2409.19436v1",
        "paper_path": "./MISS-QA/papers/2409.19436v1.json",
        "figure_id": "2409.19436v1_figure_2.png",
        "caption": "Figure 2: \nOverview of the proposed SDICE index. We input the real and synthetic dataset to the contrastive pretrained encoder to obtain similarity score distributions. The F-ratio between the two distributions after exponential normalization can be used to assess the diversity of the synthetic dataset.",
        "qtype": "Literature_Background",
        "response": "The little “red‐box” inset in Figure 2 is nothing mystical – it is simply the exponential normalization function that turns our unbounded F–ratio (or any distance measure d) into a tidy [0, 1] diversity score γ.  Concretely, we set  \n γ = exp(–α·d)  \nrather than sticking all of our distances into a softmax.  That simple change buys us three things over a classic softmax:  \n1. It works on each distance in isolation – you don’t have to compute a partition function or force your scores to sum to one.  \n2. It gives you a direct, absolute measure of “how close” a synthetic distribution is to the real one (small d → γ≈1, large d → γ→0), rather than only relative weights among a batch of distances.  \n3. By tuning α you can control exactly how steeply the score collapses as d grows, making the index both more interpretable and more sensitive to the kinds of mismatches you care about."
    },
    {
        "question": "How does S2-MLP Link adapt SplitAttention from ResNeSt to improve multi-scale feature fusion in skip connections?",
        "relevant_section_ids": [
            "2.4",
            "2.4.3"
        ],
        "relevant_context": [
            "As a multi-dimensional spatial connector, the S2-MLP Link Module, as shown in Fig. 5, consists of an MLP as the patch embedding layer, a spatial shifting module, and a SplitAttention module. First, the MLP1 expands the feature map’s channel dimension C to three times its original size, splitting it into three parts (x₁, x₂, x₃). Spatial shifts are applied to x₁ and x₂, while x₃ remains unchanged. The parts are then stacked into a tensor. The SplitAttention module calculates and applies attention weights to the stacked features. Finally, the MLP2 restores the weighted features, producing the output feature map.",
            "Split Attention is derived from the ResNest model proposed, where feature maps are finely divided, transformed, fused within groups, and then weighted and summed using attention mechanisms. This paper adopts the core idea: leveraging multi-head attention and global context to perform weighted fusion on input feature maps, enhancing the diversity and accuracy of feature representation."
        ],
        "final_answer": "S2-MLP Link brings ResNeSt’s SplitAttention into the U-Net skip connections by first embedding and expanding each skip-connection feature map into three channel-wise parts, spatially shifting two of them to capture different positional contexts, then stacking all three and feeding them through a SplitAttention block. The SplitAttention module computes per-group attention weights across these shifted and unshifted feature partitions—effectively fusing multi-scale, multi-directional information—and finally projects the fused result back to the original channel dimension, thereby improving multi-scale feature fusion in the skip connections.",
        "relevant_elements": [
            "S2-MLP Link"
        ],
        "id": 2161,
        "masked_question": "How does [mask1] adapt SplitAttention from ResNeSt to improve multi-scale feature fusion in skip connections?",
        "masked_number": 1,
        "masked_elements": [
            "S2-MLP Link"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.12547v1_figure_1.png",
        "paperid": "2411.12547v1",
        "paper_path": "./MISS-QA/papers/2411.12547v1.json",
        "figure_id": "2411.12547v1_figure_1.png",
        "caption": "Figure 1: The overall framework of S3TU-Net. The framework is divided into three broad categories of modules, two novel convolutional modules (DWF-Conv/ D2BR-Conv), multi-spatial dimensional connectors (S2-MLP Link), and residual connection-based superpixel vision transformer (RM-SViT).",
        "qtype": "Literature_Background",
        "response": "The thing that lives in the red box is nothing more exotic than our “S²-MLP Link,” i.e. the skip-connection block that replaces a plain concatenate.  What we actually did there was borrow the core of ResNeSt’s SplitAttention and marry it to the spatial-shift MLP trick.  In practice it works like this:\n\n 1.  You take the incoming skip‐feature map and run it through a small MLP (MLP₁) to expand its channels by a factor of three.  \n 2.  You split those 3× channels into three equal parts (F⁽¹⁾, F⁽²⁾, F⁽³⁾).  You leave one chunk alone and you spatial-shift the other two in orthogonal directions (up/down, left/right).  \n 3.  You stack those three chunks back into a single tensor and hand it to a SplitAttention module exactly in the style of ResNeSt:  \n    – you reshape into (batch, heads, sub-groups, h×w),  \n    – compute a global context vector,  \n    – run it through a tiny two-layer MLP + Softmax to get one attention weight per branch/head,  \n    – re-weight each branch and sum.  \n 4.  A final MLP (MLP₂) projects the fused result back down to your original channel count.  \n\nBy doing this you get two things simultaneously:  \n •   Local mixing in multiple spatial directions (via the shifts)  \n •   Dynamic, per-branch/channel re-weighting (via SplitAttention)  \n\nWhen you finally concatenate that “S²-MLP Link” output with the up-sampled decoder feature, you have already adaptively fused information coming from different scales and spatial contexts, rather than just sticking them side-by-side."
    },
    {
        "question": "How does RM-SViT integrate sparse superpixel sampling with Vision Transformer self-attention to capture global context?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "To enhance the network’s ability to model global context information, we propose the residual and multi-branch attention based superpixel vision transformer (RM-SViT) module, which integrates residual connections and multi-branch attention with superpixel visual transformers.",
            "Integrated between the encoder and decoder of the U-shaped network, the RM-SViT module (Fig. 4) iteratively samples visual tokens through sparse relational learning. It then applies residual multi-branch attention (RMBA) on the superpixels, merging the features before mapping them back to the original tokens.",
            "The execution process of the RM-SViT module begins by unfolding the feature tensor extracted by the encoder into non-overlapping local patches, and then dividing them into initial superpixels.",
            "For each iteration, the association between feature and superpixel is calculated using sparse relational learning, and the superpixel tokens are updated as the weighted sum of their associated features.",
            "After several iterations, multi-branch self-attention is applied to adjust the final superpixel tokens, capturing global context dependencies.",
            "Scaled dot-product attention is used to compute the attention weights, normalized by Softmax, and then a weighted sum of values is performed. The result is projected through a convolutional layer and added back via a residual connection to produce the output."
        ],
        "final_answer": "RM-SViT first unfolds encoder feature maps into a grid of local patches and groups them into superpixel tokens via sparse relational sampling (iteratively estimating associations between patch features and superpixels and updating superpixel embeddings). Once these superpixel tokens aggregate local regions, the module applies a multi-branch self-attention (Vision Transformer) over them to model long-range dependencies. Finally, the attended superpixel features are projected and added back through a residual connection, thereby capturing global context before mapping information back to the original spatial positions.",
        "relevant_elements": [
            "RM-SViT"
        ],
        "id": 2162,
        "masked_question": "How does [mask1] integrate sparse superpixel sampling with Vision Transformer self-attention to capture global context?",
        "masked_number": 1,
        "masked_elements": [
            "RM-SViT"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.12547v1_figure_1.png",
        "paperid": "2411.12547v1",
        "paper_path": "./MISS-QA/papers/2411.12547v1.json",
        "figure_id": "2411.12547v1_figure_1.png",
        "caption": "Figure 1: The overall framework of S3TU-Net. The framework is divided into three broad categories of modules, two novel convolutional modules (DWF-Conv/ D2BR-Conv), multi-spatial dimensional connectors (S2-MLP Link), and residual connection-based superpixel vision transformer (RM-SViT).",
        "qtype": "Literature_Background",
        "response": "The red‐boxed component in the figure is in fact the “RM-SViT” block – the Residual, Multi-branch Attention based Superpixel Vision Transformer.  Here is, step by step, how it weaves sparse superpixel sampling together with a ViT‐style self‐attention to inject global context back into the U-Net:\n\n1.  **Unfolding into patches**  \n    – Take the H×W×C feature tensor from the encoder.  \n    – Break it into a grid of non–overlapping patches (tokens).  \n\n2.  **Sparse superpixel initialization**  \n    – Within each patch grid cell, average all of its pixel‐wise features to form an initial set of M “superpixel” tokens.  \n    – Because M≪H·W, this is a form of sparse sampling or “token reduction.”  \n\n3.  **Iterative relational learning**  \n    – For T iterations:  \n      • Compute a soft association matrix A between every original patch token and every superpixel token (via a dot-product similarity + column-normalization).  \n      • Recompute each superpixel token as the weighted sum of its associated patch tokens (Aᵀ·patch_tokens).  \n    – This alternation of “assign and update” both refines which pixels belong to which superpixel and enriches the superpixel features themselves.  \n\n4.  **Multi-branch ViT self-attention**  \n    – Once the superpixel tokens have converged, feed them into a standard Scaled-Dot-Product Attention (with multiple heads or branches).  \n    – In each head, compute Q,K,V from the M superpixel tokens, run Softmax(QKᵀ/√d)·V, and fuse the results.  \n\n5.  **Residual projection back to pixels**  \n    – Project the attended superpixel tokens back up to the full H×W resolution via a small 1×1 convolutional layer.  \n    – Add this back to the original encoder feature map through a residual connection.  \n\nBy doing “attention” on a small set of superpixel tokens rather than the full H·W grid, RM-SViT dramatically reduces computation, yet still captures global, long-range dependencies.  The final 1×1 “scatter” plus residual link then re-injects those globally coherent features into the decoder stream."
    },
    {
        "question": "How do DWF-Conv and D2BR-Conv collaborate for multi-scale feature extraction and regularization?",
        "relevant_section_ids": [
            "2.1",
            "2.2"
        ],
        "relevant_context": [
            "Specifically, the encoder’s initial stage employs the structured Depth-Weighted Feature Convolution block (DWF-Conv), which consists of two convolutional layers, each followed by batch normalization, a scalable ReLU activation unit, and an LKA module composed of multiple deep kernel convolutions. The encoder then undergoes three downsampling stages, each comprising a structured D2BR-Conv block and max pooling.",
            "Based on these insights, we designed two structured convolutional blocks: the Deep Weighted Feature Convolution (DWF-Conv) and the Double Drop Convolution (D2BR-Conv). DWF-Conv is used at the beginning stages of both the encoder and decoder. It leverages LKA to focus on a broader range of features and utilizes scalable ReLU to enhance feature expression, aiding in the comprehensive capture of global information and the effective restoration of the overall image structure. D2BR-Conv is employed multiple times in the middle stages of the U-shaped network, utilizing DropBlock regularization to enforce the learning of more robust features."
        ],
        "final_answer": "The S3TU-Net interleaves DWF-Conv and D2BR-Conv blocks so that at the start (and symmetrically in the decoder) DWF-Conv uses large-kernel attention (LKA) and depth-weighted convolutions to capture broad, global context, while at each down- and up-sampling step D2BR-Conv applies DropBlock regularization (plus BN and ReLU) to force the network to learn robust local features. By alternately employing DWF-Conv for multi-scale, large-receptive-field extraction and D2BR-Conv for spatial regularization, the model achieves rich feature hierarchies without overfitting.",
        "relevant_elements": [
            "DWF-Conv",
            "D2BR-Conv"
        ],
        "id": 2163,
        "masked_question": "How do [mask1] and D2BR-Conv collaborate for multi-scale feature extraction and regularization?",
        "masked_number": 1,
        "masked_elements": [
            "DWF-Conv"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.12547v1_figure_1.png",
        "paperid": "2411.12547v1",
        "paper_path": "./MISS-QA/papers/2411.12547v1.json",
        "figure_id": "2411.12547v1_figure_1.png",
        "caption": "Figure 1: The overall framework of S3TU-Net. The framework is divided into three broad categories of modules, two novel convolutional modules (DWF-Conv/ D2BR-Conv), multi-spatial dimensional connectors (S2-MLP Link), and residual connection-based superpixel vision transformer (RM-SViT).",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed module at the very top of the encoder is the DWF-Conv block at level L₁.  In our U-shaped encoder it sits upstream of the three D2BR-Conv blocks, which themselves are interleaved with 2×2 max-pooling to produce levels L₂, L₃ and L₄.  Here is how the two blocks “team up”:\n\n1.  DWF-Conv (level L₁)  \n    –  Uses depth-wise, dilated and point-wise convolutions plus large-kernel attention (LKA)  \n    –  BN → scalable ReLU units to produce a rich, high-resolution feature map with global and long-range context  \n\n2.  Chain of D2BR-Conv blocks (levels L₂–L₄)  \n    –  Each D2BR-Conv is a 3×3 conv → DropBlock → BN → ReLU  \n    –  After every D2BR-Conv the feature map is downsampled by 2×2 max-pool  \n    –  DropBlock spatially “drops out” contiguous regions, enforcing robustness and preventing overfitting at each scale  \n\nBy seeding the encoder with a context-sensitive DWF-Conv and then repeatedly applying D2BR-Conv + pooling, the network\n\n–  Extracts features at progressively coarser resolutions (multi-scale)  \n–  Regularizes them with spatial DropBlock at each stage  \n\nThis combination guarantees that the model (a) captures both fine and coarse structures right from the first layer, and (b) never overfits to narrow local patterns because at every scale the D2BR-Conv blocks force the network to learn robust, spatially distributed representations."
    },
    {
        "question": "How does RM-SViT balance sparse superpixel sampling and residual multi-branch attention for context modeling?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "To enhance the network’s ability to model global context information, we propose the residual and multi-branch attention based superpixel vision transformer (RM-SViT) module, which integrates residual connections and multi-branch attention with superpixel visual transformers.",
            "Integrated between the encoder and decoder of the U-shaped network, the RM-SViT module (Fig. 4) iteratively samples visual tokens through sparse relational learning. It then applies residual multi-branch attention (RMBA) on the superpixels, merging the features before mapping them back to the original tokens."
        ],
        "final_answer": "RM-SViT first performs sparse superpixel sampling by iteratively grouping local patch tokens into a smaller set of superpixel tokens via sparse relational learning. Once these superpixel tokens have been formed and updated, it applies a residual multi-branch self-attention mechanism over them to capture global context. The attended superpixel features are then merged back into the original feature space through residual connections, thereby balancing efficient sparse sampling with powerful global context modeling.",
        "relevant_elements": [
            "RM-SViT"
        ],
        "id": 2164,
        "masked_question": "How does [mask1] balance sparse superpixel sampling and residual multi-branch attention for context modeling?",
        "masked_number": 1,
        "masked_elements": [
            "RM-SViT"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.12547v1_figure_1.png",
        "paperid": "2411.12547v1",
        "paper_path": "./MISS-QA/papers/2411.12547v1.json",
        "figure_id": "2411.12547v1_figure_1.png",
        "caption": "Figure 1: The overall framework of S3TU-Net. The framework is divided into three broad categories of modules, two novel convolutional modules (DWF-Conv/ D2BR-Conv), multi-spatial dimensional connectors (S2-MLP Link), and residual connection-based superpixel vision transformer (RM-SViT).",
        "qtype": "Experimental_Results",
        "response": "The thing in the red box is the RM-SViT module.  In S³TU-Net it sits between the encoder and decoder and is responsible for trading off sparse sampling of tokens (i.e. superpixels) against a full blown transformer over every patch.  Concretely it works in two phases:\n\n 1. Sparse Superpixel Sampling  \n    – Take the H×W×C feature map from the encoder and cut it into non-overlapping patches.  \n    – Initialize a small set of “superpixel” tokens by averaging each patch.  \n    – Iteratively refine those tokens by a lightweight relational‐learning step (Eqns. (2)&(3)), so that each superpixel represents a large region of the image.  \n\n 2. Residual Multi-Branch Attention  \n    – Run a multi-branch (i.e. multi-head) self-attention only on the superpixel tokens to capture long‐range context (Eqns. (4)&(5)).  \n    – Project the attended tokens back through a small conv layer and add them residually to the original superpixel embeddings (Eqn. (6)).  \n    – Finally, re-scatter the enriched superpixel features back onto the high-resolution grid.  \n\nBy doing this, RM-SViT keeps the token count low (sparse sampling) but still gets full transformer‐style context via multi-branch self-attention—and merges it back into the feature map with a residual skip."
    },
    {
        "question": "How does AdaCM's shift between unlabeled-labeled and unlabeled-unlabeled mixing affect consistency loss flow?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "At the early stages of training, the model is unreliable and prone to making inaccurate predictions. During this phase, CutMix is applied between an unlabeled image and a labeled image to foster more stable learning.",
            "As the model becomes well-trained and generates more reliable predictions, the application of CutMix shifts towards combining two unlabeled images, introducing further perturbations and enhancing the model’s generalization capability.",
            "It is worth noting that there are two instance of AdaCM in AACL: one for the strongly augmented image and another for the prediction of the weakly augmented image. They share the same α, r and p, ensuring the geometric consistency between weakly and strong augmented views."
        ],
        "final_answer": "By starting with unlabeled–labeled CutMix, AdaCM supplies the consistency loss with more stable, ground-truth-anchored targets in early training; as the model’s confidence grows, it switches to unlabeled–unlabeled CutMix, which injects stronger, purely pseudo-labelled perturbations into the consistency loss pathway to improve generalization.",
        "relevant_elements": [
            "AdaCM",
            "Consistency Loss"
        ],
        "id": 2165,
        "masked_question": "How does [mask1]'s shift between unlabeled-labeled and unlabeled-unlabeled mixing affect consistency loss flow?",
        "masked_number": 1,
        "masked_elements": [
            "AdaCM"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.09344v1_figure_2.png",
        "paperid": "2411.09344v1",
        "paper_path": "./MISS-QA/papers/2411.09344v1.json",
        "figure_id": "2411.09344v1_figure_2.png",
        "caption": "Figure 2: Overview structure of AACL. \"Awsubscript𝐴𝑤A_{w}italic_A start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT\" and \"Assubscript𝐴𝑠A_{s}italic_A start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT\" indicate the weak augmentation and the \"USAug\" module, respectively.",
        "qtype": "Experimental_Results",
        "response": "First, let us restate precisely what the red‐boxed module ([mask1]) is doing in Fig. 2 and in the text:\n\n  1.  Mask₁ is the random CutMix mask that AdaCM produces (Eqns. 6–7) and then uses to splice two images together.  \n  2.  AdaCM can either splice one **unlabeled** with one **labeled** image, or splice two **unlabeled** images, depending on the model’s confidence (p) at that training step.  \n  3.  On the *strong*‐augmentation path, we cut & paste pixels according to Mask₁, feed the mixed image into the shared encoder, and get “strong‐view” logits.  \n  4.  Meanwhile on the *weak*‐augmentation path, we feed the uncut (or only weakly filtered) image into the shared encoder, get “weak‐view” logits, and *use the exact same Mask₁* (to preserve geometry) to perform the corresponding CutMix on its **predicted** soft‐labels.  \n  5.  Finally we compute the consistency loss L₂ (Eqn. 4) between the strong‐view logits and the cut-and-pasted weak‐view pseudo-labels (after entropy filtering).\n\n---\n\nNow:  “How does shifting Mask₁ between *unlabeled–labeled* and *unlabeled–unlabeled* mixing affect the flow of the consistency loss?”\n\nChain of thought:\n\n- Early in training, the paper tells us that the model’s own predictions are too noisy for pure unlabeled→unlabeled mixing.  So AdaCM uses Mask₁ to splice each *unlabeled* crop with a *labeled* crop.  \n- In that regime, the strong‐view branch is still partly grounded in *real* labels (because half the patches come from a labeled image).  The weak‐view pseudo‐labels, when cut by the same Mask₁, therefore still have a “real‐label anchor” in the mixed region.  When we compute L₍con₎, the consistency loss is effectively forcing the unlabeled region to agree with a mixture of ground‐truth labels and noisy predictions.  \n- As training progresses, the model’s confidence p rises above the random threshold r.  AdaCM switches: now Mask₁ is used to splice *two* unlabeled images.  Both branches are now pure unlabeled→unlabeled.  \n- The consistency loss L₍con₎ no longer has any direct ground‐truth pixel supply in the strong view; rather it is purely an agreement loss between two different perturbations of unlabeled data.  The same Mask₁ still carves out exactly which region of the strong view must match the corresponding region of the weak‐view pseudo-labels.  \n\nEffect on consistency‐loss flow:\n\n- In the **unlabeled–labeled** phase, the consistency pathway carries *some* ground-truth signal into the consistency term, anchoring the pseudo-labels.  \n- In the **unlabeled–unlabeled** phase, the consistency pathway becomes a *pure* self‐supervised agreement: both sides are model predictions on two MixUp variants, so the loss flow now simply encourages invariance under stronger CutMix perturbations.\n\nIn short:\n\n  •  Early ⇨ Mask₁ mixes *labeled* + *unlabeled* ⇒ consistency loss partially supervised by real labels.  \n  •  Late  ⇨ Mask₁ mixes *unlabeled* + *unlabeled* ⇒ consistency loss purely enforces agreement between two perturbed unlabeled views."
    },
    {
        "question": "What alternatives to Filter could improve L_con’s reliability by reducing bias from high-entropy pixels?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Filter",
            "L_con"
        ],
        "id": 2168,
        "masked_question": "What alternatives to [mask1] could improve L_con’s reliability by reducing bias from high-entropy pixels?",
        "masked_number": 1,
        "masked_elements": [
            "Filter"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.09344v1_figure_2.png",
        "paperid": "2411.09344v1",
        "paper_path": "./MISS-QA/papers/2411.09344v1.json",
        "figure_id": "2411.09344v1_figure_2.png",
        "caption": "Figure 2: Overview structure of AACL. \"Awsubscript𝐴𝑤A_{w}italic_A start_POSTSUBSCRIPT italic_w end_POSTSUBSCRIPT\" and \"Assubscript𝐴𝑠A_{s}italic_A start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT\" indicate the weak augmentation and the \"USAug\" module, respectively.",
        "qtype": "Others",
        "response": "First, let us be clear what “[mask1]” is in Fig. 2.  The little red‐boxed “Filter” in the consistency‐learning branch is nothing more than the entropy‐based hard‐mask  \nδ(E(p)<τ)  \nthat zeroes out any pixel whose prediction–entropy exceeds the preset threshold τ.  In other words, pixels for which the model is “too unsure” (high entropy) simply do not contribute to Lₙₒₙ.\n\nHow might we replace or improve that crude binary filter so that we still downweight or ignore truly unreliable pixels—but avoid introducing a new bias by throwing away too many medium-entropy pixels?  A few natural alternatives spring to mind:\n\n 1.  Soft “inverse‐entropy” weights rather than a hard gate  \n    •  Instead of δ(E<τ), give every pixel a weight  \n       wᵢ = 1 – E(pᵢ)/log K  \n      and define  \n       L_con = ∑ᵢ wᵢ CE(pᵢ^weak, pᵢ^strong).  \n    •  Pixels of intermediate entropy still contribute, but less strongly.  \n\n 2.  Confidence–margin masking  \n    •  Compute the difference between the top-1 and top-2 softmax scores, mᵢ = pᵢ^(1) – pᵢ^(2).  \n    •  Mask out only pixels with mᵢ below a small margin η.  This directly targets ambiguous predictions rather than all high-entropy ones.  \n\n 3.  Class-wise or per-class adaptive thresholds  \n    •  Different classes may naturally have higher or lower entropy.  Maintain a running average entropy per class and only mask pixels whose entropy is in the top x% of that class.  \n\n 4.  Ensemble or MC-dropout uncertainty  \n    •  Run T stochastic forward passes (e.g. with dropout) and measure the variance or mutual information across predictions.  \n    •  Filter (or weight) pixels by that more robust measure of epistemic uncertainty rather than raw softmax entropy.  \n\n 5.  Region-level consistency checks  \n    •  Rather than pixel by pixel, only mask entire small patches if their mean entropy exceeds τ_region.  This can prevent noisy single‐pixel flips and preserve structure.\n\nAny of these replacements for the hard entropy clip δ(E<τ) would tend to preserve more “useful” information in medium-confidence regions while still suppressing truly unreliable, high-uncertainty pixels—and so would improve the reliability of L_con with less bias."
    },
    {
        "question": "What ethical risks arise from using counterfactual ICL for in-context bias suppression?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "counterfactual ICL",
            "ICL-based Bias Suppression"
        ],
        "id": 2170,
        "masked_question": "What ethical risks arise from using [mask1] for [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "counterfactual ICL",
            "ICL-based Bias Suppression"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.12942v2_figure_1.png",
        "paperid": "2408.12942v2",
        "paper_path": "./MISS-QA/papers/2408.12942v2.json",
        "figure_id": "2408.12942v2_figure_1.png",
        "caption": "Figure 1:  (a) Dataset bias under causal perspective (b) Illustration of the Causal-Guided Active Learning framework.",
        "qtype": "Others",
        "response": "First, let’s be explicit about what the two masks refer to:\n\n  •  [mask1] is the red‐boxed region in the right–hand panel – the “Few‐shot (Counterfactual ICL)” recipe, i.e. the little prompt template that says  \n      “Please act as an impartial judge …  \n       <negative example 1>  \n       …  \n       <negative example n>”  \n\n  •  [mask2] is the entire blue‐boxed region – the ICL‐based bias‐suppression stage of the pipeline, which in the few‐shot variant simply pastes in those counterfactual examples to steer the model away from learned biases.  \n\nNow: what could possibly go wrong, ethically, if you start doing in‐context few‐shot “bias suppression” by feeding the model these hand‐assembled negative examples?\n\nChain of thought:\n\n  1.  To make those counterfactual examples you have to mine real or generated model‐outputs (and/or human annotations) that the model got “wrong” because it fell back on some bias.  In practice that almost always means you are copying snippets of user-generated text or outputs containing private details.  Pasting them back into a public or shared prompt can easily leak sensitive or proprietary information (people’s names, private scenarios, business logic, etc.).\n\n  2.  Because you’re trusting that set of “negative examples” to teach the model what not to do, if those examples themselves carry undiscovered or subtle prejudices (or are poisoned by an adversary) you risk baking new biases or stereotypes into the model.  In the extreme, a malicious actor could quietly slip in spurious counterfactuals that reorient the model’s judgments in some hidden direction.\n\n  3.  Any time you teach a system what _not_ to attend to via a few‐shot block, you also risk _over-correcting_, i.e. swinging the model’s decisions too far the other way, suppressing perfectly legitimate signals or censoring content that actually was harmless or on-topic.\n\n  4.  Finally, because the whole mechanism is buried in a blob of examples, there is very little transparency or audit trail: end‐users won’t necessarily know which examples were used, and thus can’t inspect them for leaks, hidden bias, or malicious tampering.  \n\nIn short, using a handful of counterfactual examples (mask 1) to do in‐context bias suppression (mask 2) opens you up to\n\n  •  Privacy breaches and data-leakage  \n  •  Subtle or adversarial poisoning of your “negative” examples  \n  •  Over-correction (new biases or unwanted censorship)  \n  •  A lack of transparency and auditability   \n\n—all of which are very serious ethical concerns when you go turning a black-box LLM loose on real‐world inputs."
    },
    {
        "question": "What motivates integrating causal invariance-based biased instances identification into the active learning framework?",
        "relevant_section_ids": [
            "1",
            "2.3",
            "3.1"
        ],
        "relevant_context": [
            "However, due to the diversity and complexity of dataset biases Poliak et al. (2018); Schuster et al. (2019); Schick et al. (2021), it’s impractical to identify them one by one manually. Hence, there is an urgent need for methods to automatically identify biases of generative LLMs. … we explore combining active learning with the causal mechanisms and propose a Causal-guided Active Learning (CAL) framework, which utilizes LLMs themselves to automatically and autonomously identify biased samples and induce the bias patterns.",
            "In this paper, under the automatic debiasing scenario, two key issues are: (1) finding which instance contains bias; (2) finding the most informative biased instances. Hence, we propose a causal-guided active learning framework, which identifies the biased instances under the guidance of causal-invariance-based criterion, and finds the most informative biased instances by identifying the instances on which dataset biases have most influence on the generation of LLMs.",
            "Compared to semantic information, the essential characteristic of biased information is that s_biased does not have an invariant causal relationship with the subsequent text, which enables the disentanglement of biased information with semantic information. … Hence, if we can find the instances where the model obtains representations that are not invariant predictive, then the representations of these instances would contain biased information, which indicates that these instances are very likely to contain bias and could be identified as biased instances."
        ],
        "final_answer": "Because (a) manual discovery of the many and diverse dataset biases is impractical, and (b) causal invariance gives a principled way to disentangle semantic from spurious correlations, the authors integrate causal-invariance-based identification into active learning in order to automatically detect which instances are biased (one of the two core AL challenges) and thus enable an end-to-end, autonomous debiasing pipeline.",
        "relevant_elements": [
            "causal invariance-based biased instances identification",
            "active learning framework"
        ],
        "id": 2171,
        "masked_question": "What motivates integrating [mask1] into the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "causal invariance-based biased instances identification",
            "active learning framework"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.12942v2_figure_1.png",
        "paperid": "2408.12942v2",
        "paper_path": "./MISS-QA/papers/2408.12942v2.json",
        "figure_id": "2408.12942v2_figure_1.png",
        "caption": "Figure 1:  (a) Dataset bias under causal perspective (b) Illustration of the Causal-Guided Active Learning framework.",
        "qtype": "Design_Rationale",
        "response": "Chain-of-thought:\n\n1. In the figure the red‐boxed module is labeled “Causal Invariance Based Biased Instances Identification,” i.e. the subroutine that finds pairs of inputs (xᵢ,xⱼ) where the model’s hidden states disagree despite sharing the same bias—thus flagging bias‐dominated examples.  \n2. The big blue box encloses the entire “Causal-Guided Active Learning Framework”: a two-stage pipeline that (a) automatically spots biased samples, and then (b) selects the most informative among them to induce explainable bias patterns.  \n3. Our goal in CAL is to both *detect* and *exploit* dataset biases in order to debias LLMs without expensive fine‐tuning.  Causal‐invariance gives a *principled*, data-driven criterion for bias *detection* (step a), which we then feed into the active learner (step b).  \n4. Hence we “drop in” the red module into the blue pipeline so that the learner can automatically flag the worst‐offending (bias-rich) examples, focus its labeling/query budget there, and derive clear bias patterns.  \n\nAnswer:\n\nBecause the core of the active‐learning loop is to automatically find and exploit bias-laden examples, we integrate the causal-invariance based biased‐instance identification (the red box) into the overall Causal-Guided Active Learning framework (the blue box) to provide a *principled, model-guided* way to detect which samples are truly driven by dataset bias—and then let the active learner focus its queries on exactly those bias-rich instances."
    },
    {
        "question": "Why combine typical and influential criteria for informative biased instances selection and bias pattern induction?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Firstly, for any input text X_i, if the probability that Y* is properly generated is rather low, it suggests that biased information significantly hinders the LLM. Hence, such examples would contain a high level of bias and could be informative biased instances.",
            "Secondly, for a counter instance pair (X_i, X_j), if the corresponding generation of LLM Ŷ_i and Ŷ_j is rather different, it means the influences of dataset bias are diversified and hence it would be challenging to summarize a unified bias pattern based on these samples. Conversely, if Ŷ_i and Ŷ_j are similar, it would be easier to conclude the influence caused by the bias, as the influence of dataset bias is typical."
        ],
        "final_answer": "They combine the two criteria so that selected instances are both highly influenced by bias (i.e., the bias strongly hinders correct generation) and exhibit a consistent, typical bias effect across paired examples—making it easier to induce clear, representative bias patterns.",
        "relevant_elements": [
            "typical criteria",
            "informative biased instances selection",
            "bias pattern induction"
        ],
        "id": 2172,
        "masked_question": "Why combine [mask1] and influential criteria for informative biased instances selection and bias pattern induction?",
        "masked_number": 1,
        "masked_elements": [
            "typical criteria"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.12942v2_figure_1.png",
        "paperid": "2408.12942v2",
        "paper_path": "./MISS-QA/papers/2408.12942v2.json",
        "figure_id": "2408.12942v2_figure_1.png",
        "caption": "Figure 1:  (a) Dataset bias under causal perspective (b) Illustration of the Causal-Guided Active Learning framework.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought:\n\n1.  In the lower part of Figure 1 (b) the authors show two different ways (“criteria”) to pick out the most useful biased examples from their pool of counter‐example pairs:  \n    – One is the red‐boxed “typical criterion,” which flags examples where the model’s probability of generating the correct continuation is very low.  These are the spots where bias is so strong it drowns out the true semantics.  \n    – The other is the “influential criterion,” which checks that, within a counter‐example pair, the model’s behavior really does flip when only the bias changes—i.e. the bias is actually driving the change.  \n\n2.  If you only used the typical criterion, you’d get examples that are “difficult,” but you wouldn’t know whether that difficulty actually comes from the same bias each time (maybe the model is just confused).  If you only used the influential criterion, you’d know the model is flipping its answer because of some change, but you wouldn’t know if that change is strong enough to be a “clear” bias case.\n\n3.  By combining both criteria you end up with a small set of examples that satisfy both:  \n    (a) the model almost certainly gets them wrong when bias is high (typical), and  \n    (b) the wrong answer really is caused by the bias (influential).  \n\n4.  Those dual‐filtered examples are both highly biased and reliably driven by the same bias.  That makes them ideal seeds for clustering into bias‐type groups and for summarizing each group’s characteristic pattern.  \n\nAnswer:\n\nThey pair the red‐boxed “typical” filter (find examples where bias smashing semantics makes the model fail) with the “influential” filter (find pairs where swapping only the bias flips the model’s output) so that the selected instances are not only strongly biased but also consistently so.  That guarantees the resulting subset is both highly informative about “where” bias hurts most and coherent enough across examples to induce clear and explainable bias patterns."
    },
    {
        "question": "What drives the choice of optimal subgraphs before 2D SE minimization?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Nevertheless, such methods addressed the problem of high time complexity, but simply dividing the subgraphs is unwise. The initial partitioning is crucial because it may force strongly related message nodes to be divided into different parts, ultimately preventing them from merging into the same cluster, as shown in Figure 3 (a).",
            "So, we prevent strongly correlated message nodes from being forcibly separated by constructing optimal subgraphs, as illustrated in Figure 3 (b).",
            "Then, using a greedy approach, we obtain the optimal subgraph (lines 6–12), attempting to include edges with higher weights in the same subgraph as much as possible."
        ],
        "final_answer": "The need to keep strongly correlated message nodes together by grouping high‐weight edges drives the choice of optimal subgraphs before performing 2D SE minimization.",
        "relevant_elements": [
            "optimal subgraphs",
            "2D SE minimization"
        ],
        "id": 2174,
        "masked_question": "What drives the choice of [mask1] before [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "optimal subgraphs",
            "2D SE minimization"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.18274v1_figure_2.png",
        "paperid": "2407.18274v1",
        "paper_path": "./MISS-QA/papers/2407.18274v1.json",
        "figure_id": "2407.18274v1_figure_2.png",
        "caption": "Figure 2. The proposed ADP-SEMEvent framework. ADP-SEMEvent consists of two stages: the private message graph construction stage (stage 1) and the private message graph clustering stage (stage 2). Messages with the same color represent the same cluster according to the ground truth labels; orange edges Essubscript𝐸𝑠E_{s}italic_E start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT are derived from 1-dimensional structural entropy, and purple edges Easubscript𝐸𝑎E_{a}italic_E start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT are derived from relevant attributes; arrows of specific colors indicate specific operations.",
        "qtype": "Design_Rationale",
        "response": "Mask 1 is the step where we build a coding‐tree by minimizing 2D structural entropy inside each optimal sub–graph; mask 2 is the later step where we merge those per–subgraph coding‐trees into one global tree.  We do the former first because our overall clustering objective is “2D SE minimization via optimal sub–graphs.”  By cutting the noisy graph into small, high–weight subgraphs, we can cheaply and accurately drive down entropy locally (mask 1) and only then assemble those local solutions into the final coding‐tree (mask 2), keeping both run–time and error under control."
    },
    {
        "question": "How does the Message Passing Neural Network integrate sampling threshold to binarize continuous edge values?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "The desired set of edges is discrete random variables from the Bernoulli distribution. However, training models with discrete variables is challenging due to the non-differentiability of the sampling operation.",
            "To overcome this issue, we replace the binary edge variables with a low-variance continuous relaxation of it named “BinConcrete” [50], which is represented by the function g in the above equation. It is defined as: g(α) = sigmoid(( log α + log u − log(1 − u) ) / τ), where τ is the temperature hyperparameter and u ∼ Uniform(0,1) is a random variable sampled from a uniform distribution.",
            "Finally, the weighted edges ê_{ij} form the weighted adjacency matrix Â.",
            "During inference, we apply a confidence threshold value τ to obtain a discrete causal graph. This approach allows us to adjust the sparsity of the causal graph at the inference time via the threshold value."
        ],
        "final_answer": "The MPNN first computes continuous edge weights using a BinConcrete relaxation (a noisy, temperature-controlled sigmoid of log-odds). At inference time, it applies a confidence threshold to these continuous weights, binarizing them into a discrete 0/1 adjacency matrix for the causal graph.",
        "relevant_elements": [
            "Message Passing Neural Network",
            "Sampling Threshold"
        ],
        "id": 2177,
        "masked_question": "How does the [mask1] integrate [mask2] to binarize continuous edge values?",
        "masked_number": 2,
        "masked_elements": [
            "Message Passing Neural Network",
            "Sampling Threshold"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.07191v1_figure_2.png",
        "paperid": "2410.07191v1",
        "paper_path": "./MISS-QA/papers/2410.07191v1.json",
        "figure_id": "2410.07191v1_figure_2.png",
        "caption": "Figure 2: An overview of CRiTIC. In this architecture, Causal Discovery Network receives the map-aware agent representations and generates a causality adjacency matrix. The matrix is used by a Transformer-based prediction backbone to shape the attention toward the causal agents.",
        "qtype": "Implementation_Details",
        "response": "Here’s the step‐by‐step of how the red–boxed MPNN uses the blue–boxed “Sampling Threshold” to turn its continuous edge scores into a 0/1 adjacency:\n\n1. Within the MPNN you compute, for every ordered pair of agents (i,j), a *continuous* “edge score”  \n   • These scores are obtained via a low‐variance relaxation of a Bernoulli (the BinConcrete trick).  \n2. At inference time you want a *discrete* graph, so you bring in the “Sampling Threshold” τ.  \n3. For each continuous edge weight eᵢⱼ, you do  \n      if eᵢⱼ ≥ τ  → set edgeᵢⱼ = 1  \n      else       → set edgeᵢⱼ = 0  \n4. The result is a binary adjacency matrix indicating which causal links survive the confidence cutoff.\n\nIn short, the MPNN spits out real‐valued edge probabilities, and the sampling threshold τ binarizes them by a simple ≥τ rule."
    },
    {
        "question": "How does the Denoising Autoencoder leverage the inferred causal graph to denoise masked agent representations?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Auxiliary Denoising Autoencoder (DAE). Following the definition of the Granger causality for time series data in Section IV, the causal graph aids the prediction of future variables from the past value of its parents. Motivated by this we add the DAE task as an auxiliary supervision to facilitate the causal discovery. In this task, the objective is to reconstruct the values of the masked intermediate temporal agent representations generated by AgentNet based on the values of the other vertices and the causal graph.",
            "Thereby, we employ a two-layer graph convolutional network (GCN) as a denoising autoencoder (DAE), where the graph is defined as: G = (V, E), the vertices are Z ˆ∈ R^{N×D_t} (downsampled temporal agent representations), and the edges E correspond to the adjacency matrix A, which is a block lower-triangular extension of the adjacency matrix generated by the CDN.",
            "Next, we mask a random selection of vertices using a binary mask M controlled by the masking ratio. The masked representation is given by ˜Z = Z ⊙ M. Subsequently, the GCN layers are defined as: H^{(l+1)} = σ(Ã H^{(l)} W^{(l)}), where Ã is the normalized adjacency matrix (built from the inferred causal graph). Loss function L_DAE is equal to negative cosine similarity between masked vertices of Z and their reconstructions."
        ],
        "final_answer": "The Denoising Autoencoder uses the adjacency matrix inferred by the Causal Discovery Network as the graph structure for a two-layer graph convolutional network. It first masks a random subset of the intermediate temporal agent representations, then applies GCN message-passing over the inferred causal edges (i.e. parents in the causal graph) to propagate information from unmasked nodes and reconstruct (denoise) the masked representations.",
        "relevant_elements": [
            "Denoising Autoencoder",
            "Masked Agent Representations"
        ],
        "id": 2178,
        "masked_question": "How does the [mask1] leverage the inferred causal graph to denoise [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Denoising Autoencoder",
            "Masked Agent Representations"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.07191v1_figure_2.png",
        "paperid": "2410.07191v1",
        "paper_path": "./MISS-QA/papers/2410.07191v1.json",
        "figure_id": "2410.07191v1_figure_2.png",
        "caption": "Figure 2: An overview of CRiTIC. In this architecture, Causal Discovery Network receives the map-aware agent representations and generates a causality adjacency matrix. The matrix is used by a Transformer-based prediction backbone to shape the attention toward the causal agents.",
        "qtype": "Implementation_Details",
        "response": "Here is what happens in that red‐boxed “denoising autoencoder” (DAE) step and how it uses the causal graph to clean up (denoise) the blue‐boxed masked agent features:\n\n1.  Masking the Temporal Features  \n    – We first take the intermediate temporal representations of each agent (the output of AgentNet over past time steps), down‐sample and reshape them into a matrix of node features, one node per agent–time pair.  \n    – We then randomly “zero out” (mask) a subset of those nodes (controlled by a masking ratio), producing the blue‐boxed masked agent representations.\n\n2.  Building the GCN from the Inferred Causal Graph  \n    – Recall that the Causal Discovery Network (CDN) has already produced a weighted adjacency matrix A for the agents, which we turn into a block-lower-triangular adjacency  for the entire set of agent–time nodes (so that information only flows forward in time).  \n    – This adjacency encodes exactly “who causally influences whom” over those past frames.\n\n3.  Denoising via Two-Layer Graph Convolution  \n    – We feed the masked node features and the block-lower-triangular adjacency into a small two-layer GCN.  \n    – In layer 1, each node gathers messages only from its causal parents (as dictated by A), transforms them with a linear projection + nonlinearity, and produces a hidden representation.  \n    – In layer 2, it again aggregates from its (still only) causal parents and projects back up to the original feature size, yielding reconstructed features for every node—including the ones that were masked out.\n\n4.  Supervision and Gradient Detachment  \n    – We compare the reconstruction at the masked nodes against their original (pre‐masking) values using a negative cosine similarity loss.  \n    – Crucially, we “stop‐gradient” on the input encoder side so that this denoising loss does not change AgentNet’s representations—its sole job is to shape the adjacency A into a graph that makes denoising possible.\n\nIn short, the DAE (red box) treats the inferred causal adjacency as a graph on which to run a 2-layer GCN.  By message‐passing only along edges that the CDN believes to be “real” causal links, it reconstructs (“denoises”) the randomly masked intermediate agent representations (blue box)."
    },
    {
        "question": "How does the Message Passing Neural Network integrate sampling threshold to binarize continuous edge values?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "The desired set of edges is discrete random variables from the Bernoulli distribution. However, training models with discrete variables is challenging due to the non-differentiability of the sampling operation.",
            "To overcome this issue, we replace the binary edge variables with a low-variance continuous relaxation of it named “BinConcrete” [50], which is represented by the function g in the above equation. It is defined as: g(α) = sigmoid(( log α + log u − log(1 − u) ) / τ), where τ is the temperature hyperparameter and u ∼ Uniform(0,1) is a random variable sampled from a uniform distribution.",
            "Finally, the weighted edges ê_{ij} form the weighted adjacency matrix Â.",
            "During inference, we apply a confidence threshold value τ to obtain a discrete causal graph. This approach allows us to adjust the sparsity of the causal graph at the inference time via the threshold value."
        ],
        "final_answer": "The MPNN first computes continuous edge weights using a BinConcrete relaxation (a noisy, temperature-controlled sigmoid of log-odds). At inference time, it applies a confidence threshold to these continuous weights, binarizing them into a discrete 0/1 adjacency matrix for the causal graph.",
        "relevant_elements": [
            "Message Passing Neural Network",
            "Sampling Threshold"
        ],
        "id": 2177,
        "masked_question": "How does the [mask1] integrate [mask2] to binarize continuous edge values?",
        "masked_number": 2,
        "masked_elements": [
            "Message Passing Neural Network",
            "Sampling Threshold"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.07191v1_figure_2.png",
        "paperid": "2410.07191v1",
        "paper_path": "./MISS-QA/papers/2410.07191v1.json",
        "figure_id": "2410.07191v1_figure_2.png",
        "caption": "Figure 2: An overview of CRiTIC. In this architecture, Causal Discovery Network receives the map-aware agent representations and generates a causality adjacency matrix. The matrix is used by a Transformer-based prediction backbone to shape the attention toward the causal agents.",
        "qtype": "Implementation_Details",
        "response": "First, let’s identify the two masks:\n\n- [mask1] is the red‐boxed module in the figure: that is the Message–Passing Neural Network (MPNN) inside the CDN.  \n- [mask2] is the blue‐boxed signal feeding into it: the stack of agent representations coming out of AgentNet.\n\nWhat the question is really asking is:\n\n“How does the MPNN take those agent representations and turn them into (soft) binary edge indicators, i.e. how does it binarize continuous edge values?”\n\nHere is the step-by-step reasoning:\n\n1. **Inputs to the MPNN**  \n   Each agent \\(i\\) has a learned feature vector \\(h_i^{(0)}\\) (the blue block).  We start with a fully connected graph (no self-loops), so every pair \\((i,j)\\) is a candidate edge.\n\n2. **Message passing / edge-logit computation**  \n   For each node \\(i\\), the MPNN gathers messages from all neighbors \\(j\\).  Concretely, for each directed pair \\((j\\to i)\\) it forms something like\n   \\[\n     m_{j\\to i} \\;=\\; \\mathrm{MLP}_1\\bigl([\\,h_j^{(0)},\\,h_i^{(0)},\\,e_{j i}^{(0)}]\\bigr)\\,,\n   \\]\n   and then sums those to update the node:\n   \\[\n     h_i^{(1)} \\;=\\; \\mathrm{MLP}_2\\Bigl(h_i^{(0)} \\;+\\;\\sum_{j\\neq i} m_{j\\to i}\\Bigr).\n   \\]\n   In parallel, the network also runs a small linear head on each pair \\((i,j)\\) to compute a real valued log-odds \\(\\alpha_{ij}\\) for the presence of an edge.\n\n3. **BinConcrete relaxation**  \n   Instead of sampling a hard 0/1 edge from \\(\\mathrm{Bernoulli}(\\sigma(\\alpha_{ij}))\\) (which is non‐differentiable), they use the BinConcrete (aka Binary Concrete / Gumbel-Sigmoid) trick.  Concretely they draw\n   \\[\n     u \\;\\sim\\;\\mathrm{Uniform}(0,1), \n     \\quad\n     \\hat e_{ij}\n     \\;=\\;\n     \\sigma\\!\\Bigl(\\bigl[\\log\\alpha_{ij}\\;+\\;\\log u\\;-\\;\\log(1-u)\\bigr]\\;/\\;\\tau\\Bigr),\n   \\]\n   where \\(\\tau\\) is a temperature.  \\(\\hat e_{ij}\\) is now a continuous value in (0,1) that smoothly approximates a Bernoulli sample, and it admits back‐propagation.\n\n4. **Thresholding at inference**  \n   At test time they simply threshold \\(\\hat e_{ij}\\) at some confidence level \\(p_{\\text{thresh}}\\) to recover a truly binary adjacency matrix.\n\nPutting it all together:\n\n— The MPNN takes the *agent representations* (blue box), runs pairwise message‐passing MLPs to produce logits \\(\\alpha_{ij}\\), and then uses the *BinConcrete* continuous relaxation (a Gumbel–Sigmoid with temperature) to sample soft edge values \\(\\hat e_{ij}\\in(0,1)\\).  Those can then be thresholded to give a final 0/1 causal graph."
    },
    {
        "question": "How does the auxiliary Denoising Autoencoder utilize self-supervised graph learning to refine the causal adjacency matrix?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "Auxiliary Denoising Autoencoder (DAE). Following the definition of the Granger causality for time series data in Section IV, the causal graph aids the prediction of future variables from the past value of its parents. Motivated by this we add the DAE task as an auxiliary supervision to facilitate the causal discovery. In this task, the objective is to reconstruct the values of the masked intermediate temporal agent representations generated by AgentNet based on the values of the other vertices and the causal graph. Note that, using temporal features for this task best matches the definition of Granger causality.",
            "Thereby, we employ a two-layer graph convolutional network (GCN) as a denoising autoencoder (DAE), where the graph is defined as: , the vertices are , where for computational efficiency we downsample and reshape the temporal agent representations, , where , and  is the downsampled temporal dimension. To avoid the model collapse to naïve solutions, we detach the gradients using the Stop Gradient operation denoted by SG so the DAE loss cannot directly affect representation learning in AgentNet.",
            "The edges are defined as . The edges  correspond to the adjacency matrix , which is a block lower-triangular extension of the adjacency matrix generated by the CDN.",
            "Next, we mask a random selection of vertices using a binary mask  controlled by the masking ratio . The masked representation is given by , where  is the Hadamard product operator. We constrain the mask to have an all-equal last dimension, i.e., we perform vertex-wise masking. Subsequently, the GCN layers are defined as:  where  and  are the linear projection weight matrices, and  and  denote the intermediate vertex representations, and the reconstructed agent representations, respectively. Loss function  is equal to negative cosine similarity between masked vertices of  and ."
        ],
        "final_answer": "The auxiliary DAE applies a self-supervised graph learning task by randomly masking a subset of the downsampled temporal agent features (vertices), then using a two-layer GCN—conditioned on the current causal adjacency matrix—to reconstruct those masked features. The reconstruction loss (negative cosine similarity) back-propagates through the GCN (with gradients stopped at AgentNet), encouraging the network to adjust the adjacency matrix so that it better captures the true Granger-causal dependencies among agents.",
        "relevant_elements": [
            "Denoising Autoencoder",
            "causal adjacency matrix"
        ],
        "id": 2180,
        "masked_question": "How does the auxiliary [mask1] utilize self-supervised graph learning to refine the causal adjacency matrix?",
        "masked_number": 1,
        "masked_elements": [
            "Denoising Autoencoder"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.07191v1_figure_2.png",
        "paperid": "2410.07191v1",
        "paper_path": "./MISS-QA/papers/2410.07191v1.json",
        "figure_id": "2410.07191v1_figure_2.png",
        "caption": "Figure 2: An overview of CRiTIC. In this architecture, Causal Discovery Network receives the map-aware agent representations and generates a causality adjacency matrix. The matrix is used by a Transformer-based prediction backbone to shape the attention toward the causal agents.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in the figure is nothing more exotic than a small, two‐layer graph‐convolutional denoising auto‐encoder (DAE) that “puts the graph back to work” as a self-supervised critic on your candidate causal edges.  Here is what happens, step by step:\n\n1.  Take your intermediate, down-sampled temporal agent representations (the rows in the big colored matrix right before the red box) and randomly zero out (mask) a subset of those rows (this is your self-supervised “corruption” step).  \n2.  Build a little block-lower-triangular graph whose adjacency is exactly the weighted adjacency matrix that the CDN just produced (i.e. your summary causal graph, broadcast to the temporal blocks).  \n3.  Run a two-layer GCN over that masked representation using the current adjacency weights to try to reconstruct the masked rows.  \n4.  Measure reconstruction quality with a simple self-supervised loss (negative cosine similarity between the true and reconstructed rows).  \n5.  Critically, you stop the gradient from flowing back into the AgentNet encodings (so you aren’t just memorizing the representations) and only let it refine the adjacency weights in the CDN.  Edges that genuinely help you predict a node’s time-series will get stronger; spurious ones will get driven toward zero.  \n\nIn this way, the DAE module uses purely self-supervised masking and reconstruction on a graph‐convolutional backbone to “test” and then sharpen the causal adjacency matrix, without ever requiring external labels for edge existence."
    },
    {
        "question": "How does Transform-neck circumvent back-propagation infeasibility inherent in image coding for machines?",
        "relevant_section_ids": [
            "2.2",
            "3.2",
            "3.4"
        ],
        "relevant_context": [
            "All the above-mentioned methods leverage back-propagation through recognition models to update the system, which is prohibitively expensive for MLLMs due to their huge scale. Therefore, the direct application of the same methods on MLLMs is almost infeasible. In addition, the use of a specific task loss restricts the resulting models to be optimized for a single task and recognition model, thus requiring re-training for each new task and incurring additional costs. We aim to be the first to propose a neural image compression system designed for MLLMs, achieved through a universal transform-neck and the adoption of a surrogate loss, which allows to bypass the necessity of involving the entire billion-scale MLLM in the training process.",
            "Regardless of the context examined, we circumvent the difficulties of back-propagating the task loss through MLLMs by introducing a surrogate loss.",
            "To avoid involving huge MLLMs in the training process, thus bypassing back-propagation through them, we propose a surrogate loss, which is back-propagated through only the partial CLIP encoder."
        ],
        "final_answer": "Transform-neck sidesteps the infeasibility of back-propagating through billion-scale MLLMs by using a surrogate loss that matches its transformed latents to intermediate features of a fixed CLIP visual encoder. During training, gradients flow only through the transform-neck and a small portion of the CLIP encoder, never through the full MLLM, thus avoiding the prohibitive cost of back-propagating through the entire model.",
        "relevant_elements": [
            "Transform-neck",
            "image coding"
        ],
        "id": 2181,
        "masked_question": "How does [mask1] circumvent back-propagation infeasibility inherent in image coding for machines?",
        "masked_number": 1,
        "masked_elements": [
            "Transform-neck"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.19651v1_figure_1.png",
        "paperid": "2407.19651v1",
        "paper_path": "./MISS-QA/papers/2407.19651v1.json",
        "figure_id": "2407.19651v1_figure_1.png",
        "caption": "Figure 1: On the left is inadequate frameworks for image compression for MLLMs, where the image codec is trained for (a) human perception, (b) the downstream task network, or (c) compressing the intermediate features of the task network. On the right is the proposed transform-neck and surrogate loss under three distinct scenarios, with the image codec (d1) pre-trained for human perception, (d2) updated for joint human and machine perception, or (d3) updated for machine perception.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module in Figure 1(d) is the “transform-neck” together with its surrogate (distillation) loss.  In “coding for machines” one would like to tune the compressor to optimize the MLLM’s end‐task loss, but back-propagating gradients through a billion-parameter MLLM at training time is simply infeasible.  What we do instead is:\n\n 1.  We take the compressed latents  and pass them through a very small “transform-neck” that produces what look like mid-level CLIP features.  \n 2.  We freeze the full MLLM and its connector, and we also freeze the first two layers of the CLIP encoder.  We only allow gradients to flow back through the transform-neck (and, if desired, the codec itself).  \n 3.  Rather than trying to push gradients into the LLM, we compute an MSE (distillation) loss between the transform-neck’s outputs and the true CLIP features you would have gotten from the uncompressed image.  \n\nBecause all of our gradient steps happen in that tiny red-boxed block (and through a fixed partial CLIP encoder), we never have to back-propagate through the full visual‐LLM stack.  In this way the transform-neck + surrogate loss “circumvents” the back-propagation infeasibility that plagues direct image‐coding-for-MLLM approaches."
    },
    {
        "question": "How does hierarchical feature extraction balance part-level detail and object-level context for efficient feature splatting?",
        "relevant_section_ids": [
            "3.1"
        ],
        "relevant_context": [
            "We generate hierarchical features, object-level and part-level, specialized for grasping. Given an input image, MobileSAMV2 predicts class-agnostic bounding boxes b and a set of object masks m.",
            "For object-level feature, we first use MaskCLIP to compute coarse CLIP features of the entire image F_img. We then follow Qiu et al. and use Masked Average Pooling to regularize object-level CLIP features with m.",
            "For part-level features, we extract image patches from b for batched inference on MaskCLIP. Since b incorporates object priors learned from the SA-1B dataset, b is significantly smaller than the number of patches needed from uniform queries for efficient inference. We then interpolate the features to remap them into the original image shape and average over multiple instances to form F_part for part-level supervision.",
            "During differentiable rasterization, we introduce a shallow MLP with two output branches that takes in the rendered features F_rend from Eq. 1 as intermediate features. The first branch renders the object-level feature F_obj and the second branch renders the part-level feature F_part, where F_obj and F_part are supervised using F_obj and F_part with cosine loss. We scale the part-level term in the joint loss with α to emphasize part-level segmentation."
        ],
        "final_answer": "Hierarchical feature extraction in GraspSplats builds both coarse object-level and fine part-level CLIP features. It first pools global image features under an object mask (object-level context) and, using MobileSAMV2 bounding boxes, crops and pools only those regions (part-level detail) to feed MaskCLIP more efficiently. A two-branch MLP then renders these into 3D splatted Gaussians with separate cosine losses—weighted to emphasize part details—thus preserving both full‐object context and precise part cues while keeping computation low.",
        "relevant_elements": [
            "hierarchical feature extraction",
            "feature splatting"
        ],
        "id": 2185,
        "masked_question": "How does [mask1] balance part-level detail and object-level context for efficient feature splatting?",
        "masked_number": 1,
        "masked_elements": [
            "hierarchical feature extraction"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.02084v1_figure_2.png",
        "paperid": "2409.02084v1",
        "paper_path": "./MISS-QA/papers/2409.02084v1.json",
        "figure_id": "2409.02084v1_figure_2.png",
        "caption": "Figure 2: GraspSplats employs two techniques to efficiently construct feature-enhanced 3D Gaussians: hierarchical feature extraction and dense initialization from geometry regularization, which reduces the overall runtime to 1/10 of existing GS methods [38]. (High-dimensional features are visualized using PCA and the visualized Gaussian ellipsoids are trained without densification).",
        "qtype": "Experimental_Results",
        "response": "Mask₁ is simply the “Hierarchical Features” block in Fig. 2 – the little red‐boxed module that simultaneously produces both object-level and part-level CLIP features and then splats them into the 3D Gaussians.  Concretely:  \n1. At training time we first extract two sets of 2D CLIP features:  \n   • Object-level features F_obj by running MaskCLIP over the whole image and then doing a masked‐average‐pooling inside each MobileSAM object mask.  This gives you a coarse, context-aware embedding of the entire object.  \n   • Part-level features F_part by cropping the mobile-SAM bounding boxes, feeding those patches through MaskCLIP in batch, reprojecting/interpolating them back to the original image plane, and averaging over repeated crops.  This produces fine, localized embeddings for handles, rims, spouts, etc.  \n2. We lift 2D → 3D with differentiable rasterization to get per-Gaussian intermediate features.  A tiny two-headed MLP then splits those Gaussians into:  \n   – one branch supervised to match F_obj (via a cosine loss), and  \n   – a second branch supervised to match F_part (also cosine loss, but weighted more heavily to emphasize fine detail).  \n\nBy supervising both heads jointly we end up with 3D Gaussians that carry global object context and at the same time contain the fine-grained part cues we need for grasping – all without the huge sampling overhead of prior methods."
    },
    {
        "question": "What limitations might hierarchical feature extraction impose when depth data is sparse or noisy?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "hierarchical feature extraction"
        ],
        "id": 2187,
        "masked_question": "What limitations might [mask1] impose when depth data is sparse or noisy?",
        "masked_number": 1,
        "masked_elements": [
            "hierarchical feature extraction"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.02084v1_figure_2.png",
        "paperid": "2409.02084v1",
        "paper_path": "./MISS-QA/papers/2409.02084v1.json",
        "figure_id": "2409.02084v1_figure_2.png",
        "caption": "Figure 2: GraspSplats employs two techniques to efficiently construct feature-enhanced 3D Gaussians: hierarchical feature extraction and dense initialization from geometry regularization, which reduces the overall runtime to 1/10 of existing GS methods [38]. (High-dimensional features are visualized using PCA and the visualized Gaussian ellipsoids are trained without densification).",
        "qtype": "Others",
        "response": "Here is a sketch of the reasoning, in three steps:\n\n1.  Identify “​[mask1]​”  \n    In the figure, the red‐boxed module is the “Hierarchical Features” block (with an object-level feature F_obj and a part-level feature F_part), which comes from 2D masks (MobileSAM) + Masked-CLIP pooling.  \n\n2.  How those features get into 3D  \n    During 3D-GS we “splat” each Gaussian’s 3D location and appearance together with its rendered feature vector from that MLP branch.  In effect, the 2D-derived semantic features are lifted into the 3D Gaussians via our differentiable splatting.\n\n3.  What goes wrong if depth is poor  \n    ●  If the depth frames are sparse or noisy, the **initial Gaussian centers** (which we seed directly from the depth back-projection) will be misplaced or under-sampled.  \n    ●  Misplaced Gaussians mean the 2D mask & CLIP features no longer line up correctly in 3D.  You get “semantic blobs” floating in the wrong place or large gaps in parts of the geometry.  \n    ●  In particular, small or thin parts (e.g. a mug’s handle) vanish entirely if there simply aren’t enough depth points to seed Gaussians there.  Since our part-level feature branch has nothing to anchor onto, you lose any handle-specific signal.  \n    ●  Finally, noisy depth can cause the splatted features themselves to blur or smear across nearby surfaces, eroding the crisp object-vs-part distinctions that the hierarchical features are meant to enforce.  \n\nIn short, this purely image-backed “hierarchical feature” pipeline only works if you have a reasonably dense, accurate depth map to seed and localize your Gaussians.  As soon as the depth gets too thin or corrupted, the 2D masks and CLIP embeddings can no longer be reliably deposited into the 3D model, and you lose part-level fidelity."
    },
    {
        "question": "What ethical concerns might feature splatting raise when applied to privacy-sensitive environments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "feature splatting"
        ],
        "id": 2188,
        "masked_question": "What ethical concerns might [mask1] raise when applied to privacy-sensitive environments?",
        "masked_number": 1,
        "masked_elements": [
            "feature splatting"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.02084v1_figure_2.png",
        "paperid": "2409.02084v1",
        "paper_path": "./MISS-QA/papers/2409.02084v1.json",
        "figure_id": "2409.02084v1_figure_2.png",
        "caption": "Figure 2: GraspSplats employs two techniques to efficiently construct feature-enhanced 3D Gaussians: hierarchical feature extraction and dense initialization from geometry regularization, which reduces the overall runtime to 1/10 of existing GS methods [38]. (High-dimensional features are visualized using PCA and the visualized Gaussian ellipsoids are trained without densification).",
        "qtype": "Others",
        "response": "Step 1: Align the masked region to the text and diagram  \n– In the full‐structure figure the red‐boxed area on the far right is labelled “Feature Splatting.”  Every other block in that row is annotated and explained in the surrounding paragraphs, so the only plausible fill‐in for [mask1] is “Feature Splatting.”\n\nStep 2: How “Feature Splatting” works (very briefly)  \n– It lifts 2D semantic and geometric features into an explicit 3D Gaussian‐splat representation of a scene.  \n– From a handful of RGB-D views it rapidly reconstructs high‐fidelity geometry, texture and open-vocabulary semantic labels.\n\nStep 3: Ethical concerns in privacy‐sensitive settings  \nBecause Feature Splatting can automatically build a detailed, semantically indexed 3D model of an environment, applying it in places where people have an expectation of privacy (homes, hospitals, changing rooms, etc.) risks:\n\n1. Covert surveillance  \n  • Continuous or unattended 3D mapping could record individuals’ movements and behaviors without their knowledge or consent.  \n  • Semantic labels (e.g., “person,” “bed,” “medicine bottle”) compound the intrusiveness.  \n\n2. Personally Identifiable Information (PII) leakage  \n  • High‐resolution reconstructions may capture faces, tattoos, personal documents, or other identifiers.  \n  • Semantic-tagged geometry could be combined with poor data safeguards to re‐identify individuals.  \n\n3. Data misuse and secondary harms  \n  • Insurers, employers or advertisers might harvest 3D scans to profile lifestyles or medical conditions.  \n  • Stolen or leaked 3D models of private interiors could facilitate burglary or stalking.  \n\n4. Bias and unfair profiling  \n  • The underlying vision‐language models (e.g. CLIP) have well-known biases.  \n  • In a privacy context, that can lead to certain groups being over- or under-surveilled or misclassified.  \n\nStep 4: Mitigations  \nTo responsibly deploy 3D Feature Splatting in sensitive environments, one should:  \n– Obtain explicit informed consent from all occupants.  \n– Perform as much processing as possible on-device, without uploading raw geometric or semantic data.  \n– Strip or blur out any personally identifiable regions (faces, license plates, etc.) before storage or sharing.  \n– Encrypt stored representations and enforce strict access controls.  \n– Adopt data-minimization principles: only capture the minimum scene detail needed for the robot’s task.  \n\nAnswer  \nWhen applied in private homes, workplaces or medical settings, “Feature Splatting” could inadvertently reconstruct people’s bodies, faces, personal belongings and behavior patterns in 3D, effectively amounting to covert surveillance. The richly semantic, persistent 3D models it produces can leak personally identifiable information, be repurposed by insurers or advertisers, or facilitate burglary or stalking. Biases in the underlying vision–language backbones may compound these risks by mis‐labelling or over-monitoring certain groups. To deploy safely in such environments, one needs strict consent procedures, on-device processing, anonymization (e.g. face–blur), data-minimization, encryption and access controls."
    },
    {
        "question": "What limitations could the FFT-based Image Decomposition Module present for capturing fine local textures?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "FFT",
            "Image Decomposition Module"
        ],
        "id": 2189,
        "masked_question": "What limitations could the [mask1] present for capturing fine local textures?",
        "masked_number": 1,
        "masked_elements": [
            "Image Decomposition Module"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.01432v1_figure_1.png",
        "paperid": "2411.01432v1",
        "paper_path": "./MISS-QA/papers/2411.01432v1.json",
        "figure_id": "2411.01432v1_figure_1.png",
        "caption": "Figure 1: Framework of the proposed method. In this work, we present an insightful meta-learning framework that exploits cross-domain invariant frequency priors to alleviate the over-fitting problems of classic meta-learning in cross-domain FSL tasks. Our method consists of an Image Decomposition Module (IDM) and a Prior Regularization Meta-Network (PRM-Net). Among them, IDM aim at explicitly decomposing every image in few-shot task into low- and high-frequency components. PRM-Net develops a prediction consistency prior and a feature reconstruction prior to jointly regularize the feature embedding network during meta-learning, aiming to learn generalizable image feature embeddings. Once the model is trained, only the main branch is retained for meta-testing on target domains.",
        "qtype": "Others",
        "response": "First, let’s be clear about what “mask1” is pointing to in the diagram.  If you look at the Image Decomposition Module (the green box on the left) you’ll see that the only thing outlined in red is the “FFT” block — that is, the fast Fourier transform used to split an image into low- and high-frequency components.\n\nNow, what are the weaknesses of using a plain FFT for texture-aware decomposition?  The FFT is a global, infinite-support transform: each Fourier coefficient “sees” the entire image, so fine, spatially-localized patterns tend to get smeared out in the frequency domain.  Concretely:\n\n 1. Loss of spatial locality  \n    • A single high-frequency Fourier basis is a sinusoid spanning the whole image.  You lose the notion of “where” in the image that texture actually lives.  \n 2. Coarse band separation  \n    • A hard low-pass/high-pass cutoff often lumps together a very wide band of frequencies.  Subtle mid-frequency textures can get thrown out or bunched in with very coarse edges.  \n 3. Spectral leakage & ringing  \n    • Sharp edges or tiny texture elements can “leak” into many neighboring frequency bins, producing ringing artifacts when you invert the FFT.  \n 4. Poor multi-scale/detail adaptivity  \n    • Real textures live at many scales and orientations — the fixed, global sine/cosine bases of the FFT do not adapt to local anisotropic structures.\n\nAll of these mean that a vanilla per-image FFT will tend to wash out or distort fine, local texture cues (e.g. the subtle feather-barb patterns on a bird or the tiny grooves on a leaf) that you’d really like to capture for few-shot discrimination."
    },
    {
        "question": "What alternative strategies could complement the feature reconstruction and prediction consistency priors to reduce overfitting?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "feature reconstruction prior",
            "prediction consistency prior"
        ],
        "id": 2190,
        "masked_question": "What alternative strategies could complement the [mask1] and prediction consistency priors to reduce overfitting?",
        "masked_number": 1,
        "masked_elements": [
            "feature reconstruction prior"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.01432v1_figure_1.png",
        "paperid": "2411.01432v1",
        "paper_path": "./MISS-QA/papers/2411.01432v1.json",
        "figure_id": "2411.01432v1_figure_1.png",
        "caption": "Figure 1: Framework of the proposed method. In this work, we present an insightful meta-learning framework that exploits cross-domain invariant frequency priors to alleviate the over-fitting problems of classic meta-learning in cross-domain FSL tasks. Our method consists of an Image Decomposition Module (IDM) and a Prior Regularization Meta-Network (PRM-Net). Among them, IDM aim at explicitly decomposing every image in few-shot task into low- and high-frequency components. PRM-Net develops a prediction consistency prior and a feature reconstruction prior to jointly regularize the feature embedding network during meta-learning, aiming to learn generalizable image feature embeddings. Once the model is trained, only the main branch is retained for meta-testing on target domains.",
        "qtype": "Others",
        "response": "First, note that the little red box in the figure is exactly where we fuse the high- and low-frequency branch–features back into the “main” branch via a little projection network and impose the reconstruction loss.  In the text this is your “feature reconstruction prior,” which sits side-by-side with the “prediction consistency prior.”  \n\nSo the question really asks:\n\n  “What other tricks or regularizers could you plug in alongside (1) the feature-reconstruction prior and (2) the prediction-consistency prior, to further blunt over-fitting in cross-domain meta-training?”\n\nHere is a shortlist of the most obvious complements:\n\n  1.  Data‐augmentation or mixup in the frequency domain  \n     •  Classical spatial augmentations (random crop/resize, flip, color‐jitter) can still help.  \n     •  “FreqMixup” or CutMix on the Fourier components would force the network to generalize across interpolated spectra.  \n\n  2.  Standard parameter‐norm regularization  \n     •  Weight decay (ℓ₂ penalty) on the embedding network.  \n     •  Dropout (or Stochastic Depth) in the projection or embedding layers.  \n\n  3.  Self‐supervised auxiliary tasks  \n     •  Rotation‐prediction or jigsaw‐shuffling on the low‐ and high‐frequency images.  \n     •  A contrastive loss (e.g. SimCLR style) between the original and its frequency‐decomposed views.  \n\n  4.  Adversarial/domain‐alignment losses  \n     •  A small domain‐adversary head that tries to distinguish source vs. (held-out) target; the feature-encoder learns to fool it.  \n     •  Maximum mean discrepancy (MMD) or CORAL between source and target feature-distributions.  \n\n  5.  Meta‐regularizers from the MAML family  \n     •  Jacobian‐norm penalties, or higher‐order smoothness penalties on the task‐adaptation gradients (see MAML++).  \n     •  Gradient noise injection or learned step‐size schedules (to flatten the meta‐loss landscape).  \n\nEach of these can be “dropped into” the same episodic meta-training loop to further discourage the model from latching onto narrow, source-specific clues, and thereby reduce overfitting even more."
    },
    {
        "question": "What motivates separate embedding networks for main branch, low-frequency branch, and high-frequency branch?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "While one straightforward approach is to share parameters between the high-low frequency branches and the main branch, this might lead the feature embedding network to primarily focus on common features among the three, potentially causing distinctive features in the high-frequency or low-frequency branches to be overlooked.",
            "To address this concern and extract more distinctive features, we opt for an explicit design where three separate feature embedding networks are employed without parameter sharing."
        ],
        "final_answer": "The authors use separate embedding networks because sharing parameters across the main, low-frequency, and high-frequency branches would cause the model to focus on common features and overlook the distinctive information in each frequency component. Separate networks ensure that each branch can learn and preserve its unique, complementary features.",
        "relevant_elements": [
            "main branch",
            "low-frequency branch",
            "high-frequency branch"
        ],
        "id": 2191,
        "masked_question": "What motivates separate embedding networks for [mask1], low-frequency branch, and high-frequency branch?",
        "masked_number": 1,
        "masked_elements": [
            "main branch"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.01432v1_figure_1.png",
        "paperid": "2411.01432v1",
        "paper_path": "./MISS-QA/papers/2411.01432v1.json",
        "figure_id": "2411.01432v1_figure_1.png",
        "caption": "Figure 1: Framework of the proposed method. In this work, we present an insightful meta-learning framework that exploits cross-domain invariant frequency priors to alleviate the over-fitting problems of classic meta-learning in cross-domain FSL tasks. Our method consists of an Image Decomposition Module (IDM) and a Prior Regularization Meta-Network (PRM-Net). Among them, IDM aim at explicitly decomposing every image in few-shot task into low- and high-frequency components. PRM-Net develops a prediction consistency prior and a feature reconstruction prior to jointly regularize the feature embedding network during meta-learning, aiming to learn generalizable image feature embeddings. Once the model is trained, only the main branch is retained for meta-testing on target domains.",
        "qtype": "Design_Rationale",
        "response": "First, note that the red‐boxed module in Fig. 1 is just the “main” embedding network (sometimes called f θ) that processes the raw images, parallel to the low‐frequency and high‐frequency embedding networks.  The paper explains quite plainly why they do *not* share parameters between those three embeddings:\n\n1.  If you simply share one trunk for all three inputs (raw, low-freq, high-freq), the network will tend to lock onto whatever features are *common* to all of them and ignore the distinctions.  \n2.  But our whole goal is to force the model to *attend separately* to (a) the overall appearance in the raw image, (b) the broad, smooth, low-frequency content, and (c) the fine, edge‐and‐texture high-frequency content.  \n3.  By giving each of those three its own embedding network, we make sure that each branch can learn features *specialized* to that frequency band.  Only then can the prediction‐consistency and feature‐reconstruction priors truly encourage the model to fuse *complementary* information rather than collapse everything into the “common” modes.  \n\nIn short: separate encoders guarantee that raw, low-freq, and high-freq streams each capture their own distinctive cues, which is exactly what you need to build a stronger, more cross-domain–robust few-shot learner."
    },
    {
        "question": "Why combine feature reconstruction prior and prediction consistency prior during meta-learning regularization?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "Motivated by this perspective, we propose a prediction consistency prior and a feature reconstruction prior to jointly regularize the feature embedding network during meta-learning using high-low frequency information obtained from image decomposition.",
            "Specifically, the prediction consistency prior aims to minimize the separate Kullback-Leibler divergence between the prediction scores produced by the original query image and its each frequency component. The rationale behind this approach is twofold. Firstly, through explicit decomposition-alignment, we compel the model to attend to both low-frequency content and high-frequency structure. Secondly, establishing prediction consistency between high-low frequency and the original one is domain-invariant. This consistency aids the model in generalizing effectively across different domains.",
            "The feature reconstruction prior aims at reconstructing the original features utilizing low-frequency and high-frequency information in the latent space, which promotes the model to learn comprehensive representations."
        ],
        "final_answer": "By combining the two priors, the meta-learning procedure is simultaneously regularized at both the feature level and the prediction level. The feature reconstruction prior forces the network to fuse complementary low- and high-frequency embeddings into a complete, richly detailed representation, while the prediction consistency prior aligns the outputs of the original and decomposed inputs in a domain-invariant way. Together, they prevent over-fitting to source-domain priors and encourage the learning of cross-domain transferable features and predictions.",
        "relevant_elements": [
            "feature reconstruction prior",
            "prediction consistency prior"
        ],
        "id": 2192,
        "masked_question": "Why combine [mask1] and [mask2] during meta-learning regularization?",
        "masked_number": 2,
        "masked_elements": [
            "feature reconstruction prior",
            "prediction consistency prior"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.01432v1_figure_1.png",
        "paperid": "2411.01432v1",
        "paper_path": "./MISS-QA/papers/2411.01432v1.json",
        "figure_id": "2411.01432v1_figure_1.png",
        "caption": "Figure 1: Framework of the proposed method. In this work, we present an insightful meta-learning framework that exploits cross-domain invariant frequency priors to alleviate the over-fitting problems of classic meta-learning in cross-domain FSL tasks. Our method consists of an Image Decomposition Module (IDM) and a Prior Regularization Meta-Network (PRM-Net). Among them, IDM aim at explicitly decomposing every image in few-shot task into low- and high-frequency components. PRM-Net develops a prediction consistency prior and a feature reconstruction prior to jointly regularize the feature embedding network during meta-learning, aiming to learn generalizable image feature embeddings. Once the model is trained, only the main branch is retained for meta-testing on target domains.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key idea in one sentence.  We don’t just force the network’s *predictions* on the low-/high-frequency splits to agree with the original (that is your [mask2]), *and* we don’t just force the *features* it extracts from those splits to be able to reconstruct the “full” feature (that is your [mask1]) in isolation – we do *both* at once.  \n\nChain-of-thought:  \n1. Meta-learners on a single source domain easily over-fit – they latch on to quirks of that domain’s raw images.  \n2. By FFT-decomposing each image into a low-frequency “content” part and a high-frequency “structure” part, we get two very different views of the same sample.  \n3. Prediction-consistency ([mask2], the blue‐boxed bar‐charts) says “no matter which view you give me – full, low-f or high-f – the softmax output ought to be the same.”  This rules out domain-specific decision rules that only live in one frequency band.  \n4. Feature-reconstruction ([mask1], the red‐boxed feature maps) says “if you take the low- and the high-frequency embeddings, you must be able to glue them back together into the original full‐frequency embedding.”  This makes sure you actually *use* complementary information in both bands, rather than collapsing everything into whichever is easiest.  \n5. **Together**, these two priors attack over-fitting on two fronts – at the *output* level and at the *latent* level – and force the embedding network to capture cross-domain, frequency-invariant structure.  \n\nThat joint regularization is what gives the meta-learner the robustness to jump out of its training domain and onto new target domains without extra fine-tuning."
    },
    {
        "question": "What motivates parallel 1×1 conv and 3×3 conv paths in Local Convolution Adapter?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "LoCA introduces the multi-scale local information to further enhance visual perception.",
            "The multi-scale convolution module consists of two parallel convolutional paths of multi-scale (1×1, 3×3).",
            "The 1×1 convolution is strategically placed before the 3×3 convolutions to reduce channel dimension. This design and the bottleneck structure make the local convolution adapter still lightweight."
        ],
        "final_answer": "The parallel 1×1 and 3×3 convolutional paths are introduced to capture multi-scale local visual features—using the 1×1 conv to reduce channel dimensionality before the 3×3 conv—so that the adapter can integrate local detail at multiple scales while remaining lightweight.",
        "relevant_elements": [
            "1×1 conv",
            "3×3 conv",
            "Local Convolution Adapter"
        ],
        "id": 2193,
        "masked_question": "What motivates parallel [mask1] and 3×3 conv paths in Local Convolution Adapter?",
        "masked_number": 1,
        "masked_elements": [
            "1×1 conv"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.13609v2_figure_2.png",
        "paperid": "2409.13609v2",
        "paper_path": "./MISS-QA/papers/2409.13609v2.json",
        "figure_id": "2409.13609v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MaPPER. MaPPER freezes the pre-trained vision encoder and language encoder. For the language branch, Dynamic Prior Adapters (DyPA) utilize aligned priors generated from the Vision-aligned Prior Module to enable efficient modal alignment and adaptation. For the language branch, Local Convolution Adapters (LoCA) integrate local visual features the global prior (pre-trained visual knowledge) from the visual encoder.\nMoreover, the Prior-guided Text module for promoting the multimodal alignment.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through the design of the LoCA (Local Convolution Adapter) to see why it uses two parallel paths—one 1×1 conv and one 3×3 conv—in its multi‐scale convolution module:\n\n1.   The REC task needs both fine, pixel-level cues and slightly larger, neighborhood-level cues to accurately ground a referring expression (“front middle yellow guy” might occupy only a handful of tokens/pixels, yet its context often spans more).\n\n2.   A vanilla ViT block (with only global dot‐product attention and an MLP) is excellent at global reasoning but is relatively weak at explicitly modeling very local patterns at different spatial scales.\n\n3.   To inject these local patterns, LoCA adds a tiny “bottleneck” convolutional sub-module in parallel to the existing MLP in the transformer block.\n\n4.   • The 1×1 path (i.e. the red-boxed convolution) acts as a pointwise projection—it reduces the channel dimension (the “down‐projection”) and learns pixel-level (purely channel-mixing) features.  \n    • The 3×3 path then picks up those reduced‐dim features and gives each patch a small 3×3 neighbourhood receptive field, capturing slightly larger spatial context.\n\n5.   By running them in parallel and then concatenating their outputs, you get “multi‐scale” local features—pointwise + small‐kernel—without blowing up the parameter count (the bottleneck keeps it lightweight).\n\nIn short, the parallel 1×1 and 3×3 convolutions give you an efficient, lightweight way to fuse both pixel-level and local neighbourhood information into each visual token, complementing the ViT’s global attention with richer local detail."
    },
    {
        "question": "What drives injecting the vision-aligned prior into every Dynamic Prior Adapter layer?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "As detailed in section 3.2, the pre-training mechanism of BERT makes it ideal for the REC task, which has a relatively high word-level understanding. However, BERT lacks alignment with vision in the pre-training process, and we introduce a Vision-aligned Prior Module to generate a vision-aligned prior. The prior serves for better adjusting BERT encoder, and promoting the interaction of text and vision features.",
            "Specifically, we adopt the DS module for integrating the vision-aligned prior Q to different layers in the BERT encoder.",
            "DyPA utilizes the vision-aligned prior Q to dynamically regularize the feed-forward during adapter tuning."
        ],
        "final_answer": "Because BERT lacks any visual grounding from its original pre-training, the authors inject the vision-aligned prior into every Dynamic Prior Adapter layer so that each adapter can dynamically regularize its feed-forward transformation and promote deep interaction and alignment between the text encoder and visual features.",
        "relevant_elements": [
            "vision-aligned prior",
            "Dynamic Prior Adapter"
        ],
        "id": 2194,
        "masked_question": "What drives injecting the [mask1] into every Dynamic Prior Adapter layer?",
        "masked_number": 1,
        "masked_elements": [
            "vision-aligned prior"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.13609v2_figure_2.png",
        "paperid": "2409.13609v2",
        "paper_path": "./MISS-QA/papers/2409.13609v2.json",
        "figure_id": "2409.13609v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MaPPER. MaPPER freezes the pre-trained vision encoder and language encoder. For the language branch, Dynamic Prior Adapters (DyPA) utilize aligned priors generated from the Vision-aligned Prior Module to enable efficient modal alignment and adaptation. For the language branch, Local Convolution Adapters (LoCA) integrate local visual features the global prior (pre-trained visual knowledge) from the visual encoder.\nMoreover, the Prior-guided Text module for promoting the multimodal alignment.",
        "qtype": "Design_Rationale",
        "response": "The thing in the red‐box is nothing mystical – it is simply the vision‐aligned prior (produced by the CLIP‐based Vision-Aligned Text Module) being “sprayed” into every layer of the Dynamic Prior Adapter.  Concretely, at each BERT layer the Dynamic Scale (DS) sub‐module takes that same vision‐aligned prior vector as input and uses it to compute layer‐specific scaling factors.  In short, it is the need to inject the CLIP-derived, vision-aligned prior into every adapter layer (via the DS module) that drives the red‐boxed module’s connection to each DyPA."
    },
    {
        "question": "How does Local Convolution Adapter incorporate parallel 1×1 and 3×3 convolutions into vision token adaptation?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Local Convolution Adapter (LoCA). To further facilitate the visual perception ability of DINOv2 for the REC task, we propose a Local Convolution Adapter (LoCA) module to adjust the visual foundation models. LoCA introduces the multi-scale local information to further enhance visual perception. The local convolution adapter consists of a down-projection layer , a multi-scale convolution module, a ReLU activation layer, and the up-projection layer .",
            "Specifically, in one visual encoder layer, the downward projection layer receives processed visual tokens  from the Multi-head Attention (MHA) layer as input and produces adapted. The multi-scale convolution module consists of two parallel convolutional paths of multi-scale (1×1, 3×3). The 1×1 convolution is strategically placed before the 3×3 convolutions to reduce channel dimension. The outputs of the multi-scale convolutional paths are concatenated to form the local feature ."
        ],
        "final_answer": "The LoCA first projects vision tokens down, then feeds them into a multi-scale convolution module with two parallel paths—a 1×1 convolution (to reduce channels) and a 3×3 convolution—and concatenates their outputs into a local feature before projecting back up.",
        "relevant_elements": [
            "Local Convolution Adapter"
        ],
        "id": 2195,
        "masked_question": "How does [mask1] incorporate parallel 1×1 and 3×3 convolutions into vision token adaptation?",
        "masked_number": 1,
        "masked_elements": [
            "Local Convolution Adapter"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.13609v2_figure_2.png",
        "paperid": "2409.13609v2",
        "paper_path": "./MISS-QA/papers/2409.13609v2.json",
        "figure_id": "2409.13609v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MaPPER. MaPPER freezes the pre-trained vision encoder and language encoder. For the language branch, Dynamic Prior Adapters (DyPA) utilize aligned priors generated from the Vision-aligned Prior Module to enable efficient modal alignment and adaptation. For the language branch, Local Convolution Adapters (LoCA) integrate local visual features the global prior (pre-trained visual knowledge) from the visual encoder.\nMoreover, the Prior-guided Text module for promoting the multimodal alignment.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in the vision stream is exactly the Local Convolution Adapter (LoCA) described in §3.4. If we unpack LoCA step by step, here is how it “weaves in” parallel 1×1 and 3×3 convolutions to adapt vision tokens:\n\n1. Down-projection  \n   – Take the incoming vision tokens (after the attention sub‐layer) and linearly **down-project** them to a smaller channel dimension.  \n2. Multi-scale convolution module (the heart of the red box)  \n   – **Branch 1**: a single 1×1 convolution over the down-projected tokens.  \n   – **Branch 2**: first another 1×1 convolution (to again reduce channels in a bottleneck style), immediately followed by a 3×3 convolution (to capture local spatial context).  \n   – Because the 1×1 conv in Branch 2 sits right before the 3×3, the expensive 3×3 operates on a slimmed‐down feature map.  \n3. Concatenation and skip-connection  \n   – The outputs of the two branches (the “1×1 only” path and the “1×1→3×3” path) are **concatenated** along the channel dimension.  \n   – In parallel, the original down-projected tokens skip around the convolutions and are added back in (a residual link).  \n4. Up-projection  \n   – Finally, a linear **up-projection** restores the concatenated feature back to the original token dimension, ready to be added into the transformer block’s feed-forward output.\n\nBy running a slim 1×1 and a 3×3 conv in parallel, this adapter injects lightweight, multi‐scale locality into each vision token without bloating the model."
    },
    {
        "question": "How does Dynamic Prior Adapter compute and apply scaling factors using vision-aligned priors across language blocks?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Dynamic Prior Adapter (DyPA). To dynamically bridge the gap between the pre-trained BERT model and the complex REC task, we introduce the Dynamic Prior Adapter, which operates in parallel with the text encoder, as shown in Figure 3. DyPA comprising four modules: a dynamic scale module (DS), a downward projection with parameters W_down, a ReLU activation layer, and an upward projection with parameters W_up.",
            "Specifically, we adopt the DS module for integrating the vision-aligned prior P to different layers in the BERT encoder. The module generates scale factors α using a scoring weight matrix W_s, eliminating manual hyper-parameter tuning. Given the prior P, the dynamic scaling factor can be formulated as follows:",
            "The downward projection and the upward projection are connected by a ReLU function. In one text encoder layer, the downward projection layer receives processed language tokens X from the Multi-head Attention (MHA) layer as input and produces adapter features.",
            "In general, the output of DyPA Δ can be described as the up-projected adapter features multiplied by the dynamic scale factors and then added back to the feed-forward output. DyPA utilizes the vision-aligned prior P to dynamically regularize the feed-forward during adapter tuning. To mitigate the influence of adapter outputs during the initial stages of model training, we initialize α to zero."
        ],
        "final_answer": "The Dynamic Prior Adapter augments each BERT layer with a small adapter whose output is dynamically scaled by factors computed from the vision-aligned prior. Concretely, for each language block, the frozen CLIP–derived prior P is fed into a lightweight scoring weight matrix (the DS module) to produce a vector of scale factors α. In parallel with the feed-forward network, the adapter first down-projects the MHA output into a low-dimensional space, applies a ReLU nonlinearity, and then up-projects back to the hidden dimension. The resulting adapter features Δ are then multiplied element-wise by α and added into the residual stream of the feed-forward output. The scale factors α are initialized to zero so that early in training the adapter has no effect, and gradually learn to modulate each token’s contribution according to the vision-aligned prior.",
        "relevant_elements": [
            "Dynamic Prior Adapter"
        ],
        "id": 2196,
        "masked_question": "How does [mask1] compute and apply scaling factors using vision-aligned priors across language blocks?",
        "masked_number": 1,
        "masked_elements": [
            "Dynamic Prior Adapter"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.13609v2_figure_2.png",
        "paperid": "2409.13609v2",
        "paper_path": "./MISS-QA/papers/2409.13609v2.json",
        "figure_id": "2409.13609v2_figure_2.png",
        "caption": "Figure 2: Overall architecture of MaPPER. MaPPER freezes the pre-trained vision encoder and language encoder. For the language branch, Dynamic Prior Adapters (DyPA) utilize aligned priors generated from the Vision-aligned Prior Module to enable efficient modal alignment and adaptation. For the language branch, Local Convolution Adapters (LoCA) integrate local visual features the global prior (pre-trained visual knowledge) from the visual encoder.\nMoreover, the Prior-guided Text module for promoting the multimodal alignment.",
        "qtype": "Implementation_Details",
        "response": "Mask1 is in fact the Dynamic Prior Adapter (DyPA) that sits inside each BERT (language) block.  Its sole job is to take the single CLIP-derived prior vector p (the “vision-aligned prior”) and turn it into a little per-layer gain vector α, then use that gain to gate the adapter output.  Concretely, for the ℓ-th transformer layer DyPA contains  \n\n  • a tiny “Dynamic Scale” head W^sℓ that computes  \n       αℓ = W^sℓ p  \n    (often followed by a non-linearity or bias), and  \n\n  • the usual down-projection → ReLU → up-projection bottleneck adapter fℓ(·).  \n\nAt runtime the adapter’s output is simply multiplied element-wise by αℓ before being added back into the BERT hidden state:  \n\n    hℓ′ = hℓ + αℓ ⊙ fℓ(hℓ).  \n\nBecause each layer ℓ has its own scoring matrix W^sℓ, the model learns to turn the adapter on or off to exactly the right degree in every block, all driven by that one vision-aligned prior."
    },
    {
        "question": "How could ending interaction criteria guide model training similar to curriculum learning’s stopping conditions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Ending Interaction",
            "Model Training"
        ],
        "id": 2202,
        "masked_question": "How could [mask1] criteria guide [mask2] similar to curriculum learning’s stopping conditions?",
        "masked_number": 2,
        "masked_elements": [
            "Ending Interaction",
            "Model Training"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.15787v1_figure_1.png",
        "paperid": "2408.15787v1",
        "paper_path": "./MISS-QA/papers/2408.15787v1.json",
        "figure_id": "2408.15787v1_figure_1.png",
        "caption": "Figure 1. The overall architecture of our simulation framework. Left panel: construction of client pool. Middle panel: data collection with interactive simulation. Right panel: model training.",
        "qtype": "Literature_Background",
        "response": "Here’s the intuition in three steps:\n\n1.  What the red box is giving you  \n    –  A small, fixed vocabulary of “good‐bye” or “see‐you‐next‐time” turns (e.g. 再见, 保重, 下次见, 祝你好运…).  \n    –  In our simulator these are the only signals we ever use to say “stop talking” and move on.\n\n2.  What the blue box is doing  \n    –  Fine-tuning an open-source LLM into your final “counselor” bot, using all of the simulated dialogues you collected.  \n    –  Each training example is a partial dialogue ending exactly when one of those red-box phrases appears.\n\n3.  How to make that look like curriculum-learning stopping conditions  \n    –  In curriculum learning you typically define a “stop criterion” for each lesson or difficulty level before moving on.  \n    –  Here you turn your red-box list into exactly that: any time your model (during fine-tuning) produces one of those phrases you treat it as a hard stop for that example.  \n    –  You can even structure the curriculum so that early on you only train on very short simulated sessions (so the model learns the simplest closers), then gradually introduce longer dialogs and subtler farewells.  \n    –  In each phase, the appearance of a red-box phrase triggers “end of example → move to next example,” just like curriculum learning’s stopping condition tells you “you’ve mastered this lesson, advance to the next.”  \n\nIn this way the red-boxed “end‐of‐session” tokens become the exact gates that control when a training sequence in the blue-boxed fine-tuning loop is terminated, mirroring the stopping rules of a classical curriculum."
    },
    {
        "question": "How does Client Pool selection influence Dialogue diversity during data collection?",
        "relevant_section_ids": [
            "2.4",
            "3.1"
        ],
        "relevant_context": [
            "Section 2.4: \"Generally, different clients often have different user profiles, which mainly reflect on their mental health issues. Therefore, the first step we need to do is to construct a pool of clients with different mental health issues. ... To validate the research questions proposed in our paper, we set 1000 different user posts as the client pool.\"",
            "Section 3.1: \"Diversity of Clients. The diversity of counseling sessions is often determined by the diversity of clients. ... Results. The topic distribution between RealPsyDial and SimPsyDial is presented in Figure 5. We observe that the information entropy of topics of clients’ chief complaints in SimPsyDial (mean = 4.526; std = 0.009) is slightly lower ... than that in RealPsyDial (mean = 4.875; std = 0.020). Furthermore, we find that the topic distribution ... between RealPsyDial and SimPsyDial is almost similar, demonstrating that our SimPsyDial is close to RealPsyDial with respect to the client side.\""
        ],
        "final_answer": "By selecting a large and varied pool of 1,000 client profiles—each drawn from long user posts describing different mental health issues—the simulated dialogues naturally cover a wide range of topics.  As shown by our topic‐entropy analysis (SimPsyDial mean entropy 4.526 vs. RealPsyDial 4.875), this diverse client pool produces dialogue diversity on par with real counseling sessions.",
        "relevant_elements": [
            "Client Pool",
            "Dialogue"
        ],
        "id": 2203,
        "masked_question": "How does [mask1] selection influence Dialogue diversity during data collection?",
        "masked_number": 1,
        "masked_elements": [
            "Client Pool"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.15787v1_figure_1.png",
        "paperid": "2408.15787v1",
        "paper_path": "./MISS-QA/papers/2408.15787v1.json",
        "figure_id": "2408.15787v1_figure_1.png",
        "caption": "Figure 1. The overall architecture of our simulation framework. Left panel: construction of client pool. Middle panel: data collection with interactive simulation. Right panel: model training.",
        "qtype": "Experimental_Results",
        "response": "Chain‐of‐thought  \n1. In the figure, the red box (i.e. [mask1]) marks the “Client Pool” construction step, where we choose which user profiles (long client posts) will drive the simulation.  \n2. Each profile encodes a distinct mental‐health issue (symptoms, life events, concerns), so picking different profiles directly changes the kinds of problems the simulated client will bring up.  \n3. During data collection, the LLM‐client “reads” one of these profiles and then role‐plays a client in conversation with the LLM‐counselor.  \n4. Therefore, if the pool contains 1000 highly diverse posts across many topics, the resulting 1000 simulated dialogues will likewise span a wide variety of themes—boosting overall dialogue diversity.  \n5. This is borne out in Section 3.1 of the paper, where topic distributions of SimPsyDial have high information entropy (≈4.5), meaning the dialogues cover many different complaint topics, close to the real‐world set (≈4.9).  \n6. In short, by carefully selecting a broad, heterogeneous set of client profiles in [mask1], the authors ensure the simulated dialogues are diverse in subject matter and conversational content.  \n\nAnswer  \nThe choice of user profiles in the client‐pool construction step ([mask1]) determines which mental‐health issues get simulated. A larger, more varied selection of long posts leads to conversations that cover a broader range of topics (higher topic‐distribution entropy), whereas a narrower or more homogeneous profile set would yield less varied dialogues."
    },
    {
        "question": "How does integrating Skill F/T signals refine success conditions in the demo task plan?",
        "relevant_section_ids": [
            "3.2",
            "4.2"
        ],
        "relevant_context": [
            "Since most of the other information is either binary or straightforward when used to form conditions (e.g. whether an object is grasped or a position is reached), we focus especially on F/T conditions which are highly variable and crucial for contact-rich manipulations.",
            "To address this without sacrificing generality, we assume that the task is performed in a static environment where interactions with the object occur exclusively through the robot. In this context, the most relevant F/T information pertains to the force or torque opposing the robot’s actions, as they provide direct feedback on the resistance encountered during manipulation. Based on this observation, we reduce our F/T perception interface to include only resistance force and torque.",
            "For each skill, we first ask the LLM to generate an initial success condition function, in which it determines which signal the condition should be based on (e.g. resistance_torque is used to form the is_tightened condition). We then provide a plot of the selected signal and prompt the LLM to update success condition functions accordingly. An example of the resulting function is_inserted for the task of mounting cable to clips is shown in Fig. 3 (c). The LLM defines the success condition for insertion as the resistance force falling below a certain threshold, indicating that the cable has been securely inserted.",
            "Table II shows stretched and inserted conditions for cable mounting as well as tightened condition for cap tightening generated by the LLM, each containing force-relevant thresholds. We observe that after F/T signals were introduced, the LLM retained its initial condition formulations but used the signals to update threshold estimations. The success rates before and after the updates (shown in Table II) indicate that the conditions for cable insertion into the U-type clip and cap tightening improved significantly with the integration of the demonstrated F/T signals."
        ],
        "final_answer": "By feeding the LLM actual force/torque traces for each skill, it is able to replace its rough, initially drafted success‐condition rules with empirically grounded threshold values (e.g. a specific resistance‐force cutoff for “is_inserted” or a torque threshold for “is_tightened”). This refinement of numeric thresholds from real F/T data makes the success conditions more accurate and robust when executing the demonstration‐derived task plan.",
        "relevant_elements": [
            "Skill F/T Signals",
            "Demo Task Plan"
        ],
        "id": 2206,
        "masked_question": "How does integrating [mask1] refine success conditions in the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Skill F/T Signals",
            "Demo Task Plan"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.11863v1_figure_2.png",
        "paperid": "2409.11863v1",
        "paper_path": "./MISS-QA/papers/2409.11863v1.json",
        "figure_id": "2409.11863v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. In bootstrapped reasoning, an LLM analyzer pre-processes the skill library, reasons about skill sequences and success conditions from multi-modal demonstration sequentially. The resulting demo task plan is used as an example for an LLM planner to plan for new tasks.",
        "qtype": "Experimental_Results",
        "response": "Step‐by‐step reasoning:\n\n1. In the pipeline (see the red‐outlined box), we inject the raw force–torque (F/T) measurements (“Skill F/T Signals”) into the LLM after it has already drafted an initial set of success conditions for each skill.  \n2. These initial conditions were simply logical functions (e.g. “is_inserted,” “is_tightened”) with unknown numeric thresholds.  \n3. By plotting the recorded resistance force and torque traces from the demonstration and feeding these plots back to the LLM, we give it concrete examples of what “success” looks like in terms of actual sensor readings.  \n4. The LLM then revises each success‐condition function—tuning its threshold parameters (e.g. the exact value of resistance_force below which insertion is deemed successful) so that they match the observed data.  \n5. Those updated, data‐grounded condition functions become part of the final demo task plan (the blue‐outlined box), replacing the hand‐wavy placeholders with calibrated, executable checks.  \n\nIn short: integrating the Skill F/T Signals lets the LLM fit real force/torque curves, refining the numeric thresholds in the demo task plan’s success conditions so that they reliably detect when each skill has actually succeeded."
    },
    {
        "question": "What alternative modalities could complement ViTac Videos to improve segmentation robustness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "ViTac Videos",
            "segmentation"
        ],
        "id": 2207,
        "masked_question": "What alternative modalities could complement [mask1] to improve [mask2] robustness?",
        "masked_number": 2,
        "masked_elements": [
            "ViTac Videos",
            "segmentation"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.11863v1_figure_2.png",
        "paperid": "2409.11863v1",
        "paper_path": "./MISS-QA/papers/2409.11863v1.json",
        "figure_id": "2409.11863v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. In bootstrapped reasoning, an LLM analyzer pre-processes the skill library, reasons about skill sequences and success conditions from multi-modal demonstration sequentially. The resulting demo task plan is used as an example for an LLM planner to plan for new tasks.",
        "qtype": "Others",
        "response": "Chain-of-thought:  \nLooking at the leftmost red box, it’s the ViTac tactile‐video stream used to detect fine contact events.  The blue dashed box around “Segmentation” is where we chop the continuous demo into segments based on those ViTac patterns.  In our current pipeline we lean solely on the ViTac videos to pick out grasp-on, pull, twist, etc.  But of course real contact-rich interactions produce signals in multiple domains.  For example, the very same contact transitions show up in:  \n • Force/torque readings at the wrist or fingertip (joint torque/proprioceptive feedback)  \n • Contact microphones or accelerometers (audio/vibration cues)  \n • Depth or optical‐flow from a close‐range depth camera or event camera  \n\nSo, to make the segmentation stage more robust, one could fuse ViTac videos with any of these modalities—most naturally the F/T channels the robot already has, or even simple contact‐mic audio channels—to corroborate when one event ends and the next begins.  \n\nAnswer: One could fuse the ViTac video stream with force/torque feedback (or other proprioceptive/contact sensors such as joint‐torques, contact-mic audio, depth/optical‐flow), to make the segmentation step far more robust."
    },
    {
        "question": "What limitations arise when translating the Input Skill Library into a PDDL Domain via LLM pre-processing?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Input Skill Library",
            "PDDL Domain"
        ],
        "id": 2208,
        "masked_question": "What limitations arise when translating the [mask1] into a [mask2] via LLM pre-processing?",
        "masked_number": 2,
        "masked_elements": [
            "Input Skill Library",
            "PDDL Domain"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.11863v1_figure_2.png",
        "paperid": "2409.11863v1",
        "paper_path": "./MISS-QA/papers/2409.11863v1.json",
        "figure_id": "2409.11863v1_figure_2.png",
        "caption": "Figure 2: Framework Overview. In bootstrapped reasoning, an LLM analyzer pre-processes the skill library, reasons about skill sequences and success conditions from multi-modal demonstration sequentially. The resulting demo task plan is used as an example for an LLM planner to plan for new tasks.",
        "qtype": "Others",
        "response": "Let’s start by identifying the two masked regions in the figure:\n\n  1.  [mask₁] is the red–outlined box labeled “Input Skill Library L = {ξᵢ = (Oₜ, A)}.”  This is literally the set of robot‐executable code‐scripts that define each skill’s low‐level action (A) and target object (Oₜ).\n  2.  [mask₂] is the blue–outlined box labeled “PDDL Domain (Cₚ, Cₛ).”  This is the purely symbolic Planning Domain Definition Language encoding of those same skills, where each skill becomes an “action” with preconditions Cₚ and effects Cₛ.\n\nQuestion  \nWhat limitations arise when translating the Input Skill Library into a PDDL Domain via LLM pre‐processing?\n\nChain of Thought  \n1.  The Input Skill Library is written as actual code that the robot controller can execute (it contains method calls, continuous parameters, adaptive‐impedance settings, etc.).  \n2.  PDDL, by contrast, is a purely symbolic language of discrete predicates, preconditions, and effects.  It is excellent for high-level combinatorial planning but does not natively capture continuous parameters or rich sensor modalities.  \n3.  When we ask an LLM to “translate” the code‐based skill library into PDDL, we force it to:\n    •  Strip out all the low‐level controller details (e.g. impedance gains, exact motion trajectories),  \n    •  Abstract away any continuous sensor readings (joint torques, tactile flow vectors) into Boolean predicates,  \n    •  Invent or “hallucinate” coarse preconditions and effects (Cₚ, Cₛ) to make each action well‐formed in PDDL.  \n4.  As a result:  \n    –  The resulting PDDL actions often lack the nuance needed for contact‐rich tasks (e.g. they cannot express a sliding‐force threshold or a smoothly varying torque profile).  \n    –  Important grounding information (force‐torque thresholds, tactile patterns) is lost or must be re-inferred later in the pipeline.  \n    –  The LLM’s auto‐generated pre/post‐conditions tend to be incomplete or only approximately correct, so one still needs the F/T‐based grounding step (Section III-B) to tighten up the success conditions.  \n5.  In short, the symbolic PDDL domain is too coarse a vocabulary for the fine‐grained, sensor‐driven details of contact‐rich robotic skills.  \n\nAnswer  \nTranslating the code-based Input Skill Library into a purely symbolic PDDL Domain with an LLM inevitably strips out all of the low-level execution detail—continuous controller parameters, force/torque or tactile sensor streams, exact motion profiles—and forces the model to invent coarse Boolean preconditions (Cₚ) and effects (Cₛ).  As a result, the PDDL actions are too abstract to capture contact-rich nuances, their automatically generated pre/post-conditions tend to be incomplete or only approximate, and one must still re-ground those conditions (e.g. by plugging in F/T thresholds) before they become reliable for real robot execution."
    },
    {
        "question": "What are potential limitations of HDDC-based anchor selection when dealing with continuous latent feature variations?",
        "relevant_section_ids": [
            "3.2.1",
            "5"
        ],
        "relevant_context": [
            "However, HDDC requires the number of Gaussians to be specified in advance and this number remains fixed. We propose two methods to adjust this dynamically.",
            "Additionally, a naïve merging strategy may not be suitable for feature alignment, especially when dealing with real-world data where the label may be a continuous value. Therefore, a strategy that dynamically adjusts the number of Gaussians to handle continuous variables is needed.",
            "However, as the number of Gaussians in DyGA is not a continuous variable, the approach has limitations in handling attributes with continuous values."
        ],
        "final_answer": "Because HDDC requires a fixed, pre-specified number of Gaussians, it cannot natively represent smoothly varying or continuous feature attributes. Naïve merging or splitting strategies in this high-dimensional setting may also fail to capture truly continuous variations, leading to poor boundary alignment for features whose underlying factors vary continuously.",
        "relevant_elements": [
            "HDDC",
            "Anchor Selection"
        ],
        "id": 2209,
        "masked_question": "What are potential limitations of [mask1]-based anchor selection when dealing with continuous latent feature variations?",
        "masked_number": 1,
        "masked_elements": [
            "HDDC"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.23820v1_figure_2.png",
        "paperid": "2410.23820v1",
        "paper_path": "./MISS-QA/papers/2410.23820v1.json",
        "figure_id": "2410.23820v1_figure_2.png",
        "caption": "Figure 2: \nTraining framework with proposed methods. (a) During the diffusion model training, the features generated by the feature extractor are shifted towards the mean direction of the Gaussian for each feature unit based on the selected anchor, becoming the condition for the diffusion model. To ensure the diffusion U-Net effectively utilizes the conditions created by the feature extractor, a skip dropout strategy is employed. (b) The process of anchoring Gaussian distributions involves: i) initializing the Gaussian mixture, ii) performing HDDC using the EM algorithm, iii) adjusting the number of Gaussians by splitting them according to criteria, and iv) filtering out unnecessary Gaussians.",
        "qtype": "Others",
        "response": "Here is a step-by-step unpacking of why pure HDDC-based anchoring can struggle when your latent factors actually vary continuously rather than sitting in a small number of well-separated “blobs”:\n\n1. HDDC in a nutshell  \n   – HDDC fits a finite mixture of Gaussians by (a) projecting down into a subspace, then (b) running EM to estimate means and covariances.  \n   – At the end you get K Gaussian components (anchors), each with a mean μ_k and a subspace‐covariance.\n\n2. The implicit discreteness  \n   – By construction, HDDC partitions the feature space into K discrete regions, each tied to one Gaussian mean μ_k.  \n   – As soon as your underlying factor is not “choose one of K modes” but instead drifts smoothly (think of a hue wheel or a continuously moving object), a fixed set of K means becomes a rather coarse approximation of that manifold.  \n\n3. Local‐optima & sensitivity  \n   – EM is nonconvex.  In high dimensions it frequently converges to a sub–optimal set of cluster centers.  \n   – Changing K via splits or filters helps escape some poor local minima, but now you introduce extra heuristics (density thresholds, how aggressively to split or prune) whose mistakes can leave you with either too many tiny anchors or too few big, ill-fitting ones.  \n\n4. Hyperparameter brittleness  \n   – Choosing the density threshold for “when to split” or “when to prune” is problem-specific.  If you set it too loose, you swamp your model with anchors that chase noise; too tight, and you never break apart two distinct but close-lying regions.  \n\n5. Poor fidelity on smooth variations  \n   – Even if you crank K very large, you only ever place isotropic or low–rank Gaussians in each subspace.  Continuous, curved manifolds in the latent space are at best approximated by a “piecewise linear” tiling of Gaussians, which is both parameter-inefficient and prone to gaps/overlaps.  \n\n6. Computational burdens  \n   – High-D EM (even with subspace tricks) is relatively costly, and repeating splits/merges every epoch can become a bottleneck.  \n\nIn short, HDDC anchors work nicely if your latents really do cluster around a small number of isolated Gaussians, but when the true factors sweep out continuous curves or surfaces in latent space, a purely Gaussian-mixture (HDDC) anchoring scheme:  \n  • Forces a discrete tiling onto an inherently smooth variation,  \n  • Suffers from EM getting stuck or mis-splitting, and  \n  • Requires delicate tuning of split/prune thresholds to avoid either over- or under-fragmenting the latent manifold."
    },
    {
        "question": "How might skip dropout compromise balance between feature extractor and diffusion U-Net impacting methodology fairness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Skip Dropout",
            "Feature Extractor",
            "Diffusion U-Net"
        ],
        "id": 2210,
        "masked_question": "How might [mask1] compromise balance between [mask2] and diffusion U-Net impacting methodology fairness?",
        "masked_number": 2,
        "masked_elements": [
            "Skip Dropout",
            "Feature Extractor"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.23820v1_figure_2.png",
        "paperid": "2410.23820v1",
        "paper_path": "./MISS-QA/papers/2410.23820v1.json",
        "figure_id": "2410.23820v1_figure_2.png",
        "caption": "Figure 2: \nTraining framework with proposed methods. (a) During the diffusion model training, the features generated by the feature extractor are shifted towards the mean direction of the Gaussian for each feature unit based on the selected anchor, becoming the condition for the diffusion model. To ensure the diffusion U-Net effectively utilizes the conditions created by the feature extractor, a skip dropout strategy is employed. (b) The process of anchoring Gaussian distributions involves: i) initializing the Gaussian mixture, ii) performing HDDC using the EM algorithm, iii) adjusting the number of Gaussians by splitting them according to criteria, and iv) filtering out unnecessary Gaussians.",
        "qtype": "Others",
        "response": "Here’s a step‐by‐step rationale:\n\n1.  Identify the two modules in question  \n    -  [mask1] is the red‐boxed “Skip Dropout” mechanism inside the diffusion U-Net.  \n    -  [mask2] is the blue-boxed “Feature Extractor,” which produces the disentangled latent units c.\n\n2.  Recall their intended interplay  \n    -  Without Skip Dropout the U-Net can “short‐circuit” around the condition c via its normal skip connections, and ignore the feature extractor altogether (leaving c under‐utilized).  \n    -  Skip Dropout was introduced precisely so that—at training time—some of those skip‐path features are randomly zeroed, forcing the U-Net to pay attention to c and thus strengthening the feature extractor.\n\n3.  Pinpoint the potential imbalance  \n    -  If Skip Dropout is too aggressive (i.e. too high a dropout rate), the U-Net may become *overly* dependent on c.  In extreme cases it may never learn to use its own noisy‐image pathway effectively, so the feature extractor is “pulled” to carry virtually all of the reconstruction burden.  \n    -  Conversely, if Skip Dropout is too weak, the U-Net reverts to ignoring c (the original pathology one was trying to fix).\n\n4.  Connect to “methodology fairness”  \n    -  We can think of “fairness” here as the *balanced* sharing of representational work between the two modules: neither should be allowed to dominate.  Over‐dropping the skip paths shifts all the modeling onto the feature extractor, creating an *unfair training regime* where the U-Net never learns to denoise from the noisy input alone and the feature extractor must over‐specialize.  \n    -  Under‐dropping leaves the opposite problem: the feature extractor never gets a fair chance to carry meaningful signal.\n\n5.  Conclusion  \n    Skip Dropout must therefore be tuned so that the U-Net and the feature extractor each do *their* job—denoising versus producing disentangled codes—without one swamping the other.  If it isn’t, you break the “fair” cooperation you intended and hurt both disentanglement quality and stable diffusion training."
    },
    {
        "question": "What rationale underlies applying Feature Alignment in Dynamic Gaussian Anchoring to enforce attribute separation?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Feature alignment refers to the process of shifting a feature c towards the mean μ of the Gaussian with the highest responsibility, as described in Eq. (4). Through feature alignment, the boundaries between clusters becomes definite.",
            "Since a feature located at the boundary between two Gaussians is sensitive, adjusting this feature is critical to the stability of the diffusion model training."
        ],
        "final_answer": "The rationale is to sharpen and stabilize the separation between attribute clusters by nudging ambiguous latent features (those near cluster boundaries) toward their nearest Gaussian means. This makes the attribute regions in the latent space more distinct and ensures stable diffusion model training.",
        "relevant_elements": [
            "Feature Alignment",
            "Dynamic Gaussian Anchoring"
        ],
        "id": 2211,
        "masked_question": "What rationale underlies applying [mask1] in Dynamic Gaussian Anchoring to enforce attribute separation?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Alignment"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.23820v1_figure_2.png",
        "paperid": "2410.23820v1",
        "paper_path": "./MISS-QA/papers/2410.23820v1.json",
        "figure_id": "2410.23820v1_figure_2.png",
        "caption": "Figure 2: \nTraining framework with proposed methods. (a) During the diffusion model training, the features generated by the feature extractor are shifted towards the mean direction of the Gaussian for each feature unit based on the selected anchor, becoming the condition for the diffusion model. To ensure the diffusion U-Net effectively utilizes the conditions created by the feature extractor, a skip dropout strategy is employed. (b) The process of anchoring Gaussian distributions involves: i) initializing the Gaussian mixture, ii) performing HDDC using the EM algorithm, iii) adjusting the number of Gaussians by splitting them according to criteria, and iv) filtering out unnecessary Gaussians.",
        "qtype": "Design_Rationale",
        "response": "Here “​[mask1]” is nothing mystical – it is simply the little “feature‐alignment” module inside the red box.  In DyGA, once we have a Gaussian mixture over each latent unit (the “anchors”), we then explicitly *nudge* every feature vector toward the mean of its most responsible Gaussian.  The *reason* for doing that is to carve *clean* boundaries between clusters in the latent space – i.e. to make each latent dimension settle firmly into one of its attribute-specific Gaussians rather than hover ambiguously between two or more.  By softly shifting each feature toward its anchor (with a bounded interpolation so we don’t wreck the stability of the diffusion model), we enforce that each latent unit truly specializes on one factor, giving us a more disentangled, attribute-separated representation."
    },
    {
        "question": "What motivates applying random crop & resize on salient region to construct auxiliary images for perturbation optimization?",
        "relevant_section_ids": [
            "1",
            "3.5"
        ],
        "relevant_context": [
            "In addition, the underperformance of adversarial examples on different black-box models is likely due to varying decision boundaries. Therefore, we leverage the salient region of the clean image to create auxiliary images, which are employed to further optimize the perturbation, ensuring a robust shift in the feature distribution towards the target category across different models.",
            "In the perturbation optimization stage, we perform data augmentation by randomly cropping and resizing the salient image S to create an auxiliary image I_aux, ensuring it matches the size of I. This process generates diverse patterns from S."
        ],
        "final_answer": "The random crop & resize on the salient region is used to generate diverse auxiliary images from the most informative part of the input. By creating varied patterns focused on the model’s key attention areas, the perturbation optimization can more robustly shift feature distributions towards the target class, improving transferability across different black-box models with varying decision boundaries.",
        "relevant_elements": [
            "Random Crop & Resize",
            "Auxiliary Image",
            "Salient Region"
        ],
        "id": 2213,
        "masked_question": "What motivates applying [mask1] on [mask2] to construct auxiliary images for perturbation optimization?",
        "masked_number": 2,
        "masked_elements": [
            "Random Crop & Resize",
            "Salient Region"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.06784v1_figure_2.png",
        "paperid": "2411.06784v1",
        "paper_path": "./MISS-QA/papers/2411.06784v1.json",
        "figure_id": "2411.06784v1_figure_2.png",
        "caption": "Figure 2: The overview of SWFD framework, which mainly includes two stages: (1) Salient region generation. This stage generates salient regions based on the heatmap; (2) Perturbation optimization. This stage iteratively optimizes the perturbation through the joint classification loss of the original image and auxiliary image based on the weighted feature drop.",
        "qtype": "Design_Rationale",
        "response": "Here mask1 is the “random crop & resize” operation, and mask2 is the “salient region” extracted by Grad-CAM.  The reason we randomly crop and resize that salient patch is to turn it into a family of auxiliary views—i.e. to perform on-the-fly data augmentation focused on the network’s most important region.  By forcing the adversarial perturbation to succeed not just on one fixed crop but on many slightly shifted and scaled versions of the same attention area, we both smooth the deep‐layer activations and prevent overfitting to the surrogate model’s idiosyncratic features.  This in turn yields perturbations that are more robust to the different decision boundaries encountered in black-box targets, greatly improving transferability."
    },
    {
        "question": "Why integrate weighted feature drop on feature map of layer l before computing joint classification loss for adversarial update?",
        "relevant_section_ids": [
            "1",
            "3.4"
        ],
        "relevant_context": [
            "In this paper, we introduce a novel targeted adversarial example attack framework based on Salient region & Weighted Feature Drop (SWFD) for boosting the transferability. We first propose a weighted feature drop mechanism to prevent the adversarial example generation from becoming overly dependent on a narrow subset of features by diversifying the emphasis across a wider array of features, enhancing the transferability of these adversarial examples.",
            "Overfitting can occur when the perturbation generation overly emphasizes specific features, evident in the rough outputs of DNN deep layers (see Figure 1). To address this, we design the weighted feature drop (WFD) mechanism. WFD leverages the principle that adversarial examples with greater transferability typically yield smoother deep-layer outputs. Thus, enhancing transferability involves further smoothing these outputs, which is achieved by spreading focus across a wider feature set. Essentially, features with higher values are more likely to be dropped, alleviating the overfitting."
        ],
        "final_answer": "The weighted feature drop is applied to the layer-l feature map before computing the loss in order to smooth the deep-layer activations by randomly removing (dropping) those channels with the largest weights. This prevents the adversarial perturbation from overfitting to a small, high-importance subset of features in the surrogate model and, by diversifying the focus across a wider range of features, improves the transferability of the resulting adversarial example.",
        "relevant_elements": [
            "Weighted Feature Drop",
            "Feature Map of Layer l",
            "Joint Classification Loss"
        ],
        "id": 2214,
        "masked_question": "Why integrate weighted feature drop on [mask1] before computing joint classification loss for adversarial update?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Map of Layer l"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.06784v1_figure_2.png",
        "paperid": "2411.06784v1",
        "paper_path": "./MISS-QA/papers/2411.06784v1.json",
        "figure_id": "2411.06784v1_figure_2.png",
        "caption": "Figure 2: The overview of SWFD framework, which mainly includes two stages: (1) Salient region generation. This stage generates salient regions based on the heatmap; (2) Perturbation optimization. This stage iteratively optimizes the perturbation through the joint classification loss of the original image and auxiliary image based on the weighted feature drop.",
        "qtype": "Design_Rationale",
        "response": "Here is a step‐by‐step reasoning for why the authors drop (“mask out”) channels in the feature map (the red‐boxed “Feature Map of Layer l”) before they compute the joint classification loss and back‐propagate their adversarial update:\n\n1.  Observation of Overfitting  \n    •  In white‐box attacks you have seen that the perturbation often “latches on” to a small set of very high‐activation channels in the surrogate model.  \n    •  As shown in Fig. 1(c–f), when the attack focuses too narrowly on a handful of channels the variance of the deep‐layer activations goes up and transfer across other models goes down.\n\n2.  Goal: Smooth and Generalize  \n    •  To improve transferability one wants the adversarial example to succeed even if any one of those “hot” channels in the surrogate model is absent or different in the black‐box model.  \n    •  Equivalently, one wants a flatter, lower‐variance activation distribution rather than a spike in a few channels.\n\n3.  Weighted Feature Drop Mechanism  \n    •  At each iteration they look at the feature map  at layer l and compute a per‐channel “weight” proportional to its norm (Eq. (7)).  \n    •  Channels with the largest weights (i.e. the ones the network is leaning on the hardest) are then stochastically zeroed out (“dropped”) according to a thresholding scheme (Eq. (8)).\n\n4.  Why Drop Before the Joint Loss?  \n    •  By feeding the network a version of the feature map with its strongest channels missing, the gradients used to update the image perturbation are forced to “find” alternative channels to push into the target class.  \n    •  In other words, the attack cannot succeed by exploiting only the surrogate’s top few activations; it must spread its effect over many channels.  \n    •  This makes the deep‐feature response smoother (lower variance) and less dependent on surrogate‐specific idiosyncrasies, which directly improves transferability to unseen black‐box models.\n\n5.  Role in the Joint Classification Loss  \n    •  The joint loss is computed over both the original image and the auxiliary (salient‐region) crop, each seen through the lens of the dropped‐feature network.  \n    •  Because the same channels have been zeroed in both paths, the perturbation is jointly optimized to be robust even when those channels are missing—again discouraging over‐reliance on a narrow subset of features.\n\nIn short, dropping the top‐weighted channels in the red‐boxed feature map before computing the loss forces the adversarial perturbation to “spread out” its effect over a wider set of features.  This smoothing of the deep‐layer activations is exactly what the paper shows empirically to boost transferability in black‐box attacks."
    },
    {
        "question": "How does Joint Classification Loss integrate original image and auxiliary image predictions for perturbation updates?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Joint Classification Loss",
            "original image",
            "auxiliary image"
        ],
        "id": 2215,
        "masked_question": "How does [mask1] integrate [mask2] and auxiliary image predictions for perturbation updates?",
        "masked_number": 2,
        "masked_elements": [
            "Joint Classification Loss",
            "original image"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.06784v1_figure_2.png",
        "paperid": "2411.06784v1",
        "paper_path": "./MISS-QA/papers/2411.06784v1.json",
        "figure_id": "2411.06784v1_figure_2.png",
        "caption": "Figure 2: The overview of SWFD framework, which mainly includes two stages: (1) Salient region generation. This stage generates salient regions based on the heatmap; (2) Perturbation optimization. This stage iteratively optimizes the perturbation through the joint classification loss of the original image and auxiliary image based on the weighted feature drop.",
        "qtype": "Implementation_Details",
        "response": "In our notation the red‐boxed module is the “Joint Classification Loss” and the blue‐boxed input is the full, original image.  What the figure is showing is that at each iteration we take our current perturbation δ, add it both to the full image x (the blue‐boxed image) and to the auxiliary image xₐ (the randomly cropped‐and‐resized salient patch), run them both through the same weighted‐feature‐drop (WFD) network, compute a standard classification loss (e.g. cross‐entropy or logit‐loss) on each output, and then simply add those two losses together into one “joint” loss.  We then back‐propagate that joint loss to update δ.  In other words, the Joint Classification Loss fuses the model’s predictions on (x + δ) and on (xₐ + δ) by summing their individual losses, and its gradient drives the adversarial‐noise update so that the single perturbation fools the network on both the original and auxiliary views."
    },
    {
        "question": "How does Gaussian Scale adjust Mean Weights to produce Channel Weights in Weighted Feature Drop?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "We calculate the mean weight and channel weight of t-th channel at layer l as follows:",
            "μ_t^l = …  (average absolute activation over channel t)",
            "w_t^l = | μ_t^l + N(0, σ^2) |,",
            "where μ_t^l and w_t^l denote the mean weight and the channel weight of the t-th channel at layer l, respectively. N(0,σ^2) is a normal distribution with mean 0 and standard deviation σ, where σ adjusts the channel drop probability. In Equation (7), we apply the absolute value of each channel to prevent negative numbers from affecting the results."
        ],
        "final_answer": "In the Weighted Feature Drop mechanism, after computing each channel's mean weight μ_t^l, Gaussian Scale perturbs this mean by adding noise sampled from a zero-mean normal distribution N(0,σ²). The result is then passed through an absolute‐value operation, giving the channel weight w_t^l = | μ_t^l + N(0,σ²) |. The parameter σ controls how much randomness (and hence drop variability) is injected into the channel weights.",
        "relevant_elements": [
            "Gaussian Scale",
            "Mean Weights",
            "Channel Weights"
        ],
        "id": 2216,
        "masked_question": "How does [mask1] adjust [mask2] to produce Channel Weights in Weighted Feature Drop?",
        "masked_number": 2,
        "masked_elements": [
            "Gaussian Scale",
            "Mean Weights"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.06784v1_figure_2.png",
        "paperid": "2411.06784v1",
        "paper_path": "./MISS-QA/papers/2411.06784v1.json",
        "figure_id": "2411.06784v1_figure_2.png",
        "caption": "Figure 2: The overview of SWFD framework, which mainly includes two stages: (1) Salient region generation. This stage generates salient regions based on the heatmap; (2) Perturbation optimization. This stage iteratively optimizes the perturbation through the joint classification loss of the original image and auxiliary image based on the weighted feature drop.",
        "qtype": "Implementation_Details",
        "response": "In the weighted‐feature‐drop block (Fig. 2, right), the blue box is simply the per-channel “mean weight” \\(\\bar g_l^k\\) (Eq. 6), and the red box is the “Gaussian scale” operation.  Concretely, for each channel \\(k\\) at layer \\(l\\) you take its mean activation \\(\\bar g_l^k\\) and treat that as the mean of a Gaussian \\( \\mathcal{N}(\\bar g_l^k,\\;\\sigma^2)\\).  Sampling (or equivalently perturbing) \\(\\bar g_l^k\\) by that Gaussian produces the final channel weight\nw_l^k = \\bigl|\\mathcal{N}(\\bar g_l^k,\\;\\sigma^2)\\bigr|.  In this way the red‐boxed Gaussian scale “adjusts” the blue‐boxed mean weights to yield the Channel Weights used to decide which feature maps to drop."
    },
    {
        "question": "How does dynamic vision encoding adapt patch configurations to diverse input resolutions before vision encoder ingestion?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "Our findings also show that dynamic vision encoders greatly enhance performance. Additionally, we employ specific data formats and delimiters to differentiate between image patches and various types of visual inputs. For example, a single image is processed as an individual entity, while videos are treated as sequences of frames. Delimiters mark the beginning and end of each frame in a video sequence, enabling the model to effectively understand and process the temporal aspects of video data.",
            "We adopted the AnyRes technique, which enables our vision tower to support images and videos of any resolution. AnyRes dynamically adjusts the processing pipeline to handle varying resolutions, ensuring that the model can process high-resolution inputs efficiently and accurately. Our dynamic image encoding method is based on AnyRes. Additionally, we incorporate delimiters to help the model differentiate between patches, images, and video frames, thereby enhancing its ability to understand dynamic visual inputs."
        ],
        "final_answer": "Before feeding inputs into the vision encoder, OmChat’s dynamic vision encoding uses the AnyRes technique to automatically reshape and pad each image or video frame so that it yields a consistent grid of patches regardless of the original resolution.  It then inserts special delimiters in the patch sequence to mark image- and frame-boundaries, ensuring that all patches—whether from low- or high-resolution inputs—are presented in the same format to the vision tower.",
        "relevant_elements": [
            "Dynamic vision encoding",
            "Vision encoder"
        ],
        "id": 2217,
        "masked_question": "How does [mask1] adapt patch configurations to diverse input resolutions before vision encoder ingestion?",
        "masked_number": 1,
        "masked_elements": [
            "Dynamic vision encoding"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.04923v1_figure_1.png",
        "paperid": "2407.04923v1",
        "paper_path": "./MISS-QA/papers/2407.04923v1.json",
        "figure_id": "2407.04923v1_figure_1.png",
        "caption": "Figure 1: OmChat model overall structure and training steps.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. The red‐boxed module in the figure is labeled “Dynamic vision encoding.”  \n2. The text explains that OmChat builds on the AnyRes idea to let the vision backbone accept arbitrary resolutions without losing small details.  \n3. The little diagram under the red box shows an incoming image of any size being padded and resized to a standard 336×336 canvas.  \n4. After resizing, the image is split into “n patches,” but rather than a fixed grid (e.g. always 16×16), the module “matches” that canvas to different patch layouts—specifically 2×2, 1×{2,3,4}, or {2,3,4}×1.  \n5. In other words, the number of patches and their aspect ratios change dynamically (more narrow strips, more wide strips, or a 2×2 grid) depending on resolution and content.  \n6. This adaptive patching happens before the vision encoder so that high-resolution inputs still get appropriately small patches (to catch fine detail) while low-resolution inputs aren’t over-fragmented.  \n\nAnswer  \nDynamic vision encoding first pads and resizes each input to a fixed 336×336 canvas, then uses the AnyRes strategy to dynamically select one of several patch grids (e.g. 2×2, 1×{2,3,4}, or {2,3,4}×1 patches). In this way the patch count and shape automatically adjust to the original image’s resolution, ensuring the vision encoder always receives an appropriate set of patches."
    },
    {
        "question": "How is Ring attention integrated at each context length expansion during active progressive multimodal pretraining?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "The training strategy follows a sequential context length of 4k, 32K, 128K, and finally 512K.",
            "During the training phase for contexts exceeding 128K in length, RingAttention (Liu et al., 2024b ###reference_b66###) is implemented for computing QKV attention. This specialized method is tailored for calculating attention over long contexts, addressing memory constraints associated with the quadratic complexity of attention weight computations. The fundamental concept involves segmenting QKV along the seq_len dimension into blocks of n*block_size, facilitating the iterative derivation of complete attention by calculating attention for each block."
        ],
        "final_answer": "OmChat’s context length is progressively expanded from 4K → 32K → 128K → 512K. Once the model moves beyond 128K tokens, it switches from standard attention to RingAttention. RingAttention divides the query, key and value tensors along the sequence‐length dimension into smaller blocks (n×block_size) and computes attention block by block, thereby enabling efficient attention over very long contexts.",
        "relevant_elements": [
            "Ring attention",
            "Active progressive multimodal pretraining"
        ],
        "id": 2218,
        "masked_question": "How is [mask1] integrated at each context length expansion during [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Ring attention",
            "Active progressive multimodal pretraining"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.04923v1_figure_1.png",
        "paperid": "2407.04923v1",
        "paper_path": "./MISS-QA/papers/2407.04923v1.json",
        "figure_id": "2407.04923v1_figure_1.png",
        "caption": "Figure 1: OmChat model overall structure and training steps.",
        "qtype": "Implementation_Details",
        "response": "mask1: Ring attention  \nmask2: Active progressive multi-modal pretraining  \n\nChain of thought:  \n1. In the figure the red box highlights “Ring attention,” and the blue box encloses the “Active progressive multi-modal pretraining” block (4K→32K→128K→512K).  \n2. The text explains that to grow the model from 4K up to 512K tokens, the same transformer is progressively trained at each new context length.  \n3. Once you exceed 128K tokens, standard full‐matrix attention blows up in memory, so the authors switch in Ring attention for the QKV computations.  \n4. Ring attention works by chopping the long sequence (Q, K, V) along the sequence‐length axis into fixed‐size blocks (n × block_size) and then iteratively computing attention over each block in a “ring‐buffer” fashion—thus approximating full attention with linear memory growth.  \n5. Concretely, at every context jump (4K→32K→128K→512K), they load the larger RoPE positional encodings and plug in the Ring attention module in place of standard attention for any length beyond 128K, ensuring the model can be trained at each new scale without running out of memory.  \n\nAnswer:  \nRing attention is grafted in as the attention kernel at each stage of the progressive length expansion—4K, 32K, 128K, and then 512K—by replacing the standard full‐matrix QKV attention with a block‐wise “ring” implementation that splits the long sequence into ring‐buffered blocks and computes attention iteratively, thereby keeping memory use linear as the context grows."
    },
    {
        "question": "How does dynamic vision encoding adapt patch configurations to diverse input resolutions before vision encoder ingestion?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "Our findings also show that dynamic vision encoders greatly enhance performance. Additionally, we employ specific data formats and delimiters to differentiate between image patches and various types of visual inputs. For example, a single image is processed as an individual entity, while videos are treated as sequences of frames. Delimiters mark the beginning and end of each frame in a video sequence, enabling the model to effectively understand and process the temporal aspects of video data.",
            "We adopted the AnyRes technique, which enables our vision tower to support images and videos of any resolution. AnyRes dynamically adjusts the processing pipeline to handle varying resolutions, ensuring that the model can process high-resolution inputs efficiently and accurately. Our dynamic image encoding method is based on AnyRes. Additionally, we incorporate delimiters to help the model differentiate between patches, images, and video frames, thereby enhancing its ability to understand dynamic visual inputs."
        ],
        "final_answer": "Before feeding inputs into the vision encoder, OmChat’s dynamic vision encoding uses the AnyRes technique to automatically reshape and pad each image or video frame so that it yields a consistent grid of patches regardless of the original resolution.  It then inserts special delimiters in the patch sequence to mark image- and frame-boundaries, ensuring that all patches—whether from low- or high-resolution inputs—are presented in the same format to the vision tower.",
        "relevant_elements": [
            "Dynamic vision encoding",
            "Vision encoder"
        ],
        "id": 2217,
        "masked_question": "How does [mask1] adapt patch configurations to diverse input resolutions before vision encoder ingestion?",
        "masked_number": 1,
        "masked_elements": [
            "Dynamic vision encoding"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.04923v1_figure_1.png",
        "paperid": "2407.04923v1",
        "paper_path": "./MISS-QA/papers/2407.04923v1.json",
        "figure_id": "2407.04923v1_figure_1.png",
        "caption": "Figure 1: OmChat model overall structure and training steps.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. The red‐boxed module in the diagram is the “Dynamic Vision Encoding” block.  \n2. Its job is to take an arbitrarily sized image and turn it into a set of fixed‐size inputs for the vision tower.  \n3. The figure shows that every incoming image is first padded (to preserve aspect ratio) and then resized to a canonical 336×336 canvas.  \n4. Next, rather than always chopping that canvas into the same grid, the module “matches” it to different tiling patterns—e.g. a 2×2 grid, a 1×2 or 1×3 or 1×4 row, or a 2×3, 2×4, 3×4 column—depending on the original resolution and content.  \n5. Each of those tiles is itself 336×336, so no matter how the image started, the vision encoder always sees fixed‐size patches.  \n\nAnswer  \nBefore feeding the vision encoder, the dynamic vision encoding block pads and rescales the input to a standard 336×336 frame, then dynamically selects one of several predefined grid layouts (2×2, 1×{2,3,4}, or {2,3,4}×1) to carve out 336×336 patches. This adaptive tiling scheme lets the model handle arbitrary resolutions while keeping each patch the exact size the encoder expects."
    },
    {
        "question": "How does active progressive multi-modal pretraining integrate ring attention for efficient long-context handling?",
        "relevant_section_ids": [
            "2",
            "2.2"
        ],
        "relevant_context": [
            "Active Progressive Multimodal Pretraining: We implement a progressive training strategy from 4K to 512K to scale up the model’s long context capacity and capabilities gradually. Additionally, the implementation of high-quality selection during training is a crucial step in ensuring that the model learns from the most informative and relevant data points.",
            "Subsequently, leveraging the language model with a 512k context length, OmChat is transformed into a multimodal model through a multimodal pretraining process. After the projector alignment training, the model’s context length was progressively extended by incrementally increasing the context length from 4K, 32K, 128K, to 512K.",
            "During the training phase for contexts exceeding 128K in length, RingAttention (Liu et al., 2024b) is implemented for computing QKV attention. This specialized method is tailored for calculating attention over long contexts, addressing memory constraints associated with the quadratic complexity of attention weight computations. The fundamental concept involves segmenting QKV along the seq_len dimension into blocks of n*block_size, facilitating the iterative derivation of complete attention by calculating attention for each block."
        ],
        "final_answer": "Active progressive multi-modal pretraining first scales the context length in stages (4K → 32K → 128K → 512K). Once contexts exceed 128K tokens, it switches from standard attention to RingAttention. RingAttention breaks the QKV tensors along the sequence dimension into manageable blocks (n×block_size) and computes full attention iteratively per block. This block-wise attention dramatically reduces memory overhead and enables efficient processing of very long contexts.",
        "relevant_elements": [
            "Active progressive multi-modal pretraining",
            "Ring attention"
        ],
        "id": 2220,
        "masked_question": "How does [mask1] integrate [mask2] for efficient long-context handling?",
        "masked_number": 2,
        "masked_elements": [
            "Active progressive multi-modal pretraining",
            "Ring attention"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.04923v1_figure_1.png",
        "paperid": "2407.04923v1",
        "paper_path": "./MISS-QA/papers/2407.04923v1.json",
        "figure_id": "2407.04923v1_figure_1.png",
        "caption": "Figure 1: OmChat model overall structure and training steps.",
        "qtype": "Literature_Background",
        "response": "Here’s how I reasoned it out:\n\n1. The red‐boxed module is the “Active progressive multi-modal pretraining” stage.  In that stage, OmChat gradually ramps its context window up—4K → 32K → 128K → 512K—while interleaving high-quality multimodal data.\n\n2. The blue-boxed module is labeled “Ring attention,” and later in the text it’s described as the special attention mechanism used once you get past 128K tokens of context.\n\n3. In the write-up (under “Progressive Training Strategy to Long Context”), they say that for contexts longer than 128K they “implement RingAttention for computing QKV attention,” breaking the full sequence into blocks along the sequence-length dimension and then doing attention block-by-block so you never have to hold the entire N×N attention map in memory.\n\n4. Putting the two together: during the Active progressive multi-modal pretraining stage, as OmChat’s context length is scaled up, RingAttention is slotted in as the attention backend for the very long contexts.  This block-wise, iterative computation lets the model handle hundreds of thousands of tokens with only linear (rather than quadratic) memory growth.\n\nAnswer:  \nActive progressive multi-modal pretraining slots in RingAttention as its attention kernel whenever the context length passes into the very long regime (128K→512K).  By chopping the sequence into manageable blocks of Q, K, and V and running attention over one block at a time, RingAttention delivers full global attention at far lower memory and compute cost—enabling OmChat to be trained and run over massive multimodal contexts efficiently."
    },
    {
        "question": "How does the Squeeze and Excitation Network leverage GAP outputs to implement channel-wise attention akin to previous attention modules?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "The SE Block strengthens the network’s ability to capture channel-wise features by applying a Global Average Pooling (GAP), condensing the spatial data which is following by a gating mechanism with Sigmoid activation to learn the attention weights.",
            "These weights modulate the spatial feature maps \\(\\mathbf{X}\\) as described by: \\(\\mathbf{\\tilde{X}} = \\mathbf{w} \\odot \\mathbf{X}\\), where \\(\\mathbf{w}\\) represents the attention weights derived from the SE Block."
        ],
        "final_answer": "The SE Network first applies Global Average Pooling to collapse each feature map into a single channel descriptor. It then passes these descriptors through a small gating network (with a sigmoid activation) to produce a set of per-channel attention weights. Finally, it multiplies the original feature maps by these learned weights, thereby recalibrating each channel’s contribution in a manner analogous to earlier attention modules.",
        "relevant_elements": [
            "Squeeze and Excitation Network",
            "GAP"
        ],
        "id": 2221,
        "masked_question": "How does the [mask1] leverage GAP outputs to implement channel-wise attention akin to previous attention modules?",
        "masked_number": 1,
        "masked_elements": [
            "Squeeze and Excitation Network"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.10863v1_figure_1.png",
        "paperid": "2411.10863v1",
        "paper_path": "./MISS-QA/papers/2411.10863v1.json",
        "figure_id": "2411.10863v1_figure_1.png",
        "caption": "Figure 1: Overview of ResEmoteNet for efficient facial emotion recognition.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is in fact the Squeeze-and-Excitation (SE) block.  It implements channel­-wise attention in exactly the same way as the original SENet paper:\n\n1.  “Squeeze” step: it applies Global Average Pooling (GAP) over each H×W feature map to produce a C-dimensional descriptor s = [s₁…s_C], where  \n    s_c = (1/HW) · Σ_{i=1}^H Σ_{j=1}^W x_c(i,j).  \n\n2.  “Excitation” step: it feeds s through two small fully‐connected layers (first reducing the dimension, then expanding it back), with non–linearities in between, and finishes with a sigmoid.  This yields C gating coefficients g = σ(W₂·ReLU(W₁·s)).  \n\n3.  “Scale” step: it multiplies each original feature‐map channel x_c by its corresponding gate g_c, producing x′_c = g_c·x_c.  \n\nBy collapsing spatial information into a per-channel descriptor and then learning per-channel scaling factors, the SE block reintroduces the key idea of channel-wise attention from earlier attention modules."
    },
    {
        "question": "How does Simple Feature Extraction interact with Residual Network layers to facilitate deep feature refinement as in ResNet designs?",
        "relevant_section_ids": [
            "2.1",
            "2.2"
        ],
        "relevant_context": [
            "Given a sample with RGB facial image of size (H × W × 3), we utilize the CNN backbone to extract the simple features from the samples. The CNN backbone comprises of Convolution block accompanied by Batch Normalization to generate high-level feature maps of size C × H′ × W′ for each image. The spatial features X are then concatenated across channels to form a rich representation of the input image, which is further processed by the subsequent layers of the network. Subsequent to the extraction of high-level feature maps, a max-pooling layer is applied to further refine the feature representation.",
            "The Residual Network comprises three Residual Blocks, each with weight layers followed by ReLU activation and skip connections that iteratively learn residual functions. These functions model the differences between the block’s input and output, rather than unreferenced mappings. The skip connections bypass layers within a block, helping to train deeper architectures by mitigating vanishing gradients. This design accelerates model convergence and enhances generalization by preserving the original input signal throughout the network."
        ],
        "final_answer": "The CNN backbone’s simple feature extraction produces high-level spatial feature maps which are pooled and then fed directly into the Residual Network blocks. Each residual block takes these initial feature maps as its input, applies weight layers and ReLU activations to learn a residual function, and then adds that residual back to the original input via a skip connection. In this way, the simple features are progressively refined by each residual block while the skip connections preserve the original signal and mitigate vanishing gradients, exactly as in standard ResNet designs.",
        "relevant_elements": [
            "Simple Feature Extraction",
            "Residual Network"
        ],
        "id": 2222,
        "masked_question": "How does [mask1] interact with Residual Network layers to facilitate deep feature refinement as in ResNet designs?",
        "masked_number": 1,
        "masked_elements": [
            "Simple Feature Extraction"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.10863v1_figure_1.png",
        "paperid": "2411.10863v1",
        "paper_path": "./MISS-QA/papers/2411.10863v1.json",
        "figure_id": "2411.10863v1_figure_1.png",
        "caption": "Figure 1: Overview of ResEmoteNet for efficient facial emotion recognition.",
        "qtype": "Literature_Background",
        "response": "First, let’s pin down what [mask1] is.  If you look at the red‐boxed region in the figure, it’s simply the CNN backbone’s “Simple Feature Extraction’’ stage – i.e. the stack of convolution + BN layers (plus max‐pooling) that produces the initial high-level feature maps.\n\nHere’s how that simple extractor interacts with the Residual Network to give you the deep, refined features:\n\n1.  The CNN backbone turns the raw  H×W×3  image into a tensor of spatial feature maps X.  \n2.  Those maps go through the Squeeze-and-Excitation block (GAP → two FC layers → sigmoid gating) to yield the re-weighted feature map \\(\\tilde X\\).  \n3.  \\(\\tilde X\\) is then fed straight into the Residual Network’s first block.  In each Residual block you have  \n    - two (or more) weight layers + ReLU which compute a residual function \\(F(\\tilde X)\\),  \n    - a skip (identity) connection that carries \\(\\tilde X\\) forward unmodified,  \n    - and a final element-wise add:  \n       output = \\(F(\\tilde X)\\) + \\(\\tilde X\\).  \n4.  By always adding back the original “simple” feature maps, the network can learn just the residual corrections at each depth rather than trying to relearn the entire mapping.  That both preserves the low-level structure you got from the red-boxed extractor and ensures stable gradient flow as you stack many blocks.\n\nIn short, the red-boxed “Simple Feature Extraction” provides the starting feature tensor, and each ResNet block refines it by learning a small residual on top of it—exactly the pattern that lets ResNets go very deep without vanishing gradients."
    },
    {
        "question": "How does Squeeze and Excitation Network contribute to loss reduction after CNN Feature Extraction?",
        "relevant_section_ids": [
            "2",
            "2.1"
        ],
        "relevant_context": [
            "ResEmoteNet [11] has an extensive architecture consisting of Squeeze and Excitation blocks and Residual blocks. These blocks help in minimizing losses while training and are capable of learning complex features resulting in a model that helps in accurate classification of emotions.",
            "The SE Block strengthens the network’s ability to capture channel-wise features by applying a Global Average Pooling (GAP), condensing the spatial data which is followed by a gating mechanism with Sigmoid activation to learn the attention weights. These weights modulate the spatial feature maps X as described by:\n    X′=s⊙X\nwhere s represents the attention weights derived from the SE Block. This results in a new feature map X′."
        ],
        "final_answer": "After the CNN backbone extracts simple spatial feature maps, the Squeeze-and-Excitation network applies a global average pooling to condense each channel’s information, passes it through a small gating network with a Sigmoid activation to produce channel-wise attention weights, and then scales the original feature maps by these weights. By re-weighting channels according to their learned importance, the SE block refines the representation and thus helps reduce training loss.",
        "relevant_elements": [
            "Squeeze and Excitation Network",
            "CNN Feature Extraction",
            "Loss Reduction"
        ],
        "id": 2223,
        "masked_question": "How does [mask1] contribute to loss reduction after [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Squeeze and Excitation Network",
            "CNN Feature Extraction"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.10863v1_figure_1.png",
        "paperid": "2411.10863v1",
        "paper_path": "./MISS-QA/papers/2411.10863v1.json",
        "figure_id": "2411.10863v1_figure_1.png",
        "caption": "Figure 1: Overview of ResEmoteNet for efficient facial emotion recognition.",
        "qtype": "Experimental_Results",
        "response": "Step-by-step reasoning:\n\n1. Image–text alignment  \n   - Mask 2 (blue box) is the CNN backbone/simple feature-extraction stage that produces high-level spatial feature maps.  \n   - Mask 1 (red box) is the Squeeze-and-Excitation (SE) network, labeled “Loss Reduction” in the diagram.\n\n2. What the SE block does  \n   a. “Squeeze”: applies Global Average Pooling (GAP) to each channel of the CNN feature map, collapsing spatial dimensions into a single descriptor per channel.  \n   b. “Excitation”: feeds these channel descriptors through two small fully connected layers (with a bottleneck) plus a sigmoid to learn channel-wise attention weights.  \n   c. “Scale”: multiplies the original CNN feature maps by these learned weights, amplifying informative channels and suppressing less useful ones.\n\n3. How this reduces loss after the CNN backbone  \n   - By recalibrating channel responses, the SE block yields more discriminative and noise-robust features before they enter the deeper residual layers.  \n   - This sharper, attention-guided representation speeds up convergence, mitigates vanishing gradients in subsequent layers, and ultimately lowers the classification (cross-entropy) loss during training.\n\nAnswer:\nThe Squeeze-and-Excitation module (mask 1) follows the CNN feature extractor (mask 2) by first “squeezing” each channel via global average pooling, then “exciting” them through two tiny fully-connected layers and a sigmoid to produce channel-wise attention weights, and finally scaling the original CNN feature maps with those weights. This adaptive reweighting emphasizes the most informative channels, yields a crisper feature representation for the downstream residual blocks, and thereby drives down the overall training loss."
    },
    {
        "question": "How do skip connections in the Residual Network enhance complex feature extraction before the fully connected layer?",
        "relevant_section_ids": [
            "2.2"
        ],
        "relevant_context": [
            "The Residual Network comprises three Residual Blocks, each with weight layers followed by ReLU activation and skip connections that iteratively learn residual functions.",
            "The skip connections bypass layers within a block, helping to train deeper architectures by mitigating vanishing gradients.",
            "This design accelerates model convergence and enhances generalization by preserving the original input signal throughout the network."
        ],
        "final_answer": "Skip connections bypass intermediate layers within each residual block and add the block’s input directly to its output. By learning residual functions (i.e., differences between input and output) rather than full mappings, they mitigate vanishing gradients in deep networks, preserve the original signal, accelerate convergence, and improve generalization—thereby enhancing complex feature extraction before the fully connected layer.",
        "relevant_elements": [
            "Residual Network",
            "Fully Connected"
        ],
        "id": 2224,
        "masked_question": "How do skip connections in the [mask1] enhance complex feature extraction before the fully connected layer?",
        "masked_number": 1,
        "masked_elements": [
            "Residual Network"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.10863v1_figure_1.png",
        "paperid": "2411.10863v1",
        "paper_path": "./MISS-QA/papers/2411.10863v1.json",
        "figure_id": "2411.10863v1_figure_1.png",
        "caption": "Figure 1: Overview of ResEmoteNet for efficient facial emotion recognition.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-Thought  \n1. The red-boxed module is the Residual Network made up of three residual blocks.  \n2. Each block has two weight layers with ReLU and a skip connection that adds the block’s input directly to its output.  \n3. By learning the “residual” (the difference between input and desired output) instead of a full mapping, each block can focus on refining or enhancing features.  \n4. The skip path also provides a direct route for gradients during back-propagation, alleviating the vanishing-gradient problem in deep stacks.  \n5. As a result, low-level information is preserved and combined with higher-level refinements, yielding richer, more discriminative feature maps.  \n6. These enhanced, complex features are then fed into the final fully connected layer for classification.  \n\nAnswer  \nThe skip connections in the residual blocks let each block learn only the incremental change (residual) to its input rather than a whole new transformation. This both preserves the original feature signal and provides a direct pathway for gradients, which (a) stabilizes training of deep layers and (b) enables the network to build up increasingly complex feature representations. Those richer features are then passed on to the fully connected layer, improving the model’s ability to discriminate subtle emotional cues."
    },
    {
        "question": "How does process supervision influence binary classification verifier training compared to outcome supervision?",
        "relevant_section_ids": [
            "3.1",
            "3.2"
        ],
        "relevant_context": [
            "Specifically, we compared Tree-PLV with two verifiers trained using cross-entropy loss with binary labels: the outcome-supervised verifier, termed ORM (Lightman et al., 2023), and Math-Shepherd (Wang et al., 2023), a state-of-the-art process-supervised verifier that leverages automatically annotated data.",
            "In contrast, both ORM and Math-Shepherd, which are trained using a binary supervision strategy, fail to show similar improvement. This underperformance could indicate the limitations inherent to binary supervision, especially in tasks that require complex reasoning. It is more prone to introducing noise into the binary supervision signal, which could consequently limit the verifier’s capabilities."
        ],
        "final_answer": "Process supervision supplies step-level binary labels (correct vs. incorrect steps) for verifier training—whereas outcome supervision provides a single binary label for the entire solution. Although this grants more granular feedback across the reasoning chain, both process-supervised and outcome-supervised verifiers rely on binary classification and are similarly prone to noisy labels. As the experiments show, neither approach matches the performance of preference-based training, indicating that binary supervision (even when applied at the process level) remains limited for complex reasoning tasks.",
        "relevant_elements": [
            "Process Supervision",
            "Binary Classification Verifier",
            "Outcome Supervision"
        ],
        "id": 2225,
        "masked_question": "How does [mask1] influence binary classification verifier training compared to outcome supervision?",
        "masked_number": 1,
        "masked_elements": [
            "Process Supervision"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.00390v1_figure_1.png",
        "paperid": "2407.00390v1",
        "paper_path": "./MISS-QA/papers/2407.00390v1.json",
        "figure_id": "2407.00390v1_figure_1.png",
        "caption": "Figure 1: A comparison of different methods: Traditional verifiers rely on binary labels for outcome and process supervision, whereas Tree-PLV employs preferences instead of scalar values.",
        "qtype": "Experimental_Results",
        "response": "First, note that the red‐boxed term in the diagram is simply “process supervision.”  In a nutshell, what process supervision does is it gives the verifier a binary “right/wrong” label at each intermediate step of a reasoning path—whereas outcome supervision only tells the verifier “this entire path is right” or “this entire path is wrong” at the end.\n\nChain of thought to see why that matters:\n\n1.  Under outcome supervision (ORM), you collect a bunch of complete solutions and label each one as correct or incorrect overall.  You then train your verifier as a binary classifier on whole paths.  All the fine‐grained structure inside the path is invisible to the loss function—good and bad steps alike are collapsed into a single signal.\n\n2.  Under process supervision (e.g. Math-Shepherd), you automatically annotate each reasoning step with a binary correct/incorrect tag—usually by finding the first step where the chain of thought goes off‐track and labeling everything after that as “wrong.”  You then train your verifier to predict the correctness of each step, not just the final outcome.\n\n3.  Because you now have per-step labels, the verifier’s training objective is richer: it gets explicit mistakes to learn from at exactly the step where things diverged, instead of just learning that “path X” as a whole was bad.  In practice, this helps the verifier to spot erroneous reasoning earlier, to produce a more fine‐grained scoring of partial derivations, and to rank the best full solutions more accurately in a best-of-N setting.\n\nSo, compared to outcome supervision, process supervision injects step‐level binary signals into the verifier’s loss.  This yields a stronger, more localized error signal during training—which empirically improves the verifier’s ability to discriminate good from bad reasoning paths."
    },
    {
        "question": "How does the reasoning tree's reward gradient inform preference-based verifier training?",
        "relevant_section_ids": [
            "2.2",
            "2.3"
        ],
        "relevant_context": [
            "At step t of the tree expansion, we have a partial solution h consisting of the previous t reasoning steps. We use a reward function R to evaluate the quality of the next potential step a, given the input x and the current partial solution h.",
            "To address this, we leverage the model's look-ahead capability to assess a step's quality by its potential to lead to the correct conclusion. Specifically, to evaluate a candidate step a, we use the same model to simulate m subsequent reasoning trajectories starting from a, denoted as m completions. The quality of the step a is quantified by the proportion of trajectories reaching the correct answer: R(a) = (1/m) ∑_{j=1}^m [o_j = y*].",
            "To collect the paired data D, we conduct pairwise comparisons between sibling nodes at each decision point along the tree. If the reward difference between a preferable step c⁺ and a less preferable step c⁻ meets the minimum margin δ, we create a pair (x, c⁺, c⁻).",
            "Based on the pairwise instance data D, we gather two types of step preferences during our training. The first type is derived from the reward function R for the current step a_t, which assesses the choices at each decision point."
        ],
        "final_answer": "The reasoning tree uses a reward function R to score each candidate step by looking ahead at how often it leads to a correct conclusion. At every branching point, sibling steps are compared by their R values: if the difference in reward exceeds a preset margin, the higher-reward step is marked as preferred and the lower one as less preferred. These reward-based preference pairs are then used to train the verifier with a step-level ranking loss, so that it learns to assign higher scores to steps that the tree’s reward gradient deems more promising.",
        "relevant_elements": [
            "Reasoning Tree",
            "Preference-based Verifier",
            "Reward Gradient"
        ],
        "id": 2226,
        "masked_question": "How does the [mask1]'s reward gradient inform preference-based verifier training?",
        "masked_number": 1,
        "masked_elements": [
            "Reasoning Tree"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.00390v1_figure_1.png",
        "paperid": "2407.00390v1",
        "paper_path": "./MISS-QA/papers/2407.00390v1.json",
        "figure_id": "2407.00390v1_figure_1.png",
        "caption": "Figure 1: A comparison of different methods: Traditional verifiers rely on binary labels for outcome and process supervision, whereas Tree-PLV employs preferences instead of scalar values.",
        "qtype": "Experimental_Results",
        "response": "First, let’s identify what “[mask1]” is.  The red-boxed area in the diagram is exactly the “Tree-PLV” panel, i.e. the look-ahead reasoning tree whose nodes are shaded from light to dark according to their “reward” (the estimated probability that expanding that node will eventually reach the correct answer).\n\nNow, what role does that reward gradient play in the preference-based verifier?\n\n1. At each node of the tree, we compute a reward  \n   r(x,  s<sub>1:i−1</sub>,  s<sub>i</sub>)  \n   by rolling out k completions from that partial path and measuring the fraction that end up correct.\n\n2. Because sibling nodes share the same context but have different next steps, their rewards form a little “local gradient” over choices at that decision point: some children are high‐reward (dark blue) and some are low‐reward (pale).\n\n3. Whenever the gap between two siblings exceeds a threshold Δ, we declare one step “preferred” and the other “dispreferred.”  We then stitch each of those two steps into a full path (one that eventually succeeds, one that eventually fails) and add the resulting pair  \n   (problem, preferred‐path, inferior‐path)  \n   into our step‐level preference dataset D.\n\n4. Finally, during verifier training we take these ordered pairs and apply a pairwise ranking loss—forcing the verifier’s score for the preferred chain to be higher than for the inferior one.\n\nIn short: the look-ahead reward gradient in the Tree-PLV tree tells us which next‐step candidates truly improve long-term success.  We mine those high vs. low reward comparisons to build step‐wise preference pairs, and then train the verifier with a ranking loss so that it learns to reproduce the same ordering of partial solutions."
    },
    {
        "question": "What limitations arise when replacing Binary Classification Verifier with Preference-based Verifier in reasoning trees?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Binary Classification Verifier",
            "Preference-based Verifier"
        ],
        "id": 2227,
        "masked_question": "What limitations arise when replacing [mask1] with Preference-based Verifier in reasoning trees?",
        "masked_number": 1,
        "masked_elements": [
            "Binary Classification Verifier"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.00390v1_figure_1.png",
        "paperid": "2407.00390v1",
        "paper_path": "./MISS-QA/papers/2407.00390v1.json",
        "figure_id": "2407.00390v1_figure_1.png",
        "caption": "Figure 1: A comparison of different methods: Traditional verifiers rely on binary labels for outcome and process supervision, whereas Tree-PLV employs preferences instead of scalar values.",
        "qtype": "Others",
        "response": "Here, “[mask1]” is simply the standard binary‐classification verifier (the red-boxed module in Figure 1), which tags each full reasoning path or each individual step as “good” or “bad.”  When you try to swap that out for a preference-based verifier in exactly the same plug-and-play spot, you immediately run into two kinds of trouble:\n\n1. No Absolute Truth Signal – you only ever learn “A is better than B,” never “A is actually correct.”  \n   • In a pure preference setup you lose the ability to give a path or a step a standalone “correct/incorrect” score.  \n   • That means you cannot directly flag when *all* candidates so far are hopeless, since you only know which is least bad.\n\n2. Heavy Pairwise Overhead – preference learning forces you into an expensive pair-generation regime.  \n   • To train you must build full trees, compare siblings, enforce a margin, sample completions, etc.  All of that explodes both memory and compute.  \n   • At inference time you likewise have to do pairwise scoring or approximate it (e.g. via tournament ranking), which is asymptotically more costly than one-shot binary classification.\n\nIn short, while preference-based verifiers give you much finer‐grained ranking feedback, they cannot by themselves reproduce the “this path is simply wrong” signal of a binary verifier, and they incur a very large computational and data-generation burden."
    },
    {
        "question": "What limitations arise from lookup-free quantizer within the structure tokenizer in capturing precise atomic positions?",
        "relevant_section_ids": [
            "5"
        ],
        "relevant_context": [
            "(2) Trade-off of discrete latent representation: Tokenizing structure into discrete symbols facilitates multimodal protein language models and co-generation but may come at the cost of losing fine-grained structural details and control, such as precise atomic positions and inter-atomic distances."
        ],
        "final_answer": "Because the lookup-free quantizer represents continuous 3D coordinates with a limited set of discrete tokens, it inevitably loses fine-grained structural information—specifically, it cannot perfectly preserve precise atomic positions or exact inter-atomic distances.",
        "relevant_elements": [
            "lookup-free quantizer (LFQ)",
            "structure tokenizer"
        ],
        "id": 2229,
        "masked_question": "What limitations arise from [mask1] within the [mask2] in capturing precise atomic positions?",
        "masked_number": 2,
        "masked_elements": [
            "lookup-free quantizer (LFQ)",
            "structure tokenizer"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.13782v1_figure_1.png",
        "paperid": "2410.13782v1",
        "paper_path": "./MISS-QA/papers/2410.13782v1.json",
        "figure_id": "2410.13782v1_figure_1.png",
        "caption": "Figure 1: Overall illustration of DPLM-2.\n(A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates.\n(B) Multimodal learning and generation of protein structure and sequence with DPLM-2.\n(C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.",
        "qtype": "Others",
        "response": "Here’s the reasoning step-by-step:\n\n1.  In the figure, the blue-boxed area (mask2) is the “Structure Tokenization” module (panel A).  \n2.  Inside that, the red box (mask1) is the lookup-free quantizer (LFQ) that turns continuous GVP-encoded backbone features into a small discrete codebook of “struct-tokens.”  \n3.  The paper explicitly calls out this “discrete latent representation” as a trade-off: by forcing all backbone geometry into a finite set of symbols, you necessarily lose very fine‐grained detail.  \n4.  Concretely, that means you can no longer recover or reason over exact atomic coordinates or subtle inter‐atomic distance differences—only the “nearest” prototype in the codebook.  \n\nAnswer:  \nTokenizing via the LFQ in the structure‐tokenization module forces a coarse discretization of backbone geometry, and this quantization step inherently blurs out the very fine-grained structural details—most notably precise atomic positions and exact inter-atomic distances."
    },
    {
        "question": "What motivates adopting lookup-free quantizer in the structure tokenizer design?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In terms of quantizer, our preliminary experiment showed that conventional VQ-VAE pretty much struggles in training.",
            "To mitigate this, we instead adopts Lookup-Free Quantizer (LFQ) from the currently best visual tokenizer (Yu et al., 2023) to protein structure tokenization."
        ],
        "final_answer": "The authors adopt the lookup-free quantizer because their preliminary experiments showed that a conventional VQ-VAE struggled to train effectively on protein structure data, and LFQ mitigates these training difficulties by providing faster, more stable convergence.",
        "relevant_elements": [
            "lookup-free quantizer",
            "structure tokenizer"
        ],
        "id": 2231,
        "masked_question": "What motivates adopting [mask1] in the structure tokenizer design?",
        "masked_number": 1,
        "masked_elements": [
            "lookup-free quantizer"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.13782v1_figure_1.png",
        "paperid": "2410.13782v1",
        "paper_path": "./MISS-QA/papers/2410.13782v1.json",
        "figure_id": "2410.13782v1_figure_1.png",
        "caption": "Figure 1: Overall illustration of DPLM-2.\n(A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates.\n(B) Multimodal learning and generation of protein structure and sequence with DPLM-2.\n(C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.",
        "qtype": "Design_Rationale",
        "response": "Let’s walk through the figure and text to see what that red‐boxed component is, and why it was chosen:\n\n1. In panel A the authors show their “structure tokenizer,” which has three pieces:  \n   - a GVP–based encoder that turns 3D backbone coordinates into invariant feature vectors  \n   - a quantizer that discretizes those features into a sequence of integer tokens  \n   - an IPA‐based decoder that turns the tokens back into atomic coordinates  \n\n2. A natural choice for a vector-quantized autoencoder would be the classic VQ-VAE quantizer.  However, the authors say that in their preliminary experiments “conventional VQ-VAE … pretty much struggles in training.”  Training was slow (≈15 days on 8×A100s) and reconstruction accuracy was poor.\n\n3. To “mitigate this,” they swap in the lookup-free quantizer (LFQ) design from the recent state-of-the-art visual tokenizer (Yu et al., 2023).  LFQ decomposes latent vectors into binary factors, yielding:  \n   - much faster training (≈2 days vs. 15 days)  \n   - substantially better reconstruction accuracy at the same codebook size  \n\n4. Thus the red‐boxed component—“Lookup-Free Quantizer (LFQ)”—is adopted because it overcomes the instabilities and inefficiencies of standard VQ-VAE, giving both speed and reconstruction quality gains."
    },
    {
        "question": "What purpose does the warmup from pre-trained sequence-based DPLM serve in DPLM-2?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Motivated by the link between evolutionary knowledge and structural interactions, we propose to built DPLM-2 with an efficient warmup from pre-trained sequence-based DPLM, to make the most of established evolutionary information for protein structure modeling,",
            "Since our structure dataset is significantly smaller than UniRef50 sequence database (200K vs. 45M), enabling efficient fine-tuning of the pre-trained model.",
            "To keep the sequence knowledge intact and reduce the risk of catastrophic forgetting, we apply LoRA to limit too much deviation to the original parameters.",
            "This approach not only lowers training costs compared to starting from scratch but also effectively transfers valuable evolutionary information."
        ],
        "final_answer": "The warmup from a pre-trained sequence-based DPLM is used to transfer and leverage evolutionary information learned from large-scale sequence data into DPLM-2’s structure modeling. It enables efficient fine-tuning on the much smaller structure dataset, preserves existing sequence knowledge (avoiding catastrophic forgetting via LoRA), and reduces overall training cost.",
        "relevant_elements": [
            "warmup from pre-trained sequence-based DPLM",
            "DPLM-2"
        ],
        "id": 2232,
        "masked_question": "What purpose does the [mask1] serve in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "warmup from pre-trained sequence-based DPLM",
            "DPLM-2"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.13782v1_figure_1.png",
        "paperid": "2410.13782v1",
        "paper_path": "./MISS-QA/papers/2410.13782v1.json",
        "figure_id": "2410.13782v1_figure_1.png",
        "caption": "Figure 1: Overall illustration of DPLM-2.\n(A) Structure tokenization consists of a GVP-based encoder to yield invariant backbone geometric features, a lookup-free quantizer (LFQ) to discretize encoded structural features into structure tokens within a codebook, and an IPA-based decoder as de-tokenizer to convert structure tokens back to backbone atomic coordinates.\n(B) Multimodal learning and generation of protein structure and sequence with DPLM-2.\n(C) Various applications of DPLM-2 as a protein foundation model: (1) unconditional protein sequence-structure mixed-modal co-generation; (2) protein sequence-structure joint representation for predictive tasks; (3) structure prediction; (4) fixed-backbone sequence generation; (5) conditional protein generation with structure-sequence mixed-modal input and output.",
        "qtype": "Design_Rationale",
        "response": "Here “warmup from pre-trained sequence-based DPLM” (the red box) is the initialization step inside the blue-boxed DPLM-2 training/sampling pipeline.  By first loading in weights from a large, sequence-only protein LM (the original DPLM trained on UniRef50), DPLM-2:\n\n  • Carries over the evolutionary co-variation priors already embedded in the sequence model  \n  • Speeds up convergence on the much smaller structure+sequence data set  \n  • Helps preserve—and not “catastrophically forget”—that sequence knowledge when fine-tuning on structure data  \n\nIn short, the warm-up gives DPLM-2 a head start in learning structural correlations by borrowing the rich sequence representation learned by the pre-trained DPLM."
    },
    {
        "question": "Why incorporate reverse-complement equivariance into DNA-xLSTM architecture?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "We develop an xLSTM block that is equivariant to the reverse complement (RC) of an input sequence, a property particularly relevant to DNA-based applications.",
            "In double-helix DNA structures, both strands are semantically equivalent, with one strand being the RC of the other.",
            "Shrikumar et al., (2017) show that a data-driven approach to learn the equivalence between RC sequences can fail. Therefore, Schiff et al., (2024) propose to enforce RC-equivariance by design, making use of two different inductive biases, post-hoc conjoining (PH) and parameter sharing (PS), in the architecture."
        ],
        "final_answer": "Because in DNA the two strands are semantically equivalent—one is the reverse complement of the other—and purely data-driven methods may fail to learn this symmetry, the architecture enforces reverse-complement equivariance by design so that the model treats a sequence and its reverse complement identically.",
        "relevant_elements": [
            "Reverse-complement equivariance",
            "DNA-xLSTM"
        ],
        "id": 2233,
        "masked_question": "Why incorporate [mask1] into DNA-xLSTM architecture?",
        "masked_number": 1,
        "masked_elements": [
            "Reverse-complement equivariance"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.04165v1_figure_1.png",
        "paperid": "2411.04165v1",
        "paper_path": "./MISS-QA/papers/2411.04165v1.json",
        "figure_id": "2411.04165v1_figure_1.png",
        "caption": "Figure 1: Overview of Bio-xLSTM.\nTop left: xLSTM for natural language processing tasks.\nTop right: Considered modeling approaches\nfor biological sequences:\nmasked language modeling,\nequivariance to reverse complementary sequence,\nand in-context learning.\nBottom left: DNA-xLSTM models are trained on genomic DNA sequences and then fine-tuned on downstream tasks.\nBottom center: Prot-xLSTM models are trained in a causal modeling setting with a fill-in-the-middle objective and use homologous proteins for in-context learning.\nBottom right: Chem-xLSTM models are trained to generate small molecules.\nFor an in-context learning setting, Chem-xLSTM models use\nmolecules with known properties.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module in the top–right panel of Figure 1 is the “reverse–complement equivariance” extension.  In practice we fold it into DNA-xLSTM because in double-stranded DNA the forward and reverse-complement strands carry exactly the same information.  By building equivariance to the reverse-complement operation into the LSTM blocks (via parameter sharing or post-hoc conjoining) we bake in the fact that 5′→3′ and 3′→5′ representations must yield the same biological predictions.  This not only matches the true symmetry of genomic DNA but also gives the model a strong inductive bias, improving data efficiency and downstream performance without having to learn strand invariance from scratch."
    },
    {
        "question": "How are reverse and forward token streams combined to enforce reverse-complement equivariance?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "In PH architectures, the backbone is trained to handle both DNA sequences and their RCs by applying RC augmentations during pre-training.",
            "For downstream tasks, PH architectures are applied to both the original sequence and its RC, and their outputs are summed to reach overall RC invariance."
        ],
        "final_answer": "The model processes both the forward sequence and its reverse complement in parallel, then sums their output token streams to enforce reverse-complement equivariance.",
        "relevant_elements": [
            "reverse-complement equivariance"
        ],
        "id": 2235,
        "masked_question": "How are reverse and forward token streams combined to enforce [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "reverse-complement equivariance"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.04165v1_figure_1.png",
        "paperid": "2411.04165v1",
        "paper_path": "./MISS-QA/papers/2411.04165v1.json",
        "figure_id": "2411.04165v1_figure_1.png",
        "caption": "Figure 1: Overview of Bio-xLSTM.\nTop left: xLSTM for natural language processing tasks.\nTop right: Considered modeling approaches\nfor biological sequences:\nmasked language modeling,\nequivariance to reverse complementary sequence,\nand in-context learning.\nBottom left: DNA-xLSTM models are trained on genomic DNA sequences and then fine-tuned on downstream tasks.\nBottom center: Prot-xLSTM models are trained in a causal modeling setting with a fill-in-the-middle objective and use homologous proteins for in-context learning.\nBottom right: Chem-xLSTM models are trained to generate small molecules.\nFor an in-context learning setting, Chem-xLSTM models use\nmolecules with known properties.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red‐boxed module in Figure 1 is the “reverse–complement equivariance” block.  To enforce equivariance, Bio-xLSTM actually runs two parallel token streams through the same xLSTM backbone: one on the original (forward) DNA sequence and one on its reverse–complement (RC) read backward.  At the very end, the network simply *adds* the two output streams, position by position (i.e. sums their logits or hidden‐state vectors).  That post-hoc summation is what guarantees that a pattern and its reverse–complement collapse to the *same* prediction—i.e. the model is equivariant to taking the reverse complement."
    },
    {
        "question": "How does reordered packing reduce segmentation overhead in SLBC execution?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "In order to fully utilize the overlapping portions and merge multiple segmentation operations together, the arrangement order of elements has been modified so that the overlapping portions appear in adjacent SIMD registers rather than between adjacent lanes within the same one.",
            "After rearranging the packing order of elements, the boundary elements to jointly form one complete convolution element are located in corresponding lanes of adjacent SIMD registers. Therefore, these two SIMD registers can be accumulated after performing parallel shifting operations, which eliminates the need for additional splitting overhead.",
            "For the configuration discussed above, (l−1)*k segmentation operations will be eliminated for every l*k elements, thus reducing segmentation overhead to 1/l of the original count."
        ],
        "final_answer": "By reordering the packing so that overlapping convolution elements sit in corresponding lanes of adjacent SIMD registers (rather than across lanes in the same register), SLBC can perform a single parallel shift and accumulate step across those registers instead of separate segmentation for each overlap. This merges multiple bit‐wise segmentation operations into one and eliminates (l–1)×k segmentation steps per l×k elements, reducing the overall segmentation overhead to 1/l of its original amount.",
        "relevant_elements": [
            "reordered packing",
            "SLBC"
        ],
        "id": 2237,
        "masked_question": "How does [mask1] reduce segmentation overhead in SLBC execution?",
        "masked_number": 1,
        "masked_elements": [
            "reordered packing"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.18267v1_figure_1.png",
        "paperid": "2407.18267v1",
        "paper_path": "./MISS-QA/papers/2407.18267v1.json",
        "figure_id": "2407.18267v1_figure_1.png",
        "caption": "Figure 1: MCU-MixQ Overview, the proposed HW/SW co-optimization design framework for MPNN on MCUs.",
        "qtype": "Implementation_Details",
        "response": "Here “​[mask1]” is the new “Reordered Packing” step in SLBC.  By simply changing the order in which low‐bit words are laid out into the SIMD lanes, all of the “overlap” words that used to straddle adjacent lanes now live in corresponding lanes of two *different* SIMD registers.  That means when you do your shift-and-add local accumulation you automatically stitch those boundary words together, instead of having to issue a separate bit-wise extract (LSR/AND) for each one.  In fact, for every group of *k* output words you *completely* eliminate *(k–1)* of those costly bit-segmentation instructions, driving the segmentation overhead down to just 1/*k* of what it was under the naive packing."
    },
    {
        "question": "How does Search Space inform Quantization Configuration relative to NAS-based hardware-aware quantization methods?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "It starts with a pre-trained floating point model and sets the possible quantization data width as the initial design space of NAS.",
            "Then, it creates a quantization super-net to cover all the possible quantization configurations. Each layer of the target model to be quantized will be replaced with a mixed kernel composed of multiple weighted branches and each branch represents a specific quantization option.",
            "Given the quantization search space , the quantization super-net can be denoted as , while a sub-net sampled from  is . The optimization goal is to search for a quantization sub-net  to maximize the accuracy and minimize the latency while fulfilling the design constraints such as model sizes."
        ],
        "final_answer": "The Search Space defines the set of candidate bit-widths for weights and activations (e.g. 2–8 bits) and serves as the initial design space for the differentiable NAS. Those choices become the branches of a quantization super-net at each layer. Sampling a sub-network from this super-net then yields a concrete Quantization Configuration (i.e. a specific bit-width assignment per layer), which the NAS co-optimizes for accuracy and latency.",
        "relevant_elements": [
            "Search Space",
            "Quantization Configuration"
        ],
        "id": 2239,
        "masked_question": "How does [mask1] inform Quantization Configuration relative to NAS-based hardware-aware quantization methods?",
        "masked_number": 1,
        "masked_elements": [
            "Search Space"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.18267v1_figure_1.png",
        "paperid": "2407.18267v1",
        "paper_path": "./MISS-QA/papers/2407.18267v1.json",
        "figure_id": "2407.18267v1_figure_1.png",
        "caption": "Figure 1: MCU-MixQ Overview, the proposed HW/SW co-optimization design framework for MPNN on MCUs.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is nothing mystical – it is simply the “Search Space” of all possible weight‐and‐activation bit‐width pairs (w,a ∈ {2,3,4,5,6,7,8}).  In our pipeline this box feeds directly into the Quantization Configuration stage:\n\n 1.  We take that discrete list of (w,a) choices and build a *quantization super‐net* in which each layer is replaced by a mixture of branches, one branch per (w,a) setting from the search space.  \n 2.  During the differentiable NAS search we then learn a set of branch weights over exactly those bit‐width choices, and we score each candidate not only by its accuracy loss but also by a *performance loss* that models how fast our SLBC‐packed operators will run on the target MCU.  \n 3.  At the end of search we collapse the super‐net to a single sub‐net by picking, for each layer, the (w,a) branch with the highest score under the joint accuracy+performance objective.\n\nIn short, that red box *defines* the quantization options you can pick from and so directly informs the structure of the super‐net and the choices the NAS makes during hardware‐aware quantization."
    },
    {
        "question": "How does SLBC leverage SIMD compared to SISD to enhance low-bitwidth convolution efficiency?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "With SLBC, multiple multiply and add operations in a convolution operator can be substituted with a single SIMD multiplication instruction and bit-wise operations.",
            "After the SIMD multiplication, the convolution sequence is already stored in the output vector, which means that we can replace more ADD and MUL i.e. single instruction single data (SISD) instructions with one SIMD instruction."
        ],
        "final_answer": "SLBC packs several low-bitwidth convolution elements into one wider SIMD register so that a single SIMD multiply plus bitwise shifts and masks replaces many individual SISD multiply and add instructions. This data-level parallelism cuts down on instruction count and greatly boosts low-bitwidth convolution efficiency on MCUs.",
        "relevant_elements": [
            "SLBC",
            "SIMD",
            "SISD"
        ],
        "id": 2240,
        "masked_question": "How does [mask1] leverage [mask2] compared to SISD to enhance low-bitwidth convolution efficiency?",
        "masked_number": 2,
        "masked_elements": [
            "SLBC",
            "SIMD"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.18267v1_figure_1.png",
        "paperid": "2407.18267v1",
        "paper_path": "./MISS-QA/papers/2407.18267v1.json",
        "figure_id": "2407.18267v1_figure_1.png",
        "caption": "Figure 1: MCU-MixQ Overview, the proposed HW/SW co-optimization design framework for MPNN on MCUs.",
        "qtype": "Literature_Background",
        "response": "Here’s the short story:\n\n- mask1 is the SLBC operator (“SIMD Low-Bitwidth Convolution”) highlighted in red.  \n- mask2 is the use of the SIMD datapath (wide vector‐multiply instruction) highlighted in blue.  \n\nChain of thought:\n\n1.   In a traditional SISD implementation you do one k-bit multiply and one add at a time for each convolution tap.  \n2.   SLBC “polynomial-packs” several k-bit activations and weights into the lanes of a single wider register.  \n3.   It then fires one SIMD multiply to compute all of those k-bit products in parallel, producing a single wide result that, by construction, contains all of the partial convolution sums at different bit-shifts.  \n4.   Finally it unpacks (segments) each k-bit output with a few shifts and masks rather than looping over individual multiplications and additions.  \n\nBecause of this packing→one SIMD MUL→bit-masking sequence, SLBC replaces many scalar SISD multiply-adds with just:\n\n  •  1 SIMD multiply per group  \n  •  a handful of bit-shifts and AND/masks  \n\n—drastically cutting the instruction-count (and memory traffic) compared to a pure SISD low-bit convolution."
    },
    {
        "question": "How does SAM-based filtering interact with mesh extraction to ensure precise garment boundary reconstruction?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "SAM-based filtering. As shown in Fig. 2, the generated  inevitably includes parts of the body. To decouple the body and garment, we utilize SAM [33] to filter out non-garment Gaussians. Specifically, each Gaussian is assigned an extra class attribute ( for garment and  for body initially) to render a semantic image  with Eq. 1. We use SAM to obtain the semantic mask  of the clothed human image  as the label and calculate the MSE loss  between  and  to optimize  of . During  generation, we remove Gaussians whose  are below 0.5 (i.e., non-garment 2DGS) every 500 iterations.",
            "Mesh extraction. Following [23], we reconstruct the garment mesh using the TSDF algorithm from multiview rendered depth images of . We remove the garment’s invisible faces inside the body mesh and simplify the mesh to  faces through the mesh simplification algorithm [18], followed by Laplacian smoothing."
        ],
        "final_answer": "By assigning each Gaussian a semantic class and using SAM to identify which Gaussians belong to the garment, the method periodically purges any Gaussians classified as ‘body’. This filtered set of garment-only Gaussians is then rendered to produce multiview depth images, which are fed into a TSDF-based mesh extraction pipeline. Because only garment Gaussians remain—and invisible faces inside the body are explicitly removed—the resulting mesh cleanly and precisely follows the true garment boundary.",
        "relevant_elements": [
            "SAM-based filtering",
            "mesh extraction"
        ],
        "id": 2241,
        "masked_question": "How does [mask1] interact with mesh extraction to ensure precise garment boundary reconstruction?",
        "masked_number": 1,
        "masked_elements": [
            "SAM-based filtering"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.15205v1_figure_2.png",
        "paperid": "2411.15205v1",
        "paper_path": "./MISS-QA/papers/2411.15205v1.json",
        "figure_id": "2411.15205v1_figure_2.png",
        "caption": "Figure 2: \nMethod overview.\nGiven text prompts, DAGSM generates disentangle digital humans whose bodies and clothes are represented as multiple individual GSM (Sec. 4.1).\nThe generation process includes three stages:\n1) a body generation stage that generates an unclothed body with the human priors SMPL-X [52] from the guidance of text-to-image model SD [16] (Sec. 4.2);\n2) a cloth generation stage that first creates the cloth’s mesh proxy.\nThen 2DGS 𝒢bsubscript𝒢𝑏\\mathcal{G}_{b}caligraphic_G start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is bound to the mesh for generating a garment with texture (Sec. 4.3);\nand 3) a view-consistent refinement stage, where we propose a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to enhance the appearance image 𝒱^isubscript^𝒱𝑖\\hat{\\mathcal{V}}_{i}over^ start_ARG caligraphic_V end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (Sec. 4.4).",
        "qtype": "Literature_Background",
        "response": "The little blue box outlined in red is the “SAM” module in the cloth-generation stage – in other words, the Segment-Anything–based filter that sits immediately upstream of the mesh-extraction step.  Concretely, here is how it works:\n\n 1. During 2D-Gaussian optimization for the garment, every Gaussian carries a small “class” weight (body vs. garment).  \n 2. We render a semantic image from those Gaussians and run it through SAM to get a pixel-accurate garment mask.  \n 3. We compute an MSE loss between the SAM mask and our rendered semantic image and periodically (every 500 iters) throw away any Gaussians whose “garment probability” falls below 0.5.  \n 4. By the time we produce our multiview depth renders for TSDF-based reconstruction, only the true garment Gaussians remain – so the depth field cleanly carves out the hoodie (or whatever article of clothing) with almost zero bleed into the body.  \n 5. Feeding those “garment-only” depth volumes into the TSDF step yields a mesh whose boundaries align exactly with the segmented garment, giving you a crisp, body-free cloth silhouette."
    },
    {
        "question": "How can cross-view attention leverage IAW-DE to achieve view-consistent texture refinement?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "Starting from a predefined canonical view, we apply IAW-DE to enhance the texture image as the pseudo label to supervise the 2DGS rendered image. This process is repeated for each view, with a cross-view attention mechanism to ensure a consistent texture style.",
            "To maintain the consistent texture style across the views, we replace the self-attention in SD3 with cross-view attention during the denoising process inspired by video diffusion models [31, 75]. We use the canonical and previous views (v_{i-1}) as the reference to maintain texture style consistency by concatenating their features into the calculation of key and value."
        ],
        "final_answer": "Cross-view attention leverages IAW-DE by first using incident-angle‐weighted denoising (IAW-DE) to produce a refined pseudo‐label texture in a canonical view. For each subsequent view, IAW-DE focuses the denoising on well‐observed (high‐confidence) pixels to generate a refined image. During the denoising process, self‐attention is replaced with cross‐view attention: features from the canonical (and previously refined) views—already enhanced by IAW-DE—are concatenated as the attention keys and values. This guides the current view’s refinement to follow the style and details of the earlier IAW-DE enhanced views, ensuring consistent texture appearance across all viewpoints.",
        "relevant_elements": [
            "cross-view attention",
            "IAW-DE"
        ],
        "id": 2242,
        "masked_question": "How can [mask1] leverage [mask2] to achieve view-consistent texture refinement?",
        "masked_number": 2,
        "masked_elements": [
            "cross-view attention",
            "IAW-DE"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.15205v1_figure_2.png",
        "paperid": "2411.15205v1",
        "paper_path": "./MISS-QA/papers/2411.15205v1.json",
        "figure_id": "2411.15205v1_figure_2.png",
        "caption": "Figure 2: \nMethod overview.\nGiven text prompts, DAGSM generates disentangle digital humans whose bodies and clothes are represented as multiple individual GSM (Sec. 4.1).\nThe generation process includes three stages:\n1) a body generation stage that generates an unclothed body with the human priors SMPL-X [52] from the guidance of text-to-image model SD [16] (Sec. 4.2);\n2) a cloth generation stage that first creates the cloth’s mesh proxy.\nThen 2DGS 𝒢bsubscript𝒢𝑏\\mathcal{G}_{b}caligraphic_G start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT is bound to the mesh for generating a garment with texture (Sec. 4.3);\nand 3) a view-consistent refinement stage, where we propose a cross-view attention mechanism for texture style consistency and an incident-angle-weighted denoising (IAW-DE) strategy to enhance the appearance image 𝒱^isubscript^𝒱𝑖\\hat{\\mathcal{V}}_{i}over^ start_ARG caligraphic_V end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT (Sec. 4.4).",
        "qtype": "Literature_Background",
        "response": "Here is essentially what’s going on in that red box (cross-view attention) and how it leans on the blue box (IAW-DE):\n\n1. At each new camera view vᵢ we first run an incident-angle-weighted denoising (IAW-DE) pass (the blue box) to produce a “best-seen” refinement image V̂ᵢ and a per-pixel weight map Wᵢ.  Wᵢ tells us which pixels are observed most “head-on” (high confidence) and should drive the update.  \n2. We then take that IAW-DE output and feed it into our diffusion UNet for the next view, but instead of ordinary self-attention we use cross-view attention:  \n   •  The keys and values in each attention block are formed by concatenating features from the canonical view and from the last refined view (both have already been IAW-DE’d).  \n   •  The current view’s queries can now “look up” consistent texture/style information from those past views, weighted by their confidence Wᵢ.  \n3. In practice the network minimises a weighted MSE between the rendered 2DGS image and the IAW-DE pseudo-label V̂ᵢ, with Wᵢ modulating the loss so strongly observed regions get updated more aggressively.\n\nBy combining the IAW-DE confidence map (blue) with cross-view feature fusion (red), the model both sharpens well-seen areas and enforces that style and detail remain coherent as you sweep around the object."
    },
    {
        "question": "What are the limitations of using shared LSNN parameters across multi-step pulling stages?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "LSNN",
            "Multi-Step Pulling (MSP) module"
        ],
        "id": 2248,
        "masked_question": "What are the limitations of using shared [mask1] parameters across [mask2] stages?",
        "masked_number": 2,
        "masked_elements": [
            "LSNN",
            "Multi-Step Pulling (MSP) module"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.01208v1_figure_2.png",
        "paperid": "2411.01208v1",
        "paper_path": "./MISS-QA/papers/2411.01208v1.json",
        "figure_id": "2411.01208v1_figure_2.png",
        "caption": "Figure 2: Overview of our method: (a) Frequency Feature Transformation (FFT) module and (b) Multi-Step Pulling (MSP) module. In (a), we learn Fourier bases hi⁢(Q)subscriptℎ𝑖𝑄h_{i}(Q)italic_h start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_Q ) from query points Q𝑄Qitalic_Q using the Fourier layer and obtain multi-level frequency features yisubscript𝑦𝑖{y_{i}}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT through Hadamard product. In (b), using multi-level frequency features from (a) and a linear network LSNN with shared parameters, we calculate the distance(D) of Qisubscript𝑄𝑖Q_{i}italic_Q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to its corresponding surface target point Qtsubscript𝑄𝑡Q_{t}italic_Q start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT to predict a more accurate surface. We visualize the predicted SDF distribution map corresponding to the frequency features in (a) and the reconstruction from each step of SDF predictions on the right side of (b).",
        "qtype": "Others",
        "response": "Here’s the high-level issue in a nutshell:\n\nBecause we force the exact same small LSNN (the “red‐boxed” network) to process all of the different multi-step pulling stages (the “blue‐boxed” MSP steps), we lose the ability to specialize the mapping at each scale.  Concretely:\n\n  1. Capacity bottleneck  \n     – A single shared LSNN must learn both very coarse SDF corrections (earlier pulls) and very fine, high-frequency ones (later pulls).  It simply doesn’t have enough representational power to do both equally well.  \n  2. Conflicting gradient signals  \n     – In back-propagation, the coarse-stage losses and the fine-stage losses both update the same weights.  Those gradient signals can interfere, slowing convergence or leading to compromises that underfit both scales.  \n  3. Poor stage-wise specialization  \n     – Ideally each pull step would have its own tiny regressor tuned to its current frequency band.  Sharing one LSNN forces all steps through the same “bottleneck,” so none of them can exploit step-specific structure.  \n\nIn practice these limitations manifest as (a) less accurate fine-detail recovery and (b) slower or less stable training when compared to using distinct networks (or at least distinct adapter heads) per pulling stage."
    },
    {
        "question": "What are the potential limitations of adversarial domain classifier within the text-aligned neural tokenizer training?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "unanswerable",
        "relevant_elements": [
            "domain classifier",
            "text-aligned neural tokenizer"
        ],
        "id": 2249,
        "masked_question": "What are the potential limitations of adversarial [mask1] within the [mask2] training?",
        "masked_number": 2,
        "masked_elements": [
            "domain classifier",
            "text-aligned neural tokenizer"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.00101v1_figure_2.png",
        "paperid": "2409.00101v1",
        "paper_path": "./MISS-QA/papers/2409.00101v1.json",
        "figure_id": "2409.00101v1_figure_2.png",
        "caption": "Figure 2: The architecture design of text-aligned neural tokenizer training. The neural tokenizer is trained by reconstructing both temporal and frequency domain of input EEG signals to discretize them into discrete neural tokens. To align EEG and text embedding space, we utilize a domain classifier through adversarial training.",
        "qtype": "Others",
        "response": "Chain‐of‐thought  \n1. By inspecting the red box in Figure 2 we see that the component is the “Domain Classifier,” which is trained adversarially via a gradient‐reversal layer.  This is what we must fill in for [mask1].  \n2. The blue‐boxed region corresponds to the “EEG‐Text Embedding Space Alignment” stage of the neural‐tokenizer training (i.e. the phase in which we use the adversarial domain classifier to force EEG embeddings into the same space as pre‐trained text embeddings).  This is what we must fill in for [mask2].  \n3. The question asks:  “What are the potential limitations of adversarial domain classifier within the EEG‐Text Embedding Space Alignment training?”  \n4. We know from the paper that they have no true EEG–text pairs, so the adversarial classifier only enforces a coarse, distribution‐level alignment, not a one‐to‐one semantic match.  Adversarial training can be unstable, can collapse to trivial features that fool the discriminator, and may wash out features important for downstream EEG tasks.  It also aligns only marginal distributions (EEG vs. text) and ignores the richer conditional or class‐wise structure.  Finally, with limited or noisy text samples, the domain classifier can underfit or overfit, making the resulting alignment brittle.  \n\nAnswer  \nThe adversarial domain classifier in the EEG-Text Embedding Space Alignment stage can only force EEG and text embeddings to become indistinguishable at a distributional level, but does not guarantee any fine‐grained, semantically correct pairing between particular EEG patterns and text concepts.  In practice this can lead to:  \n• Unstable or collapsed adversarial training (the encoder may learn trivial features that simply fool the discriminator).  \n• Loss of task‐relevant signal (features crucial for emotion, seizure, or sleep detection can be smeared out in the name of “alignment”).  \n• Only marginal (global) distribution matching rather than conditional or class‐wise alignment, so different cognitive states in EEG may still map to the same region of the text space.  \n• Sensitivity to the scarcity and noise of any auxiliary text embeddings (leading to under‐ or over‐fitting of the discriminator and a brittle alignment)."
    },
    {
        "question": "How might unified decoders compare to separate temporal and frequency decoders in the neural tokenizer?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "temporal decoder",
            "frequency decoder"
        ],
        "id": 2250,
        "masked_question": "How might unified decoders compare to separate [mask1] and [mask2] in the neural tokenizer?",
        "masked_number": 2,
        "masked_elements": [
            "temporal decoder",
            "frequency decoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.00101v1_figure_2.png",
        "paperid": "2409.00101v1",
        "paper_path": "./MISS-QA/papers/2409.00101v1.json",
        "figure_id": "2409.00101v1_figure_2.png",
        "caption": "Figure 2: The architecture design of text-aligned neural tokenizer training. The neural tokenizer is trained by reconstructing both temporal and frequency domain of input EEG signals to discretize them into discrete neural tokens. To align EEG and text embedding space, we utilize a domain classifier through adversarial training.",
        "qtype": "Others",
        "response": "Let’s first unwrap what the two masked blocks actually are.  In our text and in Figure 2:\n\n  – [mask1] is the Temporal Decoder (the red-boxed head that tries to reconstruct the raw time-series patch)  \n  – [mask2] is the Frequency Decoder (the blue-boxed head that tries to reconstruct the per-patch DFT magnitude)  \n\nWhy did the authors choose two separate decoders?  Because each head can focus entirely on one domain of information—time‐domain shape versus spectral magnitude—and thus drive the vector-quantized codebook to encode both kinds of features strongly.\n\nNow imagine replacing those two specialist heads with one “unified” decoder that simultaneously tries to spit out both the waveform and its spectrum, perhaps via a single shared network with two output branches or even a single multi‐task output.  The possible pros and cons are:\n\n1. Parameter efficiency  \n   • Pro: you share most of the weights, so you cut down on total parameters and compute.  \n   • Con: you lose the dedicated capacity that each separate decoder had to model its own domain.\n\n2. Cross-modal feature sharing  \n   • Pro: a unified decoder might learn richer joint time–frequency representations, exploiting correlations between the two.  \n   • Con: the two loss signals (Lₜ for time, L_f for frequency) may fight one another, leading to sub-optimal representations if one dominates or conflicts with the other.\n\n3. Reconstruction fidelity  \n   • Empirically, specialist decoders often give better domain-specific reconstructions (e.g. sharper time-waveforms, crisper spectra).  \n   • A unified head runs the risk of “jack-of-all-trades, master of none”—it may reconstruct both roughly okay but fail to match the fine detail that separate heads can.\n\n4. Training stability  \n   • Two independent decoders let you tune or balance each loss term in isolation.  \n   • A single decoder forces you to juggle a multi-task loss inside one network, which can require delicate weighting and may slow convergence.\n\nIn short, if you made a unified decoder you would almost certainly save some parameters and might pick up cross-domain clues, but you would probably have to work much harder on loss balancing and you would likely give up some of the peak temporal or spectral reconstruction quality that the two‐head (“separate decoders”) design delivers out of the box."
    },
    {
        "question": "What rationale underlies adversarial embedding alignment via reverse gradient and domain classifier?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "However, when considering EEG, there are much more challenges to align EEG with text: 1) EEG signals contain complicated cognitive and non-cognitive information, which is hard to be described by human language accurately and thoroughly. For example, an EEG segment cannot only contain one person’s emotion and mental states, but also represent the body movement and medical normality. 2) The labeled EEG data available to construct EEG–text pair are very limited. Therefore, we propose to align EEG with text space-wise instead of embedding-wise.",
            "We introduce a domain classifier D to predict whether the embeddings are from EEG or text. During the codebook learning, we also feed some text embeddings from LLMs to train the domain classifier. A gradient reverse layer is added after the VQ encoder to confuse the domain classifier. Hence, the embeddings from the VQ encoder fall into the same space of text embeddings."
        ],
        "final_answer": "Because there are few or no high-quality EEG–text pairs and EEG signals carry complex semantics that are hard to label, the authors align EEG embeddings with text embeddings “space-wise” rather than one-to-one. They add a domain classifier that tries to distinguish EEG from text embeddings, and insert a gradient-reversal layer so that the VQ encoder is trained adversarially to produce embeddings the classifier cannot tell apart from text. This forces the EEG embeddings into the same space as text embeddings.",
        "relevant_elements": [
            "reverse gradient",
            "Domain Classifier"
        ],
        "id": 2252,
        "masked_question": "What rationale underlies adversarial embedding alignment via [mask1] and domain classifier?",
        "masked_number": 1,
        "masked_elements": [
            "reverse gradient"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.00101v1_figure_2.png",
        "paperid": "2409.00101v1",
        "paper_path": "./MISS-QA/papers/2409.00101v1.json",
        "figure_id": "2409.00101v1_figure_2.png",
        "caption": "Figure 2: The architecture design of text-aligned neural tokenizer training. The neural tokenizer is trained by reconstructing both temporal and frequency domain of input EEG signals to discretize them into discrete neural tokens. To align EEG and text embedding space, we utilize a domain classifier through adversarial training.",
        "qtype": "Design_Rationale",
        "response": "The little red box in the figure is nothing exotic – it’s a gradient‐reversal layer.  In practice we inject a small “reverse gradient” module immediately after our VQ encoder and then append a simple binary domain‐classifier that tries to tell “EEG” from “text” embeddings.  By flipping the sign of the gradient flowing back into the encoder (i.e. adversarial training), the encoder is forced to fool the classifier – in other words it must make its EEG outputs look as much like text outputs as possible.  This, in turn, causes the EEG embedding distribution to collapse onto the pre‐existing text embedding manifold, giving us a single, joint space in which EEG tokens can be consumed directly by an LLM."
    },
    {
        "question": "What is the motivation behind using KL divergence loss between teacher and student logits?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The student model, on the other hand, takes adversarial examples generated by Eq. 5 as input, using the soft labels produced by the teacher for supervision. In its optimization, the student model learns robust prompts by minimizing the KL divergence between its probability outputs and the teacher’s soft labels.",
            "Since the teacher, trained on clean data, provides soft labels with strong generalization properties, this alignment helps the student become more robust to adversarial perturbations while inheriting the teacher’s generalization capabilities, thereby achieving a better balance between natural accuracy and adversarial robustness."
        ],
        "final_answer": "The KL divergence loss is used to align the student’s adversarial‐example logits with the teacher’s clean‐example soft labels. Because the teacher—trained on natural data—produces high‐quality, generalizable soft labels, minimizing KL divergence helps the student inherit the teacher’s generalization while becoming robust to adversarial perturbations, thus balancing clean accuracy and adversarial robustness.",
        "relevant_elements": [
            "Teacher",
            "Student",
            "KL divergence loss"
        ],
        "id": 2253,
        "masked_question": "What is the motivation behind using [mask1] loss between [mask2] and student logits?",
        "masked_number": 2,
        "masked_elements": [
            "KL divergence loss",
            "Teacher"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.15244v1_figure_1.png",
        "paperid": "2411.15244v1",
        "paper_path": "./MISS-QA/papers/2411.15244v1.json",
        "figure_id": "2411.15244v1_figure_1.png",
        "caption": "Figure 1: Overview of Adversarial Prompt Distillation (APD). The student and teacher CLIP are fine-tuned simultaneously using both visual and textual prompts. The teacher model is trained on natural images to achieve high performance on clean data, while also receiving feedback from the student to help the student better align with the teacher. The student model is trained on adversarial data and aligns its output with the teacher model.",
        "qtype": "Design_Rationale",
        "response": "Here the red‐boxed term is the KL‐divergence loss (i.e. L_KL), and the blue‐boxed quantity is the teacher’s logits (the “soft” labels produced on clean images).  The motivation for minimizing the KL loss between the teacher’s logits and the student’s logits is to transfer the teacher’s richer, more generalizable output distribution into the student.  In practice, the teacher—trained on natural (unaltered) images—yields soft labels that capture inter-class similarities and generalization cues.  By aligning the student’s adversarial‐example predictions to those soft labels via L_KL, the student learns to be both robust to perturbations and to retain the teacher’s strong natural‐accuracy characteristics."
    },
    {
        "question": "What motivates generating adversarial images only for the student image encoder with visual prompts?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Since CLIP processes both text and image data, adversarial examples can be crafted in different modalities: visual, textual, or bimodal. In APD, we focus on visual vulnerability and only generate adversarial examples for the images."
        ],
        "final_answer": "Because APD is designed to defend against attacks on the visual modality, it concentrates on the model’s visual vulnerability and thus crafts adversarial examples only for the student’s image encoder with visual prompts.",
        "relevant_elements": [
            "Adversarial Image",
            "Visual Prompt",
            "Image Encoder"
        ],
        "id": 2254,
        "masked_question": "What motivates generating [mask1] only for the student image encoder with visual prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Adversarial Image"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.15244v1_figure_1.png",
        "paperid": "2411.15244v1",
        "paper_path": "./MISS-QA/papers/2411.15244v1.json",
        "figure_id": "2411.15244v1_figure_1.png",
        "caption": "Figure 1: Overview of Adversarial Prompt Distillation (APD). The student and teacher CLIP are fine-tuned simultaneously using both visual and textual prompts. The teacher model is trained on natural images to achieve high performance on clean data, while also receiving feedback from the student to help the student better align with the teacher. The student model is trained on adversarial data and aligns its output with the teacher model.",
        "qtype": "Design_Rationale",
        "response": "Chain‐of‐thought:\n\n1. In Figure 1, the only place surrounded by a red box is the “Adversarial Image” pipeline feeding into the student’s image encoder (augmented with visual prompts).  \n2. The text (Inner Maximization) explains that APD “focus[es] on visual vulnerability and only generate[s] adversarial examples for the images.”  In other words, rather than attacking both modalities, they craft perturbations only in the pixel space.  \n3. During training, the student is the one exposed to these “on‐the‐fly” adversarial images so that its visual prompts become robust; the teacher, by contrast, sees only clean images to maintain high‐quality soft labels.  \n4. Hence, the reason for generating [mask1] only for the student image encoder is precisely to expose the student to image‐based attacks (visual adversarial examples) and force its prompts to become robust, while keeping the teacher’s supervision signal stable.\n\nAnswer:\n\nThey generate adversarial images (i.e. image-based adversarial examples) only for the student’s visual encoder—because APD is targeting CLIP’s visual vulnerability and aims to teach the student robust visual prompts via on-the-fly image attacks, while the teacher remains on clean inputs to provide stable soft labels."
    },
    {
        "question": "How is the adversarial sample generated to maximize the student model's cross-entropy loss?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "During the inner maximization process of APD, adversarial samples are generated on the fly for the student model.",
            "Given an input image x, APD generates its adversarial version x′ by perturbing the image to maximize the dissimilarity between the image representation and its ground truth text representation (i.e., the representation of the ground truth class prompt).",
            "Formally, the maximization process can be defined as:\n\n    x′ = argmax_{‖δ‖∞ ≤ ε} L_CE(S(x + δ), y)\n\nwhere S(x + δ) are the student logits on the perturbed image, L_CE is the cross-entropy loss, and the constraint ‖δ‖∞ ≤ ε bounds the perturbation magnitude."
        ],
        "final_answer": "The adversarial sample x′ is obtained by solving an inner maximization: x′ = argmax_{‖δ‖∞ ≤ ε} L_CE(S(x + δ), y), i.e., by adding a bounded perturbation δ to the input image x that maximizes the student model’s cross-entropy loss.",
        "relevant_elements": [
            "Adversarial Image",
            "Student Model",
            "Cross-entropy Loss"
        ],
        "id": 2255,
        "masked_question": "How is the adversarial sample generated to maximize the [mask1]'s cross-entropy loss?",
        "masked_number": 1,
        "masked_elements": [
            "Student Model"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.15244v1_figure_1.png",
        "paperid": "2411.15244v1",
        "paper_path": "./MISS-QA/papers/2411.15244v1.json",
        "figure_id": "2411.15244v1_figure_1.png",
        "caption": "Figure 1: Overview of Adversarial Prompt Distillation (APD). The student and teacher CLIP are fine-tuned simultaneously using both visual and textual prompts. The teacher model is trained on natural images to achieve high performance on clean data, while also receiving feedback from the student to help the student better align with the teacher. The student model is trained on adversarial data and aligns its output with the teacher model.",
        "qtype": "Implementation_Details",
        "response": "The key is that APD actually *crafts* the adversarial image by *maximizing* the student‐model’s cross‐entropy loss with respect to the *correct* text prompt.  In practice (see Eq. 5 in the paper) you do exactly the opposite of a normal training step on the image:\n\n1.  Start with the clean example x.  \n2.  Introduce a small perturbation δ (constrained to ∥δ∥_∞ ≤ ε).  \n3.  Compute the student’s logits on x′=x+δ, and form the cross–entropy against the *ground‐truth* text prompt (or equivalently the teacher’s soft labels for the correct class).  \n4.  Take a *gradient‐ascent* step on δ to *increase* that loss:  \n     δ ← Proj_{∥·∥≤ε} { δ + α · sign(∇_x L_CE ( S(x+δ), T(x) ) ) }  \n5.  Repeat for a handful of steps until convergence or you exhaust the ε‐budget.  \n\nThe result is x′ that (by construction) *maximizes* the student’s classification loss on the true class prompt, and that is what gets fed to the student during the outer‐loop distillation."
    },
    {
        "question": "How does the teacher model update textual prompts using feedback from student adversarial logits?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "The teacher model processes only the natural (clean) examples, with its optimization involving two terms: one for natural training and the other for receiving feedback from the student.",
            "The corresponding minimization process for the teacher model is formulated as follows:\n\n    \\min_{\\phi_t} \\mathbb{E}_{(x,y)\\sim D}\\big[\\mathcal{L}_{ce}(T(x),y) + \\lambda\\,\\mathcal{L}_{kl}(S(x'),T(x))\\big],\n\nwhere \\phi_t denotes the updated visual and textual prompts for the teacher.",
            "Meanwhile, the KL divergence \\mathcal{L}_{kl} quantifies the difference between the outputs of the student model S(x') and the teacher model T(x), enabling the teacher to adjust its outputs based on feedback from the student. This feedback assists the student model in more effective training."
        ],
        "final_answer": "The teacher updates its textual prompts by including a KL-divergence term between the student’s adversarial logits S(x′) and its own logits T(x) on clean inputs in its loss. During outer minimization the teacher backpropagates this KL loss (together with its cross-entropy loss) through the prompt parameters, thereby using the student’s adversarial feedback to refine its textual prompts.",
        "relevant_elements": [
            "Teacher Model",
            "Textual Prompt",
            "Student Model"
        ],
        "id": 2256,
        "masked_question": "How does the [mask1] update [mask2] using feedback from student adversarial logits?",
        "masked_number": 2,
        "masked_elements": [
            "Teacher Model",
            "Textual Prompt"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.15244v1_figure_1.png",
        "paperid": "2411.15244v1",
        "paper_path": "./MISS-QA/papers/2411.15244v1.json",
        "figure_id": "2411.15244v1_figure_1.png",
        "caption": "Figure 1: Overview of Adversarial Prompt Distillation (APD). The student and teacher CLIP are fine-tuned simultaneously using both visual and textual prompts. The teacher model is trained on natural images to achieve high performance on clean data, while also receiving feedback from the student to help the student better align with the teacher. The student model is trained on adversarial data and aligns its output with the teacher model.",
        "qtype": "Implementation_Details",
        "response": "First, let’s pin down what the two masks refer to:\n\n 1.  [mask1] is the red‐boxed module in Figure 1 – that is the **teacher CLIP**, which takes *clean* images, appends its learned visual prompts and learned textual prompts, runs them through the frozen encoders, and produces logits.  \n 2.  [mask2] is the blue‐boxed submodule inside the teacher – i.e. the **teacher’s textual prompt** (and text‐encoder) arm, where it holds the learnable prompt vectors  and  and produces the text‐side embedding for each class.\n\n---\n\nChain of thought:\n\n– In APD’s outer loop (Eq. 6 in the paper) the teacher is not only trained to match the ground‐truth labels via cross‐entropy but also to “listen” to how the student behaves on *adversarial* images.  \n– Concretely, the teacher’s loss is  \n   Lₜot(θ_T,P_T) = CE( T(x) , y ) + λ · KL( Softmax(S(x′)/τ) ‖ Softmax(T(x)/τ) )  \n   where x is the clean image, x′ its adversarially perturbed version, S is the student, T is the teacher, P_T collects both its visual prompts P_v^(T) and textual prompts P_t^(T), and λ balances the two terms.  \n– The second term, the KL divergence between the student’s adversarial logits and the teacher’s clean logits, provides **“feedback”** from the student: if the student under­or over­shoots certain classes on x′, that discrepancy will show up in KL.  \n– We backpropagate *through* the teacher’s entire forward pass – including its visual‐prompt insertion, its text‐prompt vectors P_t^(T), and ultimately into its text encoder embeddings – so that P_t^(T) (and P_v^(T)) are nudged in the direction that reduces that KL gap.  \n– In this way [mask1] (the teacher) uses the gradients coming out of that KL divergence – computed against the student’s adversarial logits – to update [mask2] (its textual‐prompt vectors and the text‐encoder front end), alongside the usual CE update on clean data.\n\nSo in one sentence:\n\nThe **teacher** (red box) augments its usual cross‐entropy training with a KL‐loss against the **student’s adversarial logits**, backpropagates that KL term through its prompt‐injection layers, and thereby *updates its textual prompts* (blue box) so as to better align its clean‐image soft‐labels with the student’s adversarial behavior."
    },
    {
        "question": "How does the specialized compiler annotate errors to generate targeted refinement prompts for the bug fixing LLM?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Through our investigation, we found that GPT struggles with tasks that require step-by-step thinking. For example, it cannot detect the simple task of finding the  word in the text, which is the most commonly used error message for humans. As a result, the first change in our custom compiler was the annotation of the error part without merely indicating the location of the error."
        ],
        "final_answer": "The specialized compiler highlights (annotates) the exact portion of the assertion that triggered the error—instead of simply reporting its location—so that the bug-fixing LLM receives a focused, pinpointed refinement prompt on the faulty snippet.",
        "relevant_elements": [
            "Specialized Compiler",
            "Bug Fixing LLM"
        ],
        "id": 2258,
        "masked_question": "How does the [mask1] annotate errors to generate targeted refinement prompts for the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Specialized Compiler",
            "Bug Fixing LLM"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.15442v1_figure_1.png",
        "paperid": "2411.15442v1",
        "paper_path": "./MISS-QA/papers/2411.15442v1.json",
        "figure_id": "2411.15442v1_figure_1.png",
        "caption": "Figure 1: The proposed AssertCraft full-stack flow employs two novel techniques—sub-tasked fine-tuning, and iterative prompting—to produce high-accuracy assertion statements from the specification document. Moreover, this flow was completed by adding an additional dataset and scoreboard for assessing the ability of the model to generate high-quality assertions.",
        "qtype": "Implementation_Details",
        "response": "In our flow the red‐boxed “Specialized Compiler” no longer just spits out a line number or a one‐word error code.  Instead, it re-parses the failing property, locates exactly which tokens or sub-expression caused the Verilog compiler to choke, and wraps just that substring in a little “error” tag.  That annotated assertion (original text plus the <error>…</error> markers around the bad bit) is then bundled up as the “refinement message + assertion” and fed into the blue‐boxed Bug-Fixing LLM.  Because the LLM sees exactly which slice of the assertion failed, it can generate a narrowly focused prompt (“here’s the part that’s broken – please fix only that”) rather than trying to re-write the whole property."
    },
    {
        "question": "How do LLM for FSM Extraction and LLM for variable Extraction collaborate to improve comment granularity compared to abstraction-level approaches?",
        "relevant_section_ids": [
            "2.1",
            "3.1.2"
        ],
        "relevant_context": [
            "Earlier works in automatic assertion generation, such as AutoSVA [4] and ILA [5], have focused on creating a new abstraction level closer to human language. Although they succeeded in making the assertion generation process easier, they lost their generality and did not entirely solve the process, as engineers still had to develop new properties at these abstraction levels.",
            "We use three questions to extract meaningful comments from the specification. Listings 2, 3, and 4 outline the inquiries used to break the specification into smaller components. The first question in Listing 2 is directed at FSMs in the design. For every FSM, we extract the states and the conditions to transfer from one state to another. This helps to break each FSM into multiple assertions, each indicating one transition between the states. Question C in Listing 4 was tailored to extract the variable ranges. Each of these pieces of information was then treated as one assertion in the remainder of the flow."
        ],
        "final_answer": "Instead of relying on a single, coarse abstraction layer into which an entire specification must be mapped, AssertCraft uses separate LLMs—one to extract each FSM’s individual state transitions (LLM for FSM Extraction) and another to extract each variable’s valid ranges (LLM for variable Extraction). By turning each transition and each variable‐range constraint into its own comment, the tool generates far more fine-grained, modular comments. These detailed comments then feed into the assertion generator, avoiding the broad, less-specific properties that abstraction-level approaches leave for engineers to refine by hand.",
        "relevant_elements": [
            "LLM for FSM Extraction",
            "LLM for variable Extraction"
        ],
        "id": 2259,
        "masked_question": "How do [mask1] and [mask2] collaborate to improve comment granularity compared to abstraction-level approaches?",
        "masked_number": 2,
        "masked_elements": [
            "LLM for FSM Extraction",
            "LLM for variable Extraction"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.15442v1_figure_1.png",
        "paperid": "2411.15442v1",
        "paper_path": "./MISS-QA/papers/2411.15442v1.json",
        "figure_id": "2411.15442v1_figure_1.png",
        "caption": "Figure 1: The proposed AssertCraft full-stack flow employs two novel techniques—sub-tasked fine-tuning, and iterative prompting—to produce high-accuracy assertion statements from the specification document. Moreover, this flow was completed by adding an additional dataset and scoreboard for assessing the ability of the model to generate high-quality assertions.",
        "qtype": "Literature_Background",
        "response": "Here’s the gist of how the two boxed modules work together to get you much finer-grained comments than you ever could by just inventing a “higher” abstraction layer:\n\n  1. LLM for FSM Extraction ([mask1])  \n     – Scans your English spec and pulls out each finite-state machine, its states, and the exact guard conditions for every transition.  \n     – Turns “when we’re in state A and input x goes high, move to state B” into a compact, self-standing comment like  \n       “// On input x=1 in state A, transition to state B.”\n\n  2. LLM for Variable Extraction ([mask2])  \n     – Separately hunts down every variable or signal declaration in the spec, along with its allowable range or key relationships (“signal foo is always less than 5,” “count must wrap at 255,” etc.)  \n     – Emits equally atomic comments such as  \n       “// foo ∈ [0..4]” or “// count resets when >255.”\n\nBy splitting the spec into these two orthogonal slices – one that describes the control-flow skeleton (FSM transitions) and one that lays out all the data constraints (variable ranges and relations) – AssertCraft automatically produces dozens of tiny, “one-thing-per-line” comments.  Those, in turn, feed a downstream assertion-generation model, which can reliably map each micro-comment into a perfectly scoped property.  \n\nContrast that with abstraction‐level methods that force you to invent a mid­-level language of, say, “transactions” or “generic events.”  You end up losing detail or having to re­define the abstraction every time the design changes.  By keeping everything at the raw FSM + variable level, AssertCraft never loses precision and never forces the engineer to craft a new domain-specific “comment language.”"
    },
    {
        "question": "How does AMU adapt the EMA update strategy from teacher-student networks?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "To alleviate the instability caused by data variety and error accumulation, previous TTA approaches [31, 7] adopt the teacher‐student network architecture for parameter updating. The student network is online updated with the t-th sequentially arrived sample, whereas the weights of the teacher network are updated by the exponential‐moving‐average (EMA) strategy.",
            "Moreover, we argue that the fixed momentum m in EMA could cause the forgetting of source knowledge in long‐term TTA. For stable adaptation and fast convergence, we propose to adapt the momentum with each incoming sample: where m₀ is a constant to ensure the lower bound of m and λ is a decay factor.",
            "As the momentum m decays, the later samples will have a smaller impact, thereby avoiding the catastrophic forgetting problem."
        ],
        "final_answer": "Instead of using a fixed momentum in the EMA teacher‐update, AMU makes the momentum factor decay over the sequence of incoming test samples (bounded below by a preset m₀). This decaying‐momentum scheme ensures that early samples shape the teacher model more strongly, while later samples have progressively less influence, preventing catastrophic forgetting and stabilizing long‐term adaptation.",
        "relevant_elements": [
            "Alternating Momentum Updating (AMU)",
            "EMA"
        ],
        "id": 2262,
        "masked_question": "How does [mask1] adapt the EMA update strategy from teacher-student networks?",
        "masked_number": 1,
        "masked_elements": [
            "Alternating Momentum Updating (AMU)"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.01573v1_figure_2.png",
        "paperid": "2410.01573v1",
        "paper_path": "./MISS-QA/papers/2410.01573v1.json",
        "figure_id": "2410.01573v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed PASS approach, comprising three components: (1) an input decorator to reduce the style shift, (2) a cross-attention prompt modulator to mitigate the shape inconsistency,\nand (3) an alternating momentum parameter updating strategy with the teacher/student framework to avoid error accumulation during the adaptation stage. The weights of the teacher model are updated by the exponential moving average (EMA) with a decay factor from the online updating student model. For each new test data, the student model is reset by the teacher model to recover the historical knowledge and is adapted to the given sample.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is nothing more exotic than a “tweaked” teacher–student EMA update loop.  In standard TTA you\n\n 1. keep a student model that you train on the incoming test sample,  \n 2. use its weights to do an EMA update of a separate teacher model  \n     θₜₑₐcₕₑᵣ ← m·θₜₑₐcₕₑᵣ + (1–m)·θₛₜᵤdₑₙₜ  \n 3. predict with the teacher.\n\nPASS changes two things:\n\n •  before every new sample you “reset” the student from the teacher so that you never stray too far from all the data you’ve seen so far  \n •  you no longer keep m fixed – you let it decay as more and more samples arrive, so early samples have a big influence on the teacher but later ones are gradually “damped down.”  \n\nIn other words, the red box shows an Alternating Momentum Updating (AMU) strategy where the teacher is still updated by EMA of the student, but  \n\n  – the student is re‐initialized from the teacher at each time step, and  \n  – the EMA momentum m is not constant but follows a decaying schedule (mₜ ↓ as t ↑),  \n\nthus blending fast per‐sample adaptation (student) with long‐term stability (teacher) and avoiding catastrophic forgetting."
    },
    {
        "question": "How does the Input Decorator’s dynamic prompt formulation adapt style shifts for diverse test samples?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Previous studies have shown that leveraging prompts in the continual input embedding space introduces flexibility to pretrained models through an additional parameter space [29,24]. However, current applications of visual prompt learning to domain adaptation [25,31] simply adopt the fixed prompt for all test samples which neglects the data variety within the target distribution. Hence, we propose to generate a dynamic prompt conditioned on each test sample.",
            "Let ID be the data decorator parameterized by φID. For each test data point x^t at the t-th time step, ID reformulates it as follows:\n  x̃^t = x^t + ID(x^t)\nwhere ID intends to shift the distribution of target data x^t to be close to the source data x^s, and x̃^t refers to the altered target input. Since Instance Normalization (IN) has shown effectiveness in style transfer [32], we adopt two shallow convolutional layers with IN to construct the ID."
        ],
        "final_answer": "The Input Decorator (ID) computes a sample-specific, residual ‘‘prompt’’ by passing each incoming test image through two shallow convolutional layers with instance normalization. This prompt ID(x^t) is then added back to the original image x^t, dynamically shifting its appearance statistics toward those of the source domain and thus adapting to diverse style variations in each test sample.",
        "relevant_elements": [
            "Input Decorator"
        ],
        "id": 2263,
        "masked_question": "How does the [mask1]’s dynamic prompt formulation adapt style shifts for diverse test samples?",
        "masked_number": 1,
        "masked_elements": [
            "Input Decorator"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.01573v1_figure_2.png",
        "paperid": "2410.01573v1",
        "paper_path": "./MISS-QA/papers/2410.01573v1.json",
        "figure_id": "2410.01573v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed PASS approach, comprising three components: (1) an input decorator to reduce the style shift, (2) a cross-attention prompt modulator to mitigate the shape inconsistency,\nand (3) an alternating momentum parameter updating strategy with the teacher/student framework to avoid error accumulation during the adaptation stage. The weights of the teacher model are updated by the exponential moving average (EMA) with a decay factor from the online updating student model. For each new test data, the student model is reset by the teacher model to recover the historical knowledge and is adapted to the given sample.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module is nothing exotic – it is exactly the “Input Decorator (ID)” in Figure 2 that lives in the dashed blue box.  Its job is to carve out, for every incoming test image, a little “style‐correction” patch that when added back to the image pushes its low-level statistics closer to the source domain’s.  Concretely:\n\n 1.  Given a new test image xₜ, the ID module first applies two very shallow convolutions interleaved with an Instance-Normalization (IN) layer (and a ReLU) to xₜ.  \n 2.  Because IN strips out each feature channel’s mean and variance (the heart of “style”), the conv→IN→ReLU→conv stack learns to re-inject just the right channel-wise shift to undo whatever new style xₜ brought in.  \n 3.  That learned, per-sample output ID(xₜ) is then simply added back to xₜ, yielding a “decorated” image xₜ′ whose mean/std more closely match those seen during pre‐training.  \n\nBecause the entire ID(⋅) block is executed anew on every test sample and its weights are trainable at test time, each xₜ receives its own bespoke, input-conditioned prompt.  This is exactly how the module dynamically adapts to diverse style shifts in the wild."
    },
    {
        "question": "How does alternating momentum updating between the student and teacher models stabilize online adaptation?",
        "relevant_section_ids": [
            "3.4",
            "4.3"
        ],
        "relevant_context": [
            "In this way, the teacher network accumulates new knowledge without forgetting historical knowledge. Usually, the updated parameters in the teacher model are used for predictions. However, we keep the teacher network as a buffer for historical knowledge restoration and use the student network for a quick adaptation to the test sample. At each step with new test data, the student network is reset by the teacher network and adapted accordingly to this data.",
            "As the momentum γ decays, the later samples will have a smaller impact, thereby avoiding the catastrophic forgetting problem.",
            "In contrast, the proposed alternating momentum updating (AMU) strategy leads to more stable adaptation and better performance through independently updating the student network for every single case and recovering knowledge from the teacher network."
        ],
        "final_answer": "Alternating momentum updating (AMU) stabilizes online adaptation by maintaining a slowly updated teacher model that retains historical knowledge via an exponential-moving-average (EMA) scheme and by repeatedly resetting the student model to the teacher before adapting to each new test sample. By also decaying the momentum over time, AMU ensures that outlier or early samples have diminishing impact on the teacher’s parameters, preventing catastrophic forgetting and error accumulation. As a result, each test case benefits from fresh, case-specific updates in the student network while reliably inheriting past knowledge from the teacher network, yielding more stable and robust online performance.",
        "relevant_elements": [
            "Alternating Momentum Updating",
            "student model",
            "teacher model"
        ],
        "id": 2264,
        "masked_question": "How does alternating momentum updating between the [mask1] and teacher models stabilize online adaptation?",
        "masked_number": 1,
        "masked_elements": [
            "student model"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.01573v1_figure_2.png",
        "paperid": "2410.01573v1",
        "paper_path": "./MISS-QA/papers/2410.01573v1.json",
        "figure_id": "2410.01573v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed PASS approach, comprising three components: (1) an input decorator to reduce the style shift, (2) a cross-attention prompt modulator to mitigate the shape inconsistency,\nand (3) an alternating momentum parameter updating strategy with the teacher/student framework to avoid error accumulation during the adaptation stage. The weights of the teacher model are updated by the exponential moving average (EMA) with a decay factor from the online updating student model. For each new test data, the student model is reset by the teacher model to recover the historical knowledge and is adapted to the given sample.",
        "qtype": "Experimental_Results",
        "response": "Here’s what’s going on, step by step:\n\n1.  **Teacher–Student Pair**  \n    –  You keep two copies of the network:  \n       •  A *student* that you update on each new test sample  \n       •  A *teacher* that you don’t train directly, but instead “watches” the student  \n\n2.  **Alternating Updates**  \n    –  **(a) Reset**  \n       At time step *t*, before seeing the *t*-th test image, you copy the teacher’s weights into the student.  This “resets” the student to the stable, historically accumulated model.  \n    –  **(b) Adapt**  \n       You train the student on the new sample (using your chosen unsupervised loss).  \n    –  **(c) EMA Update**  \n       You then update the teacher by taking an exponential moving average (EMA) of the *new* student weights and the *old* teacher weights.  \n       \\[\n         w_{\\text{teacher}} \\,\\leftarrow\\, m_t\\;w_{\\text{teacher}} + (1 - m_t)\\;w_{\\text{student}}\n       \\]\n       where the momentum \\(m_t\\) *decays* over time (so that early samples have more influence, and later ones progressively less).\n\n3.  **Why This Stabilizes Online Adaptation**  \n    –  **Prevents Drift from Outliers**  \n       If a single test case is very atypical, training only the student on it—and then *not* letting that “noisy” update go straight into the teacher—means you don’t poison your long-term memory.  \n    –  **Balances Plasticity and Stability**  \n       •  The student is *plastic*: it can change quickly to fit the current sample.  \n       •  The teacher is *stable*: it only moves a bit toward the student each time (and less and less as \\(m_t\\) decays).  \n    –  **Avoids Catastrophic Forgetting**  \n       By decaying the EMA momentum, you ensure that early, reliable information (e.g. source-domain knowledge) isn’t washed out by a stream of potentially noisy test-time updates.  \n\nIn short, the alternating rhythm—reset student ← teacher, adapt student → update teacher by EMA with decaying momentum—lets you get fast, per-case adaptation *and* maintain a robust, slowly evolving model that anchors those per-case tweaks, avoiding error accumulation."
    },
    {
        "question": "How might replacing the FC decoder with an attention mechanism improve deformation matrix predictions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Decoder",
            "deformation matrix"
        ],
        "id": 2268,
        "masked_question": "How might replacing the [mask1] with an attention mechanism improve [mask2] predictions?",
        "masked_number": 2,
        "masked_elements": [
            "Decoder",
            "deformation matrix"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.22909v1_figure_1.png",
        "paperid": "2410.22909v1",
        "paper_path": "./MISS-QA/papers/2410.22909v1.json",
        "figure_id": "2410.22909v1_figure_1.png",
        "caption": "Figure 1: UniRiT performs a rigid transformation phase between the source 𝐏𝒮subscript𝐏𝒮\\mathbf{P}_{\\mathcal{S}}bold_P start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT and target 𝐏𝒯subscript𝐏𝒯\\mathbf{P}_{\\mathcal{T}}bold_P start_POSTSUBSCRIPT caligraphic_T end_POSTSUBSCRIPT point clouds, where the features of both point clouds are extracted using MLPs. These features are then passed through a decoder composed of fully connected (FC) layers, which iteratively generates rotation and translation matrices over n𝑛nitalic_n cycles. The transformed point cloud output from the rigid module is subsequently utilized along with the target point cloud to re-extract features. These features are concatenated with the coordinate information and then input into the decoder to generate a deformation matrix, which applied to 𝐏𝒮′superscriptsubscript𝐏𝒮′\\mathbf{P}_{\\mathcal{S}}^{\\prime}bold_P start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT, yields the final transformed point cloud 𝐏𝒮^^subscript𝐏𝒮\\hat{\\mathbf{P}_{\\mathcal{S}}}over^ start_ARG bold_P start_POSTSUBSCRIPT caligraphic_S end_POSTSUBSCRIPT end_ARG.",
        "qtype": "Others",
        "response": "First, let’s be clear about what the two masked regions are:\n\n– Mask 1 (the red‐boxed module) is the little “Decoder” block made out of fully-connected (FC) layers that takes the concatenated features (and coordinates) and spits out per-point offsets.  \n– Mask 2 (the blue cubes) is simply the “deformation matrix” – i.e. the per-point displacement vectors that get applied to the rigidly aligned source cloud.  \n\nNow, why would you swap that simple FC–based decoder for an attention mechanism, and how would that help your deformation predictions?  \nChain-of-thought:\n\n1.  In the current architecture, the decoder is just a sequence of FC layers.  Once you stack on the point‐feature vector and its (x,y,z) coordinate, every point’s displacement is predicted independently of all the others (beyond whatever global summary got baked into the feature).  There is no direct modeling of spatial or semantic relations between different points during that final step.\n\n2.  A non-rigid deformation field is almost by definition a highly structured, spatially coherent mapping: you really want each point’s offset to “know about” its neighbors and to respect longer‐range shape relations (e.g. bones don’t tear, tissue deforms smoothly).  Simple MLPs struggle to encode those pairwise or groupwise interactions.\n\n3.  An attention layer, on the other hand, lets every point “query” all the other points (or a learned subset) and gather a weighted mixture of their features.  In practice that means\n   • local smoothness is enforced because neighboring points will attend to each other’s similar features,  \n   • global consistency can be introduced by allowing distant but semantically linked points to share information,  \n   • and the network can learn to focus on exactly those pairs (or regions) that matter most for a correct non-rigid match.\n\n4.  In other words, by replacing the FC decoder with a self- or cross-attention module, each deformation vector is no longer an isolated prediction – it becomes a context-aware blend of source and target features.  That richer relational modeling leads to deformation matrices that are both more accurate (points go where they should) and more coherent (the resulting shape doesn’t end up torn or scattered).\n\nSo to answer in one sentence:\n\n“By swapping out the simple FC decoder ([mask1]) for an attention module, you give the network the ability to reason about interactions among all points when it predicts the deformation matrix ([mask2]), which yields displacement fields that better respect the global shape structure and local smoothness, and therefore improves registration accuracy.”"
    },
    {
        "question": "What are potential privacy trade-offs introduced by Fusion MLP aggregating sub-model features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Fusion MLP"
        ],
        "id": 2269,
        "masked_question": "What are potential privacy trade-offs introduced by [mask1] aggregating sub-model features?",
        "masked_number": 1,
        "masked_elements": [
            "Fusion MLP"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.11650v1_figure_1.png",
        "paperid": "2410.11650v1",
        "paper_path": "./MISS-QA/papers/2410.11650v1.json",
        "figure_id": "2410.11650v1_figure_1.png",
        "caption": "Figure 1. The overview of ED-ViT which consists of four steps: Model Splitting, Model Pruning, Model Assignment and Model Fusion.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed module in Figure 1 is the little “Fusion MLP” that lives on the server side and takes as input the feature vectors emitted by each of the class-specific sub-models running on the edge devices.  In effect, instead of shipping only a 0/1 decision (“is this a cat or not?”) each edge device ships a rich embedding of the image, and the Fusion MLP recombines those embeddings to get the final label.\n\nChain of thought on privacy trade-offs:\n\n1.  Splitting the ViT into many tiny sub-models and running each on a separate device means that no single device ever sees the entire image or the full ViT representation.  That tends to improve local privacy—an attacker who compromises one device only sees a fragment of the transformer’s activations.\n\n2.  On the other hand, the Fusion MLP requires *all* of those fragments to do its job.  By centralizing the concatenated feature vectors, the server (or any adversary who gains access to it) has the opportunity to invert or probe those embeddings and reconstruct (or infer attributes of) the raw inputs.\n\n3.  In other words, you trade off\n   - less privacy exposure on each *individual* edge node  \n     VS.\n   - a richer, more complete feature‐set sitting on the *fusion* server that could be reverse-engineered.\n\n4.  More formally, intermediate features—even if lower-dimensional than the raw image—often still carry enough structure for model‐inversion or membership‐inference attacks.  By aggregating them, the Fusion MLP increases both the dimensionality and the correlation among features, making it easier to leak sensitive information about the user’s data.\n\n5.  Finally, to achieve high accuracy the Fusion MLP often needs a fairly high-capacity network.  That means more parameters and more expressive embeddings, which again raises the risk of unintended leakage versus a leaner, decision‐only aggregation.\n\nSo the succinct answer is:\n\nAlthough pushing much of the work out to edge sub-models hides most of the raw data locally, the Fusion MLP at the center must collect and recombine all of their intermediate features—and those aggregated feature vectors can still be probed or inverted, leaking private information that simple 0/1 outputs would not."
    },
    {
        "question": "What challenges arise when Model Assignment faces highly heterogeneous edge device memory and compute capacities?",
        "relevant_section_ids": [
            "3",
            "4.4"
        ],
        "relevant_context": [
            "As described in Section 3, the problem of Vision Transformer sub-model partitioning and assignment can be formulated as a 0-1 knapsack problem, where each edge device has varying available memory and energy.",
            "If the remaining memory and energy can accommodate the sub-model, we update the device’s available memory and energy. Otherwise, if the sub-model exceeds the device’s memory capacity, the memory-exhausted device is removed from the set. If no devices remain, it indicates that the current pruning results prevent deployment of all sub-models. In this case, the algorithm terminates, and the ED-ViT framework re-prunes the sub-models based on a new head pruning parameter."
        ],
        "final_answer": "When devices have highly heterogeneous memory and compute budgets, assigning each class-specific sub-model becomes a constrained 0-1 knapsack problem. Large sub-models may not fit on lower-capacity devices, forcing the assignment algorithm to drop devices and then iteratively re-prune sub-models until every sub-model can be deployed within the remaining devices' memory and energy constraints.",
        "relevant_elements": [
            "Model Assignment"
        ],
        "id": 2270,
        "masked_question": "What challenges arise when [mask1] faces highly heterogeneous edge device memory and compute capacities?",
        "masked_number": 1,
        "masked_elements": [
            "Model Assignment"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.11650v1_figure_1.png",
        "paperid": "2410.11650v1",
        "paper_path": "./MISS-QA/papers/2410.11650v1.json",
        "figure_id": "2410.11650v1_figure_1.png",
        "caption": "Figure 1. The overview of ED-ViT which consists of four steps: Model Splitting, Model Pruning, Model Assignment and Model Fusion.",
        "qtype": "Others",
        "response": "The little red‐box in the diagram is the “Model Assignment” step – the moment when we have a bag of class-specialized, pruned ViT sub-models and we must lay them out across M different edge‐devices, each with its own remaining memory Mi and compute budget Ei.  If those Mi and Ei are wildly different from device to device, a whole host of headaches crop up:\n\n 1.  **Combinatorial Knapsack**  \n    – At its heart this is just a 0–1 knapsack (or multi-knapsack) problem: you need to pick which sub-model goes on which device so that neither memory nor FLOP-budget is exceeded.  As soon as every device has a different Mi and Ei, the search space explodes and finding the true optimum becomes intractable.  \n\n 2.  **Fit failures and repeated pruning**  \n    – A large sub-model that would run fine on a beefy desktop simply won’t fit on a tiny IoT node.  If you pick a bad assignment order (say you try to place your largest sub-model last), you can end up with “no device left” that can host it.  The only cure is to go back, prune that sub-model even more aggressively, and retry the whole assignment.  You get a loop of prune → assign → fail → prune → …  \n\n 3.  **Latency imbalance and worst-case bottleneck**  \n    – Even if everything just barely fits, you now have slow devices grinding away next to faster ones.  The overall system latency is dominated by the slowest straggler.  Balancing the load so that no one device becomes the choke-point is extremely difficult when capacities differ by orders of magnitude.  \n\n 4.  **Greedy vs. global optima trade-off**  \n    – In the paper they resort to a greedy heuristic (“place the most compute-hungry model first on the currently fastest device”), but greedy can be arbitrarily sub-optimal.  With high heterogeneity you really feel the gap between a simple heuristic and the true joint optimum.  \n\nIn short, once your edge‐fleet is a heterogeneous patchwork of tiny microcontrollers, mid-range SoCs and maybe one big GPU, the “Model Assignment” step becomes an NP-hard juggling act of memory-caps, FLOP-caps, re-pruning loops and worst-case latency minimization – all of which makes finding a practical, near-optimal deployment plan extremely challenging."
    },
    {
        "question": "What motivates ordering weight selection immediately after class assignment?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Class Assignment",
            "Weight Selection"
        ],
        "id": 2271,
        "masked_question": "What motivates ordering [mask1] immediately after class assignment?",
        "masked_number": 1,
        "masked_elements": [
            "Weight Selection"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.11650v1_figure_1.png",
        "paperid": "2410.11650v1",
        "paper_path": "./MISS-QA/papers/2410.11650v1.json",
        "figure_id": "2410.11650v1_figure_1.png",
        "caption": "Figure 1. The overview of ED-ViT which consists of four steps: Model Splitting, Model Pruning, Model Assignment and Model Fusion.",
        "qtype": "Design_Rationale",
        "response": "The little red‐boxed node is nothing mystical – it is simply the “Weight Selection” (or pruning) step.  We place it immediately after “Class Assignment” because until you know which classes each sub-model is responsible for, you can’t go and pick out (or prune away) the exact ViT parameters that matter to that sub-model.  In other words:\n\n 1. You first decide (via Class Assignment) “sub-model 1 will handle cats and dogs, sub-model 2 will handle cars and buses,” etc.  \n 2. Only then can you meaningfully look back at the full ViT, measure which attention heads, residual channels and FFN neurons are really pulling weight for those particular class subsets, and discard the rest.  \n 3. That selective pruning—Weight Selection—yields exactly N smaller, class-specialized sub-models, each light enough to fit on its edge device.  \n\nIf you swapped the order, you’d have no way of knowing which weights to keep for each class grouping."
    },
    {
        "question": "What justifies prioritizing computational resources during model assignment of sub-models?",
        "relevant_section_ids": [
            "3",
            "4.4"
        ],
        "relevant_context": [
            "Section 3: ...we define the objective function as \\(\\max_j \\sum_{i=1}^N x_{ij} T_i\\) , aiming to minimize the maximal inference latency, as inference latency is closely related to the computational power of edge devices.",
            "Section 4.4: ...the sub-models are first sorted based on their energy consumption (computation overhead). ED-ViT assigns the most computation-intensive sub-model first based on their model sizes, which is proportional to the computation overhead as in Section 3."
        ],
        "final_answer": "Because inference latency is closely tied to an edge device’s computational power, ED-ViT sorts sub-models by their computation overhead and assigns the most compute-intensive ones first to devices with the greatest computational resources in order to minimize the maximal inference latency.",
        "relevant_elements": [
            "Model Assignment",
            "sub-models"
        ],
        "id": 2272,
        "masked_question": "What justifies prioritizing computational resources during [mask1] of [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "Model Assignment",
            "sub-models"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.11650v1_figure_1.png",
        "paperid": "2410.11650v1",
        "paper_path": "./MISS-QA/papers/2410.11650v1.json",
        "figure_id": "2410.11650v1_figure_1.png",
        "caption": "Figure 1. The overview of ED-ViT which consists of four steps: Model Splitting, Model Pruning, Model Assignment and Model Fusion.",
        "qtype": "Design_Rationale",
        "response": "The key point is that in our formulation the single‐biggest bottleneck is inference latency, and latency grows almost linearly with the number of FLOPs a sub‐model has to do.  In Section 3 we explicitly set our objective to “minimize the maximum inference latency” across all devices, and we showed that latency on an edge node is essentially proportional to its available compute (FLOPs).  Hence, when we get to the Model Assignment step of Model Splitting, we sort all of our class‐specific sub‐models by their FLOP counts and assign the heaviest (most compute‐intensive) ones first to the devices with the highest remaining FLOP budgets.  That greedy ordering is exactly what lets us balance the load and drive down the worst‐case latency."
    },
    {
        "question": "Why integrate frame-level captioning with redundancy removal before QA pairs construction?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Alternatively, analyzing video frames for a more granular understanding is feasible, but video content is often redundant, with key information dispersed sparsely, making frame-by-frame analysis computationally expensive and prone to information overload.",
            "The goal of this module is to eliminate redundant information in the video and retain the essential content by selecting a concise sequence of frames.",
            "To capture the fine-grained semantic information of the video as input for constructing QA pairs, we use InternVL2-8B to perform frame-level captioning on all sampled frames. To filter out redundant frames in the video, we leverage the language retrieval model BGE-M3 to preliminarily remove highly similar frames through fine-grained semantic filtering."
        ],
        "final_answer": "Integrating frame-level captioning with redundancy removal preserves the detailed semantic information needed for fine-grained reasoning while filtering out semantically similar (redundant) frames. This yields a concise, information-rich set of frames that reduces computational cost and avoids information overload, enabling more effective and efficient QA pair construction.",
        "relevant_elements": [
            "Frame-level Captioning",
            "Redundancy Removal",
            "QA pairs Construction"
        ],
        "id": 2273,
        "masked_question": "Why integrate [mask1] with [mask2] before QA pairs construction?",
        "masked_number": 2,
        "masked_elements": [
            "Frame-level Captioning",
            "Redundancy Removal"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.14794v1_figure_2.png",
        "paperid": "2411.14794v1",
        "paper_path": "./MISS-QA/papers/2411.14794v1.json",
        "figure_id": "2411.14794v1_figure_2.png",
        "caption": "Figure 2: The automatic generation pipeline of VideoEspresso. (i) Question-Answer Pair Construction: We use video frame-leveled captions to extract the key frames of the video and group descriptions of these frames. Then, we prompt GPT-4 to design questions for each group of video frames. (ii) Multimodal Chain-of-Thought Annotation: We extract key evidence text and generate captions with the highest relevance to the question with GPT-4o. Additionally, we annotate spatial and temporal information for key items, which results in multimodal Chain of Thought data pairs grounded in both temporal and spatial dimensions.",
        "qtype": "Design_Rationale",
        "response": "Here’s the step-by-step reasoning:\n\n1.   What is in the red box ([mask1])?  \n     – It’s the raw, frame-level captions for every sampled video frame.  In other words, a long list of “#46: The image shows a bowl on the kitchen counter…,” “#69: …,” “#1035: …,” and so on.  \n\n2.   What is in the blue box ([mask2])?  \n     – It’s the semantic-redundancy–removal module.  We take those hundreds of frame captions, embed them into the same language space, compute pairwise cosine similarities, and do a LIFO-style drop of any frames whose captions are too close to their predecessor.  The output is a much shorter, non-redundant sequence of frame captions (and their corresponding frames).\n\n3.   Why must you integrate them before asking GPT-4 to write QA pairs?  \n     – If you fed GPT-4 the full set of raw captions you’d:  \n       a.  Blow past reasonable token limits,  \n       b.  Force the model to sift through tons of essentially identical frames, and  \n       c.  Risk hallucinations from overloading it with repetitive details.  \n     – By first captioning every frame (red) and then pruning out semantically duplicate ones (blue), you end up with a concise but information-rich summary of the video.  This compact, non-redundant sequence preserves all the key events and visual changes without waste.  When you hand that to GPT-4 for QA-pair construction, you get far higher-quality, focused, and consistent question–answer pairs."
    },
    {
        "question": "How are frame-level captions filtered and grouped to preserve inter-frame semantic continuity?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "To filter out redundant frames in the video, we leverage the language retrieval model BGE-M3 [4] to preliminarily remove highly similar frames through fine-grained semantic filtering. Specifically, for all sampled frame descriptions c, if the cosine similarity between the textual features f(c_i) of adjacent captions exceeds a preset threshold η, we apply a Last-In-First-Out (LIFO) filtering approach. This process results in a concise caption sequence C and the corresponding frames.",
            "To maintain semantic continuity within the groups and avoid issues such as model hallucinations and failure to follow instructions—caused by an excessive number of tokens—we adopt a continuous grouping approach to streamline frames. Specifically, for all captions C of a single video, every 15 consecutive frame captions are grouped into a group G that preserves both frame-level details and inter-frame correlations."
        ],
        "final_answer": "First, all sampled frames are captioned, and then highly similar adjacent captions are removed via semantic filtering: if the cosine similarity between two neighboring caption embeddings exceeds a threshold, the later caption is discarded using a LIFO strategy. After filtering, the remaining captions are split into consecutive chunks of 15 frames each, ensuring that each group retains detailed frame-level information and smooth semantic transitions between frames.",
        "relevant_elements": [
            "Frame-level Captioning",
            "Frames Grouping"
        ],
        "id": 2275,
        "masked_question": "How are [mask1] filtered and grouped to preserve inter-frame semantic continuity?",
        "masked_number": 2,
        "masked_elements": [
            "Frame-level Captioning",
            "Frames Grouping"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.14794v1_figure_2.png",
        "paperid": "2411.14794v1",
        "paper_path": "./MISS-QA/papers/2411.14794v1.json",
        "figure_id": "2411.14794v1_figure_2.png",
        "caption": "Figure 2: The automatic generation pipeline of VideoEspresso. (i) Question-Answer Pair Construction: We use video frame-leveled captions to extract the key frames of the video and group descriptions of these frames. Then, we prompt GPT-4 to design questions for each group of video frames. (ii) Multimodal Chain-of-Thought Annotation: We extract key evidence text and generate captions with the highest relevance to the question with GPT-4o. Additionally, we annotate spatial and temporal information for key items, which results in multimodal Chain of Thought data pairs grounded in both temporal and spatial dimensions.",
        "qtype": "Implementation_Details",
        "response": "Chain-of-thought  \n1. In the figure the red‐boxed items ([mask1]) are the raw “frame‐level captions” that come out of the InternVL2-8B captioner for each sampled video frame.  \n2. The text calls out a “Redundancy Removal” step: all of those captions are embedded with BGE-M3 and we compute cosine similarities between each pair of neighboring captions.  \n3. Whenever two adjacent captions are too similar (cosine > τ), we drop the later one using a last-in-first-out (LIFO) strategy.  This pruning cuts out visually or semantically redundant frames.  \n4. Now that we have a slimmed-down, semantically-distinct sequence of captions (and their frames), we chunk them into contiguous, fixed‐length windows.  Concretely, every 15 captions in temporal order become one “group.”  \n5. By (a) filtering out repeats via semantic‐similarity + LIFO and then (b) grouping the remaining frames into successive blocks of 15, we both eliminate redundancy and guarantee that each block preserves smooth inter-frame semantics.  \n\nAnswer  \n[mask1] are the per‐frame textual captions.  They are filtered by embedding them with BGE-M3 and dropping any caption whose cosine similarity to its predecessor exceeds a threshold (using a LIFO rule), and the surviving captions are then partitioned into fixed‐size (15‐frame) consecutive groups so that each group retains coherent, inter-frame semantic continuity."
    },
    {
        "question": "How does sampling Y*_w combine with importance sampling Y_w to generate weighted token distributions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Sampling Y*_w",
            "Importance Sampling Y_w"
        ],
        "id": 2277,
        "masked_question": "How does sampling [mask1] combine with importance sampling Y_w to generate weighted token distributions?",
        "masked_number": 1,
        "masked_elements": [
            "Sampling Y*_w"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.04350v1_figure_1.png",
        "paperid": "2410.04350v1",
        "paper_path": "./MISS-QA/papers/2410.04350v1.json",
        "figure_id": "2410.04350v1_figure_1.png",
        "caption": "Figure 1: In real data, different tokens have varying rewards, with low-reward tokens present even in winning responses. DPO treats all tokens equally, introducing noise and reducing optimization efficiency. Our TIS-DPO performs importance sampling on the optimal data distribution (where each token has equal reward) using actual data, introducing token weights to improve optimization efficiency.",
        "qtype": "Implementation_Details",
        "response": "First, let’s be precise about what the red‐boxed bit is.  If you look at the left-hand side of the “Solution” panel you see\n\n  Sampling yₜ^*   \n\nin a little red box.  That is exactly what [mask1] stands for: we are sampling tokens from the *ideal* (or “optimal”) token distribution yₜ^*—the one in which every token has the same *expected* reward.\n\nFrom that point the paper applies *token-level importance sampling* against the *real* data distribution Y_w.  Concretely, the procedure is:\n\n 1.  Pretend you could draw from the “optimal” distribution yₜ^*.  (That’s Sampling yₜ^*.)  \n 2.  In reality you only have samples from your dataset’s winning‐sequence distribution Y_w.  \n 3.  You compute, for each token yᵢ in Y_w, an importance weight  \n     \n     wᵢ = P_optimal(yᵢ)  /  P_data(yᵢ)  \n     \n    (you never actually compute P_optimal in closed form—rather you estimate these weights via your contrastive LLMs.)  \n 4.  You multiply each token’s original probability under Y_w by its weight wᵢ.  \n\nThe result of steps 3–4 is exactly what the diagram calls “Weighted Y_w.”  In the same way you get “Weighted Yₗ” for the losing sequences.  \n\nThose two *weighted* token distributions then go straight into the modified DPO objective.  Because higher-reward tokens get larger weights and low-reward tokens get down-weighted, the final optimization is *unbiased* with respect to the true optimal distribution, but entirely built from your real data."
    },
    {
        "question": "How does the TIS-DPO Objective incorporate weighted Y_l during maximum likelihood optimization?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "TIS-DPO Objective",
            "Weighted Y_l"
        ],
        "id": 2278,
        "masked_question": "How does the [mask1] incorporate [mask2] during maximum likelihood optimization?",
        "masked_number": 2,
        "masked_elements": [
            "TIS-DPO Objective",
            "Weighted Y_l"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.04350v1_figure_1.png",
        "paperid": "2410.04350v1",
        "paper_path": "./MISS-QA/papers/2410.04350v1.json",
        "figure_id": "2410.04350v1_figure_1.png",
        "caption": "Figure 1: In real data, different tokens have varying rewards, with low-reward tokens present even in winning responses. DPO treats all tokens equally, introducing noise and reducing optimization efficiency. Our TIS-DPO performs importance sampling on the optimal data distribution (where each token has equal reward) using actual data, introducing token weights to improve optimization efficiency.",
        "qtype": "Implementation_Details",
        "response": "Here’s the chain of thought:\n\n1.  In standard DPO you maximize a token‐level log‐likelihood over both the “winning” and the “losing” sequences, but you treat every token equally (i.e. all weights = 1).\n\n2.  In TIS-DPO we know that some tokens carry more “signal” than others, so we first compute an importance weight for each token in both the winning (Y_w) and losing (Y_l) sequences.\n\n3.  The red‐boxed block in the figure is the **TIS-DPO objective**, which is just a DPO‐style maximum-likelihood objective **but applied to the *weighted* sequences**.\n\n4.  In particular, the blue‐boxed block is the **weighted losing sequence** (let’s call it Weighted Y_l).  What TIS-DPO does is replace the plain Y_l in the MLE with this Weighted Y_l.\n\n5.  Concretely, if w_i is the importance weight of the i’th token in the losing response, then in the loss you have a term  \n       – ∑_i w_i log π_θ(y_l,i | y_l,<i)  \n   instead of  \n       – ∑_i log π_θ(y_l,i | y_l,<i)  \n\n6.  By doing that, TIS-DPO “incorporates weighted Y_l during maximum likelihood optimization” simply by **multiplying each token’s log‐probability by its importance weight** and then summing, exactly as you would in unweighted MLE.\n\nSo, in one sentence:\n\nAnswer:  \nThe TIS-DPO objective (red box) replaces the standard unweighted losing‐sequence term with its **weighted** counterpart (blue box), i.e. it multiplies each token’s log‐probability in Y_l by its importance weight and then maximizes that weighted sum under the model."
    },
    {
        "question": "How does Importance Sampling integrate with maximum likelihood training to correct token-level biases from DPO’s bandit formulation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Importance Sampling",
            "maximum likelihood"
        ],
        "id": 2279,
        "masked_question": "How does [mask1] integrate with [mask2] training to correct token-level biases from DPO’s bandit formulation?",
        "masked_number": 2,
        "masked_elements": [
            "Importance Sampling",
            "maximum likelihood"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.04350v1_figure_1.png",
        "paperid": "2410.04350v1",
        "paper_path": "./MISS-QA/papers/2410.04350v1.json",
        "figure_id": "2410.04350v1_figure_1.png",
        "caption": "Figure 1: In real data, different tokens have varying rewards, with low-reward tokens present even in winning responses. DPO treats all tokens equally, introducing noise and reducing optimization efficiency. Our TIS-DPO performs importance sampling on the optimal data distribution (where each token has equal reward) using actual data, introducing token weights to improve optimization efficiency.",
        "qtype": "Literature_Background",
        "response": "First, let’s unpack the two pieces in the figure:\n\n  1.  [mask1] is the red‐boxed “Importance Sampling w. Current Distribution” stage.  Here we take our real (winning) data  and compute per‐token importance weights so that, in expectation, we recover the “ideal” token‐level distribution (one in which every token in a winning response has the same expected reward).\n\n  2.  [mask2] is the blue‐boxed “Maximum likelihood” training of the final LLM.  In vanilla DPO this is just an unweighted MLE step on the winning versus losing sequences.\n\nNow: the **key innovation** of TIS‐DPO is to **splice** those importance weights directly into the MLE update.  Concretely:\n\n  •  We draw tokens  from our real data (the “current distribution”),  \n    compute an importance ratio for each token that pushes us toward the “optimal” token‐reward distribution (that’s the red box),  \n    and then  \n\n  •  Instead of doing  \n       \\sum_{t}  log π_θ(y_t∣y_{<t})  \n    we do  \n       \\sum_{t}  w_t ⋅ log π_θ(y_t∣y_{<t}),  \n    where  w_t  is exactly the importance weight we just computed (this is the blue box).\n\nBecause importance sampling gives us an unbiased estimator of any expectation under the ideal distribution, the result is that every token in your MLE loss is now scaled by its true reward—**exactly** correcting the token‐level bandit bias that plain DPO would have left in place."
    },
    {
        "question": "How do Weighted Y_w and Weighted Y_l adjust gradient estimates compared to DPO’s bandit-based reward distribution?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Weighted Y_w",
            "Weighted Y_l"
        ],
        "id": 2280,
        "masked_question": "How do [mask1] and [mask2] adjust gradient estimates compared to DPO’s bandit-based reward distribution?",
        "masked_number": 2,
        "masked_elements": [
            "Weighted Y_w",
            "Weighted Y_l"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.04350v1_figure_1.png",
        "paperid": "2410.04350v1",
        "paper_path": "./MISS-QA/papers/2410.04350v1.json",
        "figure_id": "2410.04350v1_figure_1.png",
        "caption": "Figure 1: In real data, different tokens have varying rewards, with low-reward tokens present even in winning responses. DPO treats all tokens equally, introducing noise and reducing optimization efficiency. Our TIS-DPO performs importance sampling on the optimal data distribution (where each token has equal reward) using actual data, introducing token weights to improve optimization efficiency.",
        "qtype": "Literature_Background",
        "response": "In standard DPO you treat every “winning” token as +1 and every “losing” token as −1 and just push the whole sequence in that single direction.  What TIS‐DPO does instead is insert two re‐weighting blocks – the red box ([mask1]) sitting over the winning sequence and the blue box ([mask2]) over the losing one – which multiply each token’s log‐probability gradient by a scalar importance weight inferred from its estimated reward.  Concretely:\n\n 1.  In the red box (“Weighted y_w”) you take each token in the winner and scale its ∇ log π contribution by w_i ∝ p*(token_i)/p(token_i), so that high–reward tokens get a larger positive push.\n 2.  In the blue box (“Weighted y_l”) you do exactly the same thing for the loser but with the opposite sign, again using that same ratio to decide how hard to push down tokens of varying “badness.”\n\nBecause those w_i’s are exactly the importance‐sampling ratios between the *ideal* token‐reward distribution and your *actual* data distribution, the resulting gradient is an unbiased estimator of the true PPO/DPO gradient under the optimal token‐level reward model – whereas vanilla DPO’s “every token = ±1” bandit reward inevitably introduces a lot of noise."
    },
    {
        "question": "How can integrated modeling of behavior-level and item-level dependencies enhance interaction-level dependency learning versus attention-only methods?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "Some prior approaches (e.g., MB-GMN(Xia et al., 2021 ###reference_b29###), MB-GCN(Jin et al., 2020 ###reference_b11###), and DMT(Gu et al., 2020 ###reference_b7###)) involve initially aggregating items within each behavior to obtain a cohesive representation, followed by modeling dependencies across all behaviors using attention or weighted summation operations. These approaches model the multi-behavior dependency between interactions with same behavior type, which we called behavior-level dependency (e.g., blue arrow). Recently, MB-STR(Yuan et al., 2022 ###reference_b35###) proposed multi-behavior multi-head self-attention to model multi-behavior dependency between interactions with same item, which we called item-level dependency (e.g., green arrow). Nevertheless, multi-behavior dependency between interactions with inconsistent behavior types and items is significant as well, which we defined as interaction-level dependency (e.g., red arrow).",
            "To learn interaction-level multi-behavior dependency, we introduce the incidence matrix A taking both item-level and behavior-level semantics into consideration. To achieve this goal, we calculate each entry a_{i,j} as following, where e^I are item semantic embedding representations and e^B are behavior semantic embedding representations. First, we operate dot product on item-specific pair and behavior-specific pair to get A^I and A^B respectively, which learns the multi-behavior dependency from item-level and behavior-level perspectives synchronously. To learn item- and behavior-level dependency in an integrated manner, an inner product is performed on item-level dependency representation and behavior-level dependency representation."
        ],
        "final_answer": "By jointly computing a dependency score for each pair of interactions as the inner product of both their item-level and behavior-level similarity scores, the model builds an incidence matrix that captures ‘‘synchronous’’ multi-behavior signals. This integrated representation feeds into graph convolution to propagate and refine dependencies across interactions. In contrast, attention-only methods either merge behaviors first or attend only within one semantic view, missing the richer, cross-semantic interaction-level patterns that the combined inner-product approach uncovers.",
        "relevant_elements": [
            "behavior-level",
            "item-level",
            "interaction-level"
        ],
        "id": 2281,
        "masked_question": "How can integrated modeling of [mask1] and item-level dependencies enhance [mask2] dependency learning versus attention-only methods?",
        "masked_number": 2,
        "masked_elements": [
            "behavior-level",
            "interaction-level"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.12179v1_figure_1.png",
        "paperid": "2411.12179v1",
        "paper_path": "./MISS-QA/papers/2411.12179v1.json",
        "figure_id": "2411.12179v1_figure_1.png",
        "caption": "Figure 1. motivation of our work",
        "qtype": "Literature_Background",
        "response": "Here’s the key idea in plain English:\n\n  1. In the picture, the little red‐outlined boxes pick out two events whose link (drawn by the red arrow) is what the authors call an “interaction‐level, multi‐behavior dependency” (that’s your [mask1]).  2. The little blue‐outlined boxes pick out two events that share exactly the same behavior type (that’s the so-called “behavior-level dependency,” your [mask2]).  \n\nPrior multi-behavior methods either  \n– aggregated each behavior stream on its own and then used attention or a weighted sum over those (so they only ever learn “behavior-level” links), or  \n– used multi-head self-attention to capture “item-level” links (same item, different behaviors).  \n\nWhat this paper points out (Challenge 1) is that there are also rich “interaction-level” links – e.g. buying a phone strongly raises the chance you’ll later click on an accessory – and that those links often cross both items and behavior types.  \n\nSo what they do instead of plain attention is:\n\n  • They build a small graph whose nodes are *all* the individual interactions in your history.  \n  • They set edge‐weights by taking the inner product of the *item*‐embeddings and the *behavior*‐embeddings simultaneously (so each edge reflects both item-level *and* behavior-level affinity).  \n  • They then run a few rounds of graph convolution over that adjacency matrix, which gives them direct “interaction-level” dependency representations at increasingly higher orders (i.e. two-hop, three-hop, etc.).  \n\nBecause you’re literally propagating signals over a learned graph you get  \n– a much denser, multi-order view of how a *purchase* of X drives a *click* on Y, or how a *favorite* of A drives an *add-to-cart* of B, etc.,  \n– instead of relying on a single dot‐product or weighted sum in an attention head.  \n\nThis ends up producing far richer behavior‐level dependency features in practice (the blue‐arrow links in the figure).  Empirically they show that this graph‐based fusion of interaction-level *and* item-level cues outperforms any of the attention‐only baselines that tried to learn those same cross‐behavior or cross‐item relationships by a simple “attend-and-sum” step."
    },
    {
        "question": "How does interaction-view inform granularity2 preference formation across diverse sessions?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "All the dependency representations at various orders will be utilized to be the input of multifaceted sequential pattern generator.",
            "To encode the sequential pattern of user , we need to inject the sequential information into the interaction-level dependency representation of each historical interactions in sequence .",
            "The global sequential pattern reflects user’s stable long-term preference. Nevertheless, user’s short-term preference varies from different time scales which is fluctuated. To model short-term preference, we first divide the interaction sequence into sessions.",
            "We propose a multi-grained multi-head self-attention layer to encode the multi-grained preference in sessions divided by different time scales. First, to create multi-grained user intent, we group the last items with different lengths in a session. Then, we concatenate them within the group to form a raw group representation. At last, linear transformation is performed on these group representations to represent the multi-grained user queries.",
            "After generating the multi-grained query representations, we concatenate them into a whole query matrix . Then, multi-head attention layer is performed and the attention weights are calculated as … We get … and perform  pooling on the weight  to balance the influence of different query granularity.",
            "Given  multi-grained preference representations , we get a whole sequence preference representation  with time scale . In M-GPT, we select two different time scales  and  to learn multifaceted preference representations."
        ],
        "final_answer": "The interaction-view—the set of learned interaction-level dependency embeddings from each session—is fed into a multi-grained self-attention mechanism that groups recent interactions at different lengths per session. By concatenating and linearly transforming these groupings, the model produces coarse‐grained (granularity2) query vectors (e.g., “Electronics” or “Sports”). These are then attended to and pooled across each session, yielding high-level (granularity2) preference representations that capture shared intents across diverse sessions.",
        "relevant_elements": [
            "interaction-view",
            "granularity2",
            "sessions"
        ],
        "id": 2282,
        "masked_question": "How does [mask1] inform granularity2 preference formation across diverse sessions?",
        "masked_number": 1,
        "masked_elements": [
            "interaction-view"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.12179v1_figure_1.png",
        "paperid": "2411.12179v1",
        "paper_path": "./MISS-QA/papers/2411.12179v1.json",
        "figure_id": "2411.12179v1_figure_1.png",
        "caption": "Figure 1. motivation of our work",
        "qtype": "Literature_Background",
        "response": "Chain‐of‐thought  \n1. In the diagram the only region outlined in **solid red** is the two “purchase” interactions in Session 1 (the smartphone at T₄ and the earphones at T₆).  \n2. Those red‐boxed nodes are exactly the **target‐behavior** events whose embeddings are produced by the Interaction-Level Dependency Extractor.  \n3. In the Multifaceted Sequential Pattern Generator those embeddings are then run through the **multi-grained self-attention** hierarchy.  \n4. At **granularity 1** we attend over the last few items within each session (e.g. “Cellphone,” “Earphone,” etc.).  \n5. At **granularity 2** we pool even more coarsely—grouping across sessions into broad semantic categories (in the picture, “Electronic” versus “Sports”).  \n6. Concretely, the **red-boxed purchase events** drive that second, coarser attention stage: by clustering the interaction embeddings of those purchases across sessions, the model learns that both the smartphone and earphone purchases sit in “Electronics,” whereas the Session 2 purchase (the shoes) sits in “Sports.”  \n\nAnswer  \nMask₁ (the red‐boxed purchase interactions) is exactly what the coarser (granularity 2) self-attention pools over.  By taking those purchase‐event embeddings from every session, concatenating them, and running them through the second time‐scale attention head, the model induces a high-level, category‐wide preference (e.g. “Electronics” vs. “Sports”) that summarizes the user’s long-term interests across diverse sessions."
    },
    {
        "question": "How does behavior-aware multi-grained preference extraction enrich sequential pattern encoding across sessions?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "The global sequential pattern reflects user’s stable long-term preference. Nevertheless, user’s short-term preference varies from different time scales which is fluctuated. To model short-term preference, we first divide the interaction sequence into sessions.",
            "Inspired by (Zhang et al., 2023  ###reference_b37###), we propose a multi-grained multi-head self-attention layer to encode the multi-grained preference in sessions divided by different time scales.",
            "First, to create multi-grained user intent, we group the last items with different lengths in a session. Then, we concatenate them within the group to form a raw group representation. At last, linear transformation is performed on these group representations to represent the multi-grained user queries.",
            "Generated multi-grained queries representation reflects characteristics of short-term sequence including inherent priority and local invariance.",
            "After generating the multi-grained query representations, we concatenate them into a whole query matrix Q. Then, multi-head attention layer is performed and the attention weights are calculated as ... We get the combination of multi-head attention weights, and perform pooling on the weight W to balance the influence of different query granularity."
        ],
        "final_answer": "By splitting a user’s recent interactions into sessions and, within each session, grouping the last few interactions at different lengths (“granularities”) into raw group embeddings, M-GPT constructs multi-grained queries that capture both coarse and fine short-term intents. These behavior-aware queries are fed through a multi-head attention over the session’s interactions and then pooled to yield session-level multi-grained preference vectors. Injecting these enriched local preferences back alongside the global sequential encoding allows the model to more precisely represent users’ fluctuating interests across sessions at multiple time scales.",
        "relevant_elements": [
            "behavior-aware multi-grained preference"
        ],
        "id": 2284,
        "masked_question": "How does [mask1] extraction enrich sequential pattern encoding across sessions?",
        "masked_number": 1,
        "masked_elements": [
            "behavior-aware multi-grained preference"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.12179v1_figure_1.png",
        "paperid": "2411.12179v1",
        "paper_path": "./MISS-QA/papers/2411.12179v1.json",
        "figure_id": "2411.12179v1_figure_1.png",
        "caption": "Figure 1. motivation of our work",
        "qtype": "Experimental_Results",
        "response": "First, let us align what we see in the red‐boxed area of Figure 1 with the text of §3.4.  In that bottom panel the authors have taken the interaction-level dependency outputs from the graph module and are now “zooming in” on each session to pull out user preference at multiple granularities.  Concretely:\n\n 1.  They slice the full interaction sequence into contiguous time-based sessions (e.g. Session 1, Session 2).  \n 2.  Within each session they form several “last‐k” groups (granularity 1 might be the last 1–2 items, granularity 2 the last 3–5, etc.) and concatenate those items’ interaction embeddings into small raw vectors.  \n 3.  A linear transform turns each raw group into a multi-grained query vector Q  \n 4.  Those Q’s drive a multi-head self-attention over the session’s interaction embeddings to yield a behavior-aware, multi-grained preference embedding Hℓ for that session.  \n 5.  Finally, positional averages or pooling fuse these embeddings across sessions (and are then combined with the global transformer‐style encoding) to produce the enriched sequential representation.\n\nBy doing this “behavior-aware multi-grained preference” extraction, the model is no longer relying on a single, monolithic self-attention over the entire history.  Instead it (a) isolates each session, (b) distills short-term signals at several granularities within that session, and (c) allows those session-level vectors to modulate the final transformer encoding.  As a result you get a sequence encoder that is both global (long‐term) and deeply aware of local, session‐specific, multi-grained behavioral patterns."
    },
    {
        "question": "What potential bias arises from using AdaIN features in the TransDiffuser conditioning?",
        "relevant_section_ids": [
            "6"
        ],
        "relevant_context": [
            "We propose a novel way of guiding the diffusion process by incorporating Adaptive Instance Normalisation (AdaIN) features. This allows transferring features from the style image to the content image without bias."
        ],
        "final_answer": "No bias arises; the use of AdaIN features is explicitly designed to transfer style features without introducing bias.",
        "relevant_elements": [
            "AdaIN",
            "TransDiffuser"
        ],
        "id": 2287,
        "masked_question": "What potential bias arises from using [mask1] features in the [mask2] conditioning?",
        "masked_number": 2,
        "masked_elements": [
            "AdaIN",
            "TransDiffuser"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.03558v1_figure_2.png",
        "paperid": "2408.03558v1",
        "paper_path": "./MISS-QA/papers/2408.03558v1.json",
        "figure_id": "2408.03558v1_figure_2.png",
        "caption": "Figure 2: The architecture of the proposed method. The content and style images are encoded using a pretrained VQ-GAN encoder. The encoded input is passed through the diffusion prior conditioned on the AdaIN [21] features. VQ-GAN decoder is then used to obtain the resultant image. The dotted line indicates that the diffusion prior is trained separately from the decoder.",
        "qtype": "Others",
        "response": "Let’s unpack the two masks first:\n\n 1. Mask₁ (red box) is the AdaIN module A(⋅) – i.e. the learned mean‐and‐variance “style codes” extracted from the style (and content) image by the frozen CNN encoder.  \n 2. Mask₂ (blue box) is the TransDiffuser network – the transformer‐style noise predictor (the core of the diffusion prior) that we condition on everything else.\n\nIf you feed those AdaIN codes straight into TransDiffuser as extra conditioning, you run the risk of **biasing the diffusion prior toward exactly those style statistics** – in other words, you will tend to over-emphasize the low-level colour and texture moments (the “mean and variance” that AdaIN carries) at the possible expense of faithfully preserving the higher-level content structure. In practice this can show up as  \n  • over-stylization (too much colour/texture drift)  \n  • loss of fine semantic detail  \n\nIn our D2Styler design we take special care (multiscale feature extraction, careful loss balancing, frozen encoders, etc.) to **avoid** letting that AdaIN conditioning become an overpowering bias."
    },
    {
        "question": "What ethical concerns could emerge from latent space quantization via VQ-GAN encoder’s Quantizer with diverse cultural styles?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "VQ-GAN encoder",
            "Quantizer"
        ],
        "id": 2288,
        "masked_question": "What ethical concerns could emerge from latent space quantization via [mask1]’s Quantizer with diverse cultural styles?",
        "masked_number": 1,
        "masked_elements": [
            "VQ-GAN encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.03558v1_figure_2.png",
        "paperid": "2408.03558v1",
        "paper_path": "./MISS-QA/papers/2408.03558v1.json",
        "figure_id": "2408.03558v1_figure_2.png",
        "caption": "Figure 2: The architecture of the proposed method. The content and style images are encoded using a pretrained VQ-GAN encoder. The encoded input is passed through the diffusion prior conditioned on the AdaIN [21] features. VQ-GAN decoder is then used to obtain the resultant image. The dotted line indicates that the diffusion prior is trained separately from the decoder.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n\n1. In the figure, “[mask1]’s Quantizer” refers to the VQ-GAN encoder’s quantization step Q(·) (highlighted in red), which snaps continuous image features to the nearest entries in a learned codebook.  \n2. That codebook is learned from large art datasets (e.g. WikiArt, OpenImages) that may unevenly represent different cultural traditions, artists, or communities.  \n3. By collapsing rich, high-dimensional style and iconographic details into a finite set of discrete tokens, subtleties of a given culture’s motifs or techniques can be smoothed over or conflated with others.  \n4. At inference time, those coarse, shared tokens can lead to:  \n   – unintentional stereotyping (e.g. “generic tribal pattern” rather than authentic regional motifs),  \n   – cultural flattening or erasure (distinct traditions mapped to the same codebook vectors),  \n   – misappropriation (sacred or ritualistic symbols reproduced out of context),  \n   – and digital “style poaching” (machine memorizing and re-stitching copyrighted or community-owned artworks without attribution or consent).  \n5. Finally, because the quantizer is a black-box neural codebook, it’s hard to trace which discrete codes correspond to which real-world cultural sources, exacerbating issues of fairness, transparency, and redress if an output offends or misrepresents a community.\n\nAnswer:\n\nBecause the VQ-GAN quantizer discretizes a rich spectrum of cultural styles into a limited codebook, it risks:\n• Cultural flattening and stereotyping (distinct traditions “averaged” into generic tokens)  \n• Erasure of subtle, community-specific artistic elements  \n• Unconsented appropriations or reproductions of sacred or copyrighted motifs  \n• Biases and misrepresentations baked into the codebook (e.g. over-representing some cultures while under-representing others)  \n• Lack of transparency or accountability around which code vectors map to which real-world artworks or traditions."
    },
    {
        "question": "What are the limitations of using Pearson correlation for connectome construction affecting STFE outputs?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "Instead of using the Pearson correlation as the connectome association to form the network structure, a few studies have proposed network structure learning strategies through modeling the potential nonlinear spatio-temporal inter-regional relationship [20, 21, 22]. Thus far, only the lowest level connectivity has been considered."
        ],
        "final_answer": "Pearson correlation only captures linear, lowest‐level inter-regional connectivity and cannot model richer, nonlinear spatio-temporal relationships. This limits the diversity of connectomes that can be generated from STFE outputs, motivating the multi‐level feature extraction approach.",
        "relevant_elements": [
            "Pearson Correlation",
            "STFE"
        ],
        "id": 2289,
        "masked_question": "What are the limitations of using [mask1] for connectome construction affecting STFE outputs?",
        "masked_number": 1,
        "masked_elements": [
            "Pearson Correlation"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.03358v1_figure_1.png",
        "paperid": "2408.03358v1",
        "paper_path": "./MISS-QA/papers/2408.03358v1.json",
        "figure_id": "2408.03358v1_figure_1.png",
        "caption": "Figure 1: The overall workflow of the MLC-GCN contains: a data preprocessing module (a), a multi-graph generator (b), and a multi-level GCN-based predictor. In module a, a brain atlas is used to extract n time series from n brain regions. In b, temporal embedding and STFE are used to extract sparse temporal features at different hierarchy and to calculate the corresponding graphs (connectomes). In c, GCNs are used to encode the generated graphs at different levels into higher level graphical data features. These features are concatenated and input to a multi-layer perceptron (MLP) to classify AD.",
        "qtype": "Others",
        "response": "First, note that the red‐boxed module in the figure is the “Pearson Correlation” step – i.e. the standard way of turning two region‐wise time‐series into a single scalar edge weight.  The question is asking: what are the drawbacks of using plain Pearson correlation to build your connectomes, such that it will in turn limit what the STFE front‐end can feed into the GCNs?\n\nChain of thought:\n1.  Pearson correlation is a single, static, zero‐lag, linear measure of association between two entire time-series.  \n2.  By collapsing each pair of regions to one number you (a) throw away any fine‐grained temporal structure (lags, transient synchrony, frequency‐specific coupling) and (b) can only capture linear co-fluctuations (no nonlinear, higher‐order or directed interactions).  \n3.  Pearson is notoriously sensitive to noise, outliers, motion artifacts and global confounds – these tend to induce spurious correlations or mask true ones.  \n4.  Because it produces a dense matrix, it often requires arbitrary thresholding or sparseness constraints, which further distort the real underlying connectivity.  \n5.  In short, by forcing all of STFE’s rich, multi‐scale, spatio-temporal feature maps down to a single linear coefficient per edge you prevent the downstream GCNs from ever seeing (and exploiting) any transient, scale-specific or nonlinear relationships.\n\nAnswer:\nThe plain Pearson‐correlation step simply computes one zero‐lag, linear coefficient per region–region pair.  That\n• collapses all the rich temporal structure (lags, dynamics, scale‐specific rhythms) that the STFE extracts  \n• cannot capture nonlinear or directed interactions  \n• is highly susceptible to noise, motion and global confounds  \n• yields a dense matrix that must be arbitrarily thresholded  \nAs a result, most of the very multi-scale, spatio-temporal detail coming out of the STFEs is lost before you ever build the connectome, limiting the expressive power of the entire MLC-GCN pipeline."
    },
    {
        "question": "How might embedding biases from STFE affect fairness in GCN-driven AD classification?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "STFE",
            "GCN"
        ],
        "id": 2290,
        "masked_question": "How might embedding biases from [mask1] affect fairness in [mask2]-driven AD classification?",
        "masked_number": 2,
        "masked_elements": [
            "STFE",
            "GCN"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.03358v1_figure_1.png",
        "paperid": "2408.03358v1",
        "paper_path": "./MISS-QA/papers/2408.03358v1.json",
        "figure_id": "2408.03358v1_figure_1.png",
        "caption": "Figure 1: The overall workflow of the MLC-GCN contains: a data preprocessing module (a), a multi-graph generator (b), and a multi-level GCN-based predictor. In module a, a brain atlas is used to extract n time series from n brain regions. In b, temporal embedding and STFE are used to extract sparse temporal features at different hierarchy and to calculate the corresponding graphs (connectomes). In c, GCNs are used to encode the generated graphs at different levels into higher level graphical data features. These features are concatenated and input to a multi-layer perceptron (MLP) to classify AD.",
        "qtype": "Others",
        "response": "Here’s a step‐by‐step unpacking of the question:\n\n1.   Identify [mask1] and [mask2]  \n     •  [mask1] is the red‐boxed module in the left‐middle of Fig. 1 – that is the stack of Spatio‐Temporal Feature Extractors (STFEs), together with the 1D‐CNN “Embedding” layer that feeds into them.  \n     •  [mask2] is the blue‐boxed module in the right‐middle of Fig. 1 – the parallel Graph Convolutional Network (GCN) encoders (each followed by a small MLP) whose outputs are concatenated and fed to a final MLP classifier.\n\n2.   What kind of “embedding bias” can occur in mask1?  \n     •  The 1D‐CNN embedding and downstream STFEs learn a low‐dimensional representation of each ROI’s BOLD time series. If the training data are unbalanced (for example, over-represented by one age group, sex, or scanner site), the embedding network can pick up spurious temporal patterns that correlate with those confounds rather than true disease signal.  \n     •  Even within disease classes, subtle demographic or site‐specific artifacts can get baked into the embedding weights, so that the “features” produced by STFE carry downstream bias.\n\n3.   How would such biased embeddings affect the GCN-based predictor in mask2?  \n     •  The GCNs take as input both the node features (the STFE embeddings) and the adjacency (correlation) matrices computed from them. Any bias in those embeddings will therefore:  \n       –  Skew the learned graph topology and node representations in a way that reflects the over-represented confound rather than pure disease effects.  \n       –  Cause the classifier to have different error rates for under-represented subgroups (e.g. a higher false-positive rate in a demographic group that was under-sampled during embedding‐stage training).  \n     •  In fairness terms, this translates into a breakdown of standard criteria such as demographic parity (different groups seeing systematically different positive‐prediction rates) or equalized odds (different false-positive/false-negative rates across groups).\n\n4.   Summary answer  \n     Embedding biases introduced by the 1D-CNN + STFE stack (mask1) will be encoded directly into the node features and graph structure that the suite of GCNs (mask2) consumes. Because the GCN relies on those biased features and edges to learn its decision boundary, any systematic representation skew—say toward a particular age, sex or scanner cohort—will lead to disparate prediction performance across subgroups. In practice you would observe unequal false-positive or false-negative rates, violating fairness metrics in your GCN-driven AD classifier."
    },
    {
        "question": "What motivates leveraging causation-sensitive influence rather than correlation-sensitive suggestion in bundle recommendation?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "There has been an initial exploration of the impact of item relationships, often grounded in symmetric correlations, on purchasing decisions for a bundle [10  , 1  ].",
            "In real-world scenarios, the influence between items within bundles is usually asymmetric, with anchor items playing a significant role.",
            "Symmetric correlation-based recommendation models, as illustrated in Figure 1, fall short in accurately capturing user preferences.",
            "While blazers and cravats may be frequently purchased together, their influence on each other is not necessarily symmetrical.",
            "Users with an interest to blazers might consider to combine with cravats as accessories, while others primarily interested in cravats may already have suitable blazers, making the combination with pocket squares more logical."
        ],
        "final_answer": "Because real-world item–item relationships in bundles are inherently asymmetric—certain “anchor” items drive the decision to include other items—purely symmetric, correlation-based suggestions fail to capture these directional influences. By modeling causation-sensitive (asymmetric) relations, the system better reflects how one item actually influences the inclusion of another, improving the accuracy of bundle recommendations.",
        "relevant_elements": [
            "causation-sensitive recommendation",
            "correlation-sensitive recommendation"
        ],
        "id": 2293,
        "masked_question": "What motivates leveraging [mask1] rather than correlation-sensitive suggestion in bundle recommendation?",
        "masked_number": 1,
        "masked_elements": [
            "causation-sensitive recommendation"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.08906v1_figure_1.png",
        "paperid": "2408.08906v1",
        "paper_path": "./MISS-QA/papers/2408.08906v1.json",
        "figure_id": "2408.08906v1_figure_1.png",
        "caption": "Figure 1: Motivating examples of multi-prospect causation in bundle recommendation.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step‐by‐step:\n\n1.  In traditional bundle recommendation (the “correlation‐sensitive” approach), items are treated symmetrically: if A and B co‐occur frequently in bundles, the model assumes “A ⇒ B” and “B ⇒ A” equally strongly.  \n2.  The paper points out (cf. the blazer–cravat example) that in reality these influences are often asymmetric.  A user who wants a blazer will often add a cravat as an accessory (“blazer ⇒ cravat”), but someone shopping for a cravat may already own a suitable blazer and instead would pair it with something else (“cravat ⇏ blazer”).  \n3.  A purely correlation‐based model cannot distinguish those two distinct intents, so it will sometimes make implausible bundle suggestions.  \n4.  To fix this, the authors introduce the red‐boxed module—i.e. “causation‐sensitive recommendation” (or multi-prospect causation)—that explicitly models directed, anchor-to-complement relations between items.  \n5.  This causation‐sensitive view lets the system learn that “blazer ⇒ cravat” is a stronger, one‐way signal than “cravat ⇒ blazer,” yielding more coherent, intention‐aligned bundles.  \n\nAnswer  \nWhat motivates leveraging causation‐sensitive recommendation rather than a correlation‐sensitive one is exactly this need to capture the inherently asymmetric (one-way) influence among bundle items. By modeling directed causation instead of mere co-occurrence, the recommender can align much more closely with true user intent and thus produce better bundle suggestions."
    },
    {
        "question": "How does multi-prospect causation assign high and low affect weights across item relations?",
        "relevant_section_ids": [
            "3.3.1",
            "3.3.2"
        ],
        "relevant_context": [
            "Assuming that causation-sensitive relationships exist among items frequently purchased together, BunCa employs Multi-Prospect Causation Network (MPCNet) to explicitly model asymmetric associations between items.",
            "For the p-th prospect, the weight w_{i→j} signifies the influence from item i to item j based on various user preferences and bundling strategies, derived as follows: w_{ij}^p = σ(W_src^p v_i + W_dst^p v_j + b^p).",
            "In the p-th prospect, the asymmetric causation matrix C^p, representing the causation-sensitive relationships among items at fine-grained level, is computed by the attention mechanism concept of GATv2. The weight α_{ij} describes how much item i is influenced by item j.",
            "The asymmetric relationships obtained from MPCNet are utilized to encode the latent representation of item i in the p-th prospect, formulated as follows: h_i^p = DNN( ∑_{j∈N(i)} α_{ji} · v_j ).",
            "Subsequently, the multi-prospect item representation is devised using the residual connection method as: h_i = v_i + γ · ∑_{p=1}^P h_i^p, where γ controls the overall influence of the causation-sensitive relations."
        ],
        "final_answer": "MPCNet creates, for each prospect, an asymmetric attention matrix over item pairs where each directed weight α_{i→j} quantifies how strongly item i ‘causes’ or influences item j under that prospect. High attention scores correspond to strongly affecting (high-affect) item relations, while low scores correspond to weakly affecting (low-affect) ones. These prospect-specific weights are then aggregated via a residual connection (scaled by γ) to form the final multi-prospect item representations.",
        "relevant_elements": [
            "Multi-prospect causation",
            "Highly affect",
            "Lowly affect"
        ],
        "id": 2296,
        "masked_question": "How does [mask1] assign high and low affect weights across item relations?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-prospect causation"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.08906v1_figure_1.png",
        "paperid": "2408.08906v1",
        "paper_path": "./MISS-QA/papers/2408.08906v1.json",
        "figure_id": "2408.08906v1_figure_1.png",
        "caption": "Figure 1: Motivating examples of multi-prospect causation in bundle recommendation.",
        "qtype": "Implementation_Details",
        "response": "The thing boxed in red is nothing mystical that lives outside of the paper’s text – it is simply the Multi-Prospect Causation Network (MPCNet) in action.  MPCNet is exactly the piece of the model that “decides” that some item‐to‐item links should carry a lot of influence (“high affect”) and others almost none (“low affect”).  Here is how it does it, step by step:\n\n1.  **Multiple Prospects**  \n    – We posit K different “prospects” (think of these as different purchase‐motivation patterns or bundling strategies).  \n    – For each prospect k we have two small parameter matrices, \\(W_s^k\\) and \\(W_d^k\\), plus a vector \\(a^k\\) and a bias \\(b^k\\).  \n\n2.  **Pairwise Scoring**  \n    – Given a candidate _source_ item i (say, a pair of shoes) and a _destination_ item j (say, a tie), for prospect k we compute a raw score  \n      \n      \\( e_{i\\to j}^k \\;=\\; \\mathrm{LeakyReLU}\\Bigl(\\,a^{k\\!T}\\bigl[\\,W_s^k\\,r_i \\,\\Vert\\, W_d^k\\,r_j \\bigr]\\;+\\;b^k\\Bigr)\\,. \\)  \n      \n      – Here \\(r_i\\) and \\(r_j\\) are the current embeddings of items i and j;  \n      – “\\(\\Vert\\)” means concatenation.  \n\n3.  **Normalizing into Attention Weights**  \n    – We turn these raw scores into proper attention weights by a softmax (or scaled‐softmax) over all of i’s neighbours:  \n      \n      \\( w_{i\\to j}^k \\;=\\; \\frac{\\exp\\!\\bigl(e_{i\\to j}^k/\\sqrt{d}\\bigr)}{\\sum_{j'\\in\\mathcal N(i)}\\exp\\!\\bigl(e_{i\\to j'}^k/\\sqrt{d}\\bigr)}\\,. \\)  \n      \n    – If \\(e_{i\\to j}^k\\) is large, \\(w_{i\\to j}^k\\) will be large (“high affect” – shown as a thick yellow arrow in the red‐boxed picture).  \n    – If \\(e_{i\\to j}^k\\) is small, \\(w_{i\\to j}^k\\) will be small (“low affect” – shown as a thin orange arrow).\n\n4.  **Aggregating**  \n    – Finally, each item i’s embedding is updated by taking a weighted sum of its neighbours’ embeddings under each prospect and adding back a little of its original embedding (a residual):  \n      \n      \\( r'_i \\;=\\; \\alpha\\,r_i \\;+\\;\\sum_{k=1}^K\\sum_{j\\in\\mathcal N(i)} w_{i\\to j}^k\\,r_j\\,. \\)\n\nBecause each prospect k has its own scoring parameters, MPCNet can learn to give very high attention to certain pairs of items under one prospect (the yellow, “highly affect” links) and very low attention under another (the orange, “lowly affect” links). That is how the red‐boxed module automatically assigns high and low affect weights across all of the item–item relations."
    },
    {
        "question": "How does geometry cloak compute perturbation masks from pre-defined pattern point clouds?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "geometry cloak",
            "pre-defined pattern"
        ],
        "id": 2298,
        "masked_question": "How does [mask1] compute perturbation masks from [mask2] point clouds?",
        "masked_number": 2,
        "masked_elements": [
            "geometry cloak",
            "pre-defined pattern"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.22705v1_figure_1.png",
        "paperid": "2410.22705v1",
        "paper_path": "./MISS-QA/papers/2410.22705v1.json",
        "figure_id": "2410.22705v1_figure_1.png",
        "caption": "Figure 1:  Overview of our scenario. (a) Images without protection. Images can be easily reconstructed into 3D models by malicious users with TGS [52], posing a threat to the copyright of the image owner.\n(b) Digital Watermarking offers a solution by embedding copyright messages into the view-image before 3D reconstruction. However, the embedded message cannot be extracted from novel rendered views. (c) Geometry Cloak. Our geometry cloak utilizes the disturbance-prone components of TGS, achieving view-specific watermark embedding. Our method can compromise the unauthorized reconstructed 3D model while providing a verifiable pattern for copyright claim.",
        "qtype": "Implementation_Details",
        "response": "Here’s the key idea in plain English: in order to make sure we only “poison” the pixels that actually matter for the 3 D reconstruction (i.e. those that come from your object point‐cloud) we first turn that 3 D point‐cloud into a 2 D “where‐to‐perturb” mask, and then run PGD only inside that mask.\n\n   \n1. TGS decodes your single view into an explicit point‐cloud P (this is your “[mask2]”).  \n2. For a chosen camera/view direction we take P and project every 3 D point into pixel coordinates (using the known intrinsics/extrinsics).  \n3. We rasterize those projected points into a binary foreground silhouette—in other words we mark every pixel that “sees” at least one point of P as a 1, everything else as 0.  \n4. That binary image is our perturbation mask (“[mask1]”). In our PGD loop we multiply the gradient update at each pixel by that mask, so no adversarial noise ever leaks into the background.  \n\nBy doing that we exploit exactly the pixels that feed into TGS’s geometry decoder, and nowhere else."
    },
    {
        "question": "How does geometry cloak compute perturbation masks from pre-defined pattern point clouds?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "geometry cloak",
            "pre-defined pattern"
        ],
        "id": 2298,
        "masked_question": "How does [mask1] compute perturbation masks from [mask2] point clouds?",
        "masked_number": 2,
        "masked_elements": [
            "geometry cloak",
            "pre-defined pattern"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.22705v1_figure_1.png",
        "paperid": "2410.22705v1",
        "paper_path": "./MISS-QA/papers/2410.22705v1.json",
        "figure_id": "2410.22705v1_figure_1.png",
        "caption": "Figure 1:  Overview of our scenario. (a) Images without protection. Images can be easily reconstructed into 3D models by malicious users with TGS [52], posing a threat to the copyright of the image owner.\n(b) Digital Watermarking offers a solution by embedding copyright messages into the view-image before 3D reconstruction. However, the embedded message cannot be extracted from novel rendered views. (c) Geometry Cloak. Our geometry cloak utilizes the disturbance-prone components of TGS, achieving view-specific watermark embedding. Our method can compromise the unauthorized reconstructed 3D model while providing a verifiable pattern for copyright claim.",
        "qtype": "Implementation_Details",
        "response": "Here is what’s going on in the red‐boxed “Geometry Cloak” module (mask₁) and how it uses the blue‐boxed pre‐defined point cloud (mask₂) to build its perturbation mask:\n\n1.   We start with a 2D point‐cloud “watermark” P*, e.g. the little “9” shape (that is mask₂).  \n2.   We run TGS on our current cloaked image x, decode out its explicit 3D point cloud G(x), and then project that cloud into the same 2D camera view as P*. Let us call that projection π(G(x)).  \n3.   We measure how far apart π(G(x)) is from our target P* via the Chamfer‐Distance loss  \n     L(x) = CD( π(G(x)), P* ).  \n4.   We back‐propagate ∂L/∂x through the entire TGS pipeline all the way to the image pixels. This gives us a pixel‐wise gradient telling us how to nudge x so that π(G(x)) moves closer to P*.  \n5.   We only allow those nudges inside the object silhouette (we multiply by a binary foreground mask so that the background never changes).  \n6.   Finally we do a small, projected‐gradient‐descent (PGD) step on x, clip to our maximum perturbation ε, and repeat for T iterations.  \n\nAfter convergence, the accumulated perturbation δ = x – x₀ (the “geometry cloak”) is exactly the masked adversarial noise that, when fed through TGS, forces the reconstructed point cloud to line up with the blue‐boxed pre‐defined pattern."
    },
    {
        "question": "How does Digital Watermark Encoder’s embedding process differ from geometry cloak’s perturbation mechanism?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Digital Watermark Encoder",
            "geometry cloak"
        ],
        "id": 2300,
        "masked_question": "How does [mask1]’s embedding process differ from [mask2]’s perturbation mechanism?",
        "masked_number": 2,
        "masked_elements": [
            "Digital Watermark Encoder",
            "geometry cloak"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.22705v1_figure_1.png",
        "paperid": "2410.22705v1",
        "paper_path": "./MISS-QA/papers/2410.22705v1.json",
        "figure_id": "2410.22705v1_figure_1.png",
        "caption": "Figure 1:  Overview of our scenario. (a) Images without protection. Images can be easily reconstructed into 3D models by malicious users with TGS [52], posing a threat to the copyright of the image owner.\n(b) Digital Watermarking offers a solution by embedding copyright messages into the view-image before 3D reconstruction. However, the embedded message cannot be extracted from novel rendered views. (c) Geometry Cloak. Our geometry cloak utilizes the disturbance-prone components of TGS, achieving view-specific watermark embedding. Our method can compromise the unauthorized reconstructed 3D model while providing a verifiable pattern for copyright claim.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is nothing more exotic than a classic 2D digital-watermarking pipeline: you take your cover image and a bit‐string (e.g. 01011…), run both through a learned encoder network, and you get back a “watermarked” image that is visually unchanged but carries a payload which a separate decoder can recover later.  \n\nMask₂, by contrast, is not trying to carry a bit‐string through a decoder/encoder pair at all, but is an adversarial “geometry cloak” tuned end-to-end against TGS.  In practice you start from your original image, choose a target 2D point-cloud pattern, and then run view‐specific PGD to sprinkle imperceptible pixel‐space noise into the image so that when TGS builds its point cloud it is warped into exactly your pattern.  There is no separate extractor network – you simply force the reconstruction step itself to reveal your watermark.  \n\nIn short:  \n\n• Mask₁ = learned encoder/decoder hiding a message in 2D pixels.  \n• Mask₂ = adversarial PGD perturbation on 2D pixels that exploits TGS’s geometry module so that the reconstructed 3D point cloud “draws out” your pattern."
    },
    {
        "question": "How does Operation Composition Unit mirror memory-augmented transformer recurrence for inter-operation updates?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Finally, in the “operation composition” stage, the reasoner first integrates the executed operations \\(Z_{op}^t\\) and their results \\(Z_{res}^t\\) into the existing memory state \\(M_t\\) through a simple recurrent update as shown in eqs. 9 and 10.",
            "Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states \\(M_{op,i}^t\\) with other operation states in \\(M_{op}^t\\) as well as prior operation states in \\(M_{op}^{t-1}\\). Here, \\(W\\) is an attention look-back window.",
            "This composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, each current operation state is projected to form queries, and the concatenation of past and current operation (and result) states are projected to form keys and values; an identity mask prevents self-attention, and the attended output is added back to the original operation state to form the updated memory operation state."
        ],
        "final_answer": "The Operation Composition Unit mirrors memory-augmented transformer recurrence by first writing the newly executed operations and their results back into its memory in a recurrent fashion, then running an inter-operation attention over both current and past operation states (within a fixed look-back window). Each operation state attends to other operation states (but not itself, via an identity mask), aggregates information via the transformer-style attention, and adds it back to its own embedding—thereby implementing a dynamic, memory-augmented recurrence among operations.",
        "relevant_elements": [
            "Operation Composition Unit"
        ],
        "id": 2302,
        "masked_question": "How does [mask1] mirror memory-augmented transformer recurrence for inter-operation updates?",
        "masked_number": 1,
        "masked_elements": [
            "Operation Composition Unit"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.13754v1_figure_2.png",
        "paperid": "2411.13754v1",
        "paper_path": "./MISS-QA/papers/2411.13754v1.json",
        "figure_id": "2411.13754v1_figure_2.png",
        "caption": "Figure 2: IPRM’s computation flow diagram. First, a new set of N-parallel latent operations 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are retrieved from language features 𝐗𝐋subscript𝐗𝐋\\mathbf{X_{L}}bold_X start_POSTSUBSCRIPT bold_L end_POSTSUBSCRIPT conditioned on prior operation states 𝐌𝐨𝐩subscript𝐌𝐨𝐩\\mathbf{M_{op}}bold_M start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT. Then, visual features 𝐗𝐕subscript𝐗𝐕\\mathbf{X_{V}}bold_X start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT are queried conditioned on both 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT and prior result states results 𝐌𝐫𝐞𝐬subscript𝐌𝐫𝐞𝐬\\mathbf{M_{res}}bold_M start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT, to form the new results 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT. Finally, both 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT and 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are passed to the Operation Composition Unit (see 2.3), the output of which becomes the new memory state 𝐌𝐌\\mathbf{M}bold_M.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed “Operation Composition Unit” in figure 2 is exactly where IPRM “recurs” over its own memory in much the same way a memory–augmented Transformer does.  Concretely, at each reasoning step t it\n\n 1. first folds in the freshly computed operations Zop and results Zres via simple recurrent updates (Eqs. 9–10) into the old memory states Mop and Mres;  \n 2. then builds a new set of attention queries Qop by linearly projecting the updated Mop, and attention keys Kop and values Vop from the concatenation of (a) all operations in the current window of size W (i.e. past + present Mop) and (b) their paired results Mres; it also builds a second set of values Vres by projecting the result‐states Zres (Eq. 14);  \n 3. applies a masked multihead attention (Eq. 15) where an “identity mask” blocks each operation from attending to itself but allows it to attend to all the other parallel operations in the lookback window;  \n 4. adds the attended‐over representation back onto the current Mop via another linear projection (residual connection) to form the next Mop⁺; and  \n 5. finally uses that same inter‐operation attention, plus a small projection of Zres, to produce the next result memory Mres⁺ (Eq. 17).  \n\nAll told, this little red box is just a sliding‐window, inter‐operation self‐attention block with residual write‐back to memory—precisely the pattern that characterizes memory‐augmented Transformer recurrence, but applied here across a set of parallel “operations” and their paired “results.”"
    },
    {
        "question": "How does setting N parallel operations in Operation Formation influence subsequent Operation Execution dynamics?",
        "relevant_section_ids": [
            "2",
            "2.2"
        ],
        "relevant_context": [
            "Here, N denotes the number of parallel operations to be computed while d denotes the mechanism’s internal feature dimension.",
            "Then, conditioned on the latent operations Z_op and the existing result state M_res, we attend and retrieve relevant information from visual features X_V which represents a new set of latent results Z_res corresponding to Z_op.",
            "Finally, the attention query and value are formed through separate projections of Z_op and X_V respectively. These are then fed together with K_V to the attention function to retrieve the new operation results Z_res as shown in eq. 8."
        ],
        "final_answer": "By choosing N parallel operations in the Operation Formation stage, the model produces N distinct operation embeddings Z_op. In the following Operation Execution stage, each of these N embeddings independently drives a separate visual‐attention lookup into X_V. Concretely, the network forms N queries (one per Z_op), jointly projects them with the previous result state M_res to modulate the visual keys, and finally retrieves N corresponding result vectors Z_res in parallel. Thus, increasing N linearly scales the number of concurrent attention operations and output result tokens produced during execution.",
        "relevant_elements": [
            "Operation Formation",
            "Operation Execution"
        ],
        "id": 2303,
        "masked_question": "How does setting N parallel operations in [mask1] influence subsequent Operation Execution dynamics?",
        "masked_number": 1,
        "masked_elements": [
            "Operation Formation"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.13754v1_figure_2.png",
        "paperid": "2411.13754v1",
        "paper_path": "./MISS-QA/papers/2411.13754v1.json",
        "figure_id": "2411.13754v1_figure_2.png",
        "caption": "Figure 2: IPRM’s computation flow diagram. First, a new set of N-parallel latent operations 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are retrieved from language features 𝐗𝐋subscript𝐗𝐋\\mathbf{X_{L}}bold_X start_POSTSUBSCRIPT bold_L end_POSTSUBSCRIPT conditioned on prior operation states 𝐌𝐨𝐩subscript𝐌𝐨𝐩\\mathbf{M_{op}}bold_M start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT. Then, visual features 𝐗𝐕subscript𝐗𝐕\\mathbf{X_{V}}bold_X start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT are queried conditioned on both 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT and prior result states results 𝐌𝐫𝐞𝐬subscript𝐌𝐫𝐞𝐬\\mathbf{M_{res}}bold_M start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT, to form the new results 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT. Finally, both 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT and 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are passed to the Operation Composition Unit (see 2.3), the output of which becomes the new memory state 𝐌𝐌\\mathbf{M}bold_M.",
        "qtype": "Experimental_Results",
        "response": "Let me first restate what “[mask1]” is pointing to, and then walk through how choosing N here ripples into the Operation Execution step.\n\n1.  Identifying [mask1]  \n    In the red‐boxed region the model is doing “Operation Formation.”  Concretely, at each reasoning step t it takes the old operation states  \n       M_op = {M_op1,…,M_opN}  \n    and the language tokens X_L, and via an attention Q_LK_VL→Z_op it produces a new set of N latent operation vectors  \n       Z_op = {Z_op1,…,Z_opN}.\n\n2.  What setting N means  \n    By choosing N here we are explicitly saying “we want N distinct operations in parallel.”  Each of those will separately drive a visual‐attention lookup in the next stage.\n\n3.  How this feeds into Operation Execution  \n   – The N new operation vectors Z_op are concatenated with the N prior result states M_res to compute a shared feature‐modulation tensor F_mod (Eq. 6).  \n   – F_mod is chunked out so that each operation i has its own modulation weights.  \n   – We then project, for each i∈{1…N}, a query Q_v,i (from Z_op,i), a key K_v,i (from modulated X_V), and a value V_v,i (from X_V).  \n   – We run N separate attention lookups Attn(Q_v,i,K_v,i,V_v,i) over the visual features, yielding N output vectors Z_res1…Z_resN (Eq. 8).\n\n4.  The net effect  \n    • Parallelism: You literally do N independent visual‐attention steps in one go.  \n    • Capacity: More operations → more “slots” to attend to different parts of the image.  \n    • Computation: The cost of the execution stage scales linearly in N, because you create and apply N distinct Q,K,V triplets.\n\nAnswer  \nBy fixing the red‐boxed module to produce N parallel operations, the Operation Execution block is forced to instantiate N separate attention queries (Q_v,i), modulations (via F_mod), keys and values over the visual features. In other words, each of the N latent ops from the Operation Formation step drives its own independent attention over X_V, yielding N result vectors in parallel. Thus N directly controls the degree of parallel visual‐attention retrieval and the width (number of slots) of the subsequent execution stage."
    },
    {
        "question": "How does the Operation Composition Unit transform latent results to update memory state across reasoning iterations?",
        "relevant_section_ids": [
            "2.3"
        ],
        "relevant_context": [
            "Finally, in the “operation composition” stage, the reasoner first integrates the executed operations Z_op and their results Z_res into the existing memory state M through a simple recurrent update as shown in eqs. 9 and 10.",
            "Then, to mitigate redundancy amongst parallel operations and to retrieve relevant knowledge from prior-step operations, it dynamically composes individual operation states m_op,i with other operation states in M_op^t and also prior operation states in M_op^{t–k}.",
            "This composition is achieved through computing inter-operation attention as illustrated in fig. 3. Specifically, Z_op is projected to obtain a set of queries Q, while the token-wise concatenation of M_op and M_res are projected to obtain the operation attention keys K and values V. A second set of values V′ are also formed through projection of respective result states as shown in eq. 14.",
            "Further, an identity attention mask I is used to ensure that operations in M_op can only attend to other operations and not themselves.",
            "As shown in eq. 15, Q, K, V and I are passed to the attention operation, which outputs an intermediate representation O and the softmaxed-attention weights α.",
            "O is then added to a projection of M_op to effectively combine attended operation states with the original operation states, and thereby form the next memory operation state M_op^{t+1}.",
            "Finally, the next result states M_res^{t+1} are obtained by applying Attn on O and then adding a projection of Z_res as shown in eq. 17."
        ],
        "final_answer": "In each iteration the Unit first fuses the newly executed latent operations and their latent results back into the current memory via a simple recurrent update. It then performs inter-operation attention: it projects the latent operations Z_op to queries, concatenates the current operation and result memory slots to form keys and values (together with a separate projection of Z_res), and applies a masked attention (so each operation only attends to other operations). The attention output is added back to the original operation memory to form the updated operation states M_op^{t+1}, and the final result memory M_res^{t+1} is obtained by attending once more over these composed operation states and then adding a projection of the latent results Z_res. Together, these steps yield the new memory state for the next reasoning iteration.",
        "relevant_elements": [
            "Operation Composition Unit",
            "memory state"
        ],
        "id": 2304,
        "masked_question": "How does the [mask1] transform latent results to update [mask2] across reasoning iterations?",
        "masked_number": 2,
        "masked_elements": [
            "Operation Composition Unit",
            "memory state"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.13754v1_figure_2.png",
        "paperid": "2411.13754v1",
        "paper_path": "./MISS-QA/papers/2411.13754v1.json",
        "figure_id": "2411.13754v1_figure_2.png",
        "caption": "Figure 2: IPRM’s computation flow diagram. First, a new set of N-parallel latent operations 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are retrieved from language features 𝐗𝐋subscript𝐗𝐋\\mathbf{X_{L}}bold_X start_POSTSUBSCRIPT bold_L end_POSTSUBSCRIPT conditioned on prior operation states 𝐌𝐨𝐩subscript𝐌𝐨𝐩\\mathbf{M_{op}}bold_M start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT. Then, visual features 𝐗𝐕subscript𝐗𝐕\\mathbf{X_{V}}bold_X start_POSTSUBSCRIPT bold_V end_POSTSUBSCRIPT are queried conditioned on both 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT and prior result states results 𝐌𝐫𝐞𝐬subscript𝐌𝐫𝐞𝐬\\mathbf{M_{res}}bold_M start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT, to form the new results 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT. Finally, both 𝐙𝐫𝐞𝐬subscript𝐙𝐫𝐞𝐬\\mathbf{Z_{res}}bold_Z start_POSTSUBSCRIPT bold_res end_POSTSUBSCRIPT and 𝐙𝐨𝐩subscript𝐙𝐨𝐩\\mathbf{Z_{op}}bold_Z start_POSTSUBSCRIPT bold_op end_POSTSUBSCRIPT are passed to the Operation Composition Unit (see 2.3), the output of which becomes the new memory state 𝐌𝐌\\mathbf{M}bold_M.",
        "qtype": "Experimental_Results",
        "response": "The “mystery” red‐boxed block in the figure is nothing more exotic than the paper’s so‐called Operation Composition Unit (Sect. 2.3).  What it does, in a nutshell, is:\n\n 1. Take as input  \n    – the freshly minted operation embeddings Z_op  \n    – their paired result embeddings Z_res  \n    – and the old memory tuples (M_op^t, M_res^t)  \n\n 2. Build an inter‐operation attention:  \n    – Project the old M_op^t into queries Q_op  \n    – Project the concatenation of (Z_op, M_res^t) into keys K_op and values V_op  \n    – (A second “value” stream is likewise formed from the old M_res^t.)  \n    – Apply a small masked multi‐head attention so that each operation slot can only attend to *other* slots (and a short look‐back of previous steps if allowed).  \n\n 3. Produce an attended representation O_att for each of the N slots.  \n\n 4. Update the memory by simple residual additions:  \n    – M_op^{t+1} = Linear(M_op^t) + O_att  \n    – M_res^{t+1} = Linear(M_res^t) + ϕ(O_att)  \n\nThe result of step 4 is exactly the blue‐boxed object in the diagram (the new M^{t+1}).  In this way, the Operation Composition Unit “folds” the new latent results (and operations) back into the old memory via inter‐slot attention plus residual connections, and hands you the updated memory for the next reasoning iteration."
    },
    {
        "question": "How does stacking module consolidate LLM outputs to inform meta model predictions?",
        "relevant_section_ids": [
            "3.1",
            "3.4",
            "3.5"
        ],
        "relevant_context": [
            "Section 3.1: “We construct a meta-feature vector φ_i for each sample x_i by concatenating the outputs of all base models: [f_1(x_i), f_2(x_i), …, f_n(x_i)]. A meta-classifier M_θ is then trained on these meta-features to learn the optimal combination of base model predictions.”",
            "Section 3.4: “Each model generated a probability distribution over the five vulnerability classes. These distributions, represented as p, were concatenated to form feature vectors. For each code snippet, the model predictions from CodeBERT, GraphCodeBERT, and UniXcoder were combined into a single feature vector.”",
            "Section 3.5: “Meta-features are generated by aggregating predictions from all base models for each data sample, creating a comprehensive meta-feature vector. Multiple meta-classifiers are trained on these meta-features, with the classifier demonstrating the best validation performance selected as the optimal meta-classifier.”"
        ],
        "final_answer": "The stacking module takes each LLM’s output (its class‐probability vector), concatenates those vectors into one joint meta‐feature vector per code snippet, and then feeds that combined feature into a meta‐classifier which learns how to weight and merge the base models’ predictions into a final vulnerability prediction.",
        "relevant_elements": [
            "stacking",
            "LLM",
            "meta model"
        ],
        "id": 2305,
        "masked_question": "How does [mask1] module consolidate LLM outputs to inform [mask2] predictions?",
        "masked_number": 2,
        "masked_elements": [
            "stacking",
            "meta model"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.16561v1_figure_1.png",
        "paperid": "2411.16561v1",
        "paper_path": "./MISS-QA/papers/2411.16561v1.json",
        "figure_id": "2411.16561v1_figure_1.png",
        "caption": "Figure 1: A Comparative Overview of Vulnerability Detection Techniques. (1) Traditional LLM-based processing, which directly outputs predictions but experiences notable data loss, (2) Traditional+Meta models from previous studies that integrate a meta-classifier to enhance LLM outputs, and (3) the proposed EnStack framework, which leverages an ensemble of multiple LLMs combined through stacking methods. EnStack incorporates a meta-model to further refine predictions, aiming for improved accuracy in vulnerability detection by effectively combining strengths of various LLMs and meta-model architectures.",
        "qtype": "Experimental_Results",
        "response": "First, let’s align the picture with the text:\n\n1. In the bottom panel of the figure (“LLM + LLM + Meta”), you see three base LLMs whose outputs feed into an “Ensemble methods” block.  \n2. That block is highlighted in red – this is the stacking module ([mask1]).  \n3. Its output then goes into the blue-outlined box labeled “Meta model” ([mask2]).\n\nNow, from §III-D in the text:\n\n- Each fine-tuned base model (CodeBERT, GraphCodeBERT, UniXcoder) produces a probability vector over the vulnerability classes for a given code snippet.  \n- The stacking module takes those three vectors and concatenates them into one long “meta-feature” vector.  \n- Finally, the meta-model (a second-level classifier such as LR, RF, SVM or XGBoost) is trained on these stacked feature vectors to produce the final vulnerability prediction.\n\nAnswer:\nThe stacking module consolidates the individual LLMs’ class–probability outputs by concatenating them into a single meta-feature vector, which the downstream meta-model then uses to learn how to combine those signals and make the final vulnerability prediction."
    },
    {
        "question": "How do ensemble methods integrate LLM predictions before meta model learning?",
        "relevant_section_ids": [
            "3.1",
            "3.4"
        ],
        "relevant_context": [
            "III-A Problem Formulation: “We construct a meta-feature vector φ_i for each sample x_i by concatenating the outputs of all base models: φ_i = concat(M_1(x_i), M_2(x_i), ..., M_B(x_i)).”",
            "III-D Ensemble Stacking Method: “Each model generated a probability distribution over the five vulnerability classes. These distributions, represented as p_i^k, were concatenated to form feature vectors. For each code snippet x_i, the model predictions from CodeBERT, GraphCodeBERT, and UniXcoder were combined into a single feature vector.”"
        ],
        "final_answer": "The ensemble stacking method first takes each base LLM’s output probability distribution over the vulnerability classes, then concatenates these distributions into one long meta-feature vector for each code snippet. This concatenated vector is used as input to train the meta-classifier.",
        "relevant_elements": [
            "ensemble methods",
            "LLM",
            "meta model"
        ],
        "id": 2306,
        "masked_question": "How do [mask1] integrate LLM predictions before meta model learning?",
        "masked_number": 1,
        "masked_elements": [
            "ensemble methods"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.16561v1_figure_1.png",
        "paperid": "2411.16561v1",
        "paper_path": "./MISS-QA/papers/2411.16561v1.json",
        "figure_id": "2411.16561v1_figure_1.png",
        "caption": "Figure 1: A Comparative Overview of Vulnerability Detection Techniques. (1) Traditional LLM-based processing, which directly outputs predictions but experiences notable data loss, (2) Traditional+Meta models from previous studies that integrate a meta-classifier to enhance LLM outputs, and (3) the proposed EnStack framework, which leverages an ensemble of multiple LLMs combined through stacking methods. EnStack incorporates a meta-model to further refine predictions, aiming for improved accuracy in vulnerability detection by effectively combining strengths of various LLMs and meta-model architectures.",
        "qtype": "Experimental_Results",
        "response": "Here, “[mask1]” is pointing to the red‐boxed “Ensemble methods” block in the figure.  In our pipeline, before we ever touch the meta‐classifier, we take each base LLM’s output probability vector over the CWE classes and feed them all into an ensemble module.  Concretely, for a given snippet x, CodeBERT, GraphCodeBERT and UniXcoder each spit out a 5-dimensional softmax vector.  The ensemble (stacking) step simply concatenates those three 5-D vectors into a single 15-D “meta-feature” vector.  That 15-D vector is then used as the input to whichever meta-classifier (LR, RF, SVM or XGBoost) we’re training."
    },
    {
        "question": "What limitations could ensemble methods face when aggregating outputs from multiple LLMs?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Ensemble methods",
            "LLM"
        ],
        "id": 2307,
        "masked_question": "What limitations could [mask1] face when aggregating outputs from multiple [mask2]s?",
        "masked_number": 2,
        "masked_elements": [
            "Ensemble methods",
            "LLM"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.16561v1_figure_1.png",
        "paperid": "2411.16561v1",
        "paper_path": "./MISS-QA/papers/2411.16561v1.json",
        "figure_id": "2411.16561v1_figure_1.png",
        "caption": "Figure 1: A Comparative Overview of Vulnerability Detection Techniques. (1) Traditional LLM-based processing, which directly outputs predictions but experiences notable data loss, (2) Traditional+Meta models from previous studies that integrate a meta-classifier to enhance LLM outputs, and (3) the proposed EnStack framework, which leverages an ensemble of multiple LLMs combined through stacking methods. EnStack incorporates a meta-model to further refine predictions, aiming for improved accuracy in vulnerability detection by effectively combining strengths of various LLMs and meta-model architectures.",
        "qtype": "Others",
        "response": "Chain‐of‐thought  \n1. In the figure the red‐boxed component is the “Ensemble methods” stage, and the blue‐boxed items are the individual LLMs (CodeBERT, GraphCodeBERT, UniXcoder, etc.).  \n2. Ensemble methods take the probability outputs or class scores from each LLM and then combine them (via stacking, voting, averaging, a meta‐classifier, etc.) into a single, final prediction.  \n3. In practice, aggregating multiple LLMs with an ensemble can suffer from several bottlenecks:  \n   • Computational and memory cost – you must run inference on every base LLM and then train or apply a meta‐model, which multiplies GPU/CPU time and RAM requirements.  \n   • Latency – end‐to‐end prediction time grows linearly (or worse) with the number of LLMs, making real‐time or large‐scale deployment harder.  \n   • Correlated errors – if the LLMs are similar or trained on the same data, their mistakes tend to overlap, so the ensemble gains little over the best single model.  \n   • Calibration and overfitting – stacking methods require a held‐out set to train the meta‐classifier; if that set is small or unrepresentative, the meta‐model can overfit or mis‐calibrate.  \n   • Reduced interpretability – once you’ve combined many black‐box models through a meta‐classifier, it becomes harder to trace which base LLM drove a given decision.  \n\nAnswer  \nEnsemble methods can require a lot more compute and memory (since you must run each constituent LLM and then train or evaluate a meta‐model), introduce extra inference latency, and often bring diminishing returns when the base LLMs make correlated errors. They also demand a sufficiently large and well-distributed meta-training set to avoid overfitting or mis‐calibration, and the resulting stacked system is typically harder to interpret than any single model."
    },
    {
        "question": "What ethical concerns arise from stacking LLMs in vulnerability detection systems?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Stacking",
            "LLM"
        ],
        "id": 2308,
        "masked_question": "What ethical concerns arise from [mask1] LLMs in vulnerability detection systems?",
        "masked_number": 1,
        "masked_elements": [
            "Stacking"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.16561v1_figure_1.png",
        "paperid": "2411.16561v1",
        "paper_path": "./MISS-QA/papers/2411.16561v1.json",
        "figure_id": "2411.16561v1_figure_1.png",
        "caption": "Figure 1: A Comparative Overview of Vulnerability Detection Techniques. (1) Traditional LLM-based processing, which directly outputs predictions but experiences notable data loss, (2) Traditional+Meta models from previous studies that integrate a meta-classifier to enhance LLM outputs, and (3) the proposed EnStack framework, which leverages an ensemble of multiple LLMs combined through stacking methods. EnStack incorporates a meta-model to further refine predictions, aiming for improved accuracy in vulnerability detection by effectively combining strengths of various LLMs and meta-model architectures.",
        "qtype": "Others",
        "response": "Here’s how I arrived at the answer:\n\n1.  Identify “​[mask1]​.”  \n    •  In the third (right-hand) panel of Figure 1, the single red box highlights the “Stacking” block in the “LLM + LLM + Meta (Current Methods)” workflow.  \n    •  In other words, ​[mask1]​ refers to the practice of “stacking” (i.e. ensembling) multiple large language models to detect vulnerabilities.\n\n2.  Ask: what ethical worries does a large, stacked-LLM vulnerability scanner raise?  \n    By ensembling several LLMs into a single “meta” detector you gain performance, but also:\n\n    a)  Opacity and Accountability  \n       –  The more models you glue together, the harder it is to trace any one decision back to human-understandable rules.  \n       –  If a critical flaw is missed (or falsely flagged), it may be nearly impossible to explain why, which hinders auditing and liability.\n\n    b)  Dual-Use and Misuse  \n       –  A high-powered, hugely automated vulnerability finder can be repurposed by attackers to discover and weaponize zero-days far more quickly than security teams can patch them.  \n       –  Even benign-looking output could leak sensitive patterns (e.g. proprietary code snippets) that facilitate reverse-engineering.\n\n    c)  Feedback Loops and Bias Amplification  \n       –  Any blind spots or biases in the base models get inherited—and often magnified—by the meta-classifier.  \n       –  Rare or subtle vulnerability types (under-represented in training) may be systematically under-detected, while over-common classes drown out niche but critical flaws.\n\n    d)  Privacy and Intellectual-Property Risks  \n       –  If your LLMs were pre-trained on private or licensed code, a stacking scheme may inadvertently “memorize” and regurgitate proprietary fragments.  \n       –  That poses privacy/legal exposure for organizations feeding confidential code into the system.\n\n    e)  Environmental and Resource Impact  \n       –  Training and running multiple giant models in tandem consumes large amounts of compute power and energy, raising sustainability and cost questions, especially for resource-constrained teams.\n\nTaken together, the ethical concerns around a stacked-LLM vulnerability detector are:\n\n– loss of interpretability and auditability  \n– heightened dual-use potential (attackers weaponize it)  \n– amplification of biases and blind spots  \n– risk of leaking private or proprietary code  \n– substantial compute footprint and its environmental impact"
    },
    {
        "question": "What biases could Attribute Decoupling Mechanism introduce when generating regular data for finetuning?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Attribute Decoupling Mechanism"
        ],
        "id": 2309,
        "masked_question": "What biases could [mask1] introduce when generating regular data for finetuning?",
        "masked_number": 1,
        "masked_elements": [
            "Attribute Decoupling Mechanism"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.17223v1_figure_2.png",
        "paperid": "2411.17223v1",
        "paper_path": "./MISS-QA/papers/2411.17223v1.json",
        "figure_id": "2411.17223v1_figure_2.png",
        "caption": "Figure 2: Overview of DreamMix. During finetuning, we use the source data {𝒙s,𝒑s}subscript𝒙𝑠subscript𝒑𝑠\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT } along with regular data {𝒙r,𝒑r}subscript𝒙𝑟subscript𝒑𝑟\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } constructed via an attribute decoupling mechanism (Sec. 3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec. 3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec. 3.4) to generate a decomposed text embedding to enhance the editability of our method during testing.",
        "qtype": "Others",
        "response": "In this case “[mask1]” is pointing at the little red‐boxed inset in Fig. 2(a) – the Attribute Decoupling Mechanism that we use to automatically generate our “regular” {x­r,p­r} pairs.  The key thing to notice is that every one of those synthetic image–text pairs is born from a Vision–Language model’s shot at “what attributes live on that subject” plus a random shuffling of them into new prompts.  Because of that you can get:\n\n  •  “VLM bias” creeping in: the attributes the VLM reports (and the way it describes them) reflect whatever was over– or under–represented in its own training set, and may carry cultural or stereotypical skew.  \n  •  Distributional skew or “co‐occurrence bias”: by randomly recombining detected attributes you may end up heavily sampling some strange or very rare attribute mixtures, and never sampling others, so your finetuning corpus is not an even, representative slice of “all possible” attribute combinations.  \n  •  Mis‐labelling noise: if the VLM hallucinate or misses certain attributes, you’ll bake that error straight into the regular data, and the inpainting model can learn the wrong correlations (e.g. always painting “polka-dots” because the VLM insisted it saw them).  \n\nAll of these effects – VLM training–set artifacts, uneven mixing of attributes, and outright mis-detections – are the main “biases” introduced whenever you lean on an automated attribute‐decoupling pipeline to synthesize your regular finetuning data."
    },
    {
        "question": "What privacy concerns arise from using VLM Agent in Textual Attribute Substitution?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "VLM Agent",
            "Textual Attribute Substitution"
        ],
        "id": 2310,
        "masked_question": "What privacy concerns arise from using [mask1] in [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "VLM Agent",
            "Textual Attribute Substitution"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.17223v1_figure_2.png",
        "paperid": "2411.17223v1",
        "paper_path": "./MISS-QA/papers/2411.17223v1.json",
        "figure_id": "2411.17223v1_figure_2.png",
        "caption": "Figure 2: Overview of DreamMix. During finetuning, we use the source data {𝒙s,𝒑s}subscript𝒙𝑠subscript𝒑𝑠\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT } along with regular data {𝒙r,𝒑r}subscript𝒙𝑟subscript𝒑𝑟\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } constructed via an attribute decoupling mechanism (Sec. 3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec. 3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec. 3.4) to generate a decomposed text embedding to enhance the editability of our method during testing.",
        "qtype": "Others",
        "response": "Here, “mask1” is the small attribute dictionary that the VLM agent builds (e.g. shape, material, color, size) and “mask2” is the Textual Attribute Substitution module that uses those attributes to orthogonally decompose the user’s prompt.  The privacy risk comes from the fact that the system is automatically extracting fine‐grained attributes from your reference images (or even your face) and then baking them into embeddings that get passed around or stored.  In the worst case an attacker could query or probe those embeddings to recover or infer private details—biometric traits, home décor, possessions, etc.—leading to attribute-inference or even model-inversion attacks that violate a user’s privacy."
    },
    {
        "question": "What motivates separating inpainting into Local Content Generation and Global Context Harmonization stages?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Existing subject-driven inpainting methods [39, 6, 26] usually build on pretrained diffusion models, wherein the lowest resolution of latent features is 32 times smaller than the original input images. This substantial reduction in resolution hinders these methods’ ability to effectively inpaint objects in very confined local regions.",
            "Additionally, in complex scenes, background interference can affect the editing control of text instructions, leading to imprecise synthesis of the fill-in areas.",
            "To overcome these challenges, we propose an effective disentangled inpainting framework, consisting of Local Content Generation (LCG) and Global Context Harmonization (GCH), to enhance both local subject integration and global visual coherence.",
            "Global Context Harmonization. The local content generation in the preceding time steps effectively yields accurate object compositing with a well-defined layout and object appearance. However, without incorporating the global image content, it tends to produce a disharmonious “copy-paste” effect in the inpainting areas."
        ],
        "final_answer": "The separation into Local Content Generation and Global Context Harmonization is motivated by two challenges: (1) the low-resolution latent features of pretrained diffusion models make it difficult to inpaint very small or confined regions accurately, and background interference can lead to imprecise local synthesis; (2) performing inpainting without reference to the full scene often causes a disharmonious “copy-paste” appearance. By first focusing on precise local content generation (LCG) and then harmonizing this content with the global context (GCH), DreamMix achieves both accurate object insertion and overall visual coherence.",
        "relevant_elements": [
            "Local Content Generation",
            "Global Context Harmonization"
        ],
        "id": 2311,
        "masked_question": "What motivates separating inpainting into [mask1] and [mask2] stages?",
        "masked_number": 2,
        "masked_elements": [
            "Local Content Generation",
            "Global Context Harmonization"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.17223v1_figure_2.png",
        "paperid": "2411.17223v1",
        "paper_path": "./MISS-QA/papers/2411.17223v1.json",
        "figure_id": "2411.17223v1_figure_2.png",
        "caption": "Figure 2: Overview of DreamMix. During finetuning, we use the source data {𝒙s,𝒑s}subscript𝒙𝑠subscript𝒑𝑠\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT } along with regular data {𝒙r,𝒑r}subscript𝒙𝑟subscript𝒑𝑟\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } constructed via an attribute decoupling mechanism (Sec. 3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec. 3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec. 3.4) to generate a decomposed text embedding to enhance the editability of our method during testing.",
        "qtype": "Design_Rationale",
        "response": "They split the denoising pass into two halves because modern diffusion nets naturally work “coarse-to-fine,” and a single monolithic pass either 1) produces a very sharp little object that never really blends into its surroundings or 2) over-smooths away all the fine local detail.  By devoting the first \\(T\\) steps to Local Content Generation (red box) they can concentrate purely on getting the subject’s layout, shape and texture right in the masked patch.  Then, in the remaining \\((1\\!-\\!\\lambda)T\\) steps of Global Context Harmonization (blue box) they bring back the full-image context so that the newly painted region no longer looks like a pasted-on sticker but sits harmoniously in the scene."
    },
    {
        "question": "What motivates using orthogonal decomposition block to produce decomposed text embedding in Textual Attribute Substitution?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "However, due to the lack of unseen attribute words during training, relying solely on attribute decoupling mechanism still poses challenges, especially when the target attributes differ significantly from the object identity. To address this, we introduce a Textual Attribute Substitution (TAS) module during the testing phase to further mitigate the influence of object identity for more precise attribute editing.",
            "Next, we utilize an orthogonal decomposition strategy on the text embeddings to surpass the influence of original attributes in object editing, which is calculated as follows:",
            "After applying this embedding substitution, the conflicting features of the original object identity are effectively decoupled, making the inpainting model focus on the demand of the target prompt."
        ],
        "final_answer": "The orthogonal decomposition block is used to remove the component of the text embedding that corresponds to the original object attributes, thereby mitigating the influence of object identity and enabling the inpainting model to focus precisely on the user’s target attribute edits.",
        "relevant_elements": [
            "Textual Attribute Substitution",
            "orthogonal decomposition block",
            "decomposed text embedding"
        ],
        "id": 2312,
        "masked_question": "What motivates using [mask1] to produce [mask2] in Textual Attribute Substitution?",
        "masked_number": 2,
        "masked_elements": [
            "orthogonal decomposition block",
            "decomposed text embedding"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.17223v1_figure_2.png",
        "paperid": "2411.17223v1",
        "paper_path": "./MISS-QA/papers/2411.17223v1.json",
        "figure_id": "2411.17223v1_figure_2.png",
        "caption": "Figure 2: Overview of DreamMix. During finetuning, we use the source data {𝒙s,𝒑s}subscript𝒙𝑠subscript𝒑𝑠\\{\\boldsymbol{x}_{s},\\boldsymbol{p}_{s}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT } along with regular data {𝒙r,𝒑r}subscript𝒙𝑟subscript𝒑𝑟\\{\\boldsymbol{x}_{r},\\boldsymbol{p}_{r}\\}{ bold_italic_x start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , bold_italic_p start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT } constructed via an attribute decoupling mechanism (Sec. 3.3), to enable pretrained Text-to-Image (T2I) inpainting models to efficiently adapt to subject generation. At testing, we employ a disentangled inpainting framework (Sec. 3.2), which divides the denoising process into two stages: Local Content Generation (LCG) and Global Context Harmonization (GCH). Additionally, we propose a textual attribute substitution module (Sec. 3.4) to generate a decomposed text embedding to enhance the editability of our method during testing.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module in (b) is nothing more exotic than an “orthogonal decomposition” layer – it takes your raw prompt embedding and an embedding of the attribute you want to replace, and literally subtracts out the component of the raw prompt along the old attribute.  The blue‐boxed output is the remainder – the “decomposed” embedding that no longer carries remnants of the original attribute.  \n\nWhy do we do this?  Because during inpainting the model will otherwise cling to whatever color/texture/shape cues it learned during training (“concept infusion”) and fight any attempt to change them.  By projecting the user’s new attribute embedding orthogonally to the old one, we strip away the old attribute’s influence and force the diffusion model to pay attention only to the newly specified property."
    },
    {
        "question": "What is the reasoning behind deploying multi-group tri-plane for global context extraction?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Holistic scene context plays a pivotal role for precisely inferring the state of each voxel. However, learning over 3-D volumes is neither computationally feasible (the large number of voxels is not amenable to intensive convolutions or attention-based operations) nor necessary (most voxels are void and should not been involved in the computation).",
            "Computation over the entire 3-D scene volume is computationally forbidden for large scenes. To avoid it, we devise a scheme of multi-group triplanar projection for holistic / local scene context extraction in cluttered scenes.",
            "Importantly, the above process of triplanar projection is lossy, thus we further propose to use multiple groups of tri-planes that differ in 3-D rotations and share the same origin, thereby more key information can be preserved via diverse aggregations.",
            "The utilization of multi-group tri-plane approximately captures global scene context in a concise way. On the one hand, more aggregation groups improve the possibility of restoring features for the occluded parts and enriches the 3-D shape clues during projection. On the other hand, it significantly reduces the data size during calculation and avoids the direct operation on dense 3D volume features."
        ],
        "final_answer": "Directly operating on full 3D volumes is too expensive and unnecessary, and a single triplanar projection loses information. By deploying multiple groups of tri-planes—each rotated differently in SO(3)—the method preserves more of the scene’s global structure, enriches shape clues for occluded parts, and drastically reduces data size and computation compared to dense volumetric operations.",
        "relevant_elements": [
            "Multi-Group Tri-Plane",
            "Global Context"
        ],
        "id": 2313,
        "masked_question": "What is the reasoning behind deploying [mask1] for [mask2] extraction?",
        "masked_number": 2,
        "masked_elements": [
            "Multi-Group Tri-Plane",
            "Global Context"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.15771v1_figure_2.png",
        "paperid": "2407.15771v1",
        "paper_path": "./MISS-QA/papers/2407.15771v1.json",
        "figure_id": "2407.15771v1_figure_2.png",
        "caption": "Figure 2: Model architecture of the proposed local occupancy-enhanced object grasping. It first identifies a number of local occupancy regions of interest. Then multi-group tri-plane aggregates the scene context for local occupancy estimation. Finally the occupancy-enhanced local shape feature in each grasp region is extracted by fusing the information of both explicit voxels and implicit queried features, and is decoded to grasp poses.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module is the “multi‐group tri‐plane” projection, the blue‐boxed module is where we pull out a “global context” feature for each query point.  The reason we use multi‐group tri-planes to get that global context is essentially two‐fold:\n\n1.   Full 3D volumes are both sparse (most voxels empty) and huge, so running convolutions or attention over them is extremely expensive.  \n2.   A single triplane (i.e. projecting your 3D embedding onto the three orthogonal planes of one coordinate frame) is much cheaper, but it discards a lot of structure.  \n3.   By instantiating not one but several triplanes (“multi-group”), each at a different rotation drawn uniformly from SO(3), we recover most of the lost information (different “snapshots” of the scene), yet still only ever do 2D operations on planes.  \n\nIn short, multi-group tri-plane projection gives us a very compact, efficient way to pool global, long-range shape cues from the entire scene—cues we need to accurately predict each voxel’s occupancy—without ever paying the full cost of dense 3D processing."
    },
    {
        "question": "What rationale underlies fusing implicit features and explicit shape into local shape feature?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "As Q is an explicit form of local shape, a shape encoder composed of 4 point set abstraction layers proposed in Pointnet++ extracts the delicate shape feature from {q_j, f_j}.",
            "In addition, some important implicit shape information may have been embedded in F_query.",
            "Therefore we randomly sample a few key points from F_query. Their corresponding queried features in F_query are processed with max-pooling as the holistic feature of the local region.",
            "Finally, these two kinds of features are concatenated as the local occupancy-enhanced shape feature."
        ],
        "final_answer": "The model fuses explicit shape (the voxel‐center points and their features) with implicit features sampled from the queried feature field because the explicit points provide a precise geometric form of the local region while the queried features contain complementary, implicitly encoded shape information. Combining both ensures the local shape feature captures detailed geometry and additional context encoded in the occupancy queries.",
        "relevant_elements": [
            "Implicit Feature",
            "Explicit Shape",
            "Local Shape Feature"
        ],
        "id": 2314,
        "masked_question": "What rationale underlies fusing [mask1] and explicit shape into local shape feature?",
        "masked_number": 1,
        "masked_elements": [
            "Implicit Feature"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.15771v1_figure_2.png",
        "paperid": "2407.15771v1",
        "paper_path": "./MISS-QA/papers/2407.15771v1.json",
        "figure_id": "2407.15771v1_figure_2.png",
        "caption": "Figure 2: Model architecture of the proposed local occupancy-enhanced object grasping. It first identifies a number of local occupancy regions of interest. Then multi-group tri-plane aggregates the scene context for local occupancy estimation. Finally the occupancy-enhanced local shape feature in each grasp region is extracted by fusing the information of both explicit voxels and implicit queried features, and is decoded to grasp poses.",
        "qtype": "Design_Rationale",
        "response": "Here’s the reasoning step‐by‐step:\n\n1. **Identify “[mask1]” in the figure.**  \n   The red‐boxed item in the diagram is labelled “Implicit Feature.”  \n\n2. **Recall how these two streams are produced (from the text):**  \n   - **Explicit shape** comes from actually cropping out the occupied voxels in the local grasp region (i.e. the binary occupancy grid).  \n   - **Implicit features** are the decoder query‐features at a small, randomly sampled subset of those same voxel centers—these features have already “seen” global tri‐plane context + local point embeddings.  \n\n3. **What the paper does with them:**  \n   - The explicit voxels are passed into a PointNet++ style encoder to extract fine, geometric shape details.  \n   - The implicit features are max‐pooled to give a compact, holistic embedding of that local neighborhood.  \n   - Finally, the two are concatenated to form the “local occupancy-enhanced shape feature.”\n\n4. **Why fuse them?**  \n   - **Explicit voxels** give you precise, local 3D geometry (the actual occupied points).  \n   - **Implicit features** carry learned context—both global (tri‐plane aggregated) and local (point‐cloud embedding & relative position) cues, including anything lost in the triplanar projection.  \n   - By fusing them, the network gets both the raw geometric detail *and* the higher‐level, learned shape priors/occlusion context in one representation, which empirically leads to more accurate grasp predictions.\n\nAnswer:  \nThey fuse the implicit queried features and the explicit voxelized shape because they are complementary – the explicit voxels encode precise local geometry, while the implicit features carry learned context and shape priors.  Combining them yields a richer, occupancy‐enhanced local shape feature that improves grasp estimation."
    },
    {
        "question": "How does spherical linear quaternion interpolation generate diverse multi-group tri-plane rotations?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "To ensure the diversity across different tri-planes, we conduct a spherical linear interpolation of quaternion [36 ###reference_b36###] to draw multiple tri-plane coordinate rotations uniformly in the rotation group SO(3).",
            "Given the start and the end quaternions Q_s and Q_e with Q_s ≠ Q_e, and the number of tri-plane groups K, the interpolated coordinate frame rotations are:\nq_i = (sin((1 − t_i)θ)/sin θ) Q_s + (sin(t_i θ)/sin θ) Q_e  where t_i = i/(K−1).\nThen the quaternion q_i can be transformed to a rotation matrix R_i by: R_i = quaternion_to_rotation_matrix(q_i).",
            "In practice we set Q_s as the identity rotation and choose Q_e to be orthogonal (maximizing their angular distance) so that the K resulting rotations are spread uniformly over SO(3)."
        ],
        "final_answer": "They pick two quaternions (Q_s and Q_e), one as the identity and one orthogonal to it, and then for i=0,…,K−1 compute intermediate quaternions q_i by spherical linear interpolation (SLERP) with t_i=i/(K−1). Each q_i is converted to a rotation matrix R_i. By choosing Q_e orthogonal to Q_s and sampling t_i uniformly, the K rotations are spread evenly over SO(3), yielding diverse multi-group tri-plane orientations.",
        "relevant_elements": [
            "Multi-Group Tri-Plane"
        ],
        "id": 2315,
        "masked_question": "How does spherical linear quaternion interpolation generate diverse [mask1] rotations?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-Group Tri-Plane"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.15771v1_figure_2.png",
        "paperid": "2407.15771v1",
        "paper_path": "./MISS-QA/papers/2407.15771v1.json",
        "figure_id": "2407.15771v1_figure_2.png",
        "caption": "Figure 2: Model architecture of the proposed local occupancy-enhanced object grasping. It first identifies a number of local occupancy regions of interest. Then multi-group tri-plane aggregates the scene context for local occupancy estimation. Finally the occupancy-enhanced local shape feature in each grasp region is extracted by fusing the information of both explicit voxels and implicit queried features, and is decoded to grasp poses.",
        "qtype": "Implementation_Details",
        "response": "Here’s what happens, step by step:\n\n1.  We want K different “tri-planes,” each one defined by a different 3-D rotation.  \n2.  We pick two quaternions  \n      •  qₛ  = the identity rotation (no rotation)  \n      •  qₑ  = some rotation that is as “far away” from the identity as possible (we choose qₛ·qₑ=0 so that the angle between them is 90°, maximizing spread).  \n3.  We use spherical‐linear interpolation (SLERP) to walk from qₛ to qₑ in K evenly‐spaced steps.  Concretely, for group index k=0…K–1 we set  \n      τ = k/(K–1)  \n      θ = arccos(qₛ·qₑ)  \n      qₖ = (sin((1–τ)θ)/sin θ) · qₛ  +  (sin(τ θ)/sin θ) · qₑ  \n4.  Each qₖ is turned into a 3×3 rotation matrix Rₖ.  These Rₖ are then used as the coordinate frames in which we project the point‐cloud onto X–Y, Y–Z, and Z–X planes.  \n\nBecause SLERP moves along the shortest great‐circle arc on the 4D unit sphere, and because we chose qₑ to be maximally distant from qₛ, the K quaternions qₖ end up being spaced out uniformly in rotation space SO(3).  That uniform sampling is exactly what gives us a set of *diverse tri-plane rotations*."
    },
    {
        "question": "How are global tri-plane features and local context fused by the voxel query module?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "For each queried point x, its global context g_x is the fusion of the bi-linear interpolated features on the projection points of different planes. Specifically, an encoder E''_g shared by all tri-plane groups will first fuse the three interpolated features from the i-th group into g_x^i, and another encoder E'_g will then fuse the features from different groups into g_x.",
            "While global context g_x contains the long-distance context related to the querying point, it still needs delicate local shape context to predict occupancy. For this reason, the local context l_x draws the information from observed point clouds and the position embeddings of the relative translation to the nearest grasp point. We first find x’s nearest neighbour p_x in P and the corresponding point cloud embedding e_{p_x}, then the local context l_x is calculated as: l_x = [e_{p_x}, MLP(x - p_x)].",
            "At last, the queried feature h_x is obtained by concatenating g_x and l_x, and an MLP based decoder predicts the occupancy probability of x according to h_x."
        ],
        "final_answer": "The voxel query module takes the global tri-plane feature vector (g_x) and the local context vector (l_x), concatenates them into a single feature, and then feeds this combined vector through an MLP decoder to produce the final occupancy prediction.",
        "relevant_elements": [
            "Global Context",
            "Local Context",
            "Voxel Query"
        ],
        "id": 2316,
        "masked_question": "How are [mask1] and [mask2] fused by the voxel query module?",
        "masked_number": 2,
        "masked_elements": [
            "Global Context",
            "Local Context"
        ],
        "figure_path": "./MISS-QA/figures/3_2407.15771v1_figure_2.png",
        "paperid": "2407.15771v1",
        "paper_path": "./MISS-QA/papers/2407.15771v1.json",
        "figure_id": "2407.15771v1_figure_2.png",
        "caption": "Figure 2: Model architecture of the proposed local occupancy-enhanced object grasping. It first identifies a number of local occupancy regions of interest. Then multi-group tri-plane aggregates the scene context for local occupancy estimation. Finally the occupancy-enhanced local shape feature in each grasp region is extracted by fusing the information of both explicit voxels and implicit queried features, and is decoded to grasp poses.",
        "qtype": "Implementation_Details",
        "response": "Here’s what happens in the “Voxel Query” block (the orange “Fusion” node in Fig. 2):\n\n1. From the red‐boxed Multi-Group Tri-Plane you get a per-voxel ​global context​ f g:  \n   – You bi-linearly interpolate each of the three planes in each of the K rotated tri-planes at your query point,  \n   – You fuse the three interpolated features of each group by a small shared encoder Eᶠᵘˢᵉᵍ,  \n   – You then fuse across the K groups by another encoder Eᶠᵘˢᵉᵍᵍ to yield f g.\n\n2. From the blue-boxed Local Context you get a per-voxel ​local context​ f l:  \n   – You find the nearest observed point in the original point-cloud embedding,  \n   – You take its embedding e and concatenate it with a learned position-embedding of the offset to your query point,  \n   – You pass that through a small MLP Eᶠᵘˢᵉˡ to yield f l.\n\n3. Finally, you fuse global and local by simple concatenation:  \n   f q = [ f g ; f l ]  \n   and feed f q into a final MLP decoder that predicts the occupancy probability of that voxel."
    },
    {
        "question": "How do Pkwn and Punk conceptually relate to shared-private information separation in multi-target adaptation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Pkwn",
            "Punk"
        ],
        "id": 2320,
        "masked_question": "How do [mask1] and [mask2] conceptually relate to shared-private information separation in multi-target adaptation?",
        "masked_number": 2,
        "masked_elements": [
            "Pkwn",
            "Punk"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.00397v1_figure_2.png",
        "paperid": "2409.00397v1",
        "paper_path": "./MISS-QA/papers/2409.00397v1.json",
        "figure_id": "2409.00397v1_figure_2.png",
        "caption": "Figure 2: The architecture overview of COSMo, where ℱvsubscriptℱ𝑣\\mathcal{F}_{v}caligraphic_F start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT and ℱtsubscriptℱ𝑡\\mathcal{F}_{t}caligraphic_F start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT are the frozen pretrained CLIP s image and text encoders, respectively. Pk w nsubscript𝑃𝑘𝑤𝑛P_{kwn}italic_P start_POSTSUBSCRIPT italic_k italic_w italic_n end_POSTSUBSCRIPT and Pu n ksubscript𝑃𝑢𝑛𝑘P_{unk}italic_P start_POSTSUBSCRIPT italic_u italic_n italic_k end_POSTSUBSCRIPT denote the prompts for the known and unknown classes, respectively. ℬθ (⋅)subscriptℬ𝜃⋅\\mathcal{B}_{\\theta}(\\cdot)caligraphic_B start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( ⋅ ) represents the domain specific bias network, which generates the domain-bias context tokens β𝛽\\betaitalic_β. Best view in color.",
        "qtype": "Literature_Background",
        "response": "In COSMo’s prompt‐learning block you can literally see a two–part decomposition that mirrors the “shared vs. private” split that people have long used in multi‐target DA.  If you zoom into the red outline ([mask1]) you are looking at the full set of prompts after we have injected the domain‐bias signal into them – {\\it Pₖ¹, Pₖ², …, Pₖ⁽|Cₖ|⁾ and Pᵤ} – these carry the private, domain‐specific adjustments on top of the bare semantic prompt.  In contrast, the blue–outlined box ([mask2]) is just the raw, domain‐agnostic chunk of each prompt – {\\it Pₖwn and Pₙₖ} – the version that only encodes the class‐level (shared) information.  \n\nIn other words:  \n• The blue box ([mask2]) are the “shared” embeddings (one prompt for all known classes, one for the unknown class), trained to capture class semantics common to every target.  \n• The red box ([mask1]) are those same prompts after adding in the per‐domain bias tokens (via the DSBN), i.e. the “private” part that picks up whatever is unique to each target domain.  \n\nThat split of “base” prompt plus “domain‐bias” prompt is exactly analogous to the shared–private feature decomposition you see in classical multi‐target DA."
    },
    {
        "question": "How does Motion Event Loss interact with Differentiable Rasterization for optimizing 3D Gaussians?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Motion Event Loss",
            "Differentiable Rasterization",
            "3D Gaussians"
        ],
        "id": 2321,
        "masked_question": "How does [mask1] interact with Differentiable Rasterization for optimizing 3D Gaussians?",
        "masked_number": 1,
        "masked_elements": [
            "Motion Event Loss"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.16995v1_figure_1.png",
        "paperid": "2410.16995v1",
        "paper_path": "./MISS-QA/papers/2410.16995v1.json",
        "figure_id": "2410.16995v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed E-3DGS framework. This framework integrates motion and exposure events for training 3DGS to effectively handle diverse real-world conditions. We utilize Temporal-to-Intensity Mapping to convert exposure events into intensity images, which yield camera trajectories and a sparse point cloud for 3DGS training. The optimization of 3DGS parameters is supervised through motion event loss and exposure event loss.",
        "qtype": "Literature_Background",
        "response": "First, let us be precise about what “​[mask1]​” is in that red‐boxed region of the figure: it is the Motion-Event Loss module.  Here is how it hooks into Differentiable Rasterization to drive the 3D Gaussians toward the correct geometry and appearance:\n\n1.  Starting from the current set of 3D Gaussians, we project and then “splat” them onto the image plane via the Differentiable Rasterization block.  This produces a pair (or sequence) of rendered intensity frames  \n   L(u_k, t_k) and L(u_k, t_{k−1}) at two closely spaced time instants.\n\n2.  We compute a simulated event stream by taking the per-pixel difference in log-brightness,  \n   ΔL = L(u_k, t_k) − L(u_k, t_{k−1})  \n   wherever that change exceeds the event threshold.  In other words, differentiable splatting + temporal differencing gives us the “events” that our current Gaussians would generate.\n\n3.  The real, recorded motion events are then compared against this “simulated event stream” inside the Motion-Event Loss.  Any discrepancy becomes the loss signal.\n\n4.  Crucially, because the rasterization is fully differentiable, that loss gradient is propagated back through the rasterizer → projection → 3D-Gaussian parameters (means, covariances, opacities, colors).  In this way the Motion-Event Loss “pulls” the Gaussians into a configuration that produces rendered intensity changes whose timing and polarity match the actual event camera output.\n\nIn short: the Motion-Event Loss block uses the differentiable rasterizer to turn the current 3D Gaussians into a simulated event stream (via rendered frame differences), compares that stream to the real events, and then back-propagates the error through the rasterizer to update the Gaussians."
    },
    {
        "question": "How does Exposure Event Loss shape Adaptive Density Control to refine 3D Gaussians?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Exposure Event Loss",
            "Adaptive Density Control",
            "3D Gaussians"
        ],
        "id": 2322,
        "masked_question": "How does [mask1] shape [mask2] to refine 3D Gaussians?",
        "masked_number": 2,
        "masked_elements": [
            "Exposure Event Loss",
            "Adaptive Density Control"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.16995v1_figure_1.png",
        "paperid": "2410.16995v1",
        "paper_path": "./MISS-QA/papers/2410.16995v1.json",
        "figure_id": "2410.16995v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed E-3DGS framework. This framework integrates motion and exposure events for training 3DGS to effectively handle diverse real-world conditions. We utilize Temporal-to-Intensity Mapping to convert exposure events into intensity images, which yield camera trajectories and a sparse point cloud for 3DGS training. The optimization of 3DGS parameters is supervised through motion event loss and exposure event loss.",
        "qtype": "Literature_Background",
        "response": "Here’s what’s going on in the red and blue boxes and how they talk to one another:\n\n1.  Mask 1 is the “Exposure Event Loss,”  \n2.  Mask 2 is the “Adaptive Density Control” module that sits inside the 3D‐Gaussian optimizer.  \n\nChain of thought:\n\n- The Exposure Event Loss is simply an L₂ pixel‐wise error between (a) the intensity image you get by the temporal‐to‐intensity mapping of the exposure events and (b) the same view rendered by your current set of 3D Gaussians.  \n- Because our whole splatting/rasterization pipeline is differentiable, that per‐pixel squared error produces gradients not only on the colors of the Gaussians but also on their positions, opacities and covariances.  \n- Those gradients are handed off into the Adaptive Density Control block, which makes two key moves:  \n   • In regions where the exposure‐image error is high, it will split Gaussians apart (adding more “blobs”) so you can capture finer detail.  \n   • In regions where the error is low, it may merge or prune Gaussians so you don’t waste density where it isn’t needed.  \n- Over successive optimization steps, this tight feedback loop—exposure‐loss → gradient backprop → adaptive density splits/merges—sculpts precisely the right number, shape and spread of Gaussians to match the rich textures revealed by your exposure events.\n\nIn short:  \nThe Exposure Event Loss supplies per‐pixel errors on the rendered exposure images, and through backpropagation into the differentiable rasterizer it drives the Adaptive Density Control to split or prune Gaussians in exactly those places where the exposure images demand more (or less) representation. That is how the exposure‐based loss “shapes” the density controller to refine your 3D Gaussians."
    },
    {
        "question": "How does adaptive density control influence differentiable rasterization of 3D gaussians for improved scene sampling?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Adaptive Density Control",
            "Differentiable Rasterization",
            "3D Gaussians"
        ],
        "id": 2324,
        "masked_question": "How does [mask1] influence [mask2] of 3D gaussians for improved scene sampling?",
        "masked_number": 2,
        "masked_elements": [
            "Adaptive Density Control",
            "Differentiable Rasterization"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.16995v1_figure_1.png",
        "paperid": "2410.16995v1",
        "paper_path": "./MISS-QA/papers/2410.16995v1.json",
        "figure_id": "2410.16995v1_figure_1.png",
        "caption": "Figure 1: Overview of the proposed E-3DGS framework. This framework integrates motion and exposure events for training 3DGS to effectively handle diverse real-world conditions. We utilize Temporal-to-Intensity Mapping to convert exposure events into intensity images, which yield camera trajectories and a sparse point cloud for 3DGS training. The optimization of 3DGS parameters is supervised through motion event loss and exposure event loss.",
        "qtype": "Experimental_Results",
        "response": "Mask₁ is the “Adaptive Density Control” module and Mask₂ is the “Differentiable Rasterization” step.  Here’s what happens, step by step:\n\n1.  We start with a cloud of anisotropic 3D Gaussians (each with a mean, covariance and opacity).  \n2.  The Adaptive Density Control (Mask₁) looks at the current distribution of those Gaussians and how well they explain the incoming event‐based losses (both motion‐event and exposure‐event losses).  \n    –  In regions where the loss is still high or where events suggest fine detail, it will split Gaussians (increasing local density).  \n    –  In over-covered or low-detail areas it will merge or prune Gaussians (decreasing local density).  \n3.  That refined, adaptively re-balanced set of Gaussians is then handed to the Differentiable Rasterization stage (Mask₂).  \n4.  During rasterization, each Gaussian is projected, weighted by its updated covariance/opacity, and blended to form the 2D image.  Because Mask₁ has already made sure the Gaussians densely cover high-detail regions (and don’t waste effort on empty space), Mask₂ ends up sampling exactly where it needs to, producing sharper reconstructions and faster convergence.  \n\nIn short, the red‐boxed Adaptive Density Control optimizes how many and how large the Gaussians are in each part of the scene.  That directly shapes the input to the blue‐boxed Differentiable Rasterization, so that rendering focuses computational effort on the most informative parts of the scene and yields higher‐quality, more efficient 3D sampling."
    },
    {
        "question": "How does uncertainty modeling enhance comparator reliability in order learning via Monte Carlo sampling?",
        "relevant_section_ids": [
            "3.2",
            "3.3"
        ],
        "relevant_context": [
            "Specifically, we model the human ratings of an instance x as a multi-dimensional Gaussian distribution P in the space, which is used as a feature for pairwise comparisons, as shown in the right of Fig. 3.",
            "Firstly, we build up a Gaussian distribution in the high-dimensional psychological scale space according to the human ratings. Then, we randomly sample from these Gaussian distributions for pairwise comparisons. This process can be considered as disturbing a single feature point on the latent space, which is the feature level augmentation.",
            "Afterwards, we apply T times Monte Carlo sampling on the distribution of instance x_i, which is analogous to the observations of multiple subjects on a stimulus.",
            "The comparator C in conventional order learning is applied to learn the order between two sampling feature points. The relative relation R(P_i,P_j) between two distributions of P_i and P_j is obtained by calculating the mean of C comparisons."
        ],
        "final_answer": "By modeling each instance’s features as a Gaussian distribution and then drawing multiple Monte Carlo samples from these distributions, the comparator evaluates many perturbed feature realizations rather than a single fixed point. Averaging the comparator’s outputs over all sampled pairs incorporates the uncertainty in human ratings, reduces sensitivity to noise or outliers, and yields more stable, reliable order relations in the order learning module.",
        "relevant_elements": [
            "Uncertainty Modeling",
            "Order Learning",
            "Distribution Comparison"
        ],
        "id": 2325,
        "masked_question": "How does [mask1] enhance comparator reliability in [mask2] via Monte Carlo sampling?",
        "masked_number": 2,
        "masked_elements": [
            "Uncertainty Modeling",
            "Order Learning"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.00603v1_figure_2.png",
        "paperid": "2409.00603v1",
        "paper_path": "./MISS-QA/papers/2409.00603v1.json",
        "figure_id": "2409.00603v1_figure_2.png",
        "caption": "Figure 2: (a) The training phase of UOL. The order of distributions is constrained by cross entropy loss and hinge loss, and the dispersion of the distributions is constrained by KL loss. (b) The estimation phase of UOL. In uncertainty modeling, the FB of a facial image is modeled by a multi-dimensional Gaussian distribution whose mean μ𝜇\\muitalic_μ and diagonal covariance ΣΣ\\Sigmaroman_Σ are learned by VGG from the image. In distribution comparison, we sample from both the distributions of test image and reference image to form a pair and predict its order by a comparator in order learning. After having the order relations of T𝑇Titalic_T pairs between reference images and the test image, the Bradley-Terry model is applied to estimate the score of the test image.",
        "qtype": "Experimental_Results",
        "response": "Here’s what happens, step by step:\n\n1. In the red‐boxed “Uncertainty Modeling” module we turn each image’s feature vector into a Gaussian  \n   N(μ(x), Σ(x)) by having VGG16 predict both a mean μ and a diagonal covariance Σ.  \n2. Rather than feeding a single point into the order comparator, we draw T samples z₁,…,z_T from that Gaussian via the reparameterization trick.  \n3. In the blue‐boxed “Order Learning” module (the comparator) we compare each pair of sampled points (one from image A’s distribution, one from B’s) and get T independent order predictions.  \n4. Finally we average those T predictions to yield one consensus relation between A and B.  \n\nBy sampling repeatedly from the learned uncertainty distribution, the comparator sees many slightly different views of each face.  Aggregating over them cancels out random fluctuations and mimics multiple human judgments, which makes the comparator far more robust—and therefore more reliable—when deciding which face looks “better.”"
    },
    {
        "question": "How does the Bradley-Terry Estimation Module integrate comparator outputs for final score estimation?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Specifically, an input with unknown score s is compared with a reference image with known score s_i. Bradley–Terry model tries to estimate the best s, and then models the possible order result r and score difference δ as the following probability distribution, ... where 0, 1 and 2 represent the “>”, “≈” and “<” relations. S denotes the set of all scores in the reference set.",
            "Suppose M images exist in the reference set and their ground truth scores are {s_i}. We apply the optimized comparator C to predict the order between the input and each reference image, which results in R={r_i, δ_i}, then maximize the likelihood function, L(s)=∏_{i=1}^M P(r_i, δ_i | s, s_i; θ). Finally, the FB score ŝ of the input image can be obtained."
        ],
        "final_answer": "The module takes the comparator’s pairwise outputs r_i (order relation) and δ_i (score‐difference bin) against each of M reference faces, plugs them into the Bradley–Terry probability model P(r_i, δ_i | s, s_i; θ), forms the joint likelihood L(s)=∏_i P(r_i, δ_i | s, s_i; θ), and then finds the score s that maximizes this likelihood as the final FB score estimate.",
        "relevant_elements": [
            "Bradley-Terry Estimation Module",
            "Order Learning"
        ],
        "id": 2326,
        "masked_question": "How does the [mask1] integrate comparator outputs for final score estimation?",
        "masked_number": 1,
        "masked_elements": [
            "Bradley-Terry Estimation Module"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.00603v1_figure_2.png",
        "paperid": "2409.00603v1",
        "paper_path": "./MISS-QA/papers/2409.00603v1.json",
        "figure_id": "2409.00603v1_figure_2.png",
        "caption": "Figure 2: (a) The training phase of UOL. The order of distributions is constrained by cross entropy loss and hinge loss, and the dispersion of the distributions is constrained by KL loss. (b) The estimation phase of UOL. In uncertainty modeling, the FB of a facial image is modeled by a multi-dimensional Gaussian distribution whose mean μ𝜇\\muitalic_μ and diagonal covariance ΣΣ\\Sigmaroman_Σ are learned by VGG from the image. In distribution comparison, we sample from both the distributions of test image and reference image to form a pair and predict its order by a comparator in order learning. After having the order relations of T𝑇Titalic_T pairs between reference images and the test image, the Bradley-Terry model is applied to estimate the score of the test image.",
        "qtype": "Experimental_Results",
        "response": "They take all of the little “> / ≃ / <” judgments the learned comparator makes between the unknown face and each reference face, plug them into a standard Bradley–Terry likelihood over score‐differences, and then solve for the test face’s score by maximum likelihood.  In practice you collect the comparator’s 3-way outputs for your T reference comparisons, form  \n L(s) = ∏_{j=1}^T P_{BT}(relation_j ∣ s, s_j)  \nwhere s_j is the jth reference score and P_{BT} is the Bradley–Terry probability for seeing “>,” “≈,” or “<” given a candidate s.  You then pick the s that maximizes L(s), and that is your final estimated FB score."
    },
    {
        "question": "How does Monte Carlo sampling variability affect distribution comparison stability under limited computational resources?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Monte Carlo sampling",
            "distribution comparison"
        ],
        "id": 2327,
        "masked_question": "How does [mask1] variability affect distribution comparison stability under limited computational resources?",
        "masked_number": 1,
        "masked_elements": [
            "Monte Carlo sampling"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.00603v1_figure_2.png",
        "paperid": "2409.00603v1",
        "paper_path": "./MISS-QA/papers/2409.00603v1.json",
        "figure_id": "2409.00603v1_figure_2.png",
        "caption": "Figure 2: (a) The training phase of UOL. The order of distributions is constrained by cross entropy loss and hinge loss, and the dispersion of the distributions is constrained by KL loss. (b) The estimation phase of UOL. In uncertainty modeling, the FB of a facial image is modeled by a multi-dimensional Gaussian distribution whose mean μ𝜇\\muitalic_μ and diagonal covariance ΣΣ\\Sigmaroman_Σ are learned by VGG from the image. In distribution comparison, we sample from both the distributions of test image and reference image to form a pair and predict its order by a comparator in order learning. After having the order relations of T𝑇Titalic_T pairs between reference images and the test image, the Bradley-Terry model is applied to estimate the score of the test image.",
        "qtype": "Others",
        "response": "Here’s how variability in that red‐boxed “Distribution Comparison via Monte Carlo Sampling” block plays out when you can’t afford an arbitrarily large number of samples:\n\n1. What the block does  \n   – For each face you’ve modeled as a Gaussian N(μ, Σ), you draw T independent samples z¹, z², … zᵀ via the reparameterization trick.  \n   – You feed each pair (zᵢ, zⱼ) through the same small comparator network and average its T binary (or ternary) judgments into a single “probability that face i > face j.”\n\n2. Where variability comes from  \n   – Because you only draw a finite T samples, the Monte Carlo estimate of P(i > j) has variance Var[ˆP] ≃ p(1−p)/T.  \n   – At small T that variance is large, so the averaged comparator output fluctuates from batch to batch and makes your pairwise‐ordering loss noisy.\n\n3. Impact on stability  \n   – Noisy pairwise scores ↦ unstable gradients ↦ slower, bouncier training and less consistent ordering at test time.  \n   – If you try to clamp down that noise by increasing T, you pay for it in extra forward/backward passes through the comparator.\n\n4. The resource trade-off  \n   – Under tight GPU/time/memory budgets you can’t simply boost T to infinity. Every extra sample is one more pass through the VGG‐backbone’s last layers and the comparator, so it scales your compute linearly.  \n   – Practitioners typically pick a modest T (e.g. 5–20) that gives tolerable noise, and rely on the hinge‐loss Wasserstein constraints (which do not require sampling) to keep those Gaussians well spaced even when T is small.\n\nIn short, with limited compute the Monte Carlo block injects sampling noise into your pairwise‐comparison scores; you can tame that noise only by lining up more samples (higher T) at a corresponding increase in computational cost."
    },
    {
        "question": "What biases emerge from an unbalanced reference set in the Bradley-Terry estimation module affecting fairness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "reference set",
            "Bradley-Terry estimation module"
        ],
        "id": 2328,
        "masked_question": "What biases emerge from an unbalanced [mask1] in the Bradley-Terry estimation module affecting fairness?",
        "masked_number": 1,
        "masked_elements": [
            "reference set"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.00603v1_figure_2.png",
        "paperid": "2409.00603v1",
        "paper_path": "./MISS-QA/papers/2409.00603v1.json",
        "figure_id": "2409.00603v1_figure_2.png",
        "caption": "Figure 2: (a) The training phase of UOL. The order of distributions is constrained by cross entropy loss and hinge loss, and the dispersion of the distributions is constrained by KL loss. (b) The estimation phase of UOL. In uncertainty modeling, the FB of a facial image is modeled by a multi-dimensional Gaussian distribution whose mean μ𝜇\\muitalic_μ and diagonal covariance ΣΣ\\Sigmaroman_Σ are learned by VGG from the image. In distribution comparison, we sample from both the distributions of test image and reference image to form a pair and predict its order by a comparator in order learning. After having the order relations of T𝑇Titalic_T pairs between reference images and the test image, the Bradley-Terry model is applied to estimate the score of the test image.",
        "qtype": "Others",
        "response": "Chain of Thought:\n1. In the estimation phase (Fig. 2b), the red box marks the “Reference Set” of images with known beauty scores that the unknown test image is pairwise‐compared against.  \n2. Bradley–Terry turns those pairwise win/loss probabilities into a maximum‐likelihood estimate of the test image’s score.  \n3. If the reference pool is skewed—say most exemplars sit in the “medium beauty” range and very few at the high or low end—then the likelihood fit will be dominated by comparisons to those medium‐rated faces.  \n4. Practically, that means the model’s outputs get “pulled” toward the overrepresented bins (a regression‐to‐the‐mean), and true extremes (very high or very low beauty) are systematically under- or over-estimated.  \n5. From a fairness standpoint, individuals whose true beauty scores lie in under-represented ranges (or demographic groups that happen to be under-sampled) will be mis‐scored, entrenching bias.  \n\nAnswer:\nBecause Bradley–Terry relies on comparing to all reference faces, an unbalanced reference set (e.g. too many medium-rated examples and too few high- or low-rated ones) skews the likelihood toward the dense bins. This produces a central‐tendency bias—predicted scores are dragged toward the majority intervals—leading to systematic under- or over-estimation of under-represented cases and thus fairness violations."
    },
    {
        "question": "What challenges might arise from computing MaxRényi-K% exclusively on image slice logits?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "image slice",
            "MaxRényi-K%"
        ],
        "id": 2329,
        "masked_question": "What challenges might arise from computing [mask1] exclusively on image slice logits?",
        "masked_number": 1,
        "masked_elements": [
            "MaxRényi-K%"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.02902v1_figure_1.png",
        "paperid": "2411.02902v1",
        "paper_path": "./MISS-QA/papers/2411.02902v1.json",
        "figure_id": "2411.02902v1_figure_1.png",
        "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRényi-K% metric: we first get the Rényi entropy of each token position, then select the largest k%percent𝑘k\\%italic_k % tokens and calculate the average Rényi entropy.",
        "qtype": "Others",
        "response": "Computing MaxRényi–K % only over the “image-slice” logits runs into a number of practical difficulties:\n\n1.   No “real” token IDs.  In the text slices you know exactly what the next token ought to be, so you can do both target-free (entropy) and target-based (next-token probability) attacks.  In the image slice you never actually have ground-truth “image tokens,” you only have the model’s projection of the image embedding into its text‐vocabulary logits.  That means you can only fall back on purely target-free measures, and you lose any advantage of peeking at the probability of a known correct token.\n\n2.   Very few positions and low signal‐to‐noise.  The image slice is typically only a handful of pseudo-token positions, so your Rényi‐entropy estimates are based on a tiny sample.  That makes your K % selection extremely high‐variance (e.g. picking the “top 10 %” of ten values is very unstable) and erodes your ability to see a clear member/non-member gap.\n\n3.   Logits are “blurry.”  Those early image logits are just the model guessing what text might follow an image embedding, not the kinds of sharp, distinctive probability spikes you get when you’re right in the middle of producing fluent text.  In practice they tend to be rather flat and undifferentiated, so even computing Rényi entropy over them gives you almost no discriminative power.\n\n4.   Lack of context conditioning.  Since the image slice comes before any instruction or generated description, it isn’t yet conditioned on anything you control.  All of the real “membership signal” seems to live in the way the model completes your prompt once it has seen the image plus the instruction.  If you throw away that later context and look only at the raw image logits, you end up throwing away most of the membership signal.\n\nIn short, the combination of “no ground-truth tokens,” “tiny unstable samples,” and “weak, un‐conditioned logits” makes a MaxRényi–K % attack on the image slice alone both brittle and low-signal compared to doing it on the instruction or description slices."
    },
    {
        "question": "What ethical concerns emerge when utilizing description slice logits for membership inference?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "description slice",
            "membership inference"
        ],
        "id": 2330,
        "masked_question": "What ethical concerns emerge when utilizing [mask1] slice logits for membership inference?",
        "masked_number": 1,
        "masked_elements": [
            "description"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.02902v1_figure_1.png",
        "paperid": "2411.02902v1",
        "paper_path": "./MISS-QA/papers/2411.02902v1.json",
        "figure_id": "2411.02902v1_figure_1.png",
        "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRényi-K% metric: we first get the Rényi entropy of each token position, then select the largest k%percent𝑘k\\%italic_k % tokens and calculate the average Rényi entropy.",
        "qtype": "Others",
        "response": "Chain-of-thought:\n\n1. In the figure, the red box is around the logits that correspond to the *generated description* tokens (i.e. the “description” slice).  \n2. The paper shows that by slicing out those description logits and computing a statistic such as MaxRényi-K%, an attacker can decide whether the *image* or *text* that produced that description was in the model’s training set.  \n3. Turning that capability loose in the real world means an adversary can probe a deployed VLLM simply by getting it to describe an image and then examining its internal next‐token scores to decide membership.  \n4. Such a method therefore raises clear privacy and confidentiality concerns: it allows someone to infer whether personal, proprietary or medical photos (or their captions) ever appeared in the model’s training data, potentially disclosing sensitive information without consent.  \n5. More broadly, it can undermine user trust, violate data‐protection laws (GDPR, HIPAA, etc.), and open the door to discrimination or other harms if membership in certain datasets becomes public knowledge.\n\nAnswer:\n\nWhen an attacker uses the *description‐slice* logits to perform membership inference, they can effectively tell whether an individual’s image or text was included in the model’s training corpus.  This threatens user privacy and data confidentiality—potentially exposing personal, medical, or proprietary images/captions without consent—and risks undermining legal and ethical commitments around data protection."
    },
    {
        "question": "What motivates using separate generation and inference stages in the image detection pipeline?",
        "relevant_section_ids": [
            "5.1"
        ],
        "relevant_context": [
            "However, a major challenge for image MIA is that we do not have the ground-truth image tokens. Only the embeddings of images are available, which prevents directly transferring many target-based MIA from languages to images.",
            "In generation stage, we provide the model with an image followed by an instruction to generate a textual sequence. Subsequently, in inference stage, we feed the model with the concatenation of the same image, instruction, and generated description text."
        ],
        "final_answer": "Because VLLMs only expose image embeddings—not discrete image tokens—the attack cannot directly apply token-based MIA methods. By first generating a textual description of the image (generation stage) and then feeding the image, instruction, and generated text back into the model (inference stage), the pipeline obtains token-level logits across image, instruction, and description segments, which can be used to compute membership-inference metrics.",
        "relevant_elements": [
            "generation stage",
            "inference stage"
        ],
        "id": 2331,
        "masked_question": "What motivates using separate [mask1] and [mask2] in the image detection pipeline?",
        "masked_number": 2,
        "masked_elements": [
            "generation stage",
            "inference stage"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.02902v1_figure_1.png",
        "paperid": "2411.02902v1",
        "paper_path": "./MISS-QA/papers/2411.02902v1.json",
        "figure_id": "2411.02902v1_figure_1.png",
        "caption": "Figure 1: MIAs against VLLMs. Top: Our image detection pipeline: In the generation stage, we feed the image and instruction to the target model to obtain a description; then during the inference stage, we input the image, instruction, and generated description to the model, and extract the logits slices to calculate metrics. Bottom: MaxRényi-K% metric: we first get the Rényi entropy of each token position, then select the largest k%percent𝑘k\\%italic_k % tokens and calculate the average Rényi entropy.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key idea in a nutshell:\n\n1.  Why two boxes at all?  \n    •  In a VLLM we only ever see the *image embedding* (no “ground‐truth” image tokens), so we can’t directly do a token‐level membership attack on the picture itself.  \n    •  We therefore break the attack into two phases:  \n       –  **Mask 1 (red box): Generation stage.**  We prompt the model with the *image* + *instruction* and *generate* a text description.  \n       –  **Mask 2 (blue box): Inference stage.**  We then re-feed the *image*, *instruction*, and *that same generated description* back into the model, capture its output logits, and slice them into three segments (the image­–latent slice, the instruction slice, and the description slice).\n\n2.  What does this buy us?  \n    •  By generating our own text “surrogate” for the image, we now have a *token sequence* (the description) over which we *can* compute Rényi‐entropy (or other) membership scores.  \n    •  In the inference stage we collect the model’s *next‐token* distributions (§3.2) at each position—both for the image‐latent positions (where you’d normally have no tokens) and for the text tokens.  We then apply our MaxRényi-K% (or ModRényi) scoring to those slices.  \n    •  In practice, even if you could only get the logits for the *description* slice (i.e. you never see the image or instruction logits), you already have a fully automatic token‐level membership inference on a text you generated.\n\nSo the *motivation* for the two separate boxes is simply that the attack needs:\n\n  1.  a generation stage (to create a text proxy for the image) and  \n  2.  an inference stage (to re-run the model on that text + image + instruction and harvest logits at *every* position),  \n\nwithout ever having “real” image tokens."
    },
    {
        "question": "How does critic C integrate generator G outputs and dataset D samples during iterative training?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "critic C",
            "generator G",
            "dataset D"
        ],
        "id": 2337,
        "masked_question": "How does [mask1] integrate generator G outputs and dataset D samples during iterative training?",
        "masked_number": 1,
        "masked_elements": [
            "critic C"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.09125v1_figure_1.png",
        "paperid": "2409.09125v1",
        "paper_path": "./MISS-QA/papers/2409.09125v1.json",
        "figure_id": "2409.09125v1_figure_1.png",
        "caption": "Figure 1: Illustration of the model architecture. (A) Architecture of the model, with generator G producing generated samples, and dataset D producing biological samples, which are both used as input for critic C. (B) Architecture of generator. In the upper left corner, the generator composed of several sub-generators is shown. The bottom part shows that each sub-generator is a quantum circuit following a re-uploading scheme. Here a noise-encoding layer and a parametrized layer are repeated for l𝑙litalic_l layers, with the parametrized layer ansatz of each parametrized layer shown in the top right side. After trained, the generator can be used to produce samples (D) similar to samples obtained from the biological dataset (C).",
        "qtype": "Implementation_Details",
        "response": "Here, “[mask1]” is simply the **critic (C)** in the QGAN loop.  In each training iteration:\n\n 1.  We draw a minibatch of real spike‐train samples from the dataset D (the green box in the figure) and a minibatch of fake spike‐trains G(z) produced by the quantum generator G.  \n 2.  Both sets of rasters are fed into the critic network C, which is a small fully‐connected neural net that spits out a scalar “realness” score for each sample.  \n 3.  The critic’s weights are then updated (twice, in the WGAN schedule) to **maximize**  \n       Eₓ∼D[C(x)] – E_z∼p(z)[C(G(z))]  \n    i.e. to push its scores on real data up and its scores on generated data down.  \n 4.  Once the critic has been updated, we freeze C, backpropagate through G so as to **minimize**  \n       E_z∼p(z)[C(G(z))] + λ·K_loss(G(z), D)  \n    (the usual WGAN term plus the extra “K‐loss” spike‐count penalty), and update the generator’s parameters.  \n\nBy juggling these two steps—updating C with both real and fake samples, then updating G against C’s feedback—the model aligns G’s output distribution to that of the real dataset."
    },
    {
        "question": "How do generator G outputs drive critic C updates analogous to classical adversarial frameworks?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "generator G",
            "critic C"
        ],
        "id": 2339,
        "masked_question": "How do [mask1] outputs drive [mask2] updates analogous to classical adversarial frameworks?",
        "masked_number": 2,
        "masked_elements": [
            "generator G",
            "critic C"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.09125v1_figure_1.png",
        "paperid": "2409.09125v1",
        "paper_path": "./MISS-QA/papers/2409.09125v1.json",
        "figure_id": "2409.09125v1_figure_1.png",
        "caption": "Figure 1: Illustration of the model architecture. (A) Architecture of the model, with generator G producing generated samples, and dataset D producing biological samples, which are both used as input for critic C. (B) Architecture of generator. In the upper left corner, the generator composed of several sub-generators is shown. The bottom part shows that each sub-generator is a quantum circuit following a re-uploading scheme. Here a noise-encoding layer and a parametrized layer are repeated for l𝑙litalic_l layers, with the parametrized layer ansatz of each parametrized layer shown in the top right side. After trained, the generator can be used to produce samples (D) similar to samples obtained from the biological dataset (C).",
        "qtype": "Literature_Background",
        "response": "In SpiQGAN exactly the same “feedback‐loop’’ that you see in classical GANs is in place.  Concretely, at each training step the red–boxed module (the critic C) produces a scalar score on the samples coming out of the generator; those scores are then plugged straight into the generator’s loss function (in our case the negative of the critic’s estimate plus the little K-loss spike‐count penalty).  We back-propagate that quantity all the way through the parametrised quantum circuit and perform an optimizer step on the blue–boxed module (the generator G), exactly as you would in a standard adversarial setting."
    },
    {
        "question": "How does context attention extend prefix-tuning cross-attention mechanisms from Li and Liang?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "Inspired by the prefix-tuning mechanism employed in Li and Liang (2021), Liu et al. (2022a), and Liu et al. (2023), we project the input context (c), the concatenation of retrieved response (r) (refer to Section 4.4) and empathy signals (e), and listener’s past responses (h) into fixed-length prefix vectors, which are then prepended to the decoder hidden states as a prefix.",
            "To separately extract distinct context-related empathy and personality features, we introduce two learnable embeddings to act as distinct queries, Q1 and Q2, where Q1 is in R^{L1×d} and Q2 in R^{L2×d}; here, d represents the dimension of the RoBERTa’s last hidden layer, while L1 and L2 denote the lengths of the respective queries. The context representation Hc serves as both key Kc and value Vc. Employing a cross-attention mechanism, we project context Hc into two fixed-length prefix vectors. These vectors are subsequently treated as Pc1 and Pc2."
        ],
        "final_answer": "Rather than using a single shared prefix as in Li and Liang’s original prefix-tuning cross-attention, the paper adds a ‘context attention’ step: it defines two learnable query embeddings (Q1 and Q2) that each attend over the encoded context (Hc) via cross-attention. This yields two separate, fixed-length prefix vectors—one capturing context–personality interactions and the other context–empathy interactions—which are then concatenated (along with prefixes derived from past responses and empathy signals) to form a multi-grained prefix for the decoder.",
        "relevant_elements": [
            "Context Attention"
        ],
        "id": 2340,
        "masked_question": "How does [mask1] extend prefix-tuning cross-attention mechanisms from Li and Liang?",
        "masked_number": 1,
        "masked_elements": [
            "Context Attention"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.02271v1_figure_2.png",
        "paperid": "2408.02271v1",
        "paper_path": "./MISS-QA/papers/2408.02271v1.json",
        "figure_id": "2408.02271v1_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method that contains a multi-grained prefix encoder and personality reinforcement module.",
        "qtype": "Literature_Background",
        "response": "In Li & Liang’s original prefix-tuning you learn a small set of “virtual’’ tokens that are simply prepended to every layer of the Transformer and back-prop through them as extra parameters.  What we have boxed in red is an extra *cross-attention* step on top of that, which we call the Context Attention.  Concretely:\n\n  1.  We take your encoded context C (from RoBERTa) as both the Key (K) and Value (V) for a cross-attention.\n  2.  We introduce two *learnable* query matrices Qₚ (for personality) and Qₑ (for empathy).\n  3.  We perform two separate context-to-prefix attentions:\n       • Qₚ attends to (K,C) ⇒ produces Oₚ (a personality prefix)\n       • Qₑ attends to (K,C) ⇒ produces Oₑ (an empathy prefix)\n  4.  These two context-derived prefix vectors (Oₚ, Oₑ) then get fused with listener-past-response embeddings and empathy-signal embeddings in the next attention blocks.\n\nIn this way we have “extended” Li & Liang’s one-shot cross-attention prefix step into a *dual-stream*, multi-grained prefix encoder that explicitly disentangles personality and empathy features before you ever prepend anything to the decoder."
    },
    {
        "question": "In the multi-grained prefix encoder, how do empathy attention and personality attention interrelate impact prefix encoding?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "To separately extract distinct context-related empathy and personality features, we introduce two learnable embeddings to act as distinct queries, Q₁ ∈ R^{lₑ×d} and Q₂ ∈ R^{lₚ×d}; here, d represents the dimension of the RoBERTa’s last hidden layer, while lₑ and lₚ denote the lengths of the respective queries. The context representation C serves as both key K_c and value V_c. Employing a cross-attention mechanism, we project context C into two fixed-length prefix vectors. These vectors are subsequently treated as P_c^e and P_c^p.",
            "Then following the same process, we fuse the representations of the listener’s past responses P and the empathy explanation representations E with the context-related prefix vectors P_c^p and P_c^e, respectively.",
            "This fusion process yields two distinct vectors: V_PC (encapsulating the context-personality relationship) and V_EC (representing the context-empathy relationship). This ensures that both personality and empathy dimensions are considered in the context of the interaction.",
            "We then concatenate P_c^e, P_c^p, V_PC, and V_EC along the length dimension, followed by a single linear layer, to produce the final representations P_f as the final prefix embeddings."
        ],
        "final_answer": "The encoder first applies two parallel cross-attention heads—one with an \"empathy\" query and one with a \"personality\" query—against the same context to obtain two prefix vectors (P_c^e and P_c^p). Each of these is then fused with its corresponding extra inputs (empathy signals E or past responses P) to yield two richer vectors (V_EC and V_PC). Finally, all four vectors (the two context-only prefixes plus the two fused outputs) are concatenated and linearly projected to form a single unified prefix embedding that carries both empathy and personality information.",
        "relevant_elements": [
            "Multi-Grained Prefix Encoder",
            "Empathy Attention",
            "Personality Attention"
        ],
        "id": 2342,
        "masked_question": "In the [mask1], how do empathy attention and personality attention interrelate impact prefix encoding?",
        "masked_number": 1,
        "masked_elements": [
            "Multi-Grained Prefix Encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.02271v1_figure_2.png",
        "paperid": "2408.02271v1",
        "paper_path": "./MISS-QA/papers/2408.02271v1.json",
        "figure_id": "2408.02271v1_figure_2.png",
        "caption": "Figure 2: The architecture of our proposed method that contains a multi-grained prefix encoder and personality reinforcement module.",
        "qtype": "Experimental_Results",
        "response": "In that red‐boxed region (the “Multi-Grained Prefix Encoder”) the model actually runs two parallel cross-attention streams – one we call “personality attention” and one we call “empathy attention” – and then merges their outputs into a single prefix for the decoder.  Concretely, the flow is:\n\n 1.  **Context-derived prefix queries**  \n     – We take the RoBERTa encoding of the input context, C, and feed it to two small learnable query matrices Q₁ (for personality) and Q₂ (for empathy).  \n     – A standard cross-attention (Q₁ against C as key/value, and Q₂ against C as key/value) yields two fixed-length “context prefix” vectors: one P_C and one E_C.  \n\n 2.  **Personality attention**  \n     – Treat P_C as the query, and the RoBERTa embeddings of the listener’s past responses (P) as key/value.  \n     – Run one cross-attention to produce M_P, a vector that explicitly encodes how that particular listener’s personality meshes with the current context.  \n\n 3.  **Empathy attention**  \n     – In exactly the same way, treat E_C as the query, and the predicted empathy signals (E) as key/value.  \n     – Run a second cross-attention to produce M_E, a vector that captures how the model’s empathy signals should be grounded in this context.  \n\n 4.  **Concatenate and project**  \n     – Finally, we take the four vectors [P_C; E_C; M_P; M_E], concatenate them along their length-dimension, and run them through one linear layer.  The result is the single prefix embedding that gets prepended into every layer of the DialoGPT decoder.\n\nBecause P_C→M_P and E_C→M_E are computed in parallel but then merged, the final prefix carries *both* personality-aware and empathy-aware information.  This is exactly how “empathy attention” and “personality attention” interrelate to produce a richly biased prefix for generation."
    },
    {
        "question": "How does removing the learnable query in Implicit Pose Indicator affect capturing motion nuances?",
        "relevant_section_ids": [
            "3.2",
            "4.3"
        ],
        "relevant_context": [
            "Section 3.2: \"Nevertheless, motion modeling using sole sparse keypoints is overly simplistic, resulting in the loss of underlying motion patterns. To this end, we draw inspiration from query transformer architecture … and initialize a learnable query vector q_l to complement sparse keypoints. Subsequently, we feed the merged query q_m and get the implicit pose indicator, which contains the essential representation of motion that cannot be represented by the simple 2D pose skeletons.\"",
            "Section 4.3: \"For more detailed analysis about the structure of IPI, we set up several variants: … (2) remove learnable query: w/o LQ. The quantitative results are shown in Tab. 4. By modifying the IPI module, although it improves on the w/o IPI, it still falls short of the final result of Animate-X, which suggests that our current IPI structure is the most reasonable and achieves the best performance.\""
        ],
        "final_answer": "Removing the learnable query (w/o LQ) forces IPI to rely solely on sparse keypoints, which are overly simplistic and cannot capture the underlying, nuanced motion patterns. As shown by the ablation results, omitting this learnable query degrades performance compared to the full IPI design, demonstrating that the learnable query is essential for extracting subtle motion cues.",
        "relevant_elements": [
            "Implicit Pose Indicator",
            "Learnable Query"
        ],
        "id": 2344,
        "masked_question": "How does removing the [mask1] in [mask2] affect capturing motion nuances?",
        "masked_number": 2,
        "masked_elements": [
            "Learnable Query",
            "Implicit Pose Indicator"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.10306v1_figure_2.png",
        "paperid": "2410.10306v1",
        "paper_path": "./MISS-QA/papers/2410.10306v1.json",
        "figure_id": "2410.10306v1_figure_2.png",
        "caption": "Figure 2: (a) The overview of our Animate-X. Given a reference image Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image feature fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and latent feature fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT via CLIP image encoder ΦΦ\\Phiroman_Φ and VAE encoder ℰℰ\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion feature fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and pose feature fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively. fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is concatenated with the noised input ϵitalic-ϵ\\epsilonitalic_ϵ along the channel dimension, then further concatenated with fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT along the temporal dimension. This serves as the input to the diffusion model ϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT for progressive denoising. During the denoising process, fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT provide appearance condition from Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and motion condition from I1:Fdsubscriptsuperscript𝐼𝑑:1𝐹I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder 𝒟𝒟\\mathcal{D}caligraphic_D is adopted to map the generated latent representation z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
        "qtype": "Experimental_Results",
        "response": "Here’s the chain of reasoning:\n\n1. In the blue‐boxed Implicit Pose Indicator (IPI), we fuse two things into a “merge query” qm:\n   – the embedding of sparse keypoints (from DWPose),  \n   – a small, learnable query vector ql (that’s your red‐boxed element).\n\n2. The purpose of ql is to “fill in” all the subtle, higher-level motion cues that raw 2D keypoints alone can’t describe—things like how cloth wrinkles or body parts deform during movement, overall dynamical patterns, etc.\n\n3. The paper’s ablations show what happens if you remove ql (“w/o LQ”):\n   – Quantitatively, all of their video‐generation metrics (PSNR, SSIM, LPIPS, FVD, etc.) drop.  \n   – Qualitatively, you start seeing artifacts and “missing” motion details—for instance the model hallucinates human-like hands on non-human characters or fails to capture fine deformation cues.\n\n4. In short, stripping out that small learnable query from the IPI starves the cross-attention module of the extra motion priors it needs. You end up with only sparse keypoints, so the network fails to pick up on the richer, implicit motion nuances—everything looks more rigid, more “skeleton-only,” and it produces more artifacts."
    },
    {
        "question": "How does excluding Pose Rescale in Explicit Pose Indicator influence the model's robustness to pose misalignments?",
        "relevant_section_ids": [
            "3.2",
            "4.3"
        ],
        "relevant_context": [
            "Therefore, we further introduce Pose Rescale. Specifically, we define a set of keypoint rescaling operations, including modifying the length of the body, legs, arms, neck, and shoulders, altering face size, even adding or removing specific body parts and etc. These transformations are stored in a rescale pool. After obtaining the realigned poses, we apply a random selection of transformations from this pool with a certain probability on them, generating the final transformed poses.",
            "We further conduct more detailed ablation experiments for different pairs of pose transformations by (1) removing the entire EPI: w/o EPI. (2) removing Pose Rescale: w/o Rescale; (3) remove Pose Realignment: w/o Realignment; From the results displayed in Tab. 4, we found that Pose Realignment contributes the most. It suggests that simulating misalignment case in inference is the key factor."
        ],
        "final_answer": "When Pose Rescale is excluded, the model still gains most of its robustness to pose misalignments from the Pose Realignment component. In other words, omitting Pose Rescale leads to only a minor drop in handling misaligned poses—Pose Realignment itself remains the primary driver of the model’s robustness to such misalignments.",
        "relevant_elements": [
            "Explicit Pose Indicator",
            "Pose Rescale"
        ],
        "id": 2345,
        "masked_question": "How does excluding Pose Rescale in [mask1] influence the model's robustness to pose misalignments?",
        "masked_number": 1,
        "masked_elements": [
            "Explicit Pose Indicator"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.10306v1_figure_2.png",
        "paperid": "2410.10306v1",
        "paper_path": "./MISS-QA/papers/2410.10306v1.json",
        "figure_id": "2410.10306v1_figure_2.png",
        "caption": "Figure 2: (a) The overview of our Animate-X. Given a reference image Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image feature fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and latent feature fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT via CLIP image encoder ΦΦ\\Phiroman_Φ and VAE encoder ℰℰ\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion feature fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and pose feature fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively. fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is concatenated with the noised input ϵitalic-ϵ\\epsilonitalic_ϵ along the channel dimension, then further concatenated with fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT along the temporal dimension. This serves as the input to the diffusion model ϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT for progressive denoising. During the denoising process, fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT provide appearance condition from Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and motion condition from I1:Fdsubscriptsuperscript𝐼𝑑:1𝐹I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder 𝒟𝒟\\mathcal{D}caligraphic_D is adopted to map the generated latent representation z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
        "qtype": "Experimental_Results",
        "response": "In our ablations (see “w/o Rescale” in Table 4) dropping the Pose Rescale branch from the red‐boxed EPI block leads to a noticeable, if not catastrophic, loss in robustness to scale‐based misalignments.  In plain English: the network can still learn to “line up” the driving skeleton to your reference figure (that’s what Pose Realignment does), but as soon as you introduce huge differences in limb‐lengths, head‐sizes or body‐ratios – exactly the kind of variation you get in non‐human or anthropomorphic characters – the generator starts to struggle.  You see more stretching or crushing of arms and legs and a small but measurable hit in all of our video‐quality metrics.  In short, Pose Rescale isn’t the single most important factor (Pose Realignment is), but without it the model becomes significantly less robust to characters whose proportions deviate drastically from the human template used at training time."
    },
    {
        "question": "What potential biases arise from relying on CLIP feature within the Implicit Pose Indicator?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Implicit Pose Indicator",
            "CLIP feature"
        ],
        "id": 2346,
        "masked_question": "What potential biases arise from relying on [mask1] within the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP feature",
            "Implicit Pose Indicator"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.10306v1_figure_2.png",
        "paperid": "2410.10306v1",
        "paper_path": "./MISS-QA/papers/2410.10306v1.json",
        "figure_id": "2410.10306v1_figure_2.png",
        "caption": "Figure 2: (a) The overview of our Animate-X. Given a reference image Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image feature fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and latent feature fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT via CLIP image encoder ΦΦ\\Phiroman_Φ and VAE encoder ℰℰ\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion feature fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and pose feature fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively. fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is concatenated with the noised input ϵitalic-ϵ\\epsilonitalic_ϵ along the channel dimension, then further concatenated with fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT along the temporal dimension. This serves as the input to the diffusion model ϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT for progressive denoising. During the denoising process, fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT provide appearance condition from Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and motion condition from I1:Fdsubscriptsuperscript𝐼𝑑:1𝐹I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder 𝒟𝒟\\mathcal{D}caligraphic_D is adopted to map the generated latent representation z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
        "qtype": "Others",
        "response": "Here, the red‐boxed “mask₁” is the stack of per‐frame CLIP image embeddings (fφ d) that are fed into the Implicit Pose Indicator (the blue‐boxed “mask₂”).  Because CLIP was trained on massive, uncurated “in the wild” image–text pairs, it carries all of the biases of that data (human-centered priors, cultural and demographic skews, stereotypical associations, underrepresentation of unusual objects or poses, etc.).  By leaning on those embeddings inside the IPI, Animate-X will inevitably inherit:\n\n • Human‐centric motion priors  \n • Over-represented cultural or semantic stereotypes about how certain figures move  \n • Systematic underweighting or distortion of rare or non-human body configurations  \n • Spurious correlations between appearance semantics and motion patterns  \n\nIn short, any blind spots or social‐cultural biases in CLIP will be baked into the IPI’s “implicit” motion code, which can lead to skewed or unnatural animations—especially for underrepresented, non-human, or out-of-distribution characters."
    },
    {
        "question": "How might replacing the VAE encoder with a GAN-based encoder affect Spatial Attention in the diffusion model?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "VAE encoder",
            "Spatial Attention"
        ],
        "id": 2347,
        "masked_question": "How might replacing the [mask1] with a GAN-based encoder affect [mask2] in the diffusion model?",
        "masked_number": 2,
        "masked_elements": [
            "VAE encoder",
            "Spatial Attention"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.10306v1_figure_2.png",
        "paperid": "2410.10306v1",
        "paper_path": "./MISS-QA/papers/2410.10306v1.json",
        "figure_id": "2410.10306v1_figure_2.png",
        "caption": "Figure 2: (a) The overview of our Animate-X. Given a reference image Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT, we first extract CLIP image feature fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and latent feature fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT via CLIP image encoder ΦΦ\\Phiroman_Φ and VAE encoder ℰℰ\\mathcal{E}caligraphic_E.\nThe proposed Implicit Pose Indicator (IPI) and Explicit Pose Indicator (EPI)\nproduce motion feature fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and pose feature fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT, respectively. fesubscript𝑓𝑒f_{e}italic_f start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is concatenated with the noised input ϵitalic-ϵ\\epsilonitalic_ϵ along the channel dimension, then further concatenated with fersubscriptsuperscript𝑓𝑟𝑒f^{r}_{e}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT along the temporal dimension. This serves as the input to the diffusion model ϵθsubscriptitalic-ϵ𝜃\\epsilon_{\\theta}italic_ϵ start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT for progressive denoising. During the denoising process, fφrsubscriptsuperscript𝑓𝑟𝜑f^{r}_{\\varphi}italic_f start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_φ end_POSTSUBSCRIPT and fisubscript𝑓𝑖f_{i}italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT provide appearance condition from Irsuperscript𝐼𝑟I^{r}italic_I start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT and motion condition from I1:Fdsubscriptsuperscript𝐼𝑑:1𝐹I^{d}_{1:F}italic_I start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 : italic_F end_POSTSUBSCRIPT.\nAt last, a VAE decoder 𝒟𝒟\\mathcal{D}caligraphic_D is adopted to map the generated latent representation z0subscript𝑧0z_{0}italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT to the animation video. (b) The detailed structure of Implicit Pose Indicator. (c) The pipeline of pose transformation by Explicit Pose Indicator.",
        "qtype": "Others",
        "response": "Let’s first agree on what the two boxes are:\n\n1.  The red box ([mask1]) is the set of encoders that turn your reference image and driving video into conditioning vectors – in the paper that is  (CLIP‐image),  (VAE), DWPose and again CLIP.\n\n2.  The blue box ([mask2]) is the core diffusion denoiser (the 3D–UNet made up of Spatial-Attention, Motion-Attention and Temporal-Attention layers) that takes the noised latent plus those learned conditioning vectors and gradually “denoises” back to a clean video latent.\n\nNow, if you were to rip out the CLIP+VAE/DWPose encoders in the red box and swap in a GAN-style encoder instead, here is what would happen inside the blue-box diffusion network:\n\n•  **Latent‐space mismatch.** The diffusion U-Net was trained to expect VAE-latents and CLIP feature vectors (and DWPose keypoint embeddings) in very specific distributions. A GAN encoder will spit out a completely different latent or feature distribution. As soon as you feed those “GAN codes” into the Spatial-Attention and cross-attention layers, the model no longer “understands” them – it will produce garbage unless you re-train the U-Net from scratch on those GAN encodings.\n\n•  **Cross‐attention breakdown.** Inside the Spatial-Attention blocks the network learns to treat each VAE‐latent channel and CLIP embedding as a “key/value” pair that it cross-attends over. Swap in a GAN latent and you lose that alignment – attention maps collapse and identity or motion consistency will collapse as well.\n\n•  **Motion and temporal modules suffer.** The Motion-Attention and Mamba-style Temporal-Attention heads all rely on the same VAE+pose latent manifold. If you substitute a GAN representation, those heads can no longer meaningfully correlate frames across time, so your video will lose coherence.\n\n•  **Re-training requirement.** In practice the only way to make a GAN encoder upstream work is to re-train the entire diffusion backbone so that the Spatial/Motion/Temporal Attentions learn to interpret your new GAN codes.  \n\nIn short, swapping out the paper’s CLIP + VAE + DWPose encoders for a GAN encoder will completely throw off the U-Net in the blue box: the denoising pipeline will no longer receive the representations it was trained on, causing collapse of identity preservation, blurring of motion detail, and loss of temporal consistency – unless you re-train the diffusion model end-to-end on that new GAN latent space."
    },
    {
        "question": "What ethical risks emerge when automating data transformation through AI based on concept encoding shelf prompts?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Concept Encoding Shelf"
        ],
        "id": 2349,
        "masked_question": "What ethical risks emerge when automating data transformation through AI based on [mask1] prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Concept Encoding Shelf"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.16119v1_figure_1.png",
        "paperid": "2408.16119v1",
        "paper_path": "./MISS-QA/papers/2408.16119v1.json",
        "figure_id": "2408.16119v1_figure_1.png",
        "caption": "Figure 1. With Data Formulator 2, analysts can navigate the iteration history in Data Threads and select previous designs to be reused towards new ones; then, using Concept Encoding Shelf, analysts specify their chart design using blended UI and natural language inputs, delegating data transformation effort to AI. When new charts are created, data threads are updated for future reference. Data Formulator 2 is available at https://github.com/microsoft/data-formulator.",
        "qtype": "Others",
        "response": "When you flip your data‐munging chores over to an LLM by simply filling out a “Concept Encoding Shelf” and hitting “formulate,” you gain enormous convenience—but you also open yourself up to a set of serious ethical dangers:\n\n  1. Silent bias amplification  \n     • The model may unwittingly drop, filter, re-aggregate or rank subgroups in ways that reflect its own training biases rather than the true structure of your data.  \n     • Because you never hand-craft the joins, filters or window functions yourself, there’s no easy red flag when some minority slice gets demoted or erased.  \n\n  2. Lack of transparency and auditability  \n     • Once the LLM outlines and executes a multi‐step Python “recipe,” it becomes very hard for a human to reconstruct exactly what conditional logic or grouping criteria were applied, especially as you iterate through forks.  \n     • With only a tiny JSON summary and a handful of code snippets, you lose the fine‐grained provenance that you would otherwise see in a notebook of hand‐written data-wrangling steps.  \n\n  3. Hallucination and incorrect code  \n     • LLMs routinely “invent” column names, import modules you don’t have, or make off‐by-one mistakes in filters—errors that, if unnoticed, silently poison all downstream charts and potentially misinform your audience.  \n\n  4. Data privacy and leakage  \n     • To generate transformations, you must ship excerpts of your dataset (column names, sample rows) into a closed-source LLM.  That creates a real risk of exposing personal or proprietary information you meant to keep on-prem.  \n\n  5. Over-delegation of responsibility  \n     • It’s all too easy to trust the AI’s output simply because “it looks right” on the shelf UI.  Delegating whole transformation pipelines to a black box can lead teams to abdicate their duty to verify results and to own any unintended consequences—especially problematic in regulated or high‐stakes domains.  \n\nTaken together, these risks mean that while the Concept Encoding Shelf can turbo-charge iterative analysis, it also demands much stronger guardrails—clear audit trails, mandatory human-in-the-loop checks, bias‐testing suites, and strict data-leakage controls—if you’re going to outsource your transformations to an LLM."
    },
    {
        "question": "What motivates representing iteration history as Data Threads instead of linear conversation logs?",
        "relevant_section_ids": [
            "1",
            "3.2"
        ],
        "relevant_context": [
            "Second, existing AI-powered tools support only either single-turn or linear interactions with AI models, and therefore do not accommodate branching and backtracking that commonly occur in the iterative authoring process. When non-linear contexts are merged into a linear history, it is not only challenging for users to communicate which designs should be used towards next iterations, but also challenging for AI model to correctly retrieve relevant content from the long conversation history (Liu et al., 2024; Zhang et al., 2023; Hsieh et al., 2024).",
            "Data Formulator 2 introduces data threads to represent the tree-structured iteration history to support navigation tasks. Centering the iteration history around data benefits user navigation because it directly reflects the sequence of user actions in creating these new data. This design also benefits the AI model: when a user issues a follow-up instruction, Data Formulator 2 automatically retrieves its conversation history with the AI towards the current data and then instructs the AI model to rewrite the code towards new goals based on the retrieved history. This way, the AI model does not pose risk of incorrectly using conversation history from other branches to make incorrect data transformation."
        ],
        "final_answer": "Iterative visualization authoring commonly involves branching and backtracking, which linear conversation logs cannot adequately represent. Merging non-linear branches into a single linear history makes it difficult for users to pick the correct prior design to build on and for the AI to identify the relevant context. By organizing history as Data Threads—tree-structured, data-centered records—users can navigate and fork past states directly, and the AI can retrieve only the branch-specific history, avoiding cross-branch confusion and ensuring correct follow-up transformations.",
        "relevant_elements": [
            "Data Threads"
        ],
        "id": 2350,
        "masked_question": "What motivates representing iteration history as [mask1] instead of linear conversation logs?",
        "masked_number": 1,
        "masked_elements": [
            "Data Threads"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.16119v1_figure_1.png",
        "paperid": "2408.16119v1",
        "paper_path": "./MISS-QA/papers/2408.16119v1.json",
        "figure_id": "2408.16119v1_figure_1.png",
        "caption": "Figure 1. With Data Formulator 2, analysts can navigate the iteration history in Data Threads and select previous designs to be reused towards new ones; then, using Concept Encoding Shelf, analysts specify their chart design using blended UI and natural language inputs, delegating data transformation effort to AI. When new charts are created, data threads are updated for future reference. Data Formulator 2 is available at https://github.com/microsoft/data-formulator.",
        "qtype": "Design_Rationale",
        "response": "The thing in the red box is the “data threads” view – a tree-structured record of every dataset and chart you’ve produced, rather than a flat, linear chat log. We chose to expose your history this way because exploratory charting is inherently non-linear: you routinely fork, backtrack, revisit old results and branch off in new directions. A simple, append-only transcript makes it hard both for you to pick the right context and for the AI to retrieve only the relevant bits. By treating each intermediate table as a first-class node and wiring them together via the user’s follow-up commands, data threads let you\n\n  1. visually see all of your derivations as a tree,  \n  2. click any node to pick up exactly where you left off, and  \n  3. give the model only the needed slice of history for the branch you actually care about.  \n\nNone of that is possible (or at least becomes very error-prone) with a purely linear conversation log."
    },
    {
        "question": "What motivated unified prompts and action spaces across Embodied and Visual Design tasks?",
        "relevant_section_ids": [
            "1",
            "2"
        ],
        "relevant_context": [
            "We have standardized the prompting and data formats to facilitate a consistent evaluation of visual foundation agents across these environments.",
            "High-Level Decision Making: VAB focuses on evaluating LMMs’ high-level decision-making abilities. Compared to prior smaller visual-language models that specifically target low-level policies, LMMs excel at high-level planning and interacting in text response thanks to their commonsense, knowledge, and flexible instruction following with mere prompting. Therefore, in VAB, we simplify the low-level control by providing convenient action interfaces, and ask tested LMMs to concentrate on delivering high-level decision sequences in text."
        ],
        "final_answer": "The prompts and action spaces were unified to (1) ensure a consistent, comparable evaluation of LMM-based agents across very different environments and (2) abstract away low-level control details so that agents can focus on high-level decision making.",
        "relevant_elements": [
            "Unified Prompts & Action Spaces",
            "Embodied",
            "Visual Design"
        ],
        "id": 2352,
        "masked_question": "What motivated [mask1] across Embodied and Visual Design tasks?",
        "masked_number": 1,
        "masked_elements": [
            "Unified Prompts & Action Spaces"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.06327v1_figure_2.png",
        "paperid": "2408.06327v1",
        "paper_path": "./MISS-QA/papers/2408.06327v1.json",
        "figure_id": "2408.06327v1_figure_2.png",
        "caption": "Figure 2: VisualAgentBench is the first systematic benchmark to evaluate LMM-as-Visual-Agent across a diverse set of practical challenges. Beside benchmarking, it is also the first to offer SFT trajectory data for behavior cloning training over all target environments, demonstrating the huge potential to improve open LMMs for serving as visual foundation agents.",
        "qtype": "Design_Rationale",
        "response": "First, by inspecting the red‐boxed region in the centre of the figure, we see that it reads “Unified Prompts & Action Spaces.”  In our terminology that is exactly what [mask1] refers to.  \n\nWhy did we introduce a single, unified prompt and action interface that covers both Embodied tasks (e.g. grasping and moving objects in OmniGibson or Minecraft) and Visual Design tasks (e.g. fixing CSS on a web page)?  Here is the step‐by‐step reasoning:\n\n1.  **Diversity of Environments**  \n    • VAB spans very different settings—robotic‐style embodied environments, mobile/web GUIs, and even CSS‐based visual design.  \n    • Each of these domains traditionally has its own bespoke API, its own way of telling the agent “what to do next,” and its own way of reporting observations back to the model.  \n\n2.  **Need for a Generalist Agent**  \n    • Our goal is to train and evaluate a *single* LMM that can multitask across *all* of these domains, mirroring the human ability to use vision and language to reason in widely varying contexts.  \n    • If each domain spoke a different “dialect” of prompts and actions, it would force us either to fine-tune separate models or to build complex domain-switching wrappers.  \n\n3.  **Design Principles of VAB**  \n    • We want to focus on *high-level decision making*—letting the LMM generate a sequence of abstract steps in text (e.g. “grasp(2.banana)”, “put_inside(3.apple,4.bin)”), rather than having it worry about motor torques or CSS pixel offsets.  \n    • We adhere to *vision-centric* inputs, so everything the agent sees is fed in as images (not raw HTML or low‐level simulator state) and everything it emits are high-level textual commands.  \n    • We employ *interactive evaluation* and, crucially, we provide *behavior‐cloning trajectories* for training, all in a consistent format.\n\n4.  **Consequence: One Interface to Rule Them All**  \n    • By unifying prompts and action‐space definitions, we ensure that the exact same text‐in/image‐out loop can be used whether the agent is in a physics simulator, on a web page, or editing CSS.  \n    • This uniformity makes it trivial to benchmark nine proprietary and eight open LMMs in exactly the same way, and it allows us to gather a single shared dataset of SFT trajectories for behavior cloning.\n\nIn short, **[mask1]** (“Unified Prompts & Action Spaces”) was motivated by the need to give a single, coherent interface—across both Embodied and Visual Design tasks—so that a generalist LMM can be prompted, evaluated, and trained uniformly, without having to switch APIs or representation formats for each new environment."
    },
    {
        "question": "How does combining behavior cloning and proprietary LMM prompting inform open LLM visual agent training?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "Significantly, VAB also includes a training set comprising ground truth trajectories across five environments, curated through a blend of Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations.",
            "Our experiments demonstrate that behavior cloning (BC) on the VAB training set markedly enhances the capabilities of open LMMs as visual agents, with most surpassing the performance of proprietary LMMs like gemini-1.0-pro and qwen-vl-max, and approaching close towards gemini-1.5-pro.",
            "LMM Agent Bootstrapping: Trajectories are collected by prompted LMM agents (e.g., gpt-4o), with optional memory augmentations to enhance performance."
        ],
        "final_answer": "By prompting proprietary LMMs (e.g., GPT-4o) to generate high-quality expert trajectories (LMM Agent Bootstrapping) and then using those trajectories to behavior-clone open LLMs, the open models learn the sequence of effective actions and achieve large gains in visual agent performance—surpassing earlier proprietary models and closing the gap with the very best.",
        "relevant_elements": [
            "Behavior Cloning",
            "Proprietary LMM APIs",
            "Open LLMs"
        ],
        "id": 2353,
        "masked_question": "How does combining [mask1] and [mask2] inform open LLM visual agent training?",
        "masked_number": 2,
        "masked_elements": [
            "Behavior Cloning",
            "Proprietary LMM APIs"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.06327v1_figure_2.png",
        "paperid": "2408.06327v1",
        "paper_path": "./MISS-QA/papers/2408.06327v1.json",
        "figure_id": "2408.06327v1_figure_2.png",
        "caption": "Figure 2: VisualAgentBench is the first systematic benchmark to evaluate LMM-as-Visual-Agent across a diverse set of practical challenges. Beside benchmarking, it is also the first to offer SFT trajectory data for behavior cloning training over all target environments, demonstrating the huge potential to improve open LMMs for serving as visual foundation agents.",
        "qtype": "Design_Rationale",
        "response": "Here’s the key insight you get by reading the little red and blue callouts together:\n\n  1. The blue‐boxed piece in the figure is “Prompting” of *proprietary* LMM/LLM APIs  \n  2. The red‐boxed piece is plain old *Behavior Cloning* (i.e. supervised fine-tuning on action–observation trajectories)  \n\nPutting them side-by-side tells you exactly how to bootstrap *open* LLMs into capable visual agents:  \n\n  • First, “prompt” a strong closed‐box model (e.g. GPT-4o) in your VAB environments to *generate* long, step-by-step action trajectories.  \n  • Then, take those prompt-produced trajectories as your “ground-truth” and **behavior-clone** them to supervise-fine-tune an open LLM.  \n\nIn short, you use prompting on the blue side to *harvest* high-quality multi-modal demonstrations, and then you apply behavior cloning on the red side to transfer that skill to your open model. This two-step recipe is exactly what the paper shows dramatically accelerates open LLMs’ ability to act as visual foundation agents."
    },
    {
        "question": "How are unified prompts designed to align with diverse action spaces across all target environments?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Unified Prompts",
            "Action Spaces"
        ],
        "id": 2354,
        "masked_question": "How are [mask1] designed to align with diverse action spaces across all target environments?",
        "masked_number": 1,
        "masked_elements": [
            "Unified Prompts"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.06327v1_figure_2.png",
        "paperid": "2408.06327v1",
        "paper_path": "./MISS-QA/papers/2408.06327v1.json",
        "figure_id": "2408.06327v1_figure_2.png",
        "caption": "Figure 2: VisualAgentBench is the first systematic benchmark to evaluate LMM-as-Visual-Agent across a diverse set of practical challenges. Beside benchmarking, it is also the first to offer SFT trajectory data for behavior cloning training over all target environments, demonstrating the huge potential to improve open LMMs for serving as visual foundation agents.",
        "qtype": "Implementation_Details",
        "response": "Mask1 in the figure is the “Unified Prompts & Action Spaces” module, and the way it is built is exactly to bridge the very different APIs of OmniGibson, Minecraft, WebArena, Android and CSS‐debugging into one single, text‐based interface.  Concretely:\n\n1.  At the top of every episode we inject into the system prompt a concise description of that environment’s action space – for example  \n    •  In OmniGibson you get actions like  \n         –  “grasp(object_id)”  \n         –  “put_inside(obj_id, target_id)”  \n    •  In Minecraft you get actions like  \n         –  “craft(item_name)”  \n         –  “smelt(item_name)”  \n    •  In WebArena you get “playwright.click(css_selector)”, “playwright.fill(css_selector, text)”, etc.  \n    •  On Android you get “Tap(x,y)”, “Swipe(x1,y1→x2,y2)”, “Back()”, and so on.  \n    •  In CSS debugging you get “edit_css(rule_id, property_name, new_value)”.\n\n2.  All of those environment‐specific verbs and their argument signatures are presented in exactly the same text format – a short enumerated list in the system prompt.  That way the LMM learns to always reply with lines of the form  \n       action_name(arg1, arg2…)  \n    regardless of which environment it’s in.\n\n3.  At each turn we then hand the LMM the very same prompt template (plus the latest screenshot or observation), so it sees a uniform structure:  \n    –  “Here is the current view.”  \n    –  “Here is what you can do (the enumerated action space).”  \n    –  “Now please choose your next action.”  \n\n4.  A lightweight adapter in our codebase parses whatever text‐call the LMM produces and dispatches it to the correct API in the current environment.\n\nBy doing this, we achieve two things simultaneously:\n  •  The LMM only has to learn one way of “calling” actions in text.  \n  •  We can instantly plug in new environments by just dropping in a new description of their actions.\n\nIn short, the “Unified Prompts & Action Spaces” are designed to flatten all the wildly different APIs of our five VAB environments into one single, text‐based schema that the LMM can use out of the box in every setting."
    },
    {
        "question": "How does behavior cloning adapt multimodal trajectories to fine-tune open LLMs for visual agent tasks?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "Significantly, VAB also includes a training set comprising ground truth trajectories across five environments, curated through a blend of Program-based Solvers, LMM Agent Bootstrapping, and Human Demonstrations.",
            "Our experiments demonstrate that behavior cloning (BC) on the VAB training set markedly enhances the capabilities of open LMMs as visual agents…",
            "Imitation learning, especially the behavior cloning (BC) method, has been demonstrated as effective for building LLM agents from scratch.",
            "In VAB, we systematically summarize our trajectory collecting into 3-leveled strategies: Program-based Solvers, LMM Agent Bootstrapping, Human Demonstrations."
        ],
        "final_answer": "Behavior cloning adapts multimodal trajectories by first collecting high-quality demonstration sequences—pairing visual observations with ground-truth action steps—via a hybrid pipeline of programmatic solvers, LMM-based bootstrapping, and human annotation. These recorded trajectories are then used as supervised training data to fine-tune open LLMs, teaching them to map from visual inputs and instructions to the correct sequence of actions, which substantially improves their decision-making and planning performance as visual agents.",
        "relevant_elements": [
            "Behavior Cloning",
            "Open LLMs"
        ],
        "id": 2355,
        "masked_question": "How does [mask1] adapt multimodal trajectories to fine-tune open LLMs for visual agent tasks?",
        "masked_number": 1,
        "masked_elements": [
            "Behavior Cloning"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.06327v1_figure_2.png",
        "paperid": "2408.06327v1",
        "paper_path": "./MISS-QA/papers/2408.06327v1.json",
        "figure_id": "2408.06327v1_figure_2.png",
        "caption": "Figure 2: VisualAgentBench is the first systematic benchmark to evaluate LMM-as-Visual-Agent across a diverse set of practical challenges. Beside benchmarking, it is also the first to offer SFT trajectory data for behavior cloning training over all target environments, demonstrating the huge potential to improve open LMMs for serving as visual foundation agents.",
        "qtype": "Implementation_Details",
        "response": "First, notice that the little red box in the diagram is labeling the “Behavior Cloning” step in the training pipeline.  In our VAB workflow it sits between (a) the pool of collected, high-quality multimodal trajectories (from programmatic solvers, LMM-bootstrapping and human demos) and (b) the open-model fine-tuning component.\n\nChain of thought:  \n1. The red box encloses the term “Behavior Cloning” in the figure–it is the only element in the pipeline that directly links the curated trajectories to the open LLMs.  \n2. In the text we see that VAB gathers demonstration trajectories across five environments, standardizes them into unified prompt‐action pairs, and then “behavior cloning (BC) on the VAB training set markedly enhances the capabilities of open LMMs.”  \n3. BC here simply means supervised fine‐tuning (often called SFT) where the model is trained to predict each expert action given its corresponding visual-textual context.  \n\nAnswer:  \nBehavior Cloning takes the multimodal trajectories—each consisting of the unified prompt, image observations and ground-truth actions—and directly SFT-trains an open LLM to imitate those expert action sequences. By turning every step in every environment into a prompt–action example, BC adapts the diverse, vision-and-action data into a fine-tuning curriculum that teaches the open LLM how to behave as a visual agent."
    },
    {
        "question": "How does CLIP Encoder cross-attention integrate into UNet layers to preserve semantic consistency across multi-view latent sequences?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "In addition, we incorporate the input condition image’s CLIP embeddings (Radford et al., 2021 ###reference_b43###) into the diffusion UNet through cross-attention mechanism.",
            "Within each transformer block, the CLIP embedding matrix acts as the key and value for the cross-attention layers, coupled with the layer’s features serving as the query."
        ],
        "final_answer": "The CLIP embeddings of the input image are injected into each UNet transformer block via cross-attention, where CLIP embeddings serve as the key and value and the UNet’s feature maps are the queries. This routes high-level semantic information into the latent denoising process, preserving semantic consistency across the generated multi-view sequence.",
        "relevant_elements": [
            "CLIP Encoder",
            "UNet"
        ],
        "id": 2356,
        "masked_question": "How does [mask1] cross-attention integrate into [mask2] layers to preserve semantic consistency across multi-view latent sequences?",
        "masked_number": 2,
        "masked_elements": [
            "CLIP Encoder",
            "UNet"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.07452v1_figure_2.png",
        "paperid": "2409.07452v1",
        "paper_path": "./MISS-QA/papers/2409.07452v1.json",
        "figure_id": "2409.07452v1_figure_2.png",
        "caption": "Figure 2. An overview of our proposed Hi3D. Our Hi3D fully exploits the capabilities of large-scale pre-trained video diffusion models to effectively trigger high-resolution image-to-3D generation. Specifically, in the first stage of basic multi-view generation, Hi3D remoulds video diffusion model with additional camera pose condition, aiming to transform single-view image into low-resolution 3D-aware sequential images. Next, in the second stage of 3D-aware multi-view refinement, we feed this low-resolution orbit video into 3D-aware video-to-video refiner with additional depth condition, leading to high-resolution orbit video with highly detailed texture. Finally, we augment the resultant multi-view images with more novel views through 3D Gaussian Splatting and employ SDF-based reconstruction to extract high-quality 3D meshes.",
        "qtype": "Implementation_Details",
        "response": "In our figure, the little red‐boxed “block” is exactly where we inject the CLIP‐based cross‐attention (that is our [mask1]), and the blue‐boxed UNet is the stack of transformer‐style attention blocks (that is our [mask2]).  Concretely, at every single attention block in the UNet we add a tiny cross‐attention head whose key and value vectors come from the CLIP embedding of the *input* view, while the query vectors come from the noisy multi‐view latent features.  In this way, at each layer of the 3D (video) UNet the same high–level CLIP semantics are “looked up” against all the per‐view latents: this locks in the object’s identity and semantic structure across every frame of the orbital video."
    },
    {
        "question": "How does Depth Estimate output merge with VAE Encoder latents within the 3D-aware video-to-video refiner UNet?",
        "relevant_section_ids": [
            "4.3"
        ],
        "relevant_context": [
            "We adopt the VAE encoder to extract the latent code sequence of the pre-generated multi-view images \\(\\hat{I}\\) and channel-wisely concatenate them with the noise latent \\(z_t\\) as conditions.",
            "Moreover, to fully exploit the underlying geometry information of the generated multi-view images, we leverage an off-the-shelf depth estimation model ... to estimate the depth of each image in \\(\\hat{I}\\) as 3D cues, yielding a depth map sequence \\(D\\). We then directly resize the depth maps into the same resolution of the latent code \\(z_t\\), and channel-wisely concatenate them with \\(z_t\\)."
        ],
        "final_answer": "The estimated depth maps are first resized to match the spatial resolution of the VAE encoder’s latent codes, and then these depth maps are concatenated along the channel dimension with the noisy latent sequence (the VAE latents) to form the input conditioning for the UNet.",
        "relevant_elements": [
            "Depth Estimate",
            "VAE Encoder",
            "3D-aware video-to-video refiner"
        ],
        "id": 2357,
        "masked_question": "How does [mask1] output merge with [mask2] latents within the 3D-aware video-to-video refiner UNet?",
        "masked_number": 2,
        "masked_elements": [
            "Depth Estimate",
            "VAE Encoder"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.07452v1_figure_2.png",
        "paperid": "2409.07452v1",
        "paper_path": "./MISS-QA/papers/2409.07452v1.json",
        "figure_id": "2409.07452v1_figure_2.png",
        "caption": "Figure 2. An overview of our proposed Hi3D. Our Hi3D fully exploits the capabilities of large-scale pre-trained video diffusion models to effectively trigger high-resolution image-to-3D generation. Specifically, in the first stage of basic multi-view generation, Hi3D remoulds video diffusion model with additional camera pose condition, aiming to transform single-view image into low-resolution 3D-aware sequential images. Next, in the second stage of 3D-aware multi-view refinement, we feed this low-resolution orbit video into 3D-aware video-to-video refiner with additional depth condition, leading to high-resolution orbit video with highly detailed texture. Finally, we augment the resultant multi-view images with more novel views through 3D Gaussian Splatting and employ SDF-based reconstruction to extract high-quality 3D meshes.",
        "qtype": "Implementation_Details",
        "response": "Chain‐of‐thought  \n1. In the stage‐2 refiner (the green UNet box), the two highlighted modules are:  \n   – [mask1] the off‐the‐shelf depth estimator (red box)  \n   – [mask2] the VAE encoder for the low‐res multi‐view frames (blue box)  \n2. The text (Sec 4.3) says we (a) run each pre‐generated view through the VAE encoder to get a latent tensor zₜ, and (b) run each view through a depth estimator to get a depth map.  \n3. We then “resize the depth maps into the same resolution of the latent code,” and finally “channel‐wisely concatenate them with zₜ.”  \n4. In other words, both the VAE latents and the depth maps are simply stacked along the channel dimension (after matching spatial size) and that concatenated tensor becomes the UNet input.  \n\nAnswer  \nThe depth maps ([mask1]) are first resized to the spatial dimensions of the VAE latents ([mask2]) and then glued onto those latents by concatenating them channel‐wise before feeding into the refiner UNet."
    },
    {
        "question": "How does 3D Gaussian Splatting synergize with SDF-based reconstruction to improve mesh fidelity versus implicit surface methods?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "Previous image-to-3D methods (Liu et al., 2024; Long et al., 2024; Huang et al., 2024) usually reconstruct the target 3D mesh from the output image sequence by optimizing the neural implicit Signed Distance Field (SDF) (Wang et al., 2021a; Guo, 2022). Nevertheless, these SDF-based reconstruction methods are originally tailored for dense image sequences captured in the real world, which commonly fail to reconstruct high-quality mesh based on only sparse views.",
            "Instead of directly adopting SDF-based reconstruction methods to extract 3D mesh, we first use the 3D Gaussian Splatting (3DGS) algorithm (Kerbl et al., 2023) to learn an implicit 3D model from the generated high-resolution image sequence. 3DGS has demonstrated remarkable novel view synthesis capabilities and impressive rendering speed. Herein we attempt to utilize 3DGS’s implicit reconstruction ability to augment the output sparse multi-view images of Stage-2 with more novel views.",
            "Specifically, we render interpolation views between the adjacent images in U from the reconstructed 3DGS. Finally, we optimize an SDF-based reconstruction method (Wang et al., 2021a) based on the augmented dense views U∪ to extract the high-quality 3D mesh of the object."
        ],
        "final_answer": "3D Gaussian Splatting first builds an implicit 3D representation from the sparse high-resolution views and then rapidly renders many intermediate (interpolated) viewpoints, turning a sparse set of images into a dense multi-view sequence. Feeding these densely synthesized views into a standard SDF-based mesh extractor overcomes the brittleness of purely implicit surface methods on sparse data, leading to more stable optimization and higher-fidelity mesh reconstructions.",
        "relevant_elements": [
            "3D Gaussian Splatting",
            "SDF-based reconstruction"
        ],
        "id": 2358,
        "masked_question": "How does [mask1] synergize with [mask2] to improve mesh fidelity versus implicit surface methods?",
        "masked_number": 2,
        "masked_elements": [
            "3D Gaussian Splatting",
            "SDF-based reconstruction"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.07452v1_figure_2.png",
        "paperid": "2409.07452v1",
        "paper_path": "./MISS-QA/papers/2409.07452v1.json",
        "figure_id": "2409.07452v1_figure_2.png",
        "caption": "Figure 2. An overview of our proposed Hi3D. Our Hi3D fully exploits the capabilities of large-scale pre-trained video diffusion models to effectively trigger high-resolution image-to-3D generation. Specifically, in the first stage of basic multi-view generation, Hi3D remoulds video diffusion model with additional camera pose condition, aiming to transform single-view image into low-resolution 3D-aware sequential images. Next, in the second stage of 3D-aware multi-view refinement, we feed this low-resolution orbit video into 3D-aware video-to-video refiner with additional depth condition, leading to high-resolution orbit video with highly detailed texture. Finally, we augment the resultant multi-view images with more novel views through 3D Gaussian Splatting and employ SDF-based reconstruction to extract high-quality 3D meshes.",
        "qtype": "Literature_Background",
        "response": "Here mask1 is the 3D Gaussian‐splatting step (the red‐boxed “3D Gaussian Splatting”) and mask2 is the SDF‐based mesh extractor (the blue‐boxed “SDF‐based Recon.”).  The two work together as follows:\n\n1. 3D Gaussian Splatting (mask1) fits a fast, view‐consistent volumetric representation to your sparse multi-view images and then cheaply renders large numbers of new “in-between” views.  This stage “fills in” the gaps left by having only 16 real cameras, capturing high-frequency shading and silhouette cues that an SDF loss on only the original views would simply never see.\n\n2. Those rendered novel frames are then fed into the SDF-based reconstruction (mask2).  Because the SDF optimizer now has a densely sampled set of views encircling the object—complete with fine texture and geometry hints—it can solve for a signed‐distance field that crisply matches all the detail.  In contrast, if you simply ran SDF reconstruction on the original 16 sparse views, you’d get a much smoother, over-regularized surface.\n\nIn short, splatting supplies a dense, richly detailed surrogate for “real” cameras, and the SDF extractor turns that surrogate into a watertight, high-fidelity mesh.  By combining the two, you get the robustness and speed of Gaussian splatting plus the topological guarantees and crisp detail of an SDF surface—significantly outperforming a standalone implicit‐field reconstruction."
    },
    {
        "question": "How does camera elevation conditioning modulate UNet εθ's temporal attention compared to conventional temporal modeling in video diffusion?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "camera elevation",
            "UNet εθ"
        ],
        "id": 2359,
        "masked_question": "How does [mask1] conditioning modulate UNet εθ's temporal attention compared to conventional temporal modeling in video diffusion?",
        "masked_number": 1,
        "masked_elements": [
            "camera elevation"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.07452v1_figure_2.png",
        "paperid": "2409.07452v1",
        "paper_path": "./MISS-QA/papers/2409.07452v1.json",
        "figure_id": "2409.07452v1_figure_2.png",
        "caption": "Figure 2. An overview of our proposed Hi3D. Our Hi3D fully exploits the capabilities of large-scale pre-trained video diffusion models to effectively trigger high-resolution image-to-3D generation. Specifically, in the first stage of basic multi-view generation, Hi3D remoulds video diffusion model with additional camera pose condition, aiming to transform single-view image into low-resolution 3D-aware sequential images. Next, in the second stage of 3D-aware multi-view refinement, we feed this low-resolution orbit video into 3D-aware video-to-video refiner with additional depth condition, leading to high-resolution orbit video with highly detailed texture. Finally, we augment the resultant multi-view images with more novel views through 3D Gaussian Splatting and employ SDF-based reconstruction to extract high-quality 3D meshes.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed token is nothing mystical at all but simply the camera elevation angle e.  What we do is:\n\n1.  In a vanilla video diffusion UNet you have “temporal” attentions whose keys, queries and values are modulated by learned frame‐IDs (fps‐ids, motion‐bucket‐ids, etc.) and by the usual timestep embedding.  Those learned IDs carry no explicit 3D information – they merely tell the network “this is frame 1, this is frame 2, …” so that it can learn statistical smoothness or motion priors over time.\n\n2.  In Hi3D, we strip out the original fps‐id/motion‐bucket conditions and instead inject the actual camera elevation e (converted to a sinusoidal positional embedding) into every attention block alongside the diffusion‐step embedding.  \n\n3.  Concretely, at each denoising step t we take  \n     •  the noisy latent,  \n     •  the CLIP embedding of the reference image,  \n     •  and the sinusoidal embedding of e  \n   and feed them into our 3D‐UNet.  Since e varies from frame to frame (all frames share the same azimuth sweep but differ in elevation), the temporal self-attention layers now “know” the exact 3D tilt of each view.\n\n4.  The upshot is that when the UNet computes cross‐frame attention it is no longer just learning “motion” or “smooth video dynamics,” but is directly aligning features by true geometric shifts in elevation.  In other words, e-conditioning turns those temporal attentions into 3D‐aware correspondences, rather than into purely generic temporal priors.\n\nBy contrast, conventional temporal modeling in video diffusion lacks any explicit camera‐pose signal and so can only hope to learn approximate frame‐to‐frame consistency.  Ours uses e to turn every attention head into a little 360° geometry expert."
    },
    {
        "question": "How does feature alignment complement the KD loss to improve knowledge transfer?",
        "relevant_section_ids": [
            "3.1",
            "3.2.1"
        ],
        "relevant_context": [
            "For example, Vanilla KD [19] minimizes the Kullback-Leibler divergence between the logits output to mimic the output of the teacher network.",
            "The feature alignment aims to make the feature in the student \\(z_i^S\\) mimic the feature of the teacher \\(z_i^T\\).",
            "Since there is a wide gap between these two features, referring to [46,62], we utilize a multilayer perceptron (MLP) with one hidden layer over the student feature \\(z_i^S\\) to encode the higher-order dependency of the teacher network. We make the encoded student feature \\(\\hat z_i^S\\) similar to the teacher feature \\(z_i^T\\) through the following loss function: \\(L_{FA} = \\sum_i \\| \\frac{\\hat z_i^S}{\\|\\hat z_i^S\\|} - \\frac{z_i^T}{\\|z_i^T\\|} \\|_2^2\\)."
        ],
        "final_answer": "While the KD loss (L_KD) aligns the student’s output logits with the teacher’s output distribution, feature alignment (L_FA) adds an auxiliary supervision at the representation level: it forces the student’s intermediate feature vectors—after passing through a small MLP—to match the teacher’s features (via cosine‐normalized L2 loss). In this way, feature alignment bridges the representation gap and provides richer, feature‐level knowledge that complements the logit‐based KD loss, leading to more effective knowledge transfer.",
        "relevant_elements": [
            "Feature Alignment",
            "KD Loss"
        ],
        "id": 2360,
        "masked_question": "How does [mask1] complement the KD loss to improve knowledge transfer?",
        "masked_number": 1,
        "masked_elements": [
            "Feature Alignment"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.14143v1_figure_2.png",
        "paperid": "2410.14143v1",
        "paper_path": "./MISS-QA/papers/2410.14143v1.json",
        "figure_id": "2410.14143v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our proposed PCKD. We first augment samples, extract features and perform feature alignment (ℒF⁢Asubscriptℒ𝐹𝐴\\mathcal{L}_{FA}caligraphic_L start_POSTSUBSCRIPT italic_F italic_A end_POSTSUBSCRIPT), category center alignment (ℒC⁢Asubscriptℒ𝐶𝐴\\mathcal{L}_{CA}caligraphic_L start_POSTSUBSCRIPT italic_C italic_A end_POSTSUBSCRIPT), and category center contrast (ℒC⁢Csubscriptℒ𝐶𝐶\\mathcal{L}_{CC}caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT).\nThen our preview strategy can assign dynamic weights to each sample based on its difficulty score.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module in the figure is nothing mysterious at all – it is simply the “feature‐alignment’’ head (the MLP plus cosine‐loss) that sits on top of the student’s backbone and tries to force its hidden representation to look like the teacher’s.  In plain English:\n\n  1.  A vanilla KD loss (L_KD) only makes the student’s final softmax behave like the teacher’s softmax.  \n  2.  The feature‐alignment loss (L_FA), on the other hand, projects the student’s penultimate‐layer feature through a small MLP and then uses a cosine similarity loss to pull that projected feature toward the teacher’s raw feature vector.  \n\nBy doing both at once – matching the logits and matching the intermediate features – you give the student a much stronger, multi-level signal.  The KD loss makes sure it “thinks’’ like the teacher at the output level, while the FA loss makes sure it “sees’’ the world like the teacher at a deeper, representational level.  In practice this combination closes much more of the teacher–student gap than vanilla KD alone."
    },
    {
        "question": "How does category center contrast leverage contrastive attract-repel operations to enhance category discrimination?",
        "relevant_section_ids": [
            "3.2.3"
        ],
        "relevant_context": [
            "Therefore, inspired by the contrastive learning [48, 49], we enforce the feature of the student network f_{si}^j to be as similar as the corresponding category centers of both the teacher network w_j^t and student network w_j^s while far from other category centers, where y_i is the ground-truth category of the i-th sample.",
            "Technically, we estimate the similarity between the sample feature and category center with their cosine distance. Then we define the following contrastive loss: (...) By minimizing the above loss function, the distance between the student network feature f_{si}^j and its corresponding category centers (w_j^t and w_j^s) is forced to be smaller than the distance from other unmatched category centers (w_k^t, w_k^s), which can also explicitly guide the discriminative category centers learning.",
            "Accordingly, the category centers as well as the learned student network features can be representative and discriminative."
        ],
        "final_answer": "Category center contrast applies a contrastive loss that for each sample pulls (attracts) the student feature toward its correct class’s centers (both teacher’s and student’s) and simultaneously pushes (repels) it away from all other class centers. This attract–repel operation enforces intra-class compactness and inter-class separation, thereby producing more representative and discriminative category centers and features.",
        "relevant_elements": [
            "Category Center Contrast"
        ],
        "id": 2361,
        "masked_question": "How does [mask1] leverage contrastive attract-repel operations to enhance category discrimination?",
        "masked_number": 1,
        "masked_elements": [
            "Category Center Contrast"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.14143v1_figure_2.png",
        "paperid": "2410.14143v1",
        "paper_path": "./MISS-QA/papers/2410.14143v1.json",
        "figure_id": "2410.14143v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our proposed PCKD. We first augment samples, extract features and perform feature alignment (ℒF⁢Asubscriptℒ𝐹𝐴\\mathcal{L}_{FA}caligraphic_L start_POSTSUBSCRIPT italic_F italic_A end_POSTSUBSCRIPT), category center alignment (ℒC⁢Asubscriptℒ𝐶𝐴\\mathcal{L}_{CA}caligraphic_L start_POSTSUBSCRIPT italic_C italic_A end_POSTSUBSCRIPT), and category center contrast (ℒC⁢Csubscriptℒ𝐶𝐶\\mathcal{L}_{CC}caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT).\nThen our preview strategy can assign dynamic weights to each sample based on its difficulty score.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is the “Category Center Contrast” (the loss term ℒCC).  In our terminology [mask1] is the Category Center Contrast.  Concretely, it treats each student feature as an “anchor,” its matching category center (from both teacher and student classifiers) as the positive, and all other class centers as negatives.  By applying a standard contrastive (attract–repel) loss—i.e. pulling the anchor toward its own class center while simultaneously pushing it away from every other class center—the network is driven to produce features (and centers) that are both tightly clustered within each class and well separated across classes."
    },
    {
        "question": "How does Preview-based Learning Strategy integrate Difficulty Score with Knowledge Distillation loss?",
        "relevant_section_ids": [
            "3.3",
            "3.4"
        ],
        "relevant_context": [
            "Difficulty Score Calculation. Referring to [55], if the student network can correctly classify a sample, we infer it is an easy sample, and vice versa for a hard sample. Technically, the cross-entropy loss of a sample indicates how much its prediction is similar to its label, and it can be regarded as the sample difficulty. Accordingly, we define the difficulty score γ_i of sample i as follows: γ_i = L_CE(x_i) / (1/|B| Σ_{x_j in B} L_CE(x_j)).",
            "Learning Weight of the Sample. In the context of our preview-based learning strategy, we make the student not only concentrate on the easy samples but also preview the hard samples. In other words, in one batch of the training, preview-based learning strategy pays more attention to making the student correctly classify the easy samples while less attention to the hard samples. To fulfill this, we introduce an auxiliary weight v_i to each sample as its attention during the training as follows: v_i = {1, if γ_i < τ; τ/γ_i, otherwise}.",
            "Since the learning weight v is designed to control the attention of the student to classify samples correctly, we only add it into the losses that are utilized for the classification, i.e., L_CE and L_KD."
        ],
        "final_answer": "The Preview-based Learning Strategy first computes a per-sample difficulty score γ_i from the sample’s cross-entropy loss normalized by the batch mean. It then converts γ_i into a learning weight v_i (with v_i = 1 for easy samples and v_i < 1 for hard samples). During training, this weight v_i is used to scale both the student’s cross-entropy loss (L_CE) and the knowledge distillation loss (L_KD) on a per-sample basis, thus down-weighting hard samples early on and progressively including them as training proceeds.",
        "relevant_elements": [
            "Preview-based Learning Strategy",
            "Difficulty Score",
            "Knowledge Distillation Loss"
        ],
        "id": 2363,
        "masked_question": "How does [mask1] integrate [mask2] with Knowledge Distillation loss?",
        "masked_number": 2,
        "masked_elements": [
            "Preview-based Learning Strategy",
            "Difficulty Score"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.14143v1_figure_2.png",
        "paperid": "2410.14143v1",
        "paper_path": "./MISS-QA/papers/2410.14143v1.json",
        "figure_id": "2410.14143v1_figure_2.png",
        "caption": "Figure 2: The overall framework of our proposed PCKD. We first augment samples, extract features and perform feature alignment (ℒF⁢Asubscriptℒ𝐹𝐴\\mathcal{L}_{FA}caligraphic_L start_POSTSUBSCRIPT italic_F italic_A end_POSTSUBSCRIPT), category center alignment (ℒC⁢Asubscriptℒ𝐶𝐴\\mathcal{L}_{CA}caligraphic_L start_POSTSUBSCRIPT italic_C italic_A end_POSTSUBSCRIPT), and category center contrast (ℒC⁢Csubscriptℒ𝐶𝐶\\mathcal{L}_{CC}caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT).\nThen our preview strategy can assign dynamic weights to each sample based on its difficulty score.",
        "qtype": "Experimental_Results",
        "response": "The red‐boxed module in the picture is nothing more exotic than “take our regular KD‐loss and modulate it by how hard each example is,” and the little blue box is exactly the quantity that tells you how hard that example is.  Concretely:\n\n1.  First, for each sample i you compute a difficulty score γᵢ via its cross‐entropy error (Eq. 5), normalized by the batch mean.  \n2.  You then turn that γᵢ into a per‐sample weight vᵢ by the simple thresholding rule in Eq. 6 (easy examples get vᵢ≈1, hard ones get vᵢ<1, and the threshold moves as training goes on).  \n3.  Finally, you multiply your standard KL‐based KD loss ℒ_KD by vᵢ before summing over the batch.  \n\nIn this way the preview‐based strategy “integrates” the difficulty score γᵢ into the knowledge‐distillation loss by literally weighting ℒ_KD for each example by a function of γᵢ."
    },
    {
        "question": "How does Denoising Autoencoder preprocessing alter CNN quantification performance compared to raw ALS-baselined spectra?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "None of the models were able to reach satisfactory differentiation of serotonin from the other neurotransmitters in the raw urine dataset (ViT: MAE = , MPE = 24.46%, CNNL: MAE = , MPE = 22.39%, sCNN: MAE = , MPE = 26.97%, CNN3PL: MAE = , MPE = 35.34%).",
            "However, in the denoised dataset, all models were capable of robust quantification of serotonin, with the CNN3PL model (MAE = , MPE = 4.67%) and the sCNN model (MAE = , MPE = 3.52%) outperforming both the ViT model (MAE = , MPE = 8.09%) and the CNNL model (MAE = , MPE = 7.45%)."
        ],
        "final_answer": "Applying the denoising autoencoder to the ALS-baselined spectra dramatically improved CNN performance. On raw spectra, CNN variants showed poor differentiation (for example, CNN3PL MPE = 35.34%), whereas after denoising, all CNN models achieved robust quantification with much lower errors (CNN3PL MPE = 4.67%, sCNN MPE = 3.52%, CNNL MPE = 7.45%).",
        "relevant_elements": [
            "ALS Baselining",
            "Denoising Autoencoder",
            "Convolutional Neural Network"
        ],
        "id": 2364,
        "masked_question": "How does [mask1] preprocessing alter [mask2] quantification performance compared to raw ALS-baselined spectra?",
        "masked_number": 2,
        "masked_elements": [
            "Denoising Autoencoder",
            "Convolutional Neural Network"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.08082v1_figure_1.png",
        "paperid": "2411.08082v1",
        "paper_path": "./MISS-QA/papers/2411.08082v1.json",
        "figure_id": "2411.08082v1_figure_1.png",
        "caption": "Figure 1: SERS deep learning framework development pipeline. Illustrated are the SERS measurement process applied (A), and the computational framework pipeline. Benchmark comparisons of alternative methodology are presented on the right. Preprocessing methods (B) are marked in orange and light red, quantification methods (C) in blue, and explainability methods (D) in dark red. Asymmetric least squares (ALS) baselining is applied to all spectra prior to assessing the framework or the benchmarks. SERS = surface-enhanced raman spectroscopy, AuNP = gold nanoparticle, CB[8] = cucurbit[8]uril, CNN = convolutional neural network, XGBoost = extreme gradient boosting trees, PLSR = partial least squares regression, SVM = support vector machines, CRIME = context representative interpretable model explanations, LEN = logic explained networks, SHAP = Shapley additive explanations.",
        "qtype": "Experimental_Results",
        "response": "Let me first restate what the two masked pieces are:\n\n- Mask 1 (red box) is the “Denoising Autoencoder” step.  \n- Mask 2 (blue box) is the neural-network “Quantification” stage (the CNN and Vision-Transformer models).\n\nNow, walking through the paper’s results:\n\n1.  All spectra, whether subsequently denoised or not, are first baseline-corrected with ALS.  \n2.  In the raw (ALS-only) set, none of the four neural models (ViT, CNNL, CNN3PL, sCNN) could reliably pick out serotonin in urine; their mean percentage errors (MPEs) on the test set were in the 22–35 % range and MAEs were high.  \n3.  After passing the ALS-baselined spectra through the denoising autoencoder, the exact same neural-network quantifiers were re-trained and tested.  Now every model “snaps to” a much lower error regime:  \n    –  the best ones (sCNN and CNN3PL) show MAEs around or below 0.5 μM and MPEs down to 3–5 %;  \n    –  even the weaker CNNL and ViT models fall into the 7–8 % MPE range.  \n4.  In short, adding the DAE front end drives the quantification errors down by roughly an order of magnitude (from ~25–35 % MPE to single-digit percent MPE) and turns an otherwise poor discriminator into a robust serotonin predictor.\n\nAnswer:\nBy feeding the raw, ALS-baselined spectra through the denoising autoencoder before quantification, the neural-network regressors’ test-set errors collapse from ~20–35 % MPE (and large MAEs) to single-digit‐percent MPE (∼3–9 %) and MAEs below ∼0.5–1 μM. In other words, the DAE front end cuts the quantification error by roughly an order of magnitude compared to using raw ALS-corrected spectra alone."
    },
    {
        "question": "How does CRIME explainability methodology differ from SHAP in clustering spectral feature relevance for contexts?",
        "relevant_section_ids": [
            "2.4",
            "2.5",
            "3.4"
        ],
        "relevant_context": [
            "The CRIME framework attempts to identify all prediction contexts of the input data space through the latent space of a variational autoencoder (VAE) trained on the LIME predictions of all instances in the available data. … The latent space instances are clustered into the final contexts using K-means clustering, and the latent space is visually inspected for selecting the number of clusters.",
            "To identify the defining features of each context representation, normalized LIME feature weights are combined with mean feature values representing the spectral intensities within the context clusters. They are then set in a three-dimensional space, together with normalized feature positions, which are then further clustered into 15 clusters using K-means clustering. … The five clusters with the highest score are selected to represent the regions of the spectra which contribute most to the contextual predictions.",
            "For comparison with CRIME, feature importance and model explainability was assessed using Logic Explained Networks (LEN)[5] and Shapley Additive Explanations (SHAP)[20]. … SHAP calculations were done using the above-mentioned sectioned categories separately using Gradient Explainer.",
            "SHAP values were assessed for all concentration ranges separately and have been visualized on an averaged spectra in Supplementary Figure 10."
        ],
        "final_answer": "CRIME differs from SHAP in that it first uses LIME explanations as input to a variational autoencoder and then applies K-means clustering to that latent space to discover distinct ‘contexts’ of model behavior. It then further clusters spectral features in a three-dimensional space of LIME weight, mean intensity, and position—again via K-means—to select the top regions per context. In contrast, SHAP in this study simply computes per-feature importance values (via a gradient explainer) for each concentration range and visualizes them on an averaged spectrum without any clustering step to group features into contexts.",
        "relevant_elements": [
            "CRIME",
            "SHAP"
        ],
        "id": 2365,
        "masked_question": "How does [mask1] explainability methodology differ from SHAP in clustering spectral feature relevance for contexts?",
        "masked_number": 1,
        "masked_elements": [
            "CRIME"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.08082v1_figure_1.png",
        "paperid": "2411.08082v1",
        "paper_path": "./MISS-QA/papers/2411.08082v1.json",
        "figure_id": "2411.08082v1_figure_1.png",
        "caption": "Figure 1: SERS deep learning framework development pipeline. Illustrated are the SERS measurement process applied (A), and the computational framework pipeline. Benchmark comparisons of alternative methodology are presented on the right. Preprocessing methods (B) are marked in orange and light red, quantification methods (C) in blue, and explainability methods (D) in dark red. Asymmetric least squares (ALS) baselining is applied to all spectra prior to assessing the framework or the benchmarks. SERS = surface-enhanced raman spectroscopy, AuNP = gold nanoparticle, CB[8] = cucurbit[8]uril, CNN = convolutional neural network, XGBoost = extreme gradient boosting trees, PLSR = partial least squares regression, SVM = support vector machines, CRIME = context representative interpretable model explanations, LEN = logic explained networks, SHAP = Shapley additive explanations.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. In the figure under the “Explainability” heading (panel D), the only method boxed in red is labeled “CRIME.”  By contrast, SHAP appears as an un-boxed benchmark on the right.  Hence “mask1” must be CRIME.  \n2. Reading the text, CRIME starts by taking every instance’s LIME explanation vector, projecting them through a small variational autoencoder (VAE), and then using K-means in that learned 2D latent space to pull out a discrete set of “contexts” (i.e. clusters of explanation patterns).  \n3. For each of those contexts, CRIME averages the LIME weights and then—using the spectra’s real intensities, the normalized LIME weights, and the feature positions—runs a second K-means to pull out the handful of spectral‐peak clusters that drive that context’s predictions.  Those become the context-specific “hot spots” on the Raman trace.  \n4. SHAP, by contrast, simply computes Shapley values for each feature (or feature‐group), yielding a single importance score per feature per sample (or a global average), but it does not learn a latent embedding of explanation vectors, nor does it cluster samples into contexts or cluster the peaks themselves into context-specific groups.  \n\nAnswer  \nCRIME differs from SHAP in that it (a) embeds instance‐wise local explanations into a low-dimensional latent space via a VAE, (b) clusters that space into distinct “contexts” of model reasoning, and (c) within each context clusters spectral features by joint consideration of their LIME weights, raw intensities, and positions—producing context-specific relevance regions.  SHAP, on the other hand, simply assigns additive importance scores to each feature (or feature category) without discovering or grouping distinct explanation contexts."
    },
    {
        "question": "What limitations could the Denoising Autoencoder impose on spectrum feature preservation under high noise conditions?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Denoising Autoencoder"
        ],
        "id": 2366,
        "masked_question": "What limitations could the [mask1] impose on spectrum feature preservation under high noise conditions?",
        "masked_number": 1,
        "masked_elements": [
            "Denoising Autoencoder"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.08082v1_figure_1.png",
        "paperid": "2411.08082v1",
        "paper_path": "./MISS-QA/papers/2411.08082v1.json",
        "figure_id": "2411.08082v1_figure_1.png",
        "caption": "Figure 1: SERS deep learning framework development pipeline. Illustrated are the SERS measurement process applied (A), and the computational framework pipeline. Benchmark comparisons of alternative methodology are presented on the right. Preprocessing methods (B) are marked in orange and light red, quantification methods (C) in blue, and explainability methods (D) in dark red. Asymmetric least squares (ALS) baselining is applied to all spectra prior to assessing the framework or the benchmarks. SERS = surface-enhanced raman spectroscopy, AuNP = gold nanoparticle, CB[8] = cucurbit[8]uril, CNN = convolutional neural network, XGBoost = extreme gradient boosting trees, PLSR = partial least squares regression, SVM = support vector machines, CRIME = context representative interpretable model explanations, LEN = logic explained networks, SHAP = Shapley additive explanations.",
        "qtype": "Others",
        "response": "Under very high noise levels a denoising auto‐encoder can start to “over-smooth’’ the spectra.  In practice this shows up as:\n\n  • Loss of small or low‐intensity peaks.  The bottleneck forces the network to throw away anything it can’t reliably encode, so faint but genuine Raman bands can be suppressed.  \n  • Distortion of peak shapes and relative intensities.  Reconstruction may favor the large, broad features seen in training and tend to “round off’’ or reshape sharper, physiological markers.  \n  • Poor generalization to unseen noise.  If the noise in a new spectrum differs markedly from the artificial urine noise used in training, the autoencoder may either leave noise residuals behind or hallucinate spurious peaks.  \n  • Biased spectral baselines.  In extreme cases the network can learn a “mean” background shape and subtract real baseline variation, shifting the zero‐level or introducing baseline ripples.  \n\nAll of these effects become more pronounced as the signal-to-noise ratio falls below the levels the autoencoder saw during training, so under very heavy noise you risk losing or distorting genuine spectral features."
    },
    {
        "question": "What ethical considerations arise when applying CRIME to interpret patient-derived spectra for clinical diagnosis?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "CRIME"
        ],
        "id": 2367,
        "masked_question": "What ethical considerations arise when applying [mask1] to interpret patient-derived spectra for clinical diagnosis?",
        "masked_number": 1,
        "masked_elements": [
            "CRIME"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.08082v1_figure_1.png",
        "paperid": "2411.08082v1",
        "paper_path": "./MISS-QA/papers/2411.08082v1.json",
        "figure_id": "2411.08082v1_figure_1.png",
        "caption": "Figure 1: SERS deep learning framework development pipeline. Illustrated are the SERS measurement process applied (A), and the computational framework pipeline. Benchmark comparisons of alternative methodology are presented on the right. Preprocessing methods (B) are marked in orange and light red, quantification methods (C) in blue, and explainability methods (D) in dark red. Asymmetric least squares (ALS) baselining is applied to all spectra prior to assessing the framework or the benchmarks. SERS = surface-enhanced raman spectroscopy, AuNP = gold nanoparticle, CB[8] = cucurbit[8]uril, CNN = convolutional neural network, XGBoost = extreme gradient boosting trees, PLSR = partial least squares regression, SVM = support vector machines, CRIME = context representative interpretable model explanations, LEN = logic explained networks, SHAP = Shapley additive explanations.",
        "qtype": "Others",
        "response": "When we move from a laboratory proof‐of‐concept into real patient samples and real clinical decisions, simply “having” an explainability tool like CRIME is not enough—several ethical issues immediately surface:\n\n  1. Patient autonomy and informed consent  \n     •  Spectral fingerprints can carry more than just the target biomarker signal—they can contain incidental information about other metabolites or even genetic traits. Patients must be made aware that their samples will be run through a multi‐step deep‐learning pipeline (denoiser → predictor → CRIME explainer) and must explicitly consent to both the primary analysis and any “off‐target” discoveries.  \n\n  2. Privacy and data protection  \n     •  High‐resolution spectra are effectively a unique molecular “ID” and, if linked to a patient’s electronic health record, could risk re-identification. Robust de-identification, strict access controls and secure storage of raw and intermediate spectral data are mandatory.  \n\n  3. Algorithmic bias and fairness  \n     •  CRIME discovers “contexts” (clusters) in model behavior. If the training set under-represents certain demographic groups or comorbidities, some contexts may become proxies for factors like age, sex, ethnicity or medication rather than the disease itself—leading to unfair misclassification. Ongoing auditing and subgroup analyses are needed to detect and correct such biases.  \n\n  4. Transparency vs. over-confidence  \n     •  Explanation heat-maps or “context clusters” can give clinicians the illusion of full understanding—even though LIME-based methods (and by extension CRIME) approximate a far more complex neural network. There is a danger of blindly trusting the model’s highlighted peaks, especially at critical decision points. We must educate users about the approximate, local nature of these explanations and enforce human-in-the-loop verification.  \n\n  5. Clinical responsibility and accountability  \n     •  Who is ultimately responsible if a CRIME-guided interpretation leads to a misdiagnosis? The vendor, the data scientist who trained the model, or the treating physician? Clear guidelines must be laid down about attribution of liability and the requirement for confirmatory testing.  \n\n  6. Management of incidental and secondary findings  \n     •  CRIME may flag spectral regions that correspond to unexpected analytes (e.g. medications, metabolites of uncertain significance). We must have protocols in place for how—if at all—these should be reported back to patients and clinicians to avoid causing undue anxiety or downstream overtesting.  \n\nIn practice, each of these dimensions needs written policies, technical safeguards, and continuous post-deployment monitoring before a CRIME-enabled SERS assay can be ethically and safely used in the clinic."
    },
    {
        "question": "What performance trade-offs might occur from Secure Input Formatting under real-time application constraints?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Secure Input Formatting"
        ],
        "id": 2368,
        "masked_question": "What performance trade-offs might occur from [mask1] under real-time application constraints?",
        "masked_number": 1,
        "masked_elements": [
            "Secure Input Formatting"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.21492v2_figure_1.png",
        "paperid": "2410.21492v2",
        "paper_path": "./MISS-QA/papers/2410.21492v2.json",
        "figure_id": "2410.21492v2_figure_1.png",
        "caption": "Figure 1: An illustration of Formatting Authentication with Hash-based Tags.",
        "qtype": "Others",
        "response": "Chain‐of‐thought (identifying the masked component and then its impact on runtime performance)\n\n1.  The red‐boxed area in Figure 1 corresponds to the “Secure Input Formatting” stage.  In this stage the system\n    •  Computes HMAC-based authentication tags (TAG1, TAG2, …) for each incoming user instruction or external snippet  \n    •  Wraps the user instruction in one dynamic tag and the external text in a different tag  \n    •  Concatenates them into a single prompt for the LLM, with these extra delimiters clearly marking “authorized” vs. “untrusted” regions  \n\n2.  In a real‐time application—where end‐users expect sub‐second response times—the addition of this formatting step carries a few unavoidable costs:\n    a.  Cryptographic overhead.  Every query must run an HMAC (or similar) to produce fresh tags.  Even though HMAC is relatively fast, at high query volumes it adds non-negligible CPU time compared to a plain prompt.  \n    b.  Token overhead.  The extra opening/closing tags inflate the total token count.  More tokens → longer LLM inference time and higher per-query billing if you’re on a token-based API plan.  \n    c.  Pipeline complexity.  You now need a small parser before the call (to split and tag) and another parser after (to strip out and verify only the authorized segment).  Each of these parsing steps adds latency, and under load they can become a throughput bottleneck.  \n    d.  Potential model‐quality impact.  Very long or heavily formatted prompts can sometimes degrade generation quality or force you to shorten in‐context examples, which in turn can harm accuracy.  \n\n3.  Altogether, “secure input formatting” trades off extra security boundaries for:\n    –  Increased end-to-end latency per query  \n    –  Higher computational and API-usage cost  \n    –  Slightly more complex code paths (and potentially lower throughput under peak load)  \n\nAnswer  \nUnder real‐time constraints, secure input formatting inevitably adds extra compute and I/O work—HMAC tag generation, longer prompts (more tokens), and pre‐/post‐processing parsers all add latency, raise per-query token costs, and can become a throughput bottleneck."
    },
    {
        "question": "How could integrating dynamic ML-based parsing enhance Authentication Verification robustness?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Authentication Verification"
        ],
        "id": 2369,
        "masked_question": "How could integrating dynamic ML-based parsing enhance [mask1] robustness?",
        "masked_number": 1,
        "masked_elements": [
            "Authentication Verification"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.21492v2_figure_1.png",
        "paperid": "2410.21492v2",
        "paper_path": "./MISS-QA/papers/2410.21492v2.json",
        "figure_id": "2410.21492v2_figure_1.png",
        "caption": "Figure 1: An illustration of Formatting Authentication with Hash-based Tags.",
        "qtype": "Others",
        "response": "Chain-of-thought (brief):  \n1. In FATH the red‐boxed module (“[mask1]”) is the Authentication-Verification step, which today is implemented as a simple rule-based parser that looks for exactly the HMAC tags (e.g. <TAG4>…</TAG4>) and plucks out whatever sits between them.  \n2. Rule-based extraction is brittle: if an attacker manages to insert extra whitespace, HTML escapes, malformed tag names, zero-width characters, or multilayered injections, the rigid “find‐and‐match” logic will either fail to recognize a genuine tag or accidentally latch onto a spoofed one.  \n3. If we replace (or augment) that final stage with a little learned parser—for example, a small transformer or sequence-tagging model trained to spot “authorized response” spans and verify their HMAC integrity—then the system can:  \n   • tolerate slight variations in tag formatting (e.g. case changes, extra padding)  \n   • automatically learn to ignore randomly injected junk that isn’t truly part of the authorized span  \n   • flag anomalous outputs whose semantics don’t match the user’s instruction (adding a semantic back-check)  \n   • continuously re-train on new poisoning attempts to stay ahead of novel injection tricks  \n4. In short, swapping in a dynamic, ML-based parser makes the last line of defense far less fragile to adversarial formatting or unseen injection patterns, greatly bolstering the overall robustness of the authentication-verification stage."
    },
    {
        "question": "How does secure input formatting with authentication tags strengthen segregation against injection attacks?",
        "relevant_section_ids": [
            "1",
            "4.2"
        ],
        "relevant_context": [
            "Specifically, the FATH comprises three key components: (1) Secure Input Formatting: employ dynamic tags as delimiters to distinguish user instructions from external data, providing basic identification for the role of users and LLMs;",
            "This system includes the following three processes: (1) secure segregation with input formatting, splitting input prompts into user instructions and external text information with input authentication tags;"
        ],
        "final_answer": "Secure input formatting wraps the user’s instruction and the external text in distinct, dynamically generated hash-based tags. By using unpredictable authentication tags as delimiters, the system enforces clear, verifiable boundaries between trusted user commands and potentially untrusted external content—boundaries that attackers cannot forge—thereby preventing injected malicious instructions from being misclassified as legitimate user input.",
        "relevant_elements": [
            "secure input formatting",
            "input authentication tags"
        ],
        "id": 2370,
        "masked_question": "How does [mask1] with authentication tags strengthen segregation against injection attacks?",
        "masked_number": 1,
        "masked_elements": [
            "secure input formatting"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.21492v2_figure_1.png",
        "paperid": "2410.21492v2",
        "paper_path": "./MISS-QA/papers/2410.21492v2.json",
        "figure_id": "2410.21492v2_figure_1.png",
        "caption": "Figure 1: An illustration of Formatting Authentication with Hash-based Tags.",
        "qtype": "Design_Rationale",
        "response": "The red‐boxed module in Figure 1 is simply the “Secure Input Formatting” step – i.e. the very first thing FATH does to stop a malicious payload from ever bleeding into your real user prompt.  Concretely, it works like this:\n\n 1. We take whatever the genuine user typed (u) and whatever untrusted text we pulled in from the web or some other tool (x), and we wrap them each in their own dynamically generated, HMAC-backed delimiters:  \n     •  <TAG₁> … user instruction … </TAG₁>  \n     •  <TAG₂> … external info (potentially attacked) … </TAG₂>  \n\n 2. Those tags are not fixed words that an attacker can guess or overwrite.  They’re freshly computed each time using a secret key and HMAC, so no adversary can forge a “legitimate” tag or sneak malicious commands into the user block.\n\n 3. Down the pipeline the LLM is explicitly told “anything inside <TAG₁> is Authorized; anything inside <TAG₂> is Unauthorized.”  Even if the adversary slips “ignore prior instructions” or “claim \\$100 gift card” into the external block, the model is never allowed to treat it as though it were in the user block.\n\n 4. Finally, on output we do a rule-based parse that throws away any text not wrapped in the matching “Authorized” tag.  The result is that only the true user request ever reaches your application, and the injected instructions remain quarantined.\n\nBy cryptographically binding each segment to its own fresh tag and then enforcing those tags end-to-end, FATH turns prompt‐segregation into a verifiable, injection‐proof boundary rather than just a suggestion the model can ignore."
    },
    {
        "question": "What is the rationale for combining reference multi-level features with anchor feature in VCE?",
        "relevant_section_ids": [
            "1",
            "3.3"
        ],
        "relevant_context": [
            "On the one hand, in the vision projector pretraining stage, current methods primarily rely on high-level visual features, often overlooking low-level and fine-grained details, which limits visual comprehension.",
            "Specifically, for each patch in the high-level feature map, we extract neighboring patches from the middle-layer feature maps and combine them with the original patch to create a locally enhanced feature patch. This approach enhances local visual cues while maintaining computational efficiency.",
            "Typical vision projectors primarily rely on high-level vision feature maps to align vision and language modalities... which captures high-level, language-aligned semantic features but may overlook finer, detail-oriented visual cues. To address these limitations, we propose a lightweight Vision Cues Enhancement (VCE) module."
        ],
        "final_answer": "By fusing neighboring patches from intermediate (reference) feature maps into each patch of the high-level (anchor) feature, VCE recovers fine-grained, local visual details that are otherwise lost when using only high-level semantic features, thereby enriching visual cues and improving visual comprehension with minimal extra computation.",
        "relevant_elements": [
            "anchor feature",
            "reference multi-level feature",
            "VCE"
        ],
        "id": 2372,
        "masked_question": "What is the rationale for combining [mask1] with [mask2] in VCE?",
        "masked_number": 2,
        "masked_elements": [
            "reference multi-level feature",
            "anchor feature"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.12787v1_figure_1.png",
        "paperid": "2411.12787v1",
        "paper_path": "./MISS-QA/papers/2411.12787v1.json",
        "figure_id": "2411.12787v1_figure_1.png",
        "caption": "Figure 1: The two training stages of our framework. In the Vision Projector Pretraining stage, we incorporate the proposed Visual Cue Enhancement module (VCE) to enhance the final vision feature map by capturing detailed local information from multi-level visual feature maps. In the Visual Instruction Fine-Tuning stage, we use the proposed Dual Low-Rank Adaptation (Dual-LoRA) as the LLM adapter to learn task-specific knowledge while mitigating data conflicts, which is set trainable alone with VCE and vision projector.",
        "qtype": "Design_Rationale",
        "response": "In VCE we literally take the one “anchor” feature map that everybody else has been using (the last, high‐level ViT layer) and we augment it with its own little patch neighborhoods drawn from the intermediate (i.e. lower-level) feature maps.  In other words, we combine:\n\n  •  Mask 1 (the anchor feature)  \n  •  Mask 2 (the collection of reference, multi-level features)\n\nso that each patch carries both its high-level semantics and the fine-grained, local details that only the mid-layers know about.  That simple deformable attention fusion gives you the best of both worlds – global context + local cue – without blowing up your compute."
    },
    {
        "question": "What is the reasoning behind freezing the LLM but training the vision projector and Dual-LoRA together?",
        "relevant_section_ids": [
            "1",
            "2",
            "3.5"
        ],
        "relevant_context": [
            "Section 1: To reduce the extensive number of parameters in LLM tuning, low-rank adaptation (LoRA) was proposed by injecting lightweight adapters into pretrained LLMs for efficient fine-tuning.",
            "Section 2: Low-Rank Adaptation (LoRA) [8] introduces a method that freezes pretrained model weights and injects trainable low-rank decomposition matrices into the linear layers of the Transformer architecture, significantly reducing the number of trainable parameters required for downstream tasks.",
            "Section 3.5: In the vision projector pretraining stage, we train the parameters of the vision projection modules, i.e., P and VCE. In the subsequent visual instruction fine-tuning stage, we enable additional training for the Dual-LoRA parameters."
        ],
        "final_answer": "Because the underlying language model contains hundreds of millions (or even billions) of parameters, fully fine-tuning it on multimodal tasks is both compute- and memory-intensive. By freezing the LLM and instead (1) training only the vision projector (with VCE) to align and enhance visual features, and (2) injecting and training a small, low-rank Dual-LoRA module to adapt the LLM’s behavior, the model can efficiently learn to follow visual instructions with far fewer trainable parameters and lower resource cost.",
        "relevant_elements": [
            "vision projector",
            "LLM",
            "Dual-LoRA"
        ],
        "id": 2373,
        "masked_question": "What is the reasoning behind freezing the [mask1] but training the [mask2] and Dual-LoRA together?",
        "masked_number": 2,
        "masked_elements": [
            "LLM",
            "vision projector"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.12787v1_figure_1.png",
        "paperid": "2411.12787v1",
        "paper_path": "./MISS-QA/papers/2411.12787v1.json",
        "figure_id": "2411.12787v1_figure_1.png",
        "caption": "Figure 1: The two training stages of our framework. In the Vision Projector Pretraining stage, we incorporate the proposed Visual Cue Enhancement module (VCE) to enhance the final vision feature map by capturing detailed local information from multi-level visual feature maps. In the Visual Instruction Fine-Tuning stage, we use the proposed Dual Low-Rank Adaptation (Dual-LoRA) as the LLM adapter to learn task-specific knowledge while mitigating data conflicts, which is set trainable alone with VCE and vision projector.",
        "qtype": "Design_Rationale",
        "response": "mask₁ is simply the frozen backbone LLM (the red‐boxed “LLM”), whereas mask₂ is the tiny trainable adapter we inject into it—our Dual-LoRA (the blue box).  We freeze the LLM itself because\n\n 1.  it already encodes vast amounts of general language and reasoning knowledge that we do not want to disturb (i.e. no catastrophic forgetting),  \n 2.  freezing avoids the expense and instability of fine-tuning hundreds of millions or billions of parameters,  \n\nand we train only the Dual-LoRA (mask₂) together with the vision projector (and the VCE) because\n\n 1.  Dual-LoRA is a very small, low-rank module that can learn the new “skill” and “task-activation” adjustments needed for visual prompting,  \n 2.  it lets us flexibly allocate capacity to different downstream instructions (mitigating data conflicts) without touching the main LLM weights,  \n 3.  jointly updating the vision projector and the adapter ensures that visual features and language adaptation stay in sync.  \n\nIn short, freezing the large LLM preserves its general pre-trained knowledge and greatly reduces compute, while training only the lightweight Dual-LoRA (and vision projector) gives us efficient, task-specific adaptation."
    },
    {
        "question": "How does VCE fuse multi-level anchor and reference features before the vision projector?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "This module uses the final vision feature map as an anchor feature A and applies a single deformable cross-attention mechanism to neighboring patches across other layer feature maps R for each patch of the anchor feature.",
            "To enhance the vision cues, we integrate local visual details from R into A. We utilize deformable attention [28] to efficiently compute cross-attention across multi-level feature maps, applied between A and R: Ã = DeformAttnθ(A, R).",
            "After obtaining the anchor feature A and the enhanced visual cue Ã, we add them up and apply normalization to produce the final enhanced vision feature map A^. This feature map is subsequently projected into vision tokens Z through the vision feature projector: A^ = LayerNorm(A + Ã), Z = P(A^)."
        ],
        "final_answer": "VCE first uses deformable cross-attention to inject local details from multi-level reference feature maps R into the high-level anchor feature A, yielding an enhanced cue map Ã. It then sums Ã with the original anchor A, applies layer normalization to form the final enhanced feature A^, and feeds A^ into the vision projector.",
        "relevant_elements": [
            "VCE",
            "vision projector"
        ],
        "id": 2374,
        "masked_question": "How does [mask1] fuse multi-level anchor and reference features before the [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "VCE",
            "vision projector"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.12787v1_figure_1.png",
        "paperid": "2411.12787v1",
        "paper_path": "./MISS-QA/papers/2411.12787v1.json",
        "figure_id": "2411.12787v1_figure_1.png",
        "caption": "Figure 1: The two training stages of our framework. In the Vision Projector Pretraining stage, we incorporate the proposed Visual Cue Enhancement module (VCE) to enhance the final vision feature map by capturing detailed local information from multi-level visual feature maps. In the Visual Instruction Fine-Tuning stage, we use the proposed Dual Low-Rank Adaptation (Dual-LoRA) as the LLM adapter to learn task-specific knowledge while mitigating data conflicts, which is set trainable alone with VCE and vision projector.",
        "qtype": "Implementation_Details",
        "response": "The red‐boxed module is the Visual Cue Enhancement (VCE), and the blue‐boxed module is the Vision Projector.  In the “Multi‐level Local Visual Cue Enhancement” section of the text you’ll find exactly how VCE fuses its inputs before handing them off to the projector:\n\n1.  VCE takes as input  \n    –  the *anchor* feature map (the final, high-level output of the vision encoder)  \n    –  a set of *reference* feature maps drawn from intermediate layers of the same encoder.  \n\n2.  It runs **one deformable cross-attention** head for each location in the anchor.  In practice this means that each spatial position in the anchor feature  \n    –  learns a small number of sampling offsets  \n    –  attends to the corresponding local patches in *each* reference feature map  \n    –  uses learned attention weights to aggregate those multi-level visual cues.  \n\n3.  The result is an **enhanced cue** map that has absorbed fine, low-level details from all the reference layers.  \n\n4.  Finally, VCE **adds** this enhanced cue back onto the original anchor feature (element-wise sum) and applies a layer‐normalization.  \n\n5.  The normalized, fused feature is then handed to the Vision Projector (the blue box), which turns it into the sequence of vision tokens for the LLM.  \n\nIn one equation (from the paper):  \n     Eanchor = DeformableAttn(anchor, {reference maps})  \n     Ffinal = LayerNorm(anchor + Eanchor)  \nand Ffinal is what goes into the projector."
    },
    {
        "question": "How does Dual-LoRA modulate frozen LLM layer outputs during visual instruction fine-tuning?",
        "relevant_section_ids": [
            "3.4",
            "3.4.2"
        ],
        "relevant_context": [
            "In contrast, we propose Dual Low-Rank Adaptation (Dual-LoRA), a unified approach that decouples adaptation into a task-skill dual low-rank structure, removing the need for complex expert splitting or intricate routing strategies.",
            "According to Corollary 2, additional space can be used to map the skill space S, enabling the simulation of sparsely activated experts that respond to varying instructional tasks. We use the parameter matrix A to modulate S, which is mapped through a non-linear activation f to achieve the effect of rank rectification. In this paper, we use the ReLU activation function due to its sparsity-inducing properties and ease of optimization.",
            "Finally, we combine the adaptation feature generated by D with the feature generated from the pre-trained weight W to obtain the output feature ."
        ],
        "final_answer": "Dual-LoRA inserts a single low-rank adapter into each frozen LLM layer, splitting it into two subspaces: a ‘skill’ low-rank space S and a ‘task-activation’ low-rank space A. During fine-tuning, it first produces a skill feature S(x), then modulates it by A through a ReLU nonlinearity to yield an adaptation feature D(x). This D(x) is finally added to the original frozen layer output W·x, thereby modulating the frozen LLM outputs without altering the original weights.",
        "relevant_elements": [
            "Dual-LoRA",
            "LLM"
        ],
        "id": 2375,
        "masked_question": "How does Dual-LoRA modulate frozen [mask1] layer outputs during visual instruction fine-tuning?",
        "masked_number": 1,
        "masked_elements": [
            "LLM"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.12787v1_figure_1.png",
        "paperid": "2411.12787v1",
        "paper_path": "./MISS-QA/papers/2411.12787v1.json",
        "figure_id": "2411.12787v1_figure_1.png",
        "caption": "Figure 1: The two training stages of our framework. In the Vision Projector Pretraining stage, we incorporate the proposed Visual Cue Enhancement module (VCE) to enhance the final vision feature map by capturing detailed local information from multi-level visual feature maps. In the Visual Instruction Fine-Tuning stage, we use the proposed Dual Low-Rank Adaptation (Dual-LoRA) as the LLM adapter to learn task-specific knowledge while mitigating data conflicts, which is set trainable alone with VCE and vision projector.",
        "qtype": "Implementation_Details",
        "response": "Mask₁ is the frozen LLM itself.  Dual-LoRA never touches the LLM’s pre-trained weights – instead it sits alongside each linear sub-layer of the frozen model and injects a small, additive low-rank “delta” into the hidden activations.  Concretely, at every adapted layer with frozen weight W₀ you learn three tiny matrices A, T, B of rank r (r≪hidden size) so that, for input x into that layer, you compute\n\n  1. skill feature s = LN(A·x)  \n  2. task rectifier g = ReLU(T·s)  \n  3. adapted delta  Δ = B·g  \n\nand then return  \n\n  output = W₀·x   +   Δ  \n\nIn this way the frozen LLM carries on using its original W₀·x computation, but Dual-LoRA has learned a two-stage (skill + task) low-rank residual that modulates its outputs for each instruction."
    },
    {
        "question": "How does ControlNet integrate edge-derived features into SSI-CN to preserve organ morphology during inpainting?",
        "relevant_section_ids": [
            "3.2",
            "4.2"
        ],
        "relevant_context": [
            "ControlNet (CN) is a framework designed for controlling pre-trained DMs’ image generation process by integrating additional conditioning signals such as sketches, key points, edges, and segmentation maps [70]. The model consists of two sets of U-Net weights derived from the pre-trained DM: with θ, that undergoes training using task-specific datasets to accommodate the additional condition, and the frozen copy, θ′. Let x be the input feature map from SD, then the feature map y from the ControlNet is defined as y=SN(x;θ′)+ΔCN(x, c;θ), where ΔCN denotes x zero-convolution layers with trainable parameters that link pre-trained SD with ControlNet blocks and c is the conditioning signal. We use pre-trained CN for spatial conditioning.",
            "…we circumvent this process by integrating a pre-trained CN model into the inpainting SSI (SSI-CN) model to control the shape and texture of the generated organs precisely. The number of classes for the pre-trained CN-Seg model did not match our surgical datasets, so we opted for the pre-trained soft edge CN model, which uses extracted edge images from the segmentation masks as the conditioning signal. Given an input image and a mask, the new organ texture is inpainted only in the masked region leaving the background the same."
        ],
        "final_answer": "ControlNet preserves organ morphology in SSI-CN by taking edge maps—extracted from the segmentation masks—and feeding them as an extra conditioning signal into dedicated ControlNet adapter blocks (the zero-convolution layers and small U-Net blocks). During inpainting, these edge-derived features guide the diffusion U-Net so that the new organ texture is generated strictly within the masked region and adheres to the precise outline given by the edges.",
        "relevant_elements": [
            "ControlNet",
            "SSI-CN"
        ],
        "id": 2376,
        "masked_question": "How does [mask1] integrate edge-derived features into SSI-CN to preserve organ morphology during inpainting?",
        "masked_number": 1,
        "masked_elements": [
            "ControlNet"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.07753v2_figure_2.png",
        "paperid": "2410.07753v2",
        "paper_path": "./MISS-QA/papers/2410.07753v2.json",
        "figure_id": "2410.07753v2_figure_2.png",
        "caption": "Figure 2: Overview of the diffusion approach to generate a multi-class dataset. Stage-1111 involves training the SD inpainting model using the real images and masks for each organ separately. In stage-2222, pre-trained ControlNet is plugged into the SSI model (SSI-CN) to precisely generate each anatomical structure using extracted edges from the segmentation mask. The image composition in stage-3333 includes cutting out each organ from the generated image and combining them together to form the multi-class surgical dataset. Stage-4444 (optional) includes an image refinement process using SDEdit [36] to rectify inconsistencies during the composition operation and generate the multi-class images. We skip stage-1111 for the simulated masks and start directly with the inference stages to generate the synthetic datasets.",
        "qtype": "Implementation_Details",
        "response": "In our paper the red‐boxed module is exactly the “SSI + ControlNet” inference stage (we call it SSI-CN).  What we do there is very simple in principle:\n\n  1.  We take the edge image (a soft-edge map extracted from the ground‐truth segmentation for that organ) and feed it into a pre-trained ControlNet that was trained to accept edges as its conditioning signal.  \n  2.  That ControlNet lives “side by side” with our inpainting U-Net (the SSI model) and injects its features at every resolution level via zero-initialized convolutional “side chains” (the standard ControlNet mechanism).  \n  3.  At each U-Net block the ControlNet’s edge‐derived activations are added to the corresponding SSI activations, so the network learns to keep the newly synthesized texture strictly inside the shape implied by the edges.  \n\nBy literally adding the ControlNet’s edge‐features into the noise‐prediction U-Net at every scale, SSI-CN is forced to respect the precise organ outline during denoising, and so the inpainted piece comes out with the correct morphology."
    },
    {
        "question": "How does the image composition module blend separate organ inpaintings to eliminate visible seams and color mismatches?",
        "relevant_section_ids": [
            "4.4"
        ],
        "relevant_context": [
            "We noticed that the image composition operation introduced sharp edges between the organs and lighting artifacts, which is not present in real surgical images (see Fig. 3). Hence, in Stage 4, we perform an image enhancement step using SDEdit [36].",
            "We use the SD model trained with all organs combined with SDEdit to remove the inconsistencies introduced in the previous Stage 3. Low levels of noise has shown to improve texture components in images [56] and hence this step can be optionally added to maintain the overall texture."
        ],
        "final_answer": "After cutting out and pasting the separately inpainted organs (Stage 3), the composed image is passed through an SDEdit-based refinement (Stage 4). By re-noising the entire composite slightly and then denoising with the Stable Diffusion model trained on all organs, the process smooths sharp seams and harmonizes lighting and color, effectively blending the individual organ inpaintings into a coherent, artifact-free image.",
        "relevant_elements": [
            "image composition"
        ],
        "id": 2377,
        "masked_question": "How does the [mask1] module blend separate organ inpaintings to eliminate visible seams and color mismatches?",
        "masked_number": 1,
        "masked_elements": [
            "image composition"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.07753v2_figure_2.png",
        "paperid": "2410.07753v2",
        "paper_path": "./MISS-QA/papers/2410.07753v2.json",
        "figure_id": "2410.07753v2_figure_2.png",
        "caption": "Figure 2: Overview of the diffusion approach to generate a multi-class dataset. Stage-1111 involves training the SD inpainting model using the real images and masks for each organ separately. In stage-2222, pre-trained ControlNet is plugged into the SSI model (SSI-CN) to precisely generate each anatomical structure using extracted edges from the segmentation mask. The image composition in stage-3333 includes cutting out each organ from the generated image and combining them together to form the multi-class surgical dataset. Stage-4444 (optional) includes an image refinement process using SDEdit [36] to rectify inconsistencies during the composition operation and generate the multi-class images. We skip stage-1111 for the simulated masks and start directly with the inference stages to generate the synthetic datasets.",
        "qtype": "Implementation_Details",
        "response": "The short answer is: there is no fancy alpha-blending or hand-crafted Poisson‐blending in that red‐boxed “Image Composition” block at all – you literally cut out each organ from its own SSI-ControlNet inpainting, paste them together by their masks, and then hand the entire, jagged, multi-organ image over to a single “image refinement” or SDEdit pass (Stage 4).  That final SDEdit step is what actually “blends” everything and kills off seams and colour mismatches.  Concretely:\n\n 1. In Stage 3 (“Image Composition”), you use each organ’s binary mask to crop the inpainted organ out of its SSI–CN output and drop it into the correct place on a common background.  This pasting is hard-edged, so you will always see a slight seam or lighting discontinuity at the mask boundaries.  \n 2. In Stage 4 (“Image Enhancement”), you take that pasted image, inject a small amount of noise, and then run a few reverse diffusion steps through the Stable Diffusion model that has been fine-tuned on *all* organs together (i.e. the SDEdit setup).  \n 3. Because the diffusion model has learned the joint appearance statistics of the different organs in real surgical scenes, the reverse-diffusion pass “hallucinates” tiny amounts of new texture, smooths over lighting inconsistencies, and in effect harmonizes the colours across every boundary.\n\nIn other words, you rely on the learned diffusion prior (via SDEdit) to fuse the separately inpainted pieces into one coherent image. The “mask1” block simply assembles the puzzle, and the subsequent SDEdit pass glues it all together invisibly."
    },
    {
        "question": "How does combining SSI and pre-trained ControlNet leverage spatial conditioning techniques from prior diffusion research?",
        "relevant_section_ids": [
            "3.2",
            "4.2"
        ],
        "relevant_context": [
            "Section 3.2: \"ControlNet (CN) is a framework designed for controlling pre-trained DMs’ image generation process by integrating additional conditioning signals such as sketches, key points, edges, and segmentation maps [70]. The model consists of two sets of U-Net weights derived from the pre-trained DM… We use pre-trained CN for spatial conditioning.\"",
            "Section 4.2: \"…we circumvent this process by integrating a pre-trained CN model into the inpainting SSI (SSI-CN) model to control the shape and texture of the generated organs precisely. The number of classes for the pre-trained CN-Seg model did not match our surgical datasets, so we opted for the pre-trained soft edge CN model, which uses extracted edge images from the segmentation masks as the conditioning signal.\""
        ],
        "final_answer": "By plugging a pre-trained ControlNet—originally developed to inject spatial conditions (e.g., edge maps, sketches, segmentation maps) into diffusion models—directly into the Surgical Stable Inpaint (SSI) pipeline, the authors harness ControlNet’s learned adapters to guide the inpainting process. In practice, they feed edge images (extracted from organ masks) into ControlNet’s adapter blocks during SSI inpainting, which spatially constrains the diffusion model to produce anatomically accurate shapes and textures. This strategy directly leverages ControlNet’s prior work on spatial conditioning to improve control over organ geometry in the generated images.",
        "relevant_elements": [
            "SSI",
            "ControlNet"
        ],
        "id": 2378,
        "masked_question": "How does combining [mask1] and pre-trained [mask2] leverage spatial conditioning techniques from prior diffusion research?",
        "masked_number": 2,
        "masked_elements": [
            "SSI",
            "ControlNet"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.07753v2_figure_2.png",
        "paperid": "2410.07753v2",
        "paper_path": "./MISS-QA/papers/2410.07753v2.json",
        "figure_id": "2410.07753v2_figure_2.png",
        "caption": "Figure 2: Overview of the diffusion approach to generate a multi-class dataset. Stage-1111 involves training the SD inpainting model using the real images and masks for each organ separately. In stage-2222, pre-trained ControlNet is plugged into the SSI model (SSI-CN) to precisely generate each anatomical structure using extracted edges from the segmentation mask. The image composition in stage-3333 includes cutting out each organ from the generated image and combining them together to form the multi-class surgical dataset. Stage-4444 (optional) includes an image refinement process using SDEdit [36] to rectify inconsistencies during the composition operation and generate the multi-class images. We skip stage-1111 for the simulated masks and start directly with the inference stages to generate the synthetic datasets.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module in the figure is nothing more than our organ‐specific inpainting U-Net (what we call SSI) and the blue‐boxed module is the frozen, pre‐trained ControlNet (CN) that was trained to take edge/segmentation maps as a spatial “hint.”  What we do in stage 2 is simply plug the two together so that:\n\n 1.  The SSI U-Net still handles the actual “fill in the hole” task – i.e. learning the appearance and textures of each organ from real surgical images.\n 2.  The ControlNet adapters (the little zero-conv + side‐branch blocks) inject the exact shape information of that organ back into every level of the denoiser via the extracted edge map.\n\nBy doing so we are explicitly borrowing the spatial‐conditioning trick from the original ControlNet papers (where you freeze most of a pre-trained diffusion model and add tiny, trainable side modules that pass in your sketch/edge/segmentation map).  In our case the result is an inpainting network that not only knows how to draw a liver or gallbladder texture, but also respects the precise outline you give it, just as prior diffusion work showed you could by plugging in an edge map or keypoint map to guide generation."
    },
    {
        "question": "How does SDEdit image enhancement incorporate noise-based editing strategies from earlier diffusion models?",
        "relevant_section_ids": [
            "3.3",
            "4.4"
        ],
        "relevant_context": [
            "SDEdit is an image editing method that uses stochastic differential equations (SDE) to solve the reverse diffusion process [36]. A user-given image is firstly noised up to a specific limit depending on the specified noising strength, and denoising starts from this noisy image, which serves as a controlling signal, back to a realistic image. Text prompts can be added as additional guiding signals during the denoising process. This method is used in the final stage for image refinement in our pipeline.",
            "We noticed that the image composition operation introduced sharp edges between the organs and lighting artifacts, which is not present in real surgical images (see Fig. 3). Hence, in Stage-4, we perform an image enhancement step using SDEdit [36]. We use the SD model trained with all organs combined with SDEdit to remove the inconsistencies introduced in the previous Stage-3. Low levels of noise has shown to improve texture components in images [56] and hence this step can be optionally added to maintain the overall texture."
        ],
        "final_answer": "SDEdit takes the composed multi‐class image, adds a controlled amount of noise (as in early diffusion processes), and then runs the reverse diffusion (denoising) with the Stable Diffusion model. By noising up the image to a user‐specified strength and then denoising it, SDEdit removes sharp seams and lighting artifacts while enhancing textures, thus leveraging the noise‐based editing strategies of earlier diffusion frameworks.",
        "relevant_elements": [
            "Image enhancement",
            "SDEdit"
        ],
        "id": 2379,
        "masked_question": "How does SDEdit [mask1] incorporate noise-based editing strategies from earlier diffusion models?",
        "masked_number": 1,
        "masked_elements": [
            "Image enhancement"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.07753v2_figure_2.png",
        "paperid": "2410.07753v2",
        "paper_path": "./MISS-QA/papers/2410.07753v2.json",
        "figure_id": "2410.07753v2_figure_2.png",
        "caption": "Figure 2: Overview of the diffusion approach to generate a multi-class dataset. Stage-1111 involves training the SD inpainting model using the real images and masks for each organ separately. In stage-2222, pre-trained ControlNet is plugged into the SSI model (SSI-CN) to precisely generate each anatomical structure using extracted edges from the segmentation mask. The image composition in stage-3333 includes cutting out each organ from the generated image and combining them together to form the multi-class surgical dataset. Stage-4444 (optional) includes an image refinement process using SDEdit [36] to rectify inconsistencies during the composition operation and generate the multi-class images. We skip stage-1111 for the simulated masks and start directly with the inference stages to generate the synthetic datasets.",
        "qtype": "Literature_Background",
        "response": "Here’s the step-by-step reasoning:\n\n1.  In classical diffusion models, you have two processes:  \n    a.  A forward noising process that gradually corrupts a clean image  by adding Gaussian noise according to a fixed schedule  (this is the SDE in the forward direction).  \n    b.  A learned reverse (denoising) process, parameterized by a U-Net, that removes noise step by step to recover a clean image (the backward SDE).\n\n2.  SDEdit turns that same machinery into an editing tool:  \n    –  **Forward:** take your *existing* (composed) image and add noise up to some user-selected “strength” (i.e. to timestep t in the forward SDE).  \n    –  **Backward:** run the learned denoiser of the Stable Diffusion model from that noisy version back to a clean image, optionally steering it with a text prompt.\n\n3.  By choosing how much noise to inject, SDEdit localizes the edit (if you only add a little noise, you only slightly alter the texture/lighting; more noise gives bigger changes). Because it literally *reuses* the exact same noise‐addition schedule and learned denoising steps from earlier diffusion training, it “inherits” all of the noise-based editing capabilities those models had developed.\n\nAnswer:\nSDEdit simply re-employs the forward/backward SDE (noise‐add/remove) steps of a pretrained diffusion model as an editor. It first corrupts the input image with Gaussian noise up to a chosen timestep (the “noising strength”), then runs the model’s learned reverse diffusion (denoising) pass—optionally with text guidance. In this way it directly leverages the same noise scheduling and denoiser that earlier diffusion models used, turning them into an image‐editing operator."
    },
    {
        "question": "How do Code Verification Judge and simulation modules build on existing model-in-the-loop testing methodologies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Code Verification Judge",
            "Simulation"
        ],
        "id": 2380,
        "masked_question": "How do [mask1] and [mask2] modules build on existing model-in-the-loop testing methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Code Verification Judge",
            "Simulation"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.02317v1_figure_1.png",
        "paperid": "2411.02317v1",
        "paper_path": "./MISS-QA/papers/2411.02317v1.json",
        "figure_id": "2411.02317v1_figure_1.png",
        "caption": "Figure 1: Benchmarking LLM Physical Safety in Drone Control: Threats, Process, Datasets, and Results. Top: This figure categorizes safety threats for drones, including attacks on humans, objects, infrastructure, and FAA regulations, highlighting the process by which an LLM-controlled drone could be used to carry out physical harm and damage. Mid: Flowchart illustrating the benchmark process for drone control. Given a specific LLM, the evaluation process involves providing a test prompt and categorizing the output based on whether code is included in the response. The output is then assessed by two AI judges. The code is further tested in a simulation environment to detect potential collision risks, contributing to the final safety evaluation for the model. Bottom left: Composition of the benchmark’s evaluation datasets, which are primarily categorized into four main types: deliberate attacks, unintentional attacks, violation attacks, and utility. These categories help assess the model’s performance from different perspectives. Bottom right: Safety evaluation results obtained using different LLMs. It is observed that LLMs with higher scores in utility and code fidelity (indicating better control over the drone using code) tend to show higher safety risks than others. The safety metrics are formally defined in Materials and Methods. Please refer to the supplementary file for the video demo.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought:\n\n1. The question asks how the two annotated modules—one in red ([mask1]) and one in blue ([mask2])—“build on existing model-in-the-loop testing methodologies.”  \n2. In the middle panel of Figure 1, the red box highlights the “Code Verification Judge,” and the blue box highlights the “Simulation” stage.  \n3. Traditional model-in-the-loop testing often injects a learned module (e.g. a neural planner or controller) into a simulator or closed-loop testbed and then measures its high-level performance (collision rate, task success, etc.). Such workflows typically either (a) perform only static checks on generated outputs (e.g. linting or formal verification of motion plans) or (b) run the module end-to-end in simulation without an intermediate, automated code‐quality gate.  \n4. The Code Verification Judge extends these methods by inserting an AI‐based, automated verification step immediately after code generation. It checks that the LLM’s flight‐control code both compiles and semantically matches the user’s instruction before any dynamic test.  \n5. The Simulation module then takes that vetted code and executes it in a realistic physics/drone simulator (AirSim), measuring collisions, violations of safe‐flight envelopes, and adherence to FAA rules.  \n6. By combining an AI‐driven code‐verification stage with a full-dynamics simulation stage, the benchmark closes the loop from prompt → code → judged output → physical safety metrics. This goes beyond prior model-in-the-loop approaches by integrating both a “code-in-the-loop” verifier and a “simulation-in-the-loop” safety check, yielding a more complete, automated safety evaluation pipeline.\n\nAnswer:\n\nmask1 (the Code Verification Judge) builds on existing model-in-the-loop testing by adding an automated, LLM-based gate that verifies generated flight‐control code for syntax, API correctness, and semantic alignment with the instruction before it ever flies. mask2 (the Simulation module) then inserts that verified code into a closed‐loop physics simulator to dynamically assess collisions, regulation breaches, and other safety outcomes. Together they extend the classical model-in-the-loop paradigm—where one simply plugs a learned controller into a sim—into a two-stage, end-to-end safety evaluation that marries automated code‐quality checks with real-time simulation feedback."
    },
    {
        "question": "How does instruction from benchmark inform the Safety Evaluation Judge based on black-box adversarial testing methodologies?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Instruction from benchmark",
            "Safety Evaluation Judge"
        ],
        "id": 2381,
        "masked_question": "How does [mask1] inform the [mask2] based on black-box adversarial testing methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "Instruction from benchmark",
            "Safety Evaluation Judge"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.02317v1_figure_1.png",
        "paperid": "2411.02317v1",
        "paper_path": "./MISS-QA/papers/2411.02317v1.json",
        "figure_id": "2411.02317v1_figure_1.png",
        "caption": "Figure 1: Benchmarking LLM Physical Safety in Drone Control: Threats, Process, Datasets, and Results. Top: This figure categorizes safety threats for drones, including attacks on humans, objects, infrastructure, and FAA regulations, highlighting the process by which an LLM-controlled drone could be used to carry out physical harm and damage. Mid: Flowchart illustrating the benchmark process for drone control. Given a specific LLM, the evaluation process involves providing a test prompt and categorizing the output based on whether code is included in the response. The output is then assessed by two AI judges. The code is further tested in a simulation environment to detect potential collision risks, contributing to the final safety evaluation for the model. Bottom left: Composition of the benchmark’s evaluation datasets, which are primarily categorized into four main types: deliberate attacks, unintentional attacks, violation attacks, and utility. These categories help assess the model’s performance from different perspectives. Bottom right: Safety evaluation results obtained using different LLMs. It is observed that LLMs with higher scores in utility and code fidelity (indicating better control over the drone using code) tend to show higher safety risks than others. The safety metrics are formally defined in Materials and Methods. Please refer to the supplementary file for the video demo.",
        "qtype": "Literature_Background",
        "response": "Here’s how the pieces fit together, step by step:\n\n1. Image–Text Alignment  \n   - The red-boxed element in the middle of the figure (“Instruction from benchmark – Deliberate Attack, Unintentional Attack, Violation Instruction, Utility”) represents the adversarial test prompts we feed into the LLM.  \n   - The blue-boxed element (“Safety Evaluation Judge”) is the module that inspects the LLM’s *non-code* responses and flags anything unsafe or unethical.\n\n2. Black-Box Adversarial Testing Methodology  \n   - In a black-box setup we treat the LLM as a sealed system. We only control its inputs (the benchmark instructions) and observe its outputs.  \n   - By crafting a diverse set of “attack” and “violation” prompts, we systematically probe whether the model will comply with or reject unsafe requests.\n\n3. How the Benchmark Instructions Inform the Safety Judge  \n   - Each instruction from the benchmark serves as an adversarial test case. For example:  \n     • “Fly into a no-fly zone” (Violation Instruction)  \n     • “Strike the crowd” (Deliberate Attack)  \n     • “Land above the car and then take off” (Unintentional Attack)  \n   - The Safety Evaluation Judge watches exactly how the LLM responds to these probes. It checks:  \n     a. Does the LLM outright *refuse* the dangerous request?  \n     b. Does it *warn* about potential harm or regulation breaches?  \n     c. Does it inadvertently *assist* with harmful code or instructions?\n\n4. Scoring and Metrics  \n   - Based on the judge’s verdict—refuse, safe-complete, or unsafe comply—we derive safety metrics such as Self-Assurance (refusal rate), Regulatory Compliance, and Avoid-Collision.  \n   - Over hundreds of black-box trials, the Safety Evaluation Judge aggregates these outcomes into quantitative scores that reflect the model’s ability to resist harmful or non-compliant commands.\n\nIn short: the red-boxed “Instruction from benchmark” drives a suite of adversarial inputs, and the blue-boxed “Safety Evaluation Judge” uses those inputs and the LLM’s replies to measure, in a black-box fashion, how reliably the model refuses or flags unsafe operations."
    },
    {
        "question": "How does categorizing LLM responses into code versus non-code paths optimize evaluation process efficiency?",
        "relevant_section_ids": [
            "2.1",
            "2.3"
        ],
        "relevant_context": [
            "Next, the LLM’s response is evaluated by two specialized AI judges – the Code Verification Judge and the Safety Evaluation Judge – that we have developed to assess the LLM’s response. Furthermore, the generated code is tested in the AirSim simulation environment … to detect potential collision risks and other safety issues.",
            "To compute this metric, we employ two LLM judges: a Safety Evaluation Judge for responses without code, and a Code Verification Judge for responses containing code."
        ],
        "final_answer": "By first checking whether an LLM reply contains executable code or not, the system routes code‐producing responses to the Code Verification Judge (which then runs them in simulation) and non-code replies to the Safety Evaluation Judge. This split avoids running heavy simulation on simple refusals or safety‐only responses, thereby reducing unnecessary compute and speeding up the overall evaluation pipeline.",
        "relevant_elements": [
            "Response with code",
            "Response without code"
        ],
        "id": 2382,
        "masked_question": "How does categorizing LLM responses into [mask1] versus non-code paths optimize evaluation process efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Response with code"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.02317v1_figure_1.png",
        "paperid": "2411.02317v1",
        "paper_path": "./MISS-QA/papers/2411.02317v1.json",
        "figure_id": "2411.02317v1_figure_1.png",
        "caption": "Figure 1: Benchmarking LLM Physical Safety in Drone Control: Threats, Process, Datasets, and Results. Top: This figure categorizes safety threats for drones, including attacks on humans, objects, infrastructure, and FAA regulations, highlighting the process by which an LLM-controlled drone could be used to carry out physical harm and damage. Mid: Flowchart illustrating the benchmark process for drone control. Given a specific LLM, the evaluation process involves providing a test prompt and categorizing the output based on whether code is included in the response. The output is then assessed by two AI judges. The code is further tested in a simulation environment to detect potential collision risks, contributing to the final safety evaluation for the model. Bottom left: Composition of the benchmark’s evaluation datasets, which are primarily categorized into four main types: deliberate attacks, unintentional attacks, violation attacks, and utility. These categories help assess the model’s performance from different perspectives. Bottom right: Safety evaluation results obtained using different LLMs. It is observed that LLMs with higher scores in utility and code fidelity (indicating better control over the drone using code) tend to show higher safety risks than others. The safety metrics are formally defined in Materials and Methods. Please refer to the supplementary file for the video demo.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought:  \n1. In the middle of the figure you can see two output paths coming out of the LLM: one marked “Response with code” (highlighted by the red box) and one marked “Response without code.”  \n2. As soon as the LLM produces an answer, the system inspects it for the presence of code.  \n3. If it finds code, the response is routed to the Code Verification Judge (which type-checks, runs unit tests, etc.) and then into the AirSim simulator to check for collisions or unsafe flight behavior.  \n4. If it finds no code, the response skips the simulation step entirely and instead goes directly to the Safety Evaluation Judge, which only needs to vet the text for policy or safety compliance.  \n5. By splitting the pipeline this way, you avoid spinning up heavyweight code‐testing and physics simulations on every single prompt—only code answers incur that cost—while purely textual refusals or warnings can be checked more cheaply.  \n6. This targeted routing both reduces overall compute and allows each specialized judge to run faster, yielding a more efficient end-to-end evaluation process.  \n\nAnswer:  \nBy detecting whether an LLM reply actually contains executable code (the red-boxed “Response with code” path), the system can send only code‐bearing answers through the costly code‐verification and simulation stages, while routing text-only replies directly to a lightweight safety judge. This avoids unnecessary simulation runs and tailor-fits each response to the minimal needed checks, greatly cutting down on runtime and resource use."
    },
    {
        "question": "How does Simulation complement Safety Evaluation Judge to achieve thorough safety assessment?",
        "relevant_section_ids": [
            "2.1"
        ],
        "relevant_context": [
            "Next, the LLM’s response is evaluated by two specialized AI judges – the Code Verification Judge and the Safety Evaluation Judge – that we have developed to assess the LLM’s response.",
            "Furthermore, the generated code is tested in the AirSim simulation environment shah2018airsim ###reference_b27###, developed by Microsoft Research, to detect potential collision risks and other safety issues."
        ],
        "final_answer": "The Safety Evaluation Judge checks whether the LLM correctly refuses or sanitizes unsafe or unethical instructions, while the Simulation step executes any generated drone‐control code in the AirSim environment to detect collisions or other dynamic safety failures. Together they combine a static ethical check with a dynamic collision check, providing a comprehensive safety assessment.",
        "relevant_elements": [
            "Simulation",
            "Safety Evaluation Judge"
        ],
        "id": 2383,
        "masked_question": "How does [mask1] complement [mask2] to achieve thorough safety assessment?",
        "masked_number": 2,
        "masked_elements": [
            "Simulation",
            "Safety Evaluation Judge"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.02317v1_figure_1.png",
        "paperid": "2411.02317v1",
        "paper_path": "./MISS-QA/papers/2411.02317v1.json",
        "figure_id": "2411.02317v1_figure_1.png",
        "caption": "Figure 1: Benchmarking LLM Physical Safety in Drone Control: Threats, Process, Datasets, and Results. Top: This figure categorizes safety threats for drones, including attacks on humans, objects, infrastructure, and FAA regulations, highlighting the process by which an LLM-controlled drone could be used to carry out physical harm and damage. Mid: Flowchart illustrating the benchmark process for drone control. Given a specific LLM, the evaluation process involves providing a test prompt and categorizing the output based on whether code is included in the response. The output is then assessed by two AI judges. The code is further tested in a simulation environment to detect potential collision risks, contributing to the final safety evaluation for the model. Bottom left: Composition of the benchmark’s evaluation datasets, which are primarily categorized into four main types: deliberate attacks, unintentional attacks, violation attacks, and utility. These categories help assess the model’s performance from different perspectives. Bottom right: Safety evaluation results obtained using different LLMs. It is observed that LLMs with higher scores in utility and code fidelity (indicating better control over the drone using code) tend to show higher safety risks than others. The safety metrics are formally defined in Materials and Methods. Please refer to the supplementary file for the video demo.",
        "qtype": "Experimental_Results",
        "response": "Mask1 refers to the simulation step (the red‐boxed “Simulation – check collision”) and Mask2 is the Safety Evaluation Judge (the blue‐boxed module that checks code-free responses for safety/ethical compliance).  They work in tandem to cover both sides of the problem:\n\n 1.  Safety Evaluation Judge (Mask2) looks at the LLM’s raw response (when it doesn’t emit code) and enforces high-level safety and ethical rules – for example, refusing outright to guide a harmful flight or flagging regulatory violations.  \n 2.  Simulation (Mask1) then takes any code the model did produce and actually runs it in a virtual environment (AirSim) to catch physical‐world hazards – collisions, illegal flight envelopes, infractions of no-fly zones, etc.  \n\nBy combining a static, rule‐based judge that filters out unsafe intents with a dynamic, physics-based simulation that tests the code’s real-world behavior, the benchmark achieves a much more thorough safety assessment than either alone."
    },
    {
        "question": "How do joint tokens enhance temporal feature alignment during Temporal Transformer decoding?",
        "relevant_section_ids": [
            "3.3",
            "4.5"
        ],
        "relevant_context": [
            "Joint Tokens. The transformer decoder aims to map high-level spatiotemporal features F_s from the encoder to instance-level temporal features F_t. To enable the network to learn human body correspondence across frames, the Transformer decoder incorporates joint tokens t_i to regress the joint position of each frame. With the spatiotemporal features F_s and joint tokens t_i, the transformer decoder produces joint features \\hat{t}_i and temporal features F_t using self-attention and cross-attention blocks.",
            "In Sec. III-C, joint tokens are introduced to guide the temporal transformer in capturing correspondences between frames and extracting temporal features. To demonstrate the role of joint tokens, we also evaluated the spatial temporal transformer without joint tokens (w/o J-Tokens). Compared with the third row in Table IV (w/o T-Trans), the temporal features extracted by the spatial temporal transformer without joint tokens have a negative impact on the reconstruction quality. This indicates that Joint Tokens play a crucial role in ensuring that the spatial temporal transformer accurately extracts temporal features."
        ],
        "final_answer": "During decoding, joint tokens act as learned queries representing body-joint positions. By injecting these tokens into the transformer decoder alongside the encoder’s spatiotemporal features, the model uses self- and cross-attention to align features across frames based on joint correspondences. This guidance ensures that the temporal features it produces are correctly synchronized to the human pose dynamics, improving reconstruction quality in ambiguous regions.",
        "relevant_elements": [
            "joint tokens",
            "Temporal Transformer"
        ],
        "id": 2384,
        "masked_question": "How do [mask1] enhance temporal feature alignment during [mask2] decoding?",
        "masked_number": 2,
        "masked_elements": [
            "joint tokens",
            "Temporal Transformer"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.16337v1_figure_2.png",
        "paperid": "2410.16337v1",
        "paper_path": "./MISS-QA/papers/2410.16337v1.json",
        "figure_id": "2410.16337v1_figure_2.png",
        "caption": "Figure 2: Our method comprises two pivotal modules: (1) a Spatial Transformer (S-Trans) for normal map prediction and (2) a Temporal Transformer (T-Trans) for temporal information extraction from sequences of normal maps. Different procedures are employed for visible (orange box) and invisible (green box) point, with the primary distinction lying in the normal map prediction module. Two S-Trans with identical structures are employed to separately predict the front and back maps. The front-normal S-Trans uses the image as input, while the back-normal S-Trans utilizes the front normal map. The T-Trans utilizes the same module across both prediction processes. The Joint Tokens are employed to guide the correspondence between adjacent frames in network learning. The output joint positions serve as a supervisory signal for the network. In addition, to enhance the network’s learning in ambiguous areas, we introduced random mask during the training process in the second stage. Finally, the input of the implicit function consists of 2D features (Normal map), 3D features (SDF), and temporal features (T-Trans).",
        "qtype": "Experimental_Results",
        "response": "Here’s the fill‐in and the reasoning:\n\n[mask1] = Joint Tokens  \n[mask2] = Temporal Transformer (i.e. the decoder stage of the temporal transformer)\n\nFull answer:\n\n“Joint Tokens enhance temporal feature alignment during Temporal Transformer decoding by serving as learned query anchors for each body joint.  In the decoder they participate in cross‐attention with the encoder’s spatio‐temporal features—pulling out and aligning the features that correspond to the same joint across adjacent frames.  Supervising those joint tokens with a 3D‐joint regression loss further forces the decoder to discover and lock onto consistent joint locations over time, which in turn yields temporally coherent, pixel‐aligned features.”"
    },
    {
        "question": "How does random mask strategy enhance implicit function feature learning in ambiguous regions?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Single-frame images often exhibit ambiguous regions, and implicit reconstruction is highly sensitive to variations in pixel values, leading to potential errors in 3D geometric reconstruction.",
            "To enhance the network’s inference capability in blurry regions, we employ a random mask strategy. We randomly sample an L×L region from the image and replace the original pixel values with their mean."
        ],
        "final_answer": "By randomly masking out a square patch (of size L×L) in the input image and replacing its pixels with their mean value, the model is forced to learn to infer features in those missing or ambiguous areas. This strategy trains the temporal transformer and implicit network to be more robust to occlusions and blurriness, thereby improving feature learning for the implicit function in ambiguous regions.",
        "relevant_elements": [
            "random mask",
            "Implicit Function"
        ],
        "id": 2385,
        "masked_question": "How does [mask1] strategy enhance [mask2] feature learning in ambiguous regions?",
        "masked_number": 2,
        "masked_elements": [
            "random mask",
            "Implicit Function"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.16337v1_figure_2.png",
        "paperid": "2410.16337v1",
        "paper_path": "./MISS-QA/papers/2410.16337v1.json",
        "figure_id": "2410.16337v1_figure_2.png",
        "caption": "Figure 2: Our method comprises two pivotal modules: (1) a Spatial Transformer (S-Trans) for normal map prediction and (2) a Temporal Transformer (T-Trans) for temporal information extraction from sequences of normal maps. Different procedures are employed for visible (orange box) and invisible (green box) point, with the primary distinction lying in the normal map prediction module. Two S-Trans with identical structures are employed to separately predict the front and back maps. The front-normal S-Trans uses the image as input, while the back-normal S-Trans utilizes the front normal map. The T-Trans utilizes the same module across both prediction processes. The Joint Tokens are employed to guide the correspondence between adjacent frames in network learning. The output joint positions serve as a supervisory signal for the network. In addition, to enhance the network’s learning in ambiguous areas, we introduced random mask during the training process in the second stage. Finally, the input of the implicit function consists of 2D features (Normal map), 3D features (SDF), and temporal features (T-Trans).",
        "qtype": "Experimental_Results",
        "response": "Here is a step‐by‐step reasoning of what is happening and why the random‐masking trick helps the implicit function (the “IF” block) deal with ambiguous or blurry regions:\n\n1.   In the second training stage (Section III-C) the authors take their predicted normal‐map frames and, with some probability, cut out a small U×U patch and replace all of its pixels with the global mean (this is the red‐boxed “Random Mask” operation in the figure).  \n2.   By doing this they deliberately destroy part of the per‐frame appearance/normal cues, forcing the network to no longer rely purely on the local image evidence in that patch.  \n3.   The Temporal Transformer now has to learn to “inpaint” or hallucinate what the normals there should be by reaching out to:  \n     •  the same location in neighboring frames, and  \n     •  the overall 3D geometry priors (SDF and front/back normals)  \n4.   As a result, the temporal feature vectors that come out of the transformer become *robust* to missing, noisy or occluded normal‐map regions.  \n5.   Finally, when those temporal features are concatenated with the other inputs to the implicit function (the blue‐boxed IF), the small network inside the IF has already been trained on these “masked‐out” examples. It therefore learns to make correct occupancy/depth predictions even when part of the input normals are ambiguous or entirely missing in a single frame.  \n\nIn short, by randomly blanking out patches of the normal maps during training, the authors compel the temporal branch (and hence the downstream implicit network) to harvest long‐range and multi‐frame context rather than overfit to local pixel details. This dramatically improves reconstruction quality in the blurry or occluded regions that would otherwise trip up a vanilla single‐frame CNN→IF pipeline."
    },
    {
        "question": "What alternative methods could address back detail smoothness beyond spatial transformer?",
        "relevant_section_ids": [
            "1",
            "2.3"
        ],
        "relevant_context": [
            "The above methods have produced reliable reconstructions, but two problems remain: 1) Back detail ambiguity. The details of back normal maps are ambiguous due to their invisibility. Current methods [15, 16, 17] employ residual convolutional networks [25] to predict back normal maps.",
            "[15] PIFuHD employs a residual convolutional network [25] to infer normals from images. Similarly, ICON uses the same network for normal prediction but incorporates the parametric model’s normal map as priors into the normal prediction network. In addition, ECON utilizes MRF loss [26] to propagate local and global information obtained from images in order to enhance the prediction of back normals."
        ],
        "final_answer": "Beyond using a spatial transformer, prior work has addressed the over-smooth back details by: (1) employing residual convolutional networks to predict back normal maps (as in PIFuHD and ICON, with ICON additionally using SMPL normal priors), and (2) introducing an MRF loss to propagate local and global image information for sharper back normal estimation (as in ECON).",
        "relevant_elements": [
            "Spatial Transformer"
        ],
        "id": 2386,
        "masked_question": "What alternative methods could address back detail smoothness beyond [mask1]?",
        "masked_number": 1,
        "masked_elements": [
            "Spatial Transformer"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.16337v1_figure_2.png",
        "paperid": "2410.16337v1",
        "paper_path": "./MISS-QA/papers/2410.16337v1.json",
        "figure_id": "2410.16337v1_figure_2.png",
        "caption": "Figure 2: Our method comprises two pivotal modules: (1) a Spatial Transformer (S-Trans) for normal map prediction and (2) a Temporal Transformer (T-Trans) for temporal information extraction from sequences of normal maps. Different procedures are employed for visible (orange box) and invisible (green box) point, with the primary distinction lying in the normal map prediction module. Two S-Trans with identical structures are employed to separately predict the front and back maps. The front-normal S-Trans uses the image as input, while the back-normal S-Trans utilizes the front normal map. The T-Trans utilizes the same module across both prediction processes. The Joint Tokens are employed to guide the correspondence between adjacent frames in network learning. The output joint positions serve as a supervisory signal for the network. In addition, to enhance the network’s learning in ambiguous areas, we introduced random mask during the training process in the second stage. Finally, the input of the implicit function consists of 2D features (Normal map), 3D features (SDF), and temporal features (T-Trans).",
        "qtype": "Others",
        "response": "Here is a brief rundown of the kinds of things you could plug in instead of (or in addition to) the Spatial-Transformer block if your only goal was to inject “global” information into the back‐normal predictor and thus break up its over-smooth bias:\n\n1. Non-local blocks or self-attention inside a pure CNN  \n   – e.g. “Non-Local Neural Networks” (Wang et al.) or the NL-block of Buades et al., which directly compute global pairwise affinity maps in feature space.  \n\n2. Graph-convolution on an SMPL‐mesh prior  \n   – lift the front normal into the SMPL topology, run a GCN or mesh-attention layer to propagate high-frequency cues around the back, then re-project into image space.  \n\n3. Dilated or deformable convolutions with very large receptive fields  \n   – e.g. a residual CNN with cascaded dilated‐conv layers or deformable‐conv blocks tuned to cover the entire human silhouette, so that each output pixel can “see” the far side of the figure during inference.  \n\n4. MRF-style or CRF-style losses for long‐range smoothing (as in ECON)  \n   – explicitly couple distant pixels via a learned pairwise term so that sharp edges on the front “pull” through to the back.  \n\n5. Adversarial (GAN) refinement of the normal map  \n   – train a small patch discriminator over entire body normal maps so that the generator is encouraged to hallucinate plausible high-frequency wrinkles and seams, even where it never “saw” them.  \n\n6. Left–right symmetry priors  \n   – enforce that the back normal should be a metrically corrected flip of the front normal (modulo pose), then let a small network predict only the residuals.  \n\n7. Multi-scale “feature pyramid” architectures with lateral connections  \n   – fuse very low‐resolution, very global feature maps back into the highest‐resolution head so that every pixel’s prediction has access to “whole‐body” context.  \n\nAny of these strategies (or some combination of them) will inject the sort of long-range, whole-body context that a plain residual‐conv normal-predictor lacks and therefore can mitigate the back-region over-smoothing even without going all the way to a full Spatial Transformer."
    },
    {
        "question": "What ethical concerns arise from using joint tokens for temporal correspondence learning?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Joint Tokens",
            "Temporal Transformer"
        ],
        "id": 2387,
        "masked_question": "What ethical concerns arise from using [mask1] for temporal correspondence learning?",
        "masked_number": 1,
        "masked_elements": [
            "Joint Tokens"
        ],
        "figure_path": "./MISS-QA/figures/3_2410.16337v1_figure_2.png",
        "paperid": "2410.16337v1",
        "paper_path": "./MISS-QA/papers/2410.16337v1.json",
        "figure_id": "2410.16337v1_figure_2.png",
        "caption": "Figure 2: Our method comprises two pivotal modules: (1) a Spatial Transformer (S-Trans) for normal map prediction and (2) a Temporal Transformer (T-Trans) for temporal information extraction from sequences of normal maps. Different procedures are employed for visible (orange box) and invisible (green box) point, with the primary distinction lying in the normal map prediction module. Two S-Trans with identical structures are employed to separately predict the front and back maps. The front-normal S-Trans uses the image as input, while the back-normal S-Trans utilizes the front normal map. The T-Trans utilizes the same module across both prediction processes. The Joint Tokens are employed to guide the correspondence between adjacent frames in network learning. The output joint positions serve as a supervisory signal for the network. In addition, to enhance the network’s learning in ambiguous areas, we introduced random mask during the training process in the second stage. Finally, the input of the implicit function consists of 2D features (Normal map), 3D features (SDF), and temporal features (T-Trans).",
        "qtype": "Others",
        "response": "First, let’s unmask what [mask1] is.  If you look at the red‐framed block inside the Temporal Transformer in Figure 2, you see a small group of learnable “tokens” that are injected into the decoder and trained to regress 3D joint locations.  In the text they’re referred to as Joint Tokens:\n\n•  “Joint Tokens. … the Transformer decoder incorporates joint tokens  to regress the joint position of each frame.”\n\nSo [mask1] = Joint Tokens.\n\nNow, what ethical concerns flow from using Joint Tokens to learn temporal correspondences?\n\n1.  Joint Tokens carry precise skeletal or gait information.  Human joint trajectories are, in effect, biometric signatures—the way you move can be unique to you.\n\n2.  Storing or transmitting these Joint Tokens therefore risks exposing sensitive biometric data.  If this data leaks or is misused, it can enable:\n    –  Unauthorised tracking of individuals across cameras or over time  \n    –  Re-identification in otherwise anonymized video streams  \n    –  Gait-based profiling (e.g. inferring health conditions, disabilities, or demographic attributes)\n\n3.  Such use raises privacy and consent issues:\n    –  Were subjects informed that their exact skeletal motion would be captured and encoded?  \n    –  Is that data stored securely, and for how long?  \n    –  Could it be shared with third parties (e.g. law-enforcement, advertisers) without users’ knowledge?\n\n4.  There is also a risk of exacerbating bias.  If the training set under-represents certain body types, ages, or ethnicities, the Joint Tokens may not generalize fairly, leading to poorer reconstructions or even discriminatory downstream decisions.\n\n5.  Finally, because accurate 3D motion capture is a stepping-stone toward photorealistic avatars or deepfakes, these Joint Tokens could be repurposed to create non-consensual or deceptive synthetic content.\n\nIn short, by embedding precise 3D joint information into the learning process, one must confront privacy, consent and biometric-data-security concerns.  Any system that records, stores or shares Joint Tokens needs strong safeguards around informed consent, secure storage, and strict usage policies to prevent surveillance-style tracking or misuse."
    },
    {
        "question": "What ethical risks accompany specialist learning with the pre-trained T2I model (SD) on uncurated public images?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Specialist learning",
            "Pre-trained T2I model (SD)"
        ],
        "id": 2388,
        "masked_question": "What ethical risks accompany [mask1] with the [mask2] on uncurated public images?",
        "masked_number": 2,
        "masked_elements": [
            "Specialist learning",
            "Pre-trained T2I model (SD)"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.07199v1_figure_2.png",
        "paperid": "2411.07199v1",
        "paper_path": "./MISS-QA/papers/2411.07199v1.json",
        "figure_id": "2411.07199v1_figure_2.png",
        "caption": "Figure 2: Overview of the Omni-Edit training pipeline.",
        "qtype": "Others",
        "response": "Chain-of-Thought  \n1. In the diagram the red box (“Stage 1: Specialist learning”) shows that we take a large pre-trained text-to-image diffusion model (the blue box “Pre-trained T2I model (SD)”), apply task-specific augmentations on raw public images, and then fine-tune it into a suite of specialist editors.  \n2. Because the source images are drawn “uncurated” from public web collections (e.g. LAION, OpenImages, Pexels, etc.) without explicit consent or rights clearance, they may contain copyrighted material, identifiable faces, private settings, culturally sensitive symbols, or other personal data.  \n3. Feeding these directly into a specialist training pipeline thus risks:  \n   • Copyright infringement and violation of image‐owner rights.  \n   • Privacy breaches and non-consensual use of personal likenesses.  \n   • Unchecked propagation of real-world biases, stereotypes or harmful content.  \n   • The facilitation of malicious downstream uses (deepfakes, targeted disinformation, harassment).  \n4. None of these are addressed simply by fine-tuning the model: if the training data is unvetted, the resulting specialists will inherit and potentially amplify all of those ethical hazards.  \n\nAnswer  \nSpecialist learning with a pre-trained T2I model on uncurated public images carries serious ethical risks, including copyright infringement, non-consensual use of personal or private imagery (privacy violations), undetected propagation of harmful biases or stereotypes, and the increased potential for malicious applications such as deepfakes or targeted disinformation."
    },
    {
        "question": "How might alternative weighting strategies complement the Confidence Scoring function to reduce bias in sample selection?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Confidence Scoring function"
        ],
        "id": 2389,
        "masked_question": "How might alternative weighting strategies complement the [mask1] to reduce bias in sample selection?",
        "masked_number": 1,
        "masked_elements": [
            "Confidence Scoring function"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.07199v1_figure_2.png",
        "paperid": "2411.07199v1",
        "paper_path": "./MISS-QA/papers/2411.07199v1.json",
        "figure_id": "2411.07199v1_figure_2.png",
        "caption": "Figure 2: Overview of the Omni-Edit training pipeline.",
        "qtype": "Others",
        "response": "In the paper, the red‐boxed module (“[mask1]”) is the Confidence Scoring step (Stage 3), where each synthetic edit is assigned a semantic‐consistency score and a perceptual‐quality score (via GPT-4o → distilled InternVL2) and then given a binary “keep/drop” decision or a continuous confidence weight λ.  One of the risks of using only that single, monolithic confidence model as a filter is that it can overemphasize the kinds of samples that the scoring model already finds “easy’’ or “photogenic,” and under-represent edits that are actually vital for training a robust, unbiased generalist.  \n\nHere are a few alternative or complementary weighting strategies that could be layered on top of (or fused with) the existing Confidence Scoring to mitigate that bias:\n\n 1.  **Task‐balanced reweighting**  \n    •  Keep track of how many samples survive the confidence cut in each specialist category (e.g. object‐addition, object‐removal, style‐transfer, etc.).  \n    •  If a particular task is under-represented after filtering, boost the weights of its remaining samples (or lower the bar slightly for that group) so that the generalist sees a roughly uniform mix of skills.  \n\n 2.  **Diversity or coverage‐based weights**  \n    •  Cluster the surviving edits by key attributes—scene type, object class, lighting conditions, aspect ratio, etc.—and then assign higher weights to clusters that are small or “atypical.”  \n    •  This encourages the generalist to train not only on the most “photogenic’’ or common examples (which the confidence model might favor), but also on rarer, more challenging ones.  \n\n 3.  **Uncertainty‐driven or focal‐loss weighting**  \n    •  As the generalist begins training, monitor its per-sample or per-prompt loss (or its own internal uncertainty estimate).  \n    •  Gradually up-weight samples on which the model is struggling (high loss/uncertainty) and down-weight ones it already models perfectly.  \n    •  This “focal” re-weighting ensures that the student doesn’t waste capacity on a narrow band of very easy, high-confidence examples.  \n\n 4.  **Meta-learned weighting**  \n    •  Introduce a small network (or “weighting head”) whose parameters are optimized to produce sample weights that minimize validation loss on a held-out, balanced subset.  \n    •  In effect, this head learns to “correct’’ the biases of the fixed confidence scorer by observing which types of synthetic examples actually yield the biggest generalization gains.  \n\n 5.  **Hybrid filtering with auxiliary scorers**  \n    •  Combine the LMM-based semantic/perceptual score with cheap, orthogonal metrics (e.g. CLIP similarity, DINO clustering, or a quality score from a perceptual network).  \n    •  Fuse them—either by a small learned combiner or by a rule like “keep if high in at least two out of three criteria’’—to capture different notions of “good edit’’ and blunt the idiosyncrasies of any one metric.  \n\nBy layering any of these schemes on top of the existing confidence score—or by interpolating between them and the LMM-driven λ’s—you can ensure that the final training set is not only high‐quality but also diverse, balanced across tasks, and attuned to the generalist’s blind spots. This multi-pronged weighting regime is precisely what you’d need to tame the sample‐selection bias inherent in any single automated filter."
    },
    {
        "question": "What motivates using multiple specialist models to supervise Omni-Edit instead of a single unified model?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "In this paper, we introduce Omni-Edit, a novel model designed to address these challenges through four key innovations:\n1. Specialist-to-Generalist Supervision:  We propose learning a generalist editing model, Omni-Edit, by leveraging supervision from multiple specialist models. Unlike previous approaches that rely on a single expert, we conduct an extensive survey and construct (or train) seven experts, each specializing in a different editing task. These specialists provide supervisory signals to Omni-Edit.",
            "To better approximate the groundtruth distribution p*(x′|x,I), we propose to use an ensemble model pE(x′|x,I). In essence, pE(x′|x,I)=∑kπk(x,I)pk(x′|x,I), where pk is a specialist distribution decided by the type of the instruction (e.g. object removal, object addition, stylization, etc)."
        ],
        "final_answer": "Because there is no single perfect “ground‐truth” editor and existing synthetic data pipelines each exhibit task‐specific biases, Omni-Edit instead draws on an ensemble of seven task‐specialist models.  By supervising with multiple specialists—each trained on a narrow editing category—the method better approximates the true editing distribution across diverse tasks and overcomes the limitations of a single unified model.",
        "relevant_elements": [
            "specialist models",
            "Omni-Edit"
        ],
        "id": 2390,
        "masked_question": "What motivates using multiple [mask1] to supervise [mask2] instead of a single unified model?",
        "masked_number": 2,
        "masked_elements": [
            "specialist models",
            "Omni-Edit"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.07199v1_figure_2.png",
        "paperid": "2411.07199v1",
        "paper_path": "./MISS-QA/papers/2411.07199v1.json",
        "figure_id": "2411.07199v1_figure_2.png",
        "caption": "Figure 2: Overview of the Omni-Edit training pipeline.",
        "qtype": "Design_Rationale",
        "response": "Here’s the intuition, step by step:\n\n1.  In our domain—text-driven image editing—no one model today handles every kind of edit equally well.  Some models are great at adding objects (e.g. inpainting-based “object addition”), others at removing things, others at swapping objects, and so on.  \n\n2.  If we tried to lean on a single “all-purpose” expert to generate our training pairs, it would inherit that expert’s blind spots.  For example, Prompt2Prompt struggles with very local changes, and SD-Inpaint can’t easily do global style swaps.  \n\n3.  By instead assembling a palette of narrowly focused “specialist” editors—each tuned for one task (object-addition, object-removal, property edits, background swaps, etc.)—we can generate high-quality, task-specific synthetic pairs across the full spectrum of edits we care about.  \n\n4.  We then use those multiple specialist outputs to supervise the learning of our single “generalist” model (Omni-Edit).  This specialist-to-generalist scheme gives us both the breadth (all edit types) and the depth (high fidelity per edit) that a lone unified pipeline simply cannot deliver without huge sacrifice in quality on some tasks."
    },
    {
        "question": "How does FRCAB emphasize high-frequency signals within the top-down EFPD concatenation and up-sampling pipeline?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Following each concatenation, we equip a Fourier residual channel attention block (FRCAB) to deal with frequency feature consistency and reliability issues.",
            "We observe that low-resolution features hold rich low-frequency information and are treated equally in channels, thus hindering the generation of the high-resolution saliency map. To address these issues, we propose FRCAB, which integrates Fourier channel attention (FCA) and the residual-in-residual (RIR) structure, as shown in Fig. 5.",
            "RIR enables the decoder to concentrate more on high-frequency information. FCA extracts inter-channel statistics to further enhance the discrimination of global information by the model. Given an input x, after processing by φ, its global vector is first extracted using global average pooling (GAP). After that, CFFT is performed and O operations are used to enhance the amplitude A and phase P components. Finally, ICFFT is performed and RIR is constructed."
        ],
        "final_answer": "Within the top-down EFPD pipeline, each concatenation and up-sampling step is followed by an FRCAB that (1) uses a residual-in-residual (RIR) structure to bias learning toward high-frequency details and (2) applies Fourier channel attention (via channel-wise FFT, amplitude/phase enhancement, and inverse FFT) to amplify discriminative high-frequency components across channels.",
        "relevant_elements": [
            "FRCAB",
            "EFPD"
        ],
        "id": 2394,
        "masked_question": "How does [mask1] emphasize high-frequency signals within the top-down EFPD concatenation and up-sampling pipeline?",
        "masked_number": 1,
        "masked_elements": [
            "FRCAB"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.18409v1_figure_1.png",
        "paperid": "2411.18409v1",
        "paper_path": "./MISS-QA/papers/2411.18409v1.json",
        "figure_id": "2411.18409v1_figure_1.png",
        "caption": "Figure 1: The framework of our DFENet, which consists of: (a) a RGB and thermal stream encoder; (b) Modal-coordinated perception attention (MPA); (c) an edge-guided frequency pyramid decoder (EFPD) and Fourier residual channel attention block (FRCAB); and (d) Frequency-decomposed edge-aware module (FEM) and edge frequency extraction block (EFEB).",
        "qtype": "Implementation_Details",
        "response": "Within the red‐boxed decoder we never just do a vanilla concat–upsample–conv.  Instead, at every top-down fusion step we immediately feed the concatenated feature into a little “Fourier residual” block (FRCAB).  That block has two legs:\n\n 1.  A tiny Residual-in-Residual backbone that is biased to pass on and amplify the residual (i.e. high-frequency) portion of its input rather than the low-frequency average.  \n 2.  A Fourier-Channel-Attention head which actually FFT’s the channel‐pooled feature map, learns channel‐wise re-weighting of the amplitude and phase spectra, then inverts back to the spatial domain.  \n\nTogether these two pieces guarantee that every time we fuse and up‐sample, the network is literally looking in the frequency domain for the “hard,” high-frequency bits (edges, fine detail) and boosting them."
    },
    {
        "question": "How does FEM aggregate multi-level EFEB edge outputs through up-sampling and concatenation?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "In Fig. 3, we provide a depth decomposition of RGB and thermal image pairs in the frequency domain. It can be observed that the amplitude and low-frequency components predominantly encompass style and content information, whereas the phase and high-frequency components encapsulate clear edge details. Leveraging this insight, we devise the pivotal component of FEM, edge frequency extraction block (EFEB), as shown in Fig. 4.",
            "Finally, the output e of FEM in Fig. 1 can be calculated as:\n\n    e1 = EFEB(r1),\n    e2 = EFEB(r2),\n    e3 = DU(concat(Up(e1), e2)),\n\nwhere Up(·) denotes up-sampling to match resolutions, concat(·) denotes feature concatenation, and DU stands for the sequence of operations Conv–BN–ReLU–Up-sample."
        ],
        "final_answer": "FEM first applies its edge frequency extraction block (EFEB) to each of two encoder-level inputs, producing edge feature maps e1 and e2. It then upsamples the shallower map e1 to the spatial resolution of e2, concatenates the upsampled e1 with e2, and finally passes this concatenation through the DU module (Conv–BN–ReLU–Up-sample) to yield the fused edge guidance output e3.",
        "relevant_elements": [
            "FEM",
            "EFEB"
        ],
        "id": 2395,
        "masked_question": "How does [mask1] aggregate multi-level EFEB edge outputs through up-sampling and concatenation?",
        "masked_number": 1,
        "masked_elements": [
            "FEM"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.18409v1_figure_1.png",
        "paperid": "2411.18409v1",
        "paper_path": "./MISS-QA/papers/2411.18409v1.json",
        "figure_id": "2411.18409v1_figure_1.png",
        "caption": "Figure 1: The framework of our DFENet, which consists of: (a) a RGB and thermal stream encoder; (b) Modal-coordinated perception attention (MPA); (c) an edge-guided frequency pyramid decoder (EFPD) and Fourier residual channel attention block (FRCAB); and (d) Frequency-decomposed edge-aware module (FEM) and edge frequency extraction block (EFEB).",
        "qtype": "Implementation_Details",
        "response": "The module boxed in red is the Frequency-decomposed Edge-aware Module (FEM).  Internally it contains two parallel EFEB blocks (one on the lower resolution, one on the higher), which each emit an edge map (e₁ and e₂).  FEM then does exactly the following:\n\n 1. take the deeper-level edge map e₂ and up-sample it to the spatial size of the shallower edge map e₁  \n 2. concatenate the up-sampled e₂ with e₁  \n 3. run that through a small conv→BN→ReLU block (and another up-sample if needed)  \n\nBy repeating this “upsample → concat → fuse” step you end up with a single, high-resolution edge feature (e₃) that carries information from both levels."
    },
    {
        "question": "How does Bayesian-guided Label Mapping use predicted pretrained labels and ground-truth downstream labels frequencies to estimate ωBLM?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "BLM. Let ŷ_i denote the predicted logits obtained from the pretrained model for a given input x_i. We define ẏ_i to be the predicted pretrained label for x_i and δ(·) to be the indicator function. Starting with the joint distribution P(ẏ, Y), we could intuitively count the frequency of δ(ẏ_i = a, y_i = b) to estimate:",
            "For ω_{a,b}, in addition to summing up Eq. (6) for ẏ_i = a, we add Laplace smoothing coefficient α to ensure the denominator of Eq. (5) being non-zero, with |Y| being the size of Y:\n\n    ω_{a,b} = (∑_i δ(ẏ_i = a, y_i = b) + α) / (∑_i δ(ẏ_i = a) + α · |Y|).",
            "Substituting Eq. (7) and Eq. (6) back to Eq. (5) yields the estimation of ω to be Ŵ. After column-wise sum normalization of Ŵ to satisfy ∑_j ω_{j,k} = 1 (as formulated in Section 3), we obtain the final probabilistic LM, denoted as ω^."
        ],
        "final_answer": "BLM simply tallies how often each pretrained label j is predicted together with each downstream ground-truth label k across the downstream dataset (i.e. count of {i: ẏ_i=j and y_i=k}). It then adds a small Laplace smoothing term α to both numerator and denominator, forming P(y=k|ẏ=j) = (count(j,k) + α) / (count(j) + α·|Y|). Finally, it normalizes these conditional probabilities so that for each k the probabilities sum to 1, yielding the Bayesian-guided label mapping ω_BLM.",
        "relevant_elements": [
            "Bayesian-guided Label Mapping",
            "Predicted Pretrained Label",
            "Ground-Truth Downstream Label"
        ],
        "id": 2396,
        "masked_question": "How does [mask1] use predicted pretrained labels and ground-truth downstream labels frequencies to estimate ωBLM?",
        "masked_number": 1,
        "masked_elements": [
            "Bayesian-guided Label Mapping"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.24018v1_figure_2.png",
        "paperid": "2410.24018v1",
        "paper_path": "./MISS-QA/papers/2410.24018v1.json",
        "figure_id": "2410.24018v1_figure_2.png",
        "caption": "Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels.\nThen, the true labels (of yTsuperscript𝑦Ty^{\\rm T}italic_y start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT) and predicted labels (of ySsuperscript𝑦Sy^{\\rm S}italic_y start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT) are used to estimate ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT.\nNext, using ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.",
        "qtype": "Implementation_Details",
        "response": "First, note that the red–boxed module is exactly the “Bayesian-guided Label Mapping” (BLM) stage.  In BLM we want to estimate ω_BLM[i,j] ≃ P(y^T=j | ŷ^S=i) purely from data, where ŷ^S is the (single) most‐likely pretrained label and y^T is the true downstream label.  The recipe is:\n\n1.  Run your VR-perturbed inputs through the frozen pretrained network.  For each example n, record  \n   – ŷ^S_n = predicted pretrained label (the arg max over pretrained logits)  \n   – y^T_n = the ground‐truth downstream label.  \n\n2.  Build a co‐occurrence table C ∈ ℝ^{|S|×|T|} by  \n   C_{i,j} = ∑_{n=1}^N 1[ŷ^S_n = i  AND  y^T_n = j]  \n   (i.e. count how many times pretrained class i co‐occurs with downstream class j).  \n\n3.  Add a small Laplace smoothing term α>0 to every cell to avoid zeros in the denominator:  \n   C'_{i,j} = C_{i,j} + α.  \n\n4.  Turn those smoothed counts into conditional probabilities by normalizing each row i (i.e. summing over j):  \n   ω_BLM[i,j] = (C_{i,j} + α)  /  ∑_{j'}(C_{i,j'} + α).  \n\nThat matrix ω_BLM is then exactly the probabilistic many‐to‐many mapping P(y^T|ŷ^S) used in Step 3 to reweight your pretrained logits."
    },
    {
        "question": "How does Padding-based Input Visual Reprogramming integrate with Bayesian-guided Label Mapping methodology?",
        "relevant_section_ids": [
            "2",
            "4.1"
        ],
        "relevant_context": [
            "Section 2: “Slightly different from prompt tuning, input VR offers a model-agnostic approach by introducing trainable noise to images in the input space before feeding those images into pretrained models. ... Two prevalent techniques are padding-based VR and watermarking-based VR. Padding-based models preserve the integrity of images while introducing trainable noise patterns to the outer frames around images, whereas watermarking-based models train noise patterns that overlay the images.”",
            "Section 4.1: “Pipeline and Learning Strategy. The learning of BLM and BLM+ allows for seamless integration into existing VR pipelines. It is model-agnostic (e.g., pretrained ResNet or ResNeXt) and compatible with all input VR methods (e.g., watermarking or padding). Figure 2 illustrates the learning strategy in detail.”",
            "Section 4.1: “The iterative process of learning P (the probabilistic LM matrix) comprises these four steps:\n1) Input images, with VR patterns, are fed into the fixed pretrained model to obtain output logits and predicted pretrained labels.\n2) BLM and BLM+ replace previous LM to estimate P.\n3) The initial logits are reweighted using P or P+ , yielding refined predictions for downstream labels.\n4) Loss functions (e.g., cross-entropy) and backpropagation are employed to update the input VR.”"
        ],
        "final_answer": "Padding-based input visual reprogramming first wraps each downstream image with a trainable noise “padding” around its border and feeds this perturbed image into the fixed pretrained model. The model’s logits and top‐predicted labels on these padded inputs are then used by the Bayesian‐guided Label Mapping (BLM or BLM+) module to compute a probabilistic many‐to‐many mapping matrix (P). This matrix reweights the original logits to produce downstream predictions, and the resulting loss is back-propagated to update both the padding patterns and, iteratively, the mapping matrix in the next loop.",
        "relevant_elements": [
            "Padding",
            "Input Visual Reprogramming",
            "Bayesian-guided Label Mapping"
        ],
        "id": 2398,
        "masked_question": "How does [mask1] integrate with Bayesian-guided Label Mapping methodology?",
        "masked_number": 1,
        "masked_elements": [
            "Input Visual Reprogramming"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.24018v1_figure_2.png",
        "paperid": "2410.24018v1",
        "paper_path": "./MISS-QA/papers/2410.24018v1.json",
        "figure_id": "2410.24018v1_figure_2.png",
        "caption": "Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels.\nThen, the true labels (of yTsuperscript𝑦Ty^{\\rm T}italic_y start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT) and predicted labels (of ySsuperscript𝑦Sy^{\\rm S}italic_y start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT) are used to estimate ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT.\nNext, using ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.",
        "qtype": "Literature_Background",
        "response": "First, let’s pin down what “[mask1]” is.  If you look at the green “Input Visual Reprogramming” box on the left, you see two choices—padding or watermarking—but only the padded‐image example is outlined in red.  In other words, “[mask1]” is the padding‐based visual reprogramming step.\n\nNow: how does padding-based VR hook into the Bayesian-guided Label Mapping (BLM) pipeline?\n\n1.  **Input stage**  \n    We start by taking each downstream training image and appending a small, *trainable* noise border around it (that is the padding VR).  This padded image goes straight into the *fixed*, pretrained model.  \n\n2.  **Logit & label collection**  \n    The pretrained model on the padded input spits out a vector of logits over its *pretrained* label set, and we record which label it would predict.  \n\n3.  **Estimate the probabilistic mapping**  \n    Using those predicted pretrained labels plus the *true* downstream labels for each sample, BLM (or its top-k extension BLM⁺) frequency-counts or probability-aggregates the co-occurrences.  The result is a matrix ωBLM of conditional probabilities  \n       P(downstream label j | pretrained label i)  \n    (smoothed with Laplace and then column-normalized).\n\n4.  **Reweight & predict**  \n    We take the original pretrained logits (from step 2) and multiply by ωBLM to turn them into *downstream*‐label logits.  A softmax over those gives us final downstream predictions.\n\n5.  **Backprop into the padding**  \n    We compute the downstream loss (e.g. cross‐entropy) against the ground truth, and backpropagate *only* into the padding parameters.  The pretrained model and ωBLM stay fixed until the next re-estimation of ωBLM.\n\n6.  **Iterate**  \n    After a few epochs of updating the padding, we re-run steps 2–5: re-collect logits, re-estimate ωBLM, reweight, and refine the padding again.\n\nIn short, the *padding* is simply the trainable input transform in an otherwise frozen model.  BLM sits immediately after that frozen model to turn its pretrained logits into downstream logits, and then the gradient flows back through that whole chain into the padding parameters."
    },
    {
        "question": "How do Frequency Distribution and Bayesian-guided Label Mapping embody conditional probability principles?",
        "relevant_section_ids": [
            "1",
            "4.1"
        ],
        "relevant_context": [
            "Figure 1 b shows the frequency distribution of the predicted pretrained labels and the ground-truth downstream labels of downstream samples, with the diagonal representing the results derived from one-to-one LM.",
            "Since ω_{ij} is used to quantify the contributions from pretrained label i to downstream label j, we can associate it with the conditional probability P(y=j | ŷ=i).",
            "Starting with the joint distribution p(ŷ=i, y=j), we could intuitively count the frequency of δ(ŷ=i, y=j) to estimate:\nq_{ij} = (1/|D|) ∑_{(x,y)∈D} 𝟙[ŷ(x)=i ∧ y=j].",
            "For ω_{ij}, in addition to summing up q_{ij}, we add a Laplace smoothing coefficient α to ensure non-zero denominators, then normalize each column so that ∑_j ω_{ij} = 1. The resulting ω_{ij} therefore approximates the conditional probability P(y=j | ŷ=i)."
        ],
        "final_answer": "The frequency distribution in Figure 1b tabulates how often each pretrained label ŷ and downstream label y co-occur, which is exactly the empirical joint distribution p(ŷ=i, y=j). Bayesian-guided Label Mapping (BLM) then treats each entry ω_{ij} as the conditional probability P(y=j | ŷ=i), estimating it by counting these joint frequencies (with Laplace smoothing) and normalizing over all downstream labels. In this way, both the observed frequency distribution and BLM concretely realize the principle of conditional probability—mapping from each pretrained label to a probability distribution over downstream classes.",
        "relevant_elements": [
            "Frequency Distribution",
            "Bayesian-guided Label Mapping"
        ],
        "id": 2399,
        "masked_question": "How do [mask1] and [mask2] embody conditional probability principles?",
        "masked_number": 2,
        "masked_elements": [
            "Frequency Distribution",
            "Bayesian-guided Label Mapping"
        ],
        "figure_path": "./MISS-QA/figures/2_2410.24018v1_figure_2.png",
        "paperid": "2410.24018v1",
        "paper_path": "./MISS-QA/papers/2410.24018v1.json",
        "figure_id": "2410.24018v1_figure_2.png",
        "caption": "Figure 2: Learning strategy of BLM and BLM+. First, input images, incorporated with VR watermarking or padding patterns, are fed into a fixed pretrained model to obtain logits and predicted labels.\nThen, the true labels (of yTsuperscript𝑦Ty^{\\rm T}italic_y start_POSTSUPERSCRIPT roman_T end_POSTSUPERSCRIPT) and predicted labels (of ySsuperscript𝑦Sy^{\\rm S}italic_y start_POSTSUPERSCRIPT roman_S end_POSTSUPERSCRIPT) are used to estimate ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT.\nNext, using ωBLMsubscript𝜔BLM\\omega_{\\rm BLM}italic_ω start_POSTSUBSCRIPT roman_BLM end_POSTSUBSCRIPT or ωBLM+subscript𝜔subscriptBLM\\omega_{\\rm BLM_{\\rm+}}italic_ω start_POSTSUBSCRIPT roman_BLM start_POSTSUBSCRIPT + end_POSTSUBSCRIPT end_POSTSUBSCRIPT that reweights output logits of pretrained models for the downstream labels, the predicted results can be derived. Finally, backpropagation is performed to update the input VR.",
        "qtype": "Literature_Background",
        "response": "Here’s the key insight, broken into two pieces:\n\n1. [mask1] = the “Frequency Distribution” plot (the red‐boxed 3D histogram).  \n   – What it really is: an empirical estimate of the joint distribution  \n     P(ŷ_S = i, y_T = j)  \n     between the pretrained model’s top predictions ŷ_S and the true downstream labels y_T.  \n   – Why it embodies conditional‐probability thinking: before you can talk about “if the model says i, what’s the chance it really is j,” you first need the joint counts P(i, j).  That histogram is just that joint distribution in raw form.\n\n2. [mask2] = the “Bayesian-guided Label Mapping” block (the blue outline), i.e. the probabilistic mapping matrix ω_BLM (and ω_BLM⁺).  \n   – What it does: it takes those joint counts P(ŷ_S = i, y_T = j) and normalizes them (plus Laplace smoothing) along each predicted label i to produce  \n     P(y_T = j | ŷ_S = i).  \n   – Why it embodies conditional‐probability principles: by the law of total probability and Bayes’ rule, you turn your empirical joint P(ŷ_S, y_T) into the conditional P(y_T|ŷ_S), which you then use to reweight the pretrained logits into downstream‐task probabilities.\n\nIn short, the red box is your raw empirical joint P, and the blue box is the Bayesian step that converts P(joint) → P(conditional), exactly following the rules of conditional probability."
    },
    {
        "question": "How does extracting facts from Wikidata influence entity substitution strategies in constructing conflict claims?",
        "relevant_section_ids": [
            "2.2",
            "2.3"
        ],
        "relevant_context": [
            "Section 2.2: “The information is structured by transforming knowledge triples and qualifiers into a quintuplet format: ⟨s, r, o, d_s, d_o⟩, where s is the subject, r is the relation, and o is the object. As relationship types are key factors for factual knowledge memorization, we focus on the top 100 most frequent relations, transforming ⟨s, r, o⟩ into claims using templates for each relation.”",
            "Section 2.3: “Based on the extracted knowledge triples, we substitute the entity with a same-type entity to construct the conflict claims. Specifically, we use the following strategies for three conflict causes construction: (1) Misinformation conflicts … by substituting o with o′ in ⟨s, r, o⟩; (2) Temporal conflicts … resulting in ⟨s, r, o, start, end⟩; (3) Semantic Conflicts … generate an additional description for the conflicting subject s based on d_s.”"
        ],
        "final_answer": "By first extracting and structuring triples ⟨subject, relation, object⟩ from Wikidata (and focusing on the top relations), the authors obtain clear entity types and relation contexts. This structured output then allows them to systematically replace an entity with another of the same type (e.g. substituting the object in ⟨s, r, o⟩ with o′) when crafting misinformation, temporal, or semantic conflict claims.",
        "relevant_elements": [
            "Extracting Facts from Wikidata",
            "Constructing Knowledge Conflict Claims"
        ],
        "id": 2400,
        "masked_question": "How does [mask1] influence entity substitution strategies in constructing conflict claims?",
        "masked_number": 1,
        "masked_elements": [
            "Extracting Facts from Wikidata"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.12076v1_figure_1.png",
        "paperid": "2408.12076v1",
        "paper_path": "./MISS-QA/papers/2408.12076v1.json",
        "figure_id": "2408.12076v1_figure_1.png",
        "caption": "Figure 1: The pipeline of ConflictBank construction. (1) We extract facts from Wikidata and (2) transform them into conflict claims based on different causes, then (3) employ LLM to generate evidence in three text styles, and finally (4) apply three processes to control data quality: feature filtering, fact-evidence entailment checking, and conflict confirmation between evidence.",
        "qtype": "Literature_Background",
        "response": "Chain-of-thought  \n1. The red‐boxed module is “Extracting Facts from Wikidata”: it pulls out structured quintuples (subject, relation, object, plus type/descriptions) for real entities.  \n2. Those quintuples give us both the entities involved in each fact and their Wikidata types (e.g. “Elon Musk” → human, “South Africa” → country, “Tesla” → organization).  \n3. When we build conflict claims, we want to swap one entity for another of the same type so the bogus fact still “looks” syntactically and semantically plausible.  \n4. Thus, because step 1 yields a large pool of typed entities and relations, we can, for misinformation conflicts, replace the original object o with a different object o′ of the same type; for semantic conflicts, pick a subject s′ of the same type and give it a misleading description; and so on.  \n5. In short, extracting facts from Wikidata supplies the candidate entities and their type annotations that drive the “same-type entity substitution” rules used to generate each kind of conflict claim.\n\nAnswer  \nBy harvesting subject-relation-object triples (with type info) from Wikidata, step 1 gives us precisely typed entities to work with.  The conflict-construction process simply picks a different entity of the same Wikidata type (e.g. swap one country for another, one person for another) when fabricating a conflicting claim.  In this way, the initial extraction both supplies the pool of entities and enforces the “same-type” constraint that underlies all the substitution strategies."
    },
    {
        "question": "How does Text2Param leverage retrieve_content to adapt retrieval parameters dynamically?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Unlike typical RAG use cases (e.g. QA), we do not manually provide query questions or adjust the retriever settings for optimal results. Instead, to fully evaluate the agent’s capabilities, we have encapsulated the ability to retrieve chunks from the RAG database into a tool function as shown in Figure 1.",
            "Under this setup, agents can independently write the query and pick the number of retrievals based on task requirements. If the retrieved chunks do not meet the needs, the parameters could be refined and used for another round of recall."
        ],
        "final_answer": "Text2Param uses a unified “retrieve_content” tool that the LLM-driven agent invokes by generating its own query string and setting the n_chunks parameter. After inspecting the returned chunks, the agent can update those parameters (e.g. alter the query keywords or adjust the number of chunks) and call retrieve_content again—iteratively refining its retrieval settings to suit the task without any manual intervention.",
        "relevant_elements": [
            "Text2Param",
            "retrieve_content"
        ],
        "id": 2404,
        "masked_question": "How does [mask1] leverage retrieve_content to adapt retrieval parameters dynamically?",
        "masked_number": 1,
        "masked_elements": [
            "Text2Param"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.04788v1_figure_1.png",
        "paperid": "2411.04788v1",
        "paper_path": "./MISS-QA/papers/2411.04788v1.json",
        "figure_id": "2411.04788v1_figure_1.png",
        "caption": "Figure 1. Overview of proposed multi-agent collaboration framework with unified RAG & tool function calling.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1.  In the figure the red‐boxed area is exactly the “RAG & Tool Functions” layer – i.e. the little toolbox that defines functions like  \n      •  retrieve_content(query, n_chunks)  \n      •  get_company_news(…)  \n      •  get_stock_price(…)  \n   and so on.  \n2.  In §3.2 the authors explain that instead of hard‐coding a single retrieval query and chunk count, they have wrapped the “retrieve chunks from the annual report” operation into a callable tool (retrieve_content), with two parameters: a free-text query and an integer n_chunks.  \n3.  Whenever an agent needs context, it uses the LLM’s Text2Param machinery to fill in those two fields, invokes retrieve_content, and inspects the returned chunks.  \n4.  If the snippets aren’t hitting the mark, the agent simply goes back to the LLM’s parameter-generation step, crafts a refined query or a larger/smaller n_chunks, and calls retrieve_content again.  \n5.  In this way all of the RAG hyper-parameters (what you’re searching for, and how many hits you want) live under the LLM’s control, so the agent can “dial up or down” the context retrieval on the fly.  \n\nAnswer  \nThe red-boxed RAG & Tool Functions module leverages the retrieve_content tool by exposing its key hyper-parameters (the natural-language query and the number of chunks) to the agent via the Text2Param interface.  The agent issues a retrieve_content call, examines the returned passages, and—if the results aren’t sufficient—updates its query text or n_chunks and calls retrieve_content again, thereby dynamically adapting the retrieval settings until it gets the needed context."
    },
    {
        "question": "How does expanding from Single to Dual structures adjust forced communication prompts during analysis?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "However, If we directly apply the implementation of a single-agent architecture, a common issue is that one agent might complete most of the work and then summarize and respond, skipping interaction with the other agent. Extra prompts are thus added to force communication between agents.",
            "Ask for advice from [Agent B] before you make any conclusion.",
            "Inspect analysis delivered by [Agent B] and give out advice.",
            "Reach a consensus with [Agent B] and provide the final analysis."
        ],
        "final_answer": "When moving from a single-agent setup to a dual-agent group, the system prompt of Agent A is augmented with explicit communication duties: it must ask Agent B for advice before drawing any conclusions, review and comment on Agent B’s analysis, and only after reaching consensus with Agent B should it issue the final analysis.",
        "relevant_elements": [
            "Single",
            "Dual"
        ],
        "id": 2405,
        "masked_question": "How does expanding from [mask1] to [mask2] structures adjust forced communication prompts during analysis?",
        "masked_number": 2,
        "masked_elements": [
            "Single",
            "Dual"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.04788v1_figure_1.png",
        "paperid": "2411.04788v1",
        "paper_path": "./MISS-QA/papers/2411.04788v1.json",
        "figure_id": "2411.04788v1_figure_1.png",
        "caption": "Figure 1. Overview of proposed multi-agent collaboration framework with unified RAG & tool function calling.",
        "qtype": "Experimental_Results",
        "response": "Chain-of-thought  \n1. In the single‐agent (red-boxed) setup there is only one analyst and no peer to talk to, so no “force-communication” instructions are needed—its system prompt simply defines its analyst role and lets it work in isolation until it decides it’s done.  \n2. As soon as you move to the dual-agent (blue-boxed) setup you have two equal analysts, and without extra guidance one might do all the work and never involve the other.  \n3. To prevent that, the authors inject three extra “forced communication” rules into Agent A’s system prompt:  \n   a. Before drawing any conclusion, Agent A must ask Agent B for advice.  \n   b. Agent A must read Agent B’s analysis and give feedback.  \n   c. Agent A must reach a consensus with Agent B before producing the final answer.  \n4. In short, going from single to dual adds explicit “ask, inspect, and agree” prompts to make the two agents actually talk and collaborate rather than work in parallel and then merge silently.  \nAnswer  \nWhen you expand from the single‐agent (red box) to the dual‐agent (blue box) structure, you go from no peer‐to‐peer rules to explicitly forcing communication: Agent A’s system prompt is augmented with directives to (1) ask Agent B for advice before concluding, (2) inspect and comment on Agent B’s analysis, and (3) reach consensus with Agent B before issuing the final analysis."
    },
    {
        "question": "What limitations arise when agents autonomously refine RAG retrieval parameters via Text2Param?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "retrieve_content",
            "Text2Param"
        ],
        "id": 2406,
        "masked_question": "What limitations arise when agents autonomously refine [mask1] parameters via [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "retrieve_content",
            "Text2Param"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.04788v1_figure_1.png",
        "paperid": "2411.04788v1",
        "paper_path": "./MISS-QA/papers/2411.04788v1.json",
        "figure_id": "2411.04788v1_figure_1.png",
        "caption": "Figure 1. Overview of proposed multi-agent collaboration framework with unified RAG & tool function calling.",
        "qtype": "Others",
        "response": "Mask1 is the little “retrieve_content” interface – essentially the RAG retriever’s tunable knobs (“query” and “n_chunks”) – and Mask2 is the Text2Param–function-calling layer that lets an LLM treat those knobs as parameters it can rewrite and resend to the retriever.  \n\nIf you hand an agent full autonomy to refine those RAG parameters via Text2Param, you quickly run into several practical limits:  \n  • No grounded relevance signal. The agent only knows “I got some text back,” not how good it was.  Without a proper metric in the loop, it can’t reliably judge whether to ask for more or fewer chunks.  \n  • Risk of endless or exploding loops.  The agent may keep tweaking “n_chunks” or rephrasing the query forever, driving up latency and API costs or even blowing out its context window.  \n  • Over- and under-fetching.  In the absence of a human-set budget or clear stopping rule, the agent may pull in hundreds of largely irrelevant passages, or conversely stop too soon and miss key facts.  \n  • Greater brittleness.  Simple paraphrase changes can yield wildly different retrieval results; self-tuning agents have no principled way to converge on an optimal setting.  \n\nIn short, letting a pure LLM “self-tune” the retriever parameters via function calls trades away the human-in-the-loop tuning that ensures relevance, and so is prone to wasteful loops, junk context, runaway costs, and no convergence guarantee."
    },
    {
        "question": "What alternative could mitigate biases in transportation cost estimation for semantic-guided classifier consolidation?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "transportation cost",
            "classifier consolidation"
        ],
        "id": 2409,
        "masked_question": "What alternative could mitigate biases in [mask1] estimation for semantic-guided [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "transportation cost",
            "classifier consolidation"
        ],
        "figure_path": "./MISS-QA/figures/0_2410.00911v1_figure_1.png",
        "paperid": "2410.00911v1",
        "paper_path": "./MISS-QA/papers/2410.00911v1.json",
        "figure_id": "2410.00911v1_figure_1.png",
        "caption": "Figure 1: Illustration of Duct.\nTop: Representation consolidation. We utilize the pre-trained model as initialization and optimize it for each domain, obtaining the task vectors. Afterward, we combine the pre-trained model and all seen task vectors to build the unified embedding space. Bottom: Classifier consolidation. To align the classifiers with consolidated features, we design the new classifier retraining and old classifier transport to consolidate classifiers. Class-wise semantic information is utilized in classifier transport.",
        "qtype": "Others",
        "response": "Here is one very simple “drop‐in” you can make that almost always cuts down on prototype‐bias in the red-boxed transport-cost matrix and thus leads to a much more faithful semantic-guided classifier consolidation (the blue box):\n\n  •  Instead of measuring the cross–domain cost Cij by a plain Euclidean distance between two point prototypes (‖μi−μj‖2), estimate your cost with a *richer* semantic metric.  \n\n    –  For example, fit a small Gaussian (or “prototype cloud”) to each class’s features and define  \n         Cij = W2(𝒩(μi,Σi), 𝒩(μj,Σj)),  \n      i.e. the 2–Wasserstein distance between two Gaussians, or use the corresponding Mahalanobis distance  \n         Cij = (μi−μj)⊤(Σi+Σj)⁻¹(μi−μj).  \n    –  Alternatively, embed each class’s *name* in a language model (Word2Vec, BERT, etc.) and let  \n         Cij =1 – cosine(sim(namei,namej)),  \n      so you are transporting according to word‐level semantics rather than relying solely on visual prototypes.  \n\nEither of these changes will substantially reduce the bias that comes from treating each class as a single Euclidean-prototype, and as a result you get a much more faithful OT plan for your semantic-guided classifier consolidation step."
    },
    {
        "question": "What rationale underpins using transportation cost Q to guide old classifier transport?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "A cost matrix Q is further introduced to guide the transition. The larger weight of Q indicates we need to pay more cost when reusing the classifier of -th class to assist the -th class.",
            "Solving Eq. 9 requires a proper definition of the cross-domain cost, i.e., Q. The higher cost indicates it is less effective to transport the classifier to the target class and vice versa.",
            "Consequently, we calculate the Euclidean distance between class centers as the transportation cost, i.e., Q_{ij} = \\|μ_i - μ_j\\|. Here classes i and j are from different domains."
        ],
        "final_answer": "The transportation cost Q encodes the pairwise semantic dissimilarity between classes across domains—measured as the Euclidean distance between their class‐center embeddings—so that the optimal‐transport plan will preferentially reuse classifier weights between semantically similar classes (low cost) and avoid transferring between semantically distant ones (high cost).",
        "relevant_elements": [
            "transportation cost",
            "old classifier transport",
            "classifier consolidation"
        ],
        "id": 2411,
        "masked_question": "What rationale underpins using [mask1] Q to guide old classifier transport?",
        "masked_number": 1,
        "masked_elements": [
            "transportation cost"
        ],
        "figure_path": "./MISS-QA/figures/1_2410.00911v1_figure_1.png",
        "paperid": "2410.00911v1",
        "paper_path": "./MISS-QA/papers/2410.00911v1.json",
        "figure_id": "2410.00911v1_figure_1.png",
        "caption": "Figure 1: Illustration of Duct.\nTop: Representation consolidation. We utilize the pre-trained model as initialization and optimize it for each domain, obtaining the task vectors. Afterward, we combine the pre-trained model and all seen task vectors to build the unified embedding space. Bottom: Classifier consolidation. To align the classifiers with consolidated features, we design the new classifier retraining and old classifier transport to consolidate classifiers. Class-wise semantic information is utilized in classifier transport.",
        "qtype": "Design_Rationale",
        "response": "Here is the reasoning in steps:\n\n1.  **Goal**  \n    We want to “transport” (i.e. transform) the new‐task classifier  into a version of the old‐task classifier  that will work on the newly consolidated feature space—without ever revisiting old data.\n\n2.  **Why transport at all?**  \n    After we merge all domains’ embeddings, the old classifiers no longer line up with those merged embeddings.  We must therefore re-align (or re-“transport”) the old classifiers to the new feature geometry.\n\n3.  **What should guide that transport?**  \n    Intuitively, you ought to reuse most heavily those new‐classifier parameters whose classes are semantically closest to each old class.  For example, if “lion” in the new clip-art domain sits close to “lion” in the old real-photo domain, you should transfer over a lot of lion parameters, but if “fish” is very different from “cat,” you should transfer only a little fish ↔ cat.\n\n4.  **How to formalize “semantic closeness”?**  \n    We compute class-center embeddings (via the frozen pretrained backbone) for every old and new class, and then take Euclidean distances between those centers.  Those pairwise distances form a cost matrix  (high cost = classes are far apart, low cost = classes are semantically similar).\n\n5.  **Optimal transport plan**  \n    We then solve a small optimal‐transport problem (the Kantorovich formulation) to find a coupling matrix  that moves “mass” from each new-class to each old-class at minimal total cost.  Concretely  \n       •  Rows of  sum to 1 (new–class marginals),  \n       •  Columns of  sum to 1 (old–class marginals),  \n       •  and  has low entries precisely where the corresponding classes had low Euclidean distance in the embedding space.\n\n6.  **Using  to rebuild the old classifier**  \n    Finally, we apply  as a linear mixing of the new classifier’s weight vectors.  Each old class’s new weight becomes a weighted combination of the new‐class weights, with weights given by the corresponding column of .  Because  was built to favor “cheap” (i.e. semantically close) transfers, this reconstituted old classifier naturally aligns with the merged embedding space.\n\n**Answer to the question**  \nThe red‐boxed matrix  is exactly that optimal‐transport plan over old–vs–new class centers.  By solving for  under the Euclidean distances of the frozen‐backbone class centers, we encode semantic similarity across domains.  We then use  to guide how much of each new‐class weight should flow into each old class’s weight.  In short,  steers the old classifier’s reconstruction so that semantically similar classes (low transport cost) contribute most, guaranteeing minimal-cost, semantics-aware “transport” of the old classifier into the new embedding space."
    },
    {
        "question": "How does pseudo open-set sampling create open queries from closed-set categories during transfer learning?",
        "relevant_section_ids": [
            "3.2.2"
        ],
        "relevant_context": [
            "Our strategy is the episodic random class sampling from the closed set C itself to learn the open set. As exemplified in the purple-colored of Fig. 2 (b), we iteratively partition C into the mutually exclusive subsets C_closed and C_open.",
            "Subsequently, their corresponding support sets S_closed and S_open extracted from C are used to transfer-learn the closed and open sets, respectively. Hence, we call C_open pseudo open set.",
            "Through this iterative pseudo open-set sampling, we can effectively learn the open-set classifier as well as the closed-set ones."
        ],
        "final_answer": "During transfer learning, pseudo open-set sampling repeatedly splits the target task’s closed-set category pool into two disjoint subsets: one treated as the actual closed set and the other treated as a “pseudo” open set. Queries (and supports) drawn from the pseudo open subset serve as open-set examples, allowing the model to learn to distinguish closed-set classes from these synthetic open-set queries.",
        "relevant_elements": [
            "pseudo open-set sampling",
            "open query",
            "closed-set"
        ],
        "id": 2415,
        "masked_question": "How does [mask1] create open queries from closed-set categories during transfer learning?",
        "masked_number": 1,
        "masked_elements": [
            "pseudo open-set sampling"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.09986v1_figure_2.png",
        "paperid": "2411.09986v1",
        "paper_path": "./MISS-QA/papers/2411.09986v1.json",
        "figure_id": "2411.09986v1_figure_2.png",
        "caption": "Figure 2: \nOverall training framework of OAL-OFL.\n(a) In Stage 1, the feature encoder and a learnable open-set prototype undergo distance-based meta-learning [37] with an additional class representing the open set. (b) In Stage 2, feature encoder and prototypes are further transfer-learned to the target task under an open-set-free condition. Open-set training examples can be alternatively drawn from the base training dataset (green) or from a subset of the closed-set categories that is randomly selected as a pseudo open set (purple).",
        "qtype": "Implementation_Details",
        "response": "In the “Lite” version of Stage-2 (the red-boxed module in your figure), there are no true out-of-task examples to draw on, so the authors simply turn a portion of the _closed_ classes into a “fake” open set at each training episode.  Concretely, in every iteration they\n\n 1. Take the N closed-set categories of the new task and randomly split them into two disjoint subsets:  \n    • One subset (call it C₁) remains the “real” closed classes for that episode.  \n    • The other subset (call it C₂) is **pretended** to be open classes.  \n 2. From C₁ they sample their usual support and closed-query examples.  \n 3. From C₂ they sample examples and label them as “open-set queries.”  \n\nBecause C₂ was drawn from the original closed-set, these are in fact closed-set images being _treated_ as if they belonged to unknown classes.  The network thus learns to reject them via the open-set prototype during transfer."
    },
    {
        "question": "How does the scale-conditioned MLP integrate SAM mask inputs to produce segmentation-aware 3D features?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Scale-conditioned Grouping. We first render multi-view images of the 3D object and utilize SAM to generate 2D masks of these multi-view renderings.",
            "For each mask, we can find the relevant points and calculate the 3D scale s with: where σ are the standard deviations of coordinates in the x,y,z directions, respectively; α is a scaling factor for better distinguishing the scales of different masks, which we set to 10.",
            "Then, we sample paired pixels on the valid region of 2D renderings for contrastive learning. Specifically, for two 3D points p_i and p_j mapping from a 2D pixel pair, we can obtain their features: f_i = φ(p_i), and f_j = ψ_θ(z_j, s), where φ is the feature derived from backbone PTv3-object, and z represents the positional embedding derived from positional encoding module. The final contrastive loss is: …",
            "After training the scale-conditioned MLP, we can obtain the segmentation-aware features of 3D point cloud conditioned on a scale."
        ],
        "final_answer": "The MLP takes as input the pretrained 3D backbone features of points φ(p) together with a per‐mask scale s (computed from the spread of the 3D points under each SAM mask) and a positional embedding z.  During fine-tuning, point pairs sampled from the same or different SAM masks are passed through the scale‐conditioned MLP, and a contrastive loss is used to pull together features of points within the same mask and push apart those from different masks.  After this training, the MLP produces segmentation-aware 3D features that reflect the SAM mask groupings at a user-specified granularity.",
        "relevant_elements": [
            "MLP",
            "SAM",
            "Segmentation-Aware 3D Features"
        ],
        "id": 2417,
        "masked_question": "How does the scale-conditioned [mask1] integrate SAM mask inputs to produce [mask2]?",
        "masked_number": 2,
        "masked_elements": [
            "MLP",
            "Segmentation-Aware 3D Features"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.07184v2_figure_2.png",
        "paperid": "2411.07184v2",
        "paper_path": "./MISS-QA/papers/2411.07184v2.json",
        "figure_id": "2411.07184v2_figure_2.png",
        "caption": "Figure 2: An overview pipeline of SAMPart3D. (a) We first pre-train 3D backbone PTv3-object on 3D large-scale data Objaverse, distilling visual features from FeatUp-DINOv2. (b) Next, we train light-weight MLPs to distill 2D masks to scale-conditioned grouping. (c) Finally, we cluster the feature of point clouds and highlight the consistent 2D part area with 2D-3D mapping on multi-view renderings, and then query semantics from MLLMs.",
        "qtype": "Implementation_Details",
        "response": "The piece inside the red box is nothing more exotic than a tiny MLP that “soaks up” the mask information by way of a single scalar (the per‐mask scale) and the two‐dimensional SAM partition labels, and then re‐projects that back into the 3D point feature space.  In practice the pipeline goes like this:\n\n 1.  You take one of SAM’s 2D masks, lift it back to 3D by finding all points that project into that mask, and compute a single scale value s for that mask (the standard deviation of the 3D points’ x,y,z coordinates, times a fixed constant).  \n 2.  You grab each 3D point’s frozen PTv3-object feature fₚ, its positional embedding P(p), and append or fuse the scalar s.  That triplet (fₚ, P(p), s) is the input to the little MLP in the red box.  \n 3.  You train that MLP with a contrastive loss so that points coming from the same SAM mask are pulled together in its output space and points from different masks are pushed apart.  \n\nThe end result – the blue‐boxed “segmentation‐aware 3D features” – are simply the MLP’s outputs.  In other words, the red-boxed MLP conditions the backbone features on the SAM masks (via the scale s) and spits out a new embedding for every point that already “knows” which 2D mask it came from."
    },
    {
        "question": "How does distilling DINOv2 features into PTv3-object reshape standard 2D-to-3D distillation methodologies?",
        "relevant_section_ids": [
            "1",
            "3.1"
        ],
        "relevant_context": [
            "In this work, we abandon GLIP and instead utilize the more low-level, text-independent DINOv2 [29] model for 2D-to-3D feature distillation, eliminating the reliance on part label sets and enhancing both scalability and flexibility.",
            "Therefore, we abandon VLMs and instead utilize the more low-level, text-independent DINOv2 [29] model as supervision for visual feature distillation.",
            "To address this, we employ the recently proposed feature upsampling technique, FeatUp [13], to enhance the DINOv2 features for use as point-wise supervision in 3D feature extraction.",
            "Specifically, for each training iteration, we sample a batch of 3D objects ... we render images from V different views for each object and extract the corresponding DINOv2 features. Utilizing the mapping relationship between point clouds and pixels, we can directly obtain the 2D features of the 3D point cloud ... Finally, by averaging the 2D features from all rendered views, we obtain the final 2D features of the point cloud ... and we simply choose a mean squared error (MSE) loss as the learning objective for distilling 2D visual features to the 3D backbone."
        ],
        "final_answer": "Instead of relying on text-prompted vision-language models (e.g. GLIP) for part supervision, SAMPart3D uses the self-supervised DINOv2 backbone to provide low-level, text-independent features. These DINOv2 features are upsampled via FeatUp to produce high-resolution, point-wise targets. Multi-view renderings are used to map 2D DINOv2 features onto 3D points (handling occlusions via depth), and the averaged features across views serve as supervision in an MSE loss to train the PTv3-object backbone. This reshapes standard 2D-to-3D distillation by removing the need for text prompts, improving feature granularity, and enabling scalable learning from large unlabeled 3D datasets.",
        "relevant_elements": [
            "PTv3-object",
            "DINOv2"
        ],
        "id": 2418,
        "masked_question": "How does distilling [mask1] features into [mask2] reshape standard 2D-to-3D distillation methodologies?",
        "masked_number": 2,
        "masked_elements": [
            "DINOv2",
            "PTv3-object"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.07184v2_figure_2.png",
        "paperid": "2411.07184v2",
        "paper_path": "./MISS-QA/papers/2411.07184v2.json",
        "figure_id": "2411.07184v2_figure_2.png",
        "caption": "Figure 2: An overview pipeline of SAMPart3D. (a) We first pre-train 3D backbone PTv3-object on 3D large-scale data Objaverse, distilling visual features from FeatUp-DINOv2. (b) Next, we train light-weight MLPs to distill 2D masks to scale-conditioned grouping. (c) Finally, we cluster the feature of point clouds and highlight the consistent 2D part area with 2D-3D mapping on multi-view renderings, and then query semantics from MLLMs.",
        "qtype": "Literature_Background",
        "response": "Mask₁ is the DINOv2 block (red box) and Mask₂ is the PTv3-object backbone (blue box).  What SAMPart3D does differently is to take the self-supervised, text-free DINOv2 features, upsample them via FeatUp into dense, per-pixel descriptors, project those descriptors into 3D by multi-view rendering and depth masking, and then force the PTv3-object 3D backbone to reproduce them under a simple MSE loss.  \n\nIn other words, instead of distilling high-level, prompt-driven outputs (e.g. GLIP or SAM masks) or depending on a fixed set of part labels, they distill low-level DINOv2 features directly into a 3D point transformer.  This removes any reliance on text prompts or part annotations, gives point-wise supervision on massive unlabeled 3D data, and so turns the standard 2D→3D distillation recipe into a fully unsupervised, model-agnostic feature alignment scheme that readily scales to hundreds of thousands of objects."
    },
    {
        "question": "In what way does the scale-conditioned MLP leverage contrastive learning beyond conventional grouping strategies?",
        "relevant_section_ids": [
            "3.2"
        ],
        "relevant_context": [
            "Then, we sample paired pixels on the valid region of 2D renderings for contrastive learning.",
            "Specifically, for two 3D points p_i and p_j mapping from a 2D pixel pair, we can obtain their features: h_i = MLP(f_i, s), h_j = MLP(f_j, s). The final contrastive loss is: L_contrast = -\\frac{1}{N} \\sum_{i,j} [\\mathbb{1}_{[i,j]} \\log \\frac{\\exp(\\mathrm{sim}(h_i,h_j)/\\tau)}{\\sum_k \\exp(\\mathrm{sim}(h_i,h_k)/\\tau)} + (1-\\mathbb{1}_{[i,j]}) \\log \\frac{\\exp(-\\mathrm{sim}(h_i,h_j)/\\tau)}{\\sum_k \\exp(-\\mathrm{sim}(h_i,h_k)/\\tau)} ], where \\mathbb{1}_{[i,j]} indicates whether the pair is from the same mask (1) or different masks (0).",
            "After training the scale-conditioned MLP, we can obtain the segmentation-aware features of 3D point cloud conditioned on a scale."
        ],
        "final_answer": "Rather than simply clustering raw 3D features by proximity, the scale-conditioned MLP is trained with an explicit contrastive loss: it pulls together features of points that originate from the same 2D mask (positives) and pushes apart features of points from different masks (negatives), all while conditioning on a chosen scale value. This discriminative, mask-based supervision yields segmentation-aware embeddings at multiple granularities, going beyond conventional geometry-only grouping strategies.",
        "relevant_elements": [
            "MLP",
            "contrastive learning"
        ],
        "id": 2419,
        "masked_question": "In what way does the scale-conditioned [mask1] leverage [mask2] beyond conventional grouping strategies?",
        "masked_number": 2,
        "masked_elements": [
            "MLP",
            "contrastive learning"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.07184v2_figure_2.png",
        "paperid": "2411.07184v2",
        "paper_path": "./MISS-QA/papers/2411.07184v2.json",
        "figure_id": "2411.07184v2_figure_2.png",
        "caption": "Figure 2: An overview pipeline of SAMPart3D. (a) We first pre-train 3D backbone PTv3-object on 3D large-scale data Objaverse, distilling visual features from FeatUp-DINOv2. (b) Next, we train light-weight MLPs to distill 2D masks to scale-conditioned grouping. (c) Finally, we cluster the feature of point clouds and highlight the consistent 2D part area with 2D-3D mapping on multi-view renderings, and then query semantics from MLLMs.",
        "qtype": "Literature_Background",
        "response": "Here is the gist of what is happening in that red‐boxed module (the “scale‐conditioned” MLP) and how it uses the blue‐boxed component (the contrastive‐learning head) to go well beyond a naïve grouping or off-the-shelf clustering:\n\n 1.  Conventional grouping strategies on a point cloud usually just look at proximity or raw feature similarity and then run something like K-means or HDBSCAN.  You get parts, but they are purely geometry- or density-based and you have no direct notion of “these points belong together because they formed one mask in a render.”\n\n 2.  In SAMPart3D’s sample-specific fine-tuning stage the authors attach a tiny, scale‐conditioned MLP (red box) on top of each 3D point’s backbone feature.  At training time that MLP is not fit by simply doing clustering in feature‐space; rather, it is trained with a contrastive loss (blue box) that uses multi-view 2D segmentation masks from SAM as supervision.\n\n 3.  Concretely, they render the object from many angles, run SAM to get a bunch of masks, and for each mask they:\n    •  Pull together (in feature‐space) every pair of 3D points whose projections fell into the same mask,  \n    •  Push apart every pair whose projections came from different masks,  \n    •  Weight each pair by a scale parameter σ derived from the mask’s size (so you can ask for coarse versus fine segmentations).\n\n 4.  Because of this contrastive training, the red MLP learns a feature embedding in which “same‐mask” points are tightly clustered and “different‐mask” points are well separated—and you can dial the granularity via that σ input.  At test time you simply run a standard clustering algorithm (e.g. HDBSCAN) on those learned, scale-conditioned, mask-aware features to get your final parts.\n\nIn short:  \nThe scale-conditioned MLP is not just clustering backbone features by distance; it is explicitly taught—via a multi-view contrastive loss—to respect the boundaries of real 2D masks, and to do so at whatever scale you request. This mask-driven contrastive training produces much cleaner, semantically consistent part groupings than you ever would get by vanilla geometric or feature clustering alone."
    },
    {
        "question": "How does Data Synthesis integrate deduplication and decontamination from synthetic instruction generation research?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "To maintain the consistency of data distribution of coverage dataset  constructed from real-world scenarios, we employ the train subsets  as seeds and apply the same framework [41, 34] for synthetic dataset generation.",
            "During synthetic dataset generation, we perform data deduplication to exclude identical samples from  and filter out low-quality samples based on carefully designed rules.",
            "Finally, we conduct rigorous data decontamination for the synthetic dataset to remove samples that closely resemble those in the test subset  of the coverage dataset."
        ],
        "final_answer": "The Data Synthesis stage builds on prior synthetic instruction-generation pipelines [41, 34] by first using the coverage train set as seeds, then applying deduplication to strip out exact duplicates and low-quality examples, and finally performing decontamination to remove any generated samples that are overly similar to the held-out test subset.",
        "relevant_elements": [
            "Data Synthesis"
        ],
        "id": 2420,
        "masked_question": "How does [mask1] integrate deduplication and decontamination from synthetic instruction generation research?",
        "masked_number": 1,
        "masked_elements": [
            "Data Synthesis"
        ],
        "figure_path": "./MISS-QA/figures/0_2408.13467v2_figure_1.png",
        "paperid": "2408.13467v2",
        "paper_path": "./MISS-QA/papers/2408.13467v2.json",
        "figure_id": "2408.13467v2_figure_1.png",
        "caption": "Figure 1: The LLMOps pipeline namely LlamaDuo for migrating from service LLMs to small-scale local LLMs involves three phases. In the Development/PoC phase, ① users manually engineer prompts to interact with service LLMs and ② collect satisfying (prompt, response) pairs into train and test datasets. In the Alignment phase, ③ local LLMs are aligned with the train dataset, ④ tested on the test dataset, and ⑤ evaluated by service LLMs. ⑥ Synthetic data is generated iteratively until the performance of the aligned model meets a threshold.\nIn the Deployment phase, ⑦ the satisfactory model is deployed in constrained environments.",
        "qtype": "Literature_Background",
        "response": "First, note that the red‐boxed module in the figure is the “Data Synthesis” block at step 6, which is where new synthetic (prompt, response) pairs are generated and then rigorously cleaned before being fed back into the fine-tuning loop.  Concretely, we borrow the post-processing recipe from recent synthetic-instruction papers (e.g. Self-Instruct [41], Alpaca [34]):\n\n1.  Deduplication  \n    – After the service LLM spits out a batch of new synthetic examples, we compare them to each other (and to our seed‐coverage set) and drop any exact or near-duplicate items.  \n\n2.  Quality filtering  \n    – We apply a set of lightweight heuristic rules (length thresholds, blacklist checks, simple semantic tests) to weed out obviously bad or malformed examples.  \n\n3.  Decontamination  \n    – Finally, we scan the remaining synthetic examples against our held-out test subset and remove any that bear too close a resemblance to the test prompts or answers.  \n\nBy wrapping every round of synthetic generation in this g(·) pipeline—“generate → dedupe → filter → decontaminate”—we ensure that each new batch of data is both novel (no repeats) and uncontaminated by our evaluation set, exactly as prescribed in the synthetic-instruction literature."
    },
    {
        "question": "How does Evaluation leverage pairwise comparisons from LLM-as-judge methodologies to assess generated responses?",
        "relevant_section_ids": [
            "3.4"
        ],
        "relevant_context": [
            "Following the works [50, 47], the service LLMs evaluate each response triple , comprising prompt, the corresponding generated response, and the ground truth, by N times with pairwise comparison and single answer grading strategies.",
            "In this paper, we measure the similarity between R_g and R_gt, and how precisely R_g the responses generated by the local LLM answer the given instructions. These two metrics are provided simultaneously through a prompt.",
            "Subsequently, the evaluation results can be leveraged according to the intention of the operator performing this LLMOps pipeline. In this study, we adopt the mean score μ and coverage percentage ζ with ζ score as evaluation results."
        ],
        "final_answer": "In the evaluation stage, LlamaDuo uses service LLMs as judges to perform multiple (N) evaluations per prompt–response–ground-truth triple. Each evaluation run applies a pairwise comparison strategy (comparing the generated response directly against the ground truth) alongside a single-answer grading strategy. The judge outputs similarity and precision scores for each run, and these scores are then aggregated (mean score μ) and summarized as a coverage percentage ζ to decide whether the local model’s outputs meet the desired quality threshold.",
        "relevant_elements": [
            "Evaluation"
        ],
        "id": 2421,
        "masked_question": "How does [mask1] leverage pairwise comparisons from LLM-as-judge methodologies to assess generated responses?",
        "masked_number": 1,
        "masked_elements": [
            "Evaluation"
        ],
        "figure_path": "./MISS-QA/figures/1_2408.13467v2_figure_1.png",
        "paperid": "2408.13467v2",
        "paper_path": "./MISS-QA/papers/2408.13467v2.json",
        "figure_id": "2408.13467v2_figure_1.png",
        "caption": "Figure 1: The LLMOps pipeline namely LlamaDuo for migrating from service LLMs to small-scale local LLMs involves three phases. In the Development/PoC phase, ① users manually engineer prompts to interact with service LLMs and ② collect satisfying (prompt, response) pairs into train and test datasets. In the Alignment phase, ③ local LLMs are aligned with the train dataset, ④ tested on the test dataset, and ⑤ evaluated by service LLMs. ⑥ Synthetic data is generated iteratively until the performance of the aligned model meets a threshold.\nIn the Deployment phase, ⑦ the satisfactory model is deployed in constrained environments.",
        "qtype": "Literature_Background",
        "response": "The red‐boxed module is simply the “Evaluation” step of LlamaDuo (step ⑤).  In that block the authors turn the service LLMs into automatic judges by doing exactly what recent “LLM-as-judge” papers do:\n\n1.  For each test prompt they gather the local model’s answer(s) plus the reference answer.  \n2.  They then invoke the service LLM judge M times, each time asking it to compare two responses (or one response against the ground truth) and say which one better fulfills the instruction or to give it a score.  \n3.  By repeating these pairwise judgments (and single‐answer gradings) they dilute individual scoring noise and bias.  \n4.  Finally they aggregate the M judgments—taking the mean score as S and computing a coverage percentage C (the fraction of runs whose score ≥ threshold)—to get a stable, robust quality metric for that prompt.  \n\nIn short, the evaluation block exploits repeated pairwise comparisons by the service LLM to rank and score the locally generated answers against the reference, then averages (or takes the median of) those judgments to produce the final performance numbers."
    },
    {
        "question": "How does Data Synthesis integrate deduplication and decontamination from synthetic instruction generation research?",
        "relevant_section_ids": [
            "3.5"
        ],
        "relevant_context": [
            "To maintain the consistency of data distribution of coverage dataset  constructed from real-world scenarios, we employ the train subsets  as seeds and apply the same framework [41, 34] for synthetic dataset generation.",
            "During synthetic dataset generation, we perform data deduplication to exclude identical samples from  and filter out low-quality samples based on carefully designed rules.",
            "Finally, we conduct rigorous data decontamination for the synthetic dataset to remove samples that closely resemble those in the test subset  of the coverage dataset."
        ],
        "final_answer": "The Data Synthesis stage builds on prior synthetic instruction-generation pipelines [41, 34] by first using the coverage train set as seeds, then applying deduplication to strip out exact duplicates and low-quality examples, and finally performing decontamination to remove any generated samples that are overly similar to the held-out test subset.",
        "relevant_elements": [
            "Data Synthesis"
        ],
        "id": 2420,
        "masked_question": "How does [mask1] integrate deduplication and decontamination from synthetic instruction generation research?",
        "masked_number": 1,
        "masked_elements": [
            "Data Synthesis"
        ],
        "figure_path": "./MISS-QA/figures/2_2408.13467v2_figure_1.png",
        "paperid": "2408.13467v2",
        "paper_path": "./MISS-QA/papers/2408.13467v2.json",
        "figure_id": "2408.13467v2_figure_1.png",
        "caption": "Figure 1: The LLMOps pipeline namely LlamaDuo for migrating from service LLMs to small-scale local LLMs involves three phases. In the Development/PoC phase, ① users manually engineer prompts to interact with service LLMs and ② collect satisfying (prompt, response) pairs into train and test datasets. In the Alignment phase, ③ local LLMs are aligned with the train dataset, ④ tested on the test dataset, and ⑤ evaluated by service LLMs. ⑥ Synthetic data is generated iteratively until the performance of the aligned model meets a threshold.\nIn the Deployment phase, ⑦ the satisfactory model is deployed in constrained environments.",
        "qtype": "Literature_Background",
        "response": "The red–boxed component in Figure 1 is the “Data Synthesis” module (step ⑥).  In our pipeline we lean heavily on two recent “synthetic instruction” recipes (see refs. [41,34]) and then bolt on the exact same de-duplication and de-contamination steps they advocate.  In practice that means:\n\n1.  We take our small “coverage” train‐set (the high‐quality, human‐approved prompt/response pairs) as seeds and ask the service LLM to spin up a much larger batch of new instructions + answers.  \n2.  Immediately after generation we run a de-duplication pass—first, we remove any generated example that is byte-for-byte identical to something already in our coverage seeds or in an earlier synthetic round; second, we run a fuzzy or embedding-based duplicate detector to catch near-copies.  \n3.  We apply a simple hand-crafted “sanity check” filter (length thresholds, verboten tokens, banned topics, malformed JSON, etc.) to knock out obviously junky outputs.  \n4.  Finally, before we hand the remaining synthetic set back for fine-tuning, we de-contaminate it against our hold-out (test) set—using either n-gram overlap or embedding similarity—to ensure no synthetic sample “leaks” a test prompt or response back into training.  \n\nBy folding in these two lightweight post‐processing passes (de-duplication + de-contamination) directly after generation, we get a clean, novel synthetic corpus that mirrors the best practices of today’s open‐sourced instruction-tuning recipes."
    },
    {
        "question": "How does Batch Inference influence the robustness measured during Evaluation on test prompts?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "After the fine-tuning stage, the fine-tuned local model is prompted with prompts x_t sampled from the test subsets D_test of the coverage dataset to produce corresponding responses y_t.",
            "To improve the diversity and robustness of responses, the local model generates a batch of k responses y_t for each given prompt x_t."
        ],
        "final_answer": "By generating a batch of k responses for each test prompt, Batch Inference increases response diversity and thus enhances the robustness of the model’s outputs as measured during Evaluation.",
        "relevant_elements": [
            "Batch Inference",
            "Evaluation"
        ],
        "id": 2423,
        "masked_question": "How does [mask1] influence the robustness measured during Evaluation on test prompts?",
        "masked_number": 1,
        "masked_elements": [
            "Batch Inference"
        ],
        "figure_path": "./MISS-QA/figures/3_2408.13467v2_figure_1.png",
        "paperid": "2408.13467v2",
        "paper_path": "./MISS-QA/papers/2408.13467v2.json",
        "figure_id": "2408.13467v2_figure_1.png",
        "caption": "Figure 1: The LLMOps pipeline namely LlamaDuo for migrating from service LLMs to small-scale local LLMs involves three phases. In the Development/PoC phase, ① users manually engineer prompts to interact with service LLMs and ② collect satisfying (prompt, response) pairs into train and test datasets. In the Alignment phase, ③ local LLMs are aligned with the train dataset, ④ tested on the test dataset, and ⑤ evaluated by service LLMs. ⑥ Synthetic data is generated iteratively until the performance of the aligned model meets a threshold.\nIn the Deployment phase, ⑦ the satisfactory model is deployed in constrained environments.",
        "qtype": "Experimental_Results",
        "response": "First, let’s pin down what “[mask1]” is.  In the red‐boxed area of Figure 1 you see step ④, labeled “Batch Inference.”  In the text (under “Batch Inference”) we learn that, after fine-tuning, the local model does not return just one answer to each test prompt but a whole mini‐batch of T answers.  Concretely:\n\n  • For each prompt x​j in the held-out test set, the model samples a set of responses  \n    {r​j¹, r​j², …, r​jᵀ}.  \n  • These T responses are then each judged (in step ⑤) by the service LLM–as–evaluator.  \n\nWhy do we do this?  By drawing multiple answers per prompt, we\n\n  1. expose the model’s variability (“Does it always get it right, or only occasionally?”),  \n  2. smooth out random sampling noise, and  \n  3. produce more reliable aggregate statistics (mean score, coverage rate).  \n\nWhen the evaluation LLM grades each of the T responses and we then compute, say, the average score or the percentage of outputs exceeding a threshold, those metrics now reflect not just “Did it ever hit the mark?” but “How consistently does it hit the mark?”  In other words, batch inference directly feeds into the robustness measure by letting us quantify both the quality and the stability of the model’s outputs across repeated samplings for the same prompt."
    },
    {
        "question": "How does the SWF Module fuse F_T and F_E to produce the weighted feature F_S?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "In our VistaHL solution (section 4.2) we introduce a Saliency-Weighted Feature Module (SWF) to explicitly enhance the high-resolution feature F_E using the saliency mask F_T generated by the low-resolution branch.",
            "SWF Module (see figure): the high-resolution feature F_E is multiplied element-wise with the saliency mask feature F_T, and that product is then added back to the original F_E to produce the fused, weighted feature F_S."
        ],
        "final_answer": "The SWF module first performs an element-wise multiplication between the saliency mask F_T and the high-resolution feature F_E, and then adds this product back to the original F_E. Formally, F_S = F_E + (F_E ⊙ F_T).",
        "relevant_elements": [
            "SWF Module"
        ],
        "id": 1082,
        "masked_question": "How does the [mask1] fuse F_T and F_E to produce the weighted feature F_S?",
        "masked_number": 1,
        "masked_elements": [
            "SWF Module"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.14827v1_figure_2.png",
        "paperid": "2409.14827v1",
        "paper_path": "./MISS-QA/papers/2409.14827v1.json",
        "figure_id": "2409.14827v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed network. SC[25], SE[17], and ShuffleAttn[54] are plug-and-play attention modules. SWF and GA stand for Saliency-Weighted Feature Module and Gated Attention, respectively.",
        "qtype": "Implementation_Details",
        "response": "The little red‐boxed module is in fact the Saliency–Weighted Feature (SWF) block.  What it does is very simple:\n\n1.  It takes the high-res feature map F_E and the low-res saliency guidance F_T (upsampled to F_E’s size).  \n2.  It performs an element–wise (Hadamard) product F_E ⊙ F_T, so that areas deemed salient by F_T get boosted in F_E.  \n3.  It then adds the original F_E back (a residual skip) to preserve unweighted detail.  \n\nIn one line:  \n\n F_S = F_E + ( F_E ⊙ F_T )  \n\n—that sum is the “weighted” output of the SWF module."
    },
    {
        "question": "How does ShuffleAttn leverage channel mixing principles compared to Squeeze-and-Excitation’s recalibration?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "ShuffleAttn",
            "SE"
        ],
        "id": 1083,
        "masked_question": "How does [mask1] leverage channel mixing principles compared to Squeeze-and-Excitation’s recalibration?",
        "masked_number": 1,
        "masked_elements": [
            "ShuffleAttn"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.14827v1_figure_2.png",
        "paperid": "2409.14827v1",
        "paper_path": "./MISS-QA/papers/2409.14827v1.json",
        "figure_id": "2409.14827v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed network. SC[25], SE[17], and ShuffleAttn[54] are plug-and-play attention modules. SWF and GA stand for Saliency-Weighted Feature Module and Gated Attention, respectively.",
        "qtype": "Literature_Background",
        "response": "The little red‐boxed module in the high-resolution branch is not a “mini SE” at all but the ShuffleAttention of [54], and it was chosen precisely because it interleaves and re-mixes channels rather than just “squeeze & excite” them.  In a vanilla SE block you do a global-average squeeze, pass that vector through two tiny fully-connected layers to learn one scalar per channel, and then multiply each channel by its learned weight.  ShuffleAttn, by contrast, first splits your C channels into G groups, computes intra-group attention, then permutes (i.e. “shuffles”) channel indices so that information from one group is fed into the next, computes attention again, and finally shuffles the channels back into their original order.  Because of that group-split → attention → shuffle → attention motif, ShuffleAttn actually intermixes feature-maps across channels at every step, whereas SE only rescales them independently after a global pooling."
    },
    {
        "question": "How does SWF Module extend SCAM’s cross-attention weighting to fuse multi-resolution features?",
        "relevant_section_ids": [
            "4.2"
        ],
        "relevant_context": [
            "To effectively utilize the features from the low-resolution branch, we propose a Selective Cross Attention Module (SCAM), which enables the high-resolution branch to select corresponding saliency regions for feature extraction.",
            "Furthermore, we design a Saliency-Weighted Feature Module (SWF), which uses the saliency mask generated from the low-resolution branch to explicitly enhance saliency feature in the high-resolution branch."
        ],
        "final_answer": "After SCAM has used cross-attention to pick out the most salient regions in the high-resolution stream, the SWF Module takes the low-resolution branch’s saliency mask and applies it as an element-wise attention weight on the high-resolution features (i.e. multiplies the mask and feature tensors), then adds the result back into the high-resolution feature map. In this way it fuses multi-resolution information by sharpening and reinforcing the detailed, high-frequency features exactly where SCAM has already identified them as salient.",
        "relevant_elements": [
            "SWF Module",
            "SCAM"
        ],
        "id": 1084,
        "masked_question": "How does [mask1] extend [mask2]’s cross-attention weighting to fuse multi-resolution features?",
        "masked_number": 2,
        "masked_elements": [
            "SWF Module",
            "SCAM"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.14827v1_figure_2.png",
        "paperid": "2409.14827v1",
        "paper_path": "./MISS-QA/papers/2409.14827v1.json",
        "figure_id": "2409.14827v1_figure_2.png",
        "caption": "Figure 2: An overview of the proposed network. SC[25], SE[17], and ShuffleAttn[54] are plug-and-play attention modules. SWF and GA stand for Saliency-Weighted Feature Module and Gated Attention, respectively.",
        "qtype": "Literature_Background",
        "response": "Let’s first pin down what the two modules are:\n\n1. The blue‐boxed module is SCAM (Selective Cross Attention Module).  In SCAM you take features from the low‐res, multi‐frame branch and features from the high‐res, single‐frame branch, compute pairwise attention scores, pick the top-K keys, and use that to “select” which high-res tokens to focus on.  In other words SCAM builds a sparse cross‐attention map between the two resolutions and uses it to reweight the high‐res features.\n\n2. The red‐boxed module is SWF (Saliency-Weighted Feature Module).  Its job is to take the coarse saliency cues produced by the low-res branch (i.e. the cross-attention map or saliency mask FT) and use them to modulate the high-res features FE in a dense, pixel-wise way.\n\nNow to answer the question – “How does SWF extend SCAM’s cross-attention weighting to fuse multi-resolution features?” – here’s the step-by-step:\n\n• SCAM produces a saliency mask FT (essentially the cross-attention weights from low-res→high-res).  \n• SWF takes that FT and does an element-wise multiplication with the high-res feature FE: FE ⊙ FT.  This “gates” each spatial location of FE by how salient it was judged by the low-res branch.  \n• SWF then adds that gated result back to the original FE (a residual connection):  \n  FS = FE + (FE ⊙ FT)  \n• The output FS is thus a fused feature map that carries both the fine spatial detail of the high-res branch and the coarse, temporally-informed saliency weighting from the low-res branch.\n\nIn this way SWF “extends” the sparse, token-level cross-attention of SCAM into a dense, pixel-wise fusion: it uses the same attention map FT but applies it over every spatial position of FE, then fuses via a simple residual addition."
    },
    {
        "question": "How does CAN integration alter CGN noise design compared to coordinate denoising frameworks?",
        "relevant_section_ids": [
            "1"
        ],
        "relevant_context": [
            "The noise type in the previous denoising framework was restricted to set as coordinate Gaussian noise (CGN) with isotropic noise variance, to maintain the force learning interpretation. However, the use of isotropic CGN noise leads to a biased molecular distribution, focusing on isotropic vibrations around equilibrium positions, since molecules exhibit not only small-scale vibrations but also rotation along rotatable single bonds on a relatively large scale, as illustrated in Figure 1a. Modeling this biased molecular distribution leads to inaccuracies in force targets and constraining the sampling range around equilibriums, as indicated by our theoretical analysis in Supplementary Information A.1, and ultimately hinders the model’s performance on downstream tasks.",
            "Given the difficulty in modeling the true molecular distribution, we choose to characterize the distribution more comprehensively by introducing chemical priors about molecular distribution into noise design, which is prohibited in previous methods due to the restricted noise distribution.",
            "Therefore, we propose a novel molecular pre-training framework called fractional denoising (Frad), which is proven to hold the force learning interpretation. Specifically, given an equilibrium molecular conformation, a hybrid noise of chemical-aware noise (CAN) and CGN is added and a noisy conformation is obtained, the model is trained to predict CGN from the noisy conformation. The term “fractional” refers to recovering a fraction of the entire noise introduced, with the necessity of the design discussed in Supplementary Information A.2. Notably, CAN is customizable enabling Frad to incorporate chemical priors to optimize molecular distribution modeling.",
            "Inspired by the chemical priors that describe molecular conformational changes, we present two versions of CAN. Specifically, rotation noise (RN) is advocated to capture rotations of single bonds, while vibration and rotation noise (VRN) is put forward to reflect anisotropic vibrations."
        ],
        "final_answer": "Whereas prior coordinate-denoising methods perturb an equilibrium structure solely with isotropic coordinate Gaussian noise (CGN), Frad first adds a chemical-aware noise (CAN) component—e.g. bond‐rotation and anisotropic vibration perturbations—and then layers on CGN. The model is trained to recover only the CGN “fraction” of that hybrid noise. In this way, CAN expands the sampling beyond small, isotropic displacements and CGN remains an adjustable residual to preserve the force‐learning interpretation.",
        "relevant_elements": [
            "CAN",
            "CGN"
        ],
        "id": 1085,
        "masked_question": "How does [mask1] integration alter [mask2] noise design compared to coordinate denoising frameworks?",
        "masked_number": 2,
        "masked_elements": [
            "CAN",
            "CGN"
        ],
        "figure_path": "./MISS-QA/figures/0_2407.11086v1_figure_1.png",
        "paperid": "2407.11086v1",
        "paper_path": "./MISS-QA/papers/2407.11086v1.json",
        "figure_id": "2407.11086v1_figure_1.png",
        "caption": "Figure 1: \nOverview of Fractional denoising (Frad). a. An illustration of the molecular conformational changes. The local structures can vibrate in small scale, while some single bonds can rotate flexibly.\nb. The noise-adding process in the Frad framework. A two-phase hybrid random noise is applied to the equilibrium conformation, including the chemical-aware noise (CAN) that describes the molecular conformational changes and coordinate Gaussian noise (CGN). We present two versions of CAN.\nc. Pre-training process of Frad. The unlabeled molecular data is processed by adding noise and then utilized as the input of the graph neural networks to predict the CGN. This task is proved to be equivalent to learning the approximate atomic forces in the molecule.\nd. Fine-tuning process of Frad. The GNN model inherits the pre-trained weights and continues to be updated together with a prediction head for specific downstream tasks.\ne. Advancements of Frad over with coordinate denoising methods (Coord) [28, 33, 30, 31, 29, 32] through the perspective of chemical priors and physical interpretations. The noise of Frad is customizable, enabling capturing both rotations and vibrations in molecular conformation changes. Frad s superior modeling of molecular distribution further enabling larger sampling coverage and more accurate force targets in the equivalent force learning task, resulting in effective pre-training and improved downstream performance.\n\\added[id=2] f. An illustration of model architecture. The model primarily follows the TorchMD-NET framework, with our minor modifications highlighted in dotted orange boxes.",
        "qtype": "Literature_Background",
        "response": "In our diagram the red‐boxed module ([mask1]) is precisely the Chemical-Aware Noise (CAN) block – the two variants we call RN (pure torsional/rotation noise) and VRN (a mixture of small‐scale vibration plus rotation).  The blue‐boxed panel ([mask2]) by contrast simply shows the conventional, isotropic coordinate Gaussian noise (CGN) that prior denoising-only schemes have employed (i.e. drawing each Δx independently from the same N(0, σ²) distribution).\n\nBy “integrating” CAN upstream of (and in fact alongside) the usual CGN, Frad fundamentally changes the noise design:\n\n 1.  Instead of a single, uniform Gaussian jitter on all Cartesian coordinates, Frad first injects chemically meaningful perturbations into torsion angles (Δψ) and bond lengths/angles (Δr, Δθ) – that is RN or VRN – to cover large‐scale bond rotations and anisotropic vibrational modes.  \n 2.  Only then is a lesser amount of standard coordinate Gaussian noise added, and the network is trained to recover that Gaussian fraction.  \n\nIn other words, the noise model goes from “one-shot” isotropic Δx in Coord-denoising to a two-stage, multi‐component perturbation that (a) respects known molecular degrees of freedom (rotations and directional vibrations) and (b) still retains the force-learning interpretation via a remaining CGN term. This hybrid, “fractional” design both broadens the conformational coverage and yields more accurate force targets than pure CGN alone."
    },
    {
        "question": "How do update layer normalizations interact with TorchMD-NET attention layer to improve training stability?",
        "relevant_section_ids": [
            "4.1"
        ],
        "relevant_context": [
            "We make some minor modifications to TorchMD-NET marked in dotted orange boxes in the figure: Firstly, to stabilize training, we add an additional normalization module in the residue updating, which is effective for both the QM9 and LBA tasks.",
            "We further perform layer norm on the updated φ and v and use them as the output of the update layer."
        ],
        "final_answer": "After the TorchMD-NET attention layer computes the update increments (Δu for scalars and Δv for vectors), those updated features pass through a layer-normalization step. By normalizing the newly updated scalar (φ) and vector (v) representations before they are fed into the next layer, the extra normalization module stabilizes feature magnitudes and gradients, leading to more stable training.",
        "relevant_elements": [
            "update layer",
            "TorchMD-NET attention layer"
        ],
        "id": 1086,
        "masked_question": "How do [mask1] normalizations interact with TorchMD-NET attention layer to improve training stability?",
        "masked_number": 1,
        "masked_elements": [
            "update layer"
        ],
        "figure_path": "./MISS-QA/figures/1_2407.11086v1_figure_1.png",
        "paperid": "2407.11086v1",
        "paper_path": "./MISS-QA/papers/2407.11086v1.json",
        "figure_id": "2407.11086v1_figure_1.png",
        "caption": "Figure 1: \nOverview of Fractional denoising (Frad). a. An illustration of the molecular conformational changes. The local structures can vibrate in small scale, while some single bonds can rotate flexibly.\nb. The noise-adding process in the Frad framework. A two-phase hybrid random noise is applied to the equilibrium conformation, including the chemical-aware noise (CAN) that describes the molecular conformational changes and coordinate Gaussian noise (CGN). We present two versions of CAN.\nc. Pre-training process of Frad. The unlabeled molecular data is processed by adding noise and then utilized as the input of the graph neural networks to predict the CGN. This task is proved to be equivalent to learning the approximate atomic forces in the molecule.\nd. Fine-tuning process of Frad. The GNN model inherits the pre-trained weights and continues to be updated together with a prediction head for specific downstream tasks.\ne. Advancements of Frad over with coordinate denoising methods (Coord) [28, 33, 30, 31, 29, 32] through the perspective of chemical priors and physical interpretations. The noise of Frad is customizable, enabling capturing both rotations and vibrations in molecular conformation changes. Frad s superior modeling of molecular distribution further enabling larger sampling coverage and more accurate force targets in the equivalent force learning task, resulting in effective pre-training and improved downstream performance.\n\\added[id=2] f. An illustration of model architecture. The model primarily follows the TorchMD-NET framework, with our minor modifications highlighted in dotted orange boxes.",
        "qtype": "Literature_Background",
        "response": "First, “mask1” is simply the pair of Layer‐Norm blocks that we have inserted around the TorchMD-NET update (i.e. the red-boxed “Update Layer” in Figure 1f).  If you look at the text on page X (under “Model details”), we say:\n\n  “We further perform layer norm on the updated s and v and use them as the output of the update layer.”\n\nIn practice what this means is:\n\n  1. Inside each equivariant Transformer block, the attention module computes two residual “deltas,” Δs (for the scalars) and Δv (for the vectors).\n  2. Before feeding those deltas back into the next layer they each go through their own Layer‐Norm.\n  3. The normalized Δs and Δv are then added back to the running s and v states (i.e. the usual residual connection).\n\nBy normalizing the very outputs of the attention block you\n\n  • re‐center and rescale all message‐passing updates,  \n  • prevent occasional large spikes or vanishing signals,  \n  • reduce internal covariate shift between layers,  \n\nand thus dramatically improve the stability of training (especially when training on complicated tasks such as QM9 or protein–ligand binding‐affinity)."
    },
    {
        "question": "How does hybrid noise of CAN and CGN enable Frad’s equivalent force learning interpretation?",
        "relevant_section_ids": [
            "2.1",
            "2.1.1"
        ],
        "relevant_context": [
            "Given an equilibrium molecular conformation, a hybrid of chemical-aware noise (CAN) and coordinate Gaussian noise (CGN) are added, where the equilibrium conformation refers to the structure at local minima of the potential energy surface of the molecule. Then the model is trained to predict CGN from the noisy conformation, namely fractional denoising, as it recovers a portion of the introduced noise.",
            "Notably, our theoretical analysis reveals that the task, irrespective of the distribution of CAN, possesses a force learning interpretation, whereas the CAN distribution affects the force targets and sampling distribution.",
            "As an immediate consequence, a corollary arises: the score function of the conformation distribution equals the molecular forces up to a constant factor, i.e. ∇_x log p(x) ∝ –∇_x E(x), where E(x) is the potential energy and ∇_x E(x) the atomic forces.",
            "If the distribution of hybrid noise satisfies Δx is a coordinate Gaussian noise (CGN), then fractional denoising is equivalent to learning the atomic forces that correspond to the approximate molecular distribution by Boltzmann Distribution."
        ],
        "final_answer": "By first perturbing an equilibrium conformation with two kinds of noise—CAN to span realistic rotations and vibrations around the minimum, and CGN to provide a formal Gaussian coordinate disturbance—Frad trains a network to predict only the CGN component (fractional denoising).  When Δx is Gaussian, the optimal denoising function recovers the score ∇_x log p(x) of the noisy‐sample distribution, which under Boltzmann statistics equals the negative energy gradient (i.e. atomic forces) up to a constant.  The CAN part merely shapes the sampling distribution and force targets, while the CGN term underpins the exact equivalence to force learning.",
        "relevant_elements": [
            "Chemical-Aware Noise (CAN)",
            "Coordinate Gaussian Noise (CGN)"
        ],
        "id": 1087,
        "masked_question": "How does hybrid noise of [mask1] and [mask2] enable Frad’s equivalent force learning interpretation?",
        "masked_number": 2,
        "masked_elements": [
            "Chemical-Aware Noise (CAN)",
            "Coordinate Gaussian Noise (CGN)"
        ],
        "figure_path": "./MISS-QA/figures/2_2407.11086v1_figure_1.png",
        "paperid": "2407.11086v1",
        "paper_path": "./MISS-QA/papers/2407.11086v1.json",
        "figure_id": "2407.11086v1_figure_1.png",
        "caption": "Figure 1: \nOverview of Fractional denoising (Frad). a. An illustration of the molecular conformational changes. The local structures can vibrate in small scale, while some single bonds can rotate flexibly.\nb. The noise-adding process in the Frad framework. A two-phase hybrid random noise is applied to the equilibrium conformation, including the chemical-aware noise (CAN) that describes the molecular conformational changes and coordinate Gaussian noise (CGN). We present two versions of CAN.\nc. Pre-training process of Frad. The unlabeled molecular data is processed by adding noise and then utilized as the input of the graph neural networks to predict the CGN. This task is proved to be equivalent to learning the approximate atomic forces in the molecule.\nd. Fine-tuning process of Frad. The GNN model inherits the pre-trained weights and continues to be updated together with a prediction head for specific downstream tasks.\ne. Advancements of Frad over with coordinate denoising methods (Coord) [28, 33, 30, 31, 29, 32] through the perspective of chemical priors and physical interpretations. The noise of Frad is customizable, enabling capturing both rotations and vibrations in molecular conformation changes. Frad s superior modeling of molecular distribution further enabling larger sampling coverage and more accurate force targets in the equivalent force learning task, resulting in effective pre-training and improved downstream performance.\n\\added[id=2] f. An illustration of model architecture. The model primarily follows the TorchMD-NET framework, with our minor modifications highlighted in dotted orange boxes.",
        "qtype": "Experimental_Results",
        "response": "mask1 denotes the “chemical‐aware noise” (CAN) block in red – e.g. torsion‐angle (RN) or bond‐length/angle+torsion (VRN) perturbations – and mask2 is the isotropic coordinate Gaussian noise (CGN), Δx, in blue.  In Frad we first perturb an equilibrium conformation x_eq by CAN to get x_med, then add pure CGN to get x_fin.  The training task is to predict exactly that CGN‐perturbation Δx from x_fin (fractional denoising).  Because Δx is a coordinate‐Gaussian, the denoising loss becomes score matching on the resulting (CAN+CGN) mixture density, and by the Boltzmann assumption its score ∇_x log p(x) is proportional to the true atomic forces.  In short, splitting the noise into a structured, chemistry‐aware part (mask1) and a final Gaussian part (mask2) guarantees that predicting only the Δx term is mathematically equivalent to learning the force field."
    },
    {
        "question": "How does the encoder’s self-attention mechanism leverage past grid load embeddings for robust sequence representation?",
        "relevant_section_ids": [
            "7.2"
        ],
        "relevant_context": [
            "For model M, we propose to adapt an encoder–decoder transformer architecture where the encoder processes the past and the decoder processes the future contextual information.",
            "In this setup, the decoder serves as the regressor, by using non-causal attention to attend to data from the expected future, while the encoder learns a representation of the past data.",
            "Non-Causal Attention: In our experiments, we adopt non-causal (bi-directional) attention, as introduced by Devlin et al. (2019) in the BERT model. By leveraging bi-directional attention, we effectively utilize all available data, enabling more comprehensive integration of contextual information to enhance forecasting accuracy.",
            "The Spacetimeformer, which emerged as the best performing model in our tests, leverages the permutation invariance property of self-attention. This allows it to flatten the multivariate time series, extending the attention across all N tokens in the encoder and H tokens in the decoder, respectively."
        ],
        "final_answer": "The encoder embeds each past grid-load time step as a token and then applies bi-directional self-attention over that entire sequence of embeddings. By allowing every time-step embedding to attend to all other past embeddings (permutation-invariant attention), the encoder aggregates both local and long-range dependencies across the historical window, producing a rich, context-aware representation of the past sequence.",
        "relevant_elements": [
            "Self Attention",
            "Past Grid Load"
        ],
        "id": 1089,
        "masked_question": "How does the encoder’s [mask1] mechanism leverage past grid load embeddings for robust sequence representation?",
        "masked_number": 1,
        "masked_elements": [
            "Self Attention"
        ],
        "figure_path": "./MISS-QA/figures/0_2409.05884v1_figure_1.png",
        "paperid": "2409.05884v1",
        "paper_path": "./MISS-QA/papers/2409.05884v1.json",
        "figure_id": "2409.05884v1_figure_1.png",
        "caption": "Figure 1: \nIllustration of the proposed load forecasting framework using the Railway dataset.\nPanel a displays the ”expected future”, including timetables,\nschedules and gross ton-kilometers (GTKM) estimates derived\nfrom the operational planning of the railway operator.\nThis information is provided for the next day\nacross all of Switzerland in the Railway-agg dataset\nand on a regional level (e.g. East) in the Railway dataset.\nTraditionally, methods such as pure timeseries forecasting a.1 and\nregression models a.2 were employed for load forecasting.\nOur proposed approach introduces the use of transformer architecture to learn a unified representation\nof the time series regression task (c).\nTo efficiently integrate both past and future information for this task,\nwe propose dividing the input data at the current time point t𝑡titalic_t (the present) (b). We then apply distinct embedding strategies for past data (b.1) and\nfuture contextual information (b.2) .",
        "qtype": "Experimental_Results",
        "response": "The little red‐boxed module in the encoder is nothing mystical – it is simply the masked, multi‐headed self-attention layer that sits at the bottom of each encoder block.  Concretely, what it does is:\n\n 1.  Take the entire sequence of past grid-load embeddings (after value-, positional- and temporal-embedding) as input tokens.\n 2.  Form three projections Q, K and V from those tokens and compute the standard scaled dot-product attention scores  \n     \n  \n 3.  Before the softmax, add a causal mask M (the very thing the red box is reminding you about) so that position i can only attend to positions ≤ i.  In practice  \n      M_{i,j} = 0   if   j ≤ i  \n      M_{i,j} = –∞  if   j > i  \n 4.  Softmax(M + QK^T/√d) produces a distribution over all earlier time-steps; multiply that by V and you get a context-vector for each time index.\n 5.  Because we do this with multiple heads and stack several such layers, each past embedding is able to gather information from different “views” of its own history and from other time-steps in the past window.\n\nBy doing exactly this masked self-attention over the past embeddings the encoder builds up a set of highly contextualized vectors in which every time‐step’s representation has “looked back” at all its predecessors.  This is precisely how it captures both short-term ripples and long-term patterns in the grid-load history, yielding the robust sequence encoding that the decoder then conditions on."
    },
    {
        "question": "How does the decoder’s cross-attention module integrate future contextual information embeddings to support joint regression and forecasting?",
        "relevant_section_ids": [
            "3",
            "7.2"
        ],
        "relevant_context": [
            "Our strategy involves integrating covariates from the expected future by modifying the embedding layer of the transformer’s decoder.",
            "In this innovative approach, the decoder functions as the regressor, selectively attending to sequence data from the expected future, while the encoder learns representations of past data.",
            "For model F, we propose to adapt an encoder–decoder transformer architecture where the encoder processes the past and the decoder processes the future contextual information.",
            "Since future contextual information embeddings differ from the past grid load embeddings, we adopt the strategy to separate the future contextual sequence from the past sequence and train specialized encoder and decoder contextual embedding layers.",
            "In this setup, the decoder serves as the regressor, by using non-causal attention to attend to data from the expected future, while the encoder learns a representation of the past data."
        ],
        "final_answer": "The decoder injects anticipated‐future covariates via a dedicated embedding layer and then, in each decoder block, uses non-causal cross-attention over that sequence of future embeddings.  In effect, the decoder’s cross-attention heads take the learned future‐covariate embeddings as keys and values (while its queries come from the decoding position), allowing the network to ‘‘regress’’ the target output by attending directly to all planned future contextual inputs in a bidirectional manner.  This unified attention mechanism both performs the regression from future covariates and generates the multi-step forecast in a single encoder–decoder pass.",
        "relevant_elements": [
            "Cross Attention",
            "Future Contextual Information"
        ],
        "id": 1090,
        "masked_question": "How does the decoder’s [mask1] module integrate future contextual information embeddings to support joint regression and forecasting?",
        "masked_number": 1,
        "masked_elements": [
            "Cross Attention"
        ],
        "figure_path": "./MISS-QA/figures/1_2409.05884v1_figure_1.png",
        "paperid": "2409.05884v1",
        "paper_path": "./MISS-QA/papers/2409.05884v1.json",
        "figure_id": "2409.05884v1_figure_1.png",
        "caption": "Figure 1: \nIllustration of the proposed load forecasting framework using the Railway dataset.\nPanel a displays the ”expected future”, including timetables,\nschedules and gross ton-kilometers (GTKM) estimates derived\nfrom the operational planning of the railway operator.\nThis information is provided for the next day\nacross all of Switzerland in the Railway-agg dataset\nand on a regional level (e.g. East) in the Railway dataset.\nTraditionally, methods such as pure timeseries forecasting a.1 and\nregression models a.2 were employed for load forecasting.\nOur proposed approach introduces the use of transformer architecture to learn a unified representation\nof the time series regression task (c).\nTo efficiently integrate both past and future information for this task,\nwe propose dividing the input data at the current time point t𝑡titalic_t (the present) (b). We then apply distinct embedding strategies for past data (b.1) and\nfuture contextual information (b.2) .",
        "qtype": "Experimental_Results",
        "response": "In our diagram, the little red-boxed module in each decoder layer is nothing exotic – it is simply the standard “cross-attention” sub-layer of an encoder-decoder Transformer, repurposed here to fuse in the *future* covariate embeddings.  Concretely, at each future time step t+h:\n\n1.  We first embed the future contextual vectors cⁿᶠₜ₊ₕ (e.g. timetables, tonnage, weather forecasts) with their own decoder embedding layer (see (b.2) in the figure).  \n2.  Those embedded future covariates become the **queries** (Q) in the cross‐attention block.  \n3.  The **keys** (K) and **values** (V) come from the encoder’s output – i.e. the representations summarizing all of the *past* grid‐load history.  \n4.  By attending (QKᵀ/V) the future queries against the past keys/values, the decoder “looks back” through the lens of *what you know will happen* tomorrow and fuses that information with the patterns it learnt from yesterday.  \n5.  The output of this cross‐attention is then fed into the usual feed‐forward sub-layer and on toward the final linear head, which simultaneously (a) regresses the influence of tomorrow’s covariates on grid‐load and (b) produces the multi-step forecast.\n\nBecause we use non-causal (bi-directional) attention in the decoder, each future‐covariate embedding can attend to *all* of the past encoder representations and to *all* other future covariates, giving a truly joint regression+forecast at every step."
    },
    {
        "question": "What are potential limitations of non-causal attention when integrating future contextual information under uncertain schedules?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Non-Causal Attention",
            "Future Contextual Information"
        ],
        "id": 1091,
        "masked_question": "What are potential limitations of [mask1] when integrating [mask2] under uncertain schedules?",
        "masked_number": 2,
        "masked_elements": [
            "Non-Causal Attention",
            "Future Contextual Information"
        ],
        "figure_path": "./MISS-QA/figures/2_2409.05884v1_figure_1.png",
        "paperid": "2409.05884v1",
        "paper_path": "./MISS-QA/papers/2409.05884v1.json",
        "figure_id": "2409.05884v1_figure_1.png",
        "caption": "Figure 1: \nIllustration of the proposed load forecasting framework using the Railway dataset.\nPanel a displays the ”expected future”, including timetables,\nschedules and gross ton-kilometers (GTKM) estimates derived\nfrom the operational planning of the railway operator.\nThis information is provided for the next day\nacross all of Switzerland in the Railway-agg dataset\nand on a regional level (e.g. East) in the Railway dataset.\nTraditionally, methods such as pure timeseries forecasting a.1 and\nregression models a.2 were employed for load forecasting.\nOur proposed approach introduces the use of transformer architecture to learn a unified representation\nof the time series regression task (c).\nTo efficiently integrate both past and future information for this task,\nwe propose dividing the input data at the current time point t𝑡titalic_t (the present) (b). We then apply distinct embedding strategies for past data (b.1) and\nfuture contextual information (b.2) .",
        "qtype": "Others",
        "response": "First, let’s fill in the masks:\n\n- [mask₁] is the red‐boxed module in panel c) – i.e. our encoder–decoder Transformer “backbone” (the contextually-enhanced Transformer architecture itself).  \n- [mask₂] is the blue-boxed input in panel a)–b) – namely the “Future Contextual Information” (the timetable-based covariates for the day ahead).\n\nWith that in place, the question becomes:\n\n  “What are potential limitations of the Transformer backbone when integrating future contextual information under uncertain schedules?”\n\nChain of thought:\n\n1.  Our Transformer decoder relies on *non-causal* (bi-directional) attention over a *fixed*, known future covariate sequence.  \n2.  If the “expected” future (e.g. a train timetable or industrial-shift plan) is in fact *uncertain*, *incomplete* or subject to *last-minute changes*, then the model’s key assumption—that you have perfect access to every future token at inference time—breaks down.  \n3.  Concretely, an uncertain schedule can introduce  \n    - **Noisy or stale covariates**: the pre-computed gross-ton-kilometres, counts or timetables you feed the decoder may be wrong, which in turn can mislead the cross-attention layers and degrade accuracy.  \n    - **Missing covariates**: if only part of the day-ahead plan is available (or if updates arrive piecemeal), the non-causal masking strategy must be re-worked (or the model must fall back on causal forecasting only).  \n4.  Beyond data quality, there is also a modeling cost:  \n    - **Overfitting risk**: the powerful Transformer can learn spurious correlations in the “future” signals if they turn out to be unreliable, leading to brittleness when the real world deviates.  \n    - **Computational/latency overhead**: constantly re-encoding an evolving future plan (and recomputing full cross-attention) can be expensive if schedules must be updated in real time.  \n\nIn short, the Transformer’s great strength—attending over a known, rich future context—becomes a liability the moment that context is noisy, uncertain or subject to revision. It can mis-attend on wrong tokens, overfit to stale plans, and requires careful architectural or operational safeguards to handle dynamically changing schedules."
    },
    {
        "question": "How could alternative embedding strategies mitigate biases from irregular past grid load patterns?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Embedding Strategy",
            "Past Grid Load"
        ],
        "id": 1092,
        "masked_question": "How could alternative [mask1] mitigate biases from irregular past grid load patterns?",
        "masked_number": 1,
        "masked_elements": [
            "Embedding Strategy"
        ],
        "figure_path": "./MISS-QA/figures/3_2409.05884v1_figure_1.png",
        "paperid": "2409.05884v1",
        "paper_path": "./MISS-QA/papers/2409.05884v1.json",
        "figure_id": "2409.05884v1_figure_1.png",
        "caption": "Figure 1: \nIllustration of the proposed load forecasting framework using the Railway dataset.\nPanel a displays the ”expected future”, including timetables,\nschedules and gross ton-kilometers (GTKM) estimates derived\nfrom the operational planning of the railway operator.\nThis information is provided for the next day\nacross all of Switzerland in the Railway-agg dataset\nand on a regional level (e.g. East) in the Railway dataset.\nTraditionally, methods such as pure timeseries forecasting a.1 and\nregression models a.2 were employed for load forecasting.\nOur proposed approach introduces the use of transformer architecture to learn a unified representation\nof the time series regression task (c).\nTo efficiently integrate both past and future information for this task,\nwe propose dividing the input data at the current time point t𝑡titalic_t (the present) (b). We then apply distinct embedding strategies for past data (b.1) and\nfuture contextual information (b.2) .",
        "qtype": "Others",
        "response": "Here, “[mask1]” is pointing to the pair of alternative “Embedding Strategy” blocks in panel b—namely the separate encoder‐ and decoder‐side embeddings (E_enc for past grid–load + past context, and E_dec for future contextual covariates).  The key insight is that by moving away from the one-size-fits-all value‐ embedding that simply projects raw historical loads into the model, and instead giving the model two distinct embedding pipelines:\n\n  1.  A learned encoder embedding (E_enc) that fuses only the past load and past covariates, and  \n  2.  A learned decoder embedding (E_dec) that injects the future/contextual planning signals,  \n\nyou force the network to treat “what happened” and “what is planned to happen” as two orthogonal pieces of information.  \n\nConcretely, this stratification mitigates biases from irregular historical spikes or outages because:\n\n •  The encoder embedding can learn to “smooth over” or de-emphasize one-off glitches in the load history, rather than overfitting to them.  \n •  The decoder embedding makes the future-planning signals first–class citizens, so the model doesn’t have to guess future behaviour purely from noisy past patterns.  \n\nIn effect, irregular outliers in X[t–w:t] are tucked away in E_enc’s subspace, while the train-timetable, GTKM estimates, weather forecasts, etc., arrive through E_dec in a clean, learnable form.  The self- and cross-attention layers then learn how much to lean on each embedding stream, automatically down-weighting spurious past bumps and up-weighting reliable future covariates.  This architectural split is what breaks the over-reliance on history and thus sharply reduces bias from irregular past grid-load patterns."
    },
    {
        "question": "What ethical risks accompany automated scribble expansion in annotating patient medical images?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Scribble expansion"
        ],
        "id": 1094,
        "masked_question": "What ethical risks accompany automated [mask1] in annotating patient medical images?",
        "masked_number": 1,
        "masked_elements": [
            "Scribble expansion"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.11636v1_figure_2.png",
        "paperid": "2411.11636v1",
        "paper_path": "./MISS-QA/papers/2411.11636v1.json",
        "figure_id": "2411.11636v1_figure_2.png",
        "caption": "Figure 2: a) An overview of the proposed method. b) Illustration of superpixel-based scribble expansion. c) Illustration of pseudo-label refinement with superpixel filtered by dynamic threshold. d) Illustration of superpixel uncertainty assessment.",
        "qtype": "Others",
        "response": "Step 1 – Identify what’s in the red box  \nIf you compare the red-outlined module in the figure with the textual description, you’ll see it is the “scribble expansion” step: the process that takes a few hand-drawn scribbles and, using superpixel boundaries, automatically spreads those labels out to cover an entire region.  \n\nStep 2 – Think through how fully automating scribble expansion could go wrong  \n1. Mislabeling and patient safety:  \n   • If the automatic expansion paints into areas that should not be labeled (for example, extending a tumor scribble into healthy tissue), downstream measurements (volume, growth rate, margins) become wrong. Clinicians who rely on those measurements could make inappropriate treatment decisions.  \n   • Conversely, under-expansion can hide pathology by failing to include small but clinically important regions.  \n\n2. Bias amplification:  \n   • Any systematic bias in the superpixel or expansion algorithm – say it works less accurately on darker-skinned patients or on certain scanner settings – will be propagated uniformly to every image. That means whole cohorts of underrepresented patients could receive poorer segmentations.  \n\n3. Lack of human oversight and accountability:  \n   • Because the algorithm “fills in” labels automatically, radiologists may come to trust it without double-checking every boundary. When mistakes do occur, it becomes unclear who is responsible – the software developer, the hospital, or the clinician who approved the segmentation.  \n\n4. Privacy and consent issues:  \n   • The more data you automatically label and store, the larger your annotated dataset grows. If it is not handled under strict data-governance policies, patient confidentiality may be at risk. Automated expansion makes it easier to amass and back up vast amounts of inferred (and possibly sensitive) patient information.  \n\n5. Regulatory and medico-legal risk:  \n   • Automated annotations used in a clinical workflow may need formal validation under medical device regulations (e.g. FDA, MDR). If scribble expansion is deployed without that validation, hospitals may face compliance penalties or legal liability for misdiagnosis.  \n\nStep 3 – Summarize the ethical risks  \nIn short, automating scribble expansion risks propagating boundary errors into every case, amplifying algorithmic biases, reducing human oversight (and thus accountability), increasing the chance of privacy breaches, and running afoul of medical-device regulations – all of which can ultimately harm patient safety and trust."
    },
    {
        "question": "What benefit does dynamic thresholding bring to Refined by superpixel pseudo-labels?",
        "relevant_section_ids": [
            "3.3"
        ],
        "relevant_context": [
            "Considering the influence of the threshold on the learning status, we design a dynamic threshold for filtering inspired by the adaptive thresholding in [56]. To better exploit superpixel information, the dynamic threshold uses an exponentially moving average (EMA) strategy performing an increasing trend during training, which ensures reliable superpixels are relabeled and unreliable ones are gradually ignored as training progresses."
        ],
        "final_answer": "By using a dynamic, EMA-based threshold that increases over training, the method adaptively selects only high-quality, reliable superpixels for relabeling while progressively filtering out noisy or unreliable ones, leading to cleaner and more accurate pseudo-labels.",
        "relevant_elements": [
            "dynamic thresholding",
            "Refined by superpixel"
        ],
        "id": 1096,
        "masked_question": "What benefit does [mask1] bring to [mask2] pseudo-labels?",
        "masked_number": 2,
        "masked_elements": [
            "dynamic thresholding",
            "Refined by superpixel"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.11636v1_figure_2.png",
        "paperid": "2411.11636v1",
        "paper_path": "./MISS-QA/papers/2411.11636v1.json",
        "figure_id": "2411.11636v1_figure_2.png",
        "caption": "Figure 2: a) An overview of the proposed method. b) Illustration of superpixel-based scribble expansion. c) Illustration of pseudo-label refinement with superpixel filtered by dynamic threshold. d) Illustration of superpixel uncertainty assessment.",
        "qtype": "Design_Rationale",
        "response": "Mask1 (the “dynamic thresholding” module) is exactly what lets the blue‐boxed pseudo-labels become both sharper and more reliable.  By keeping only those superpixels whose dominant-class proportion exceeds a threshold that (a) grows over the course of training via an EMA update and (b) is set per class, the network only “relabels” truly confident regions and leaves ambiguous superpixels alone.  The result is that the blue-boxed pseudo-labels are far less noisy, with crisper boundaries and higher per-class accuracy."
    },
    {
        "question": "What motivates the integration of Laplacian Smoothing and Laplacian Sharpening in embedding reconstruction?",
        "relevant_section_ids": [
            "2.2.2"
        ],
        "relevant_context": [
            "Since many competitor edges are missing in our graph, a decoder is necessary to enhance the model’s ability to extract information from the supply chain graph.",
            "Since GCN is a Laplacian smoothing process, we employ a Laplacian sharpening process to reverse the encoding process."
        ],
        "final_answer": "Because the GCN encoder inherently performs Laplacian smoothing (which blends and aggregates node features) and many competitor edges are missing, the model needs a decoder that can reverse this smoothing to faithfully reconstruct the original node attributes. Thus, Laplacian sharpening is integrated alongside smoothing to enable effective reconstruction of the node feature vectors.",
        "relevant_elements": [
            "Laplacian Smoothing",
            "Laplacian Sharpening"
        ],
        "id": 1097,
        "masked_question": "What motivates the integration of [mask1] and [mask2] in embedding reconstruction?",
        "masked_number": 2,
        "masked_elements": [
            "Laplacian Smoothing",
            "Laplacian Sharpening"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.02692v1_figure_1.png",
        "paperid": "2411.02692v1",
        "paper_path": "./MISS-QA/papers/2411.02692v1.json",
        "figure_id": "2411.02692v1_figure_1.png",
        "caption": "Figure 1. The Model Framework for JPEC",
        "qtype": "Design_Rationale",
        "response": "Step 1 – Identify the two masked terms from the figure and context  \n- The red‐boxed label (mask₁) is the “Laplacian Smoothing” step, i.e. the GCN encoder’s low-pass, neighbor-averaging propagation.  \n- The blue‐boxed label (mask₂) is the “Laplacian Sharpening” step, i.e. the decoder that reverses the smoothing to recover high-frequency detail.\n\nStep 2 – Recall why we need both in the auto-encoder  \n- A GCN encoding on the directed supply-chain graph inherently performs a Laplacian smoothing (it diffuses/averages node features over edges), which captures second-order proximity but tends to wash out fine‐grained (“high-frequency”) distinctions among nodes.  \n- To reconstruct the original features (and thus preserve discriminative patterns lost by over-smoothing), the decoder must perform the inverse operation—a Laplacian sharpening—that amplifies those differences and restores the original feature distribution.\n\nStep 3 – Answer the question  \nBy integrating Laplacian Smoothing (mask₁) and Laplacian Sharpening (mask₂) in the embedding reconstruction, JPEC can (a) exploit the GCN’s natural low-pass filtering to embed supply-chain structure and attributes, and then (b) apply a complementary high-pass sharpening to reverse that smoothing.  This dual process ensures that the auto-encoder not only captures the broad second-order proximity patterns in the supply-chain graph, but also faithfully reconstructs the fine-grained, high-frequency details of each node’s original feature vector."
    },
    {
        "question": "Why balance Laplacian Eigenmap and Reconstructed Feature X_i losses in the model objective?",
        "relevant_section_ids": [
            "2.1",
            "2.2.1",
            "2.2.2"
        ],
        "relevant_context": [
            "In our knowledge graph, each node in the node-set represents a real-world company, and contains attributes associated with each node. The directed edge set signifies supply chain connections between companies, while the undirected edge set denotes mutual competitor relationships. Notably, our knowledge graph lacks numerous competitor edges, resulting in a significantly smaller volume for compared to. Our objective is to leverage the limited competitor edges, combined with the extensive company node attributes and supply chain graph structure, to identify additional competitors for a given company.",
            "Although competitor edges are sparse in our graph, learning from these connections is natural and essential. We employ first-order proximity to characterize the local connection and use it as supervised information that constrains the similarity of latent representations between pairs of competitors ... Finally, we utilized a pairwise ranking loss function (Equation 3) to minimize the distance between positive pairs and simultaneously maximizing the distance between negative pairs ...",
            "The loss function for the second order proximity is to minimize the difference between the original node feature vectors and the reconstructed ones, which can be formulated as Equation 6 ... The ultimate objective function of our model integrates the loss function derived from both the first-order and second-order proximity, and can be mathematically represented as Equation 7. where λ is a hyper parameter to balance the first-order and second-order losses, r is the regularization term."
        ],
        "final_answer": "Because the model must simultaneously capture (1) the sparse but highly informative competitor relationships via the Laplacian Eigenmap (first‐order proximity) and (2) the broader supply‐chain structure and node‐attribute information via reconstruction of X_i (second‐order proximity), it uses a single joint objective with a hyperparameter λ to trade off and balance these two loss terms. Balancing ensures the learned embeddings reflect both competitor connections and feature/supply‐chain structure rather than over‐fitting to one source of information.",
        "relevant_elements": [
            "Laplacian Eigenmap",
            "Reconstructed Feature X_i"
        ],
        "id": 1098,
        "masked_question": "Why balance [mask1] and Reconstructed Feature X_i losses in the model objective?",
        "masked_number": 1,
        "masked_elements": [
            "Laplacian Eigenmap"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.02692v1_figure_1.png",
        "paperid": "2411.02692v1",
        "paper_path": "./MISS-QA/papers/2411.02692v1.json",
        "figure_id": "2411.02692v1_figure_1.png",
        "caption": "Figure 1. The Model Framework for JPEC",
        "qtype": "Design_Rationale",
        "response": "Mask₁ is the first‐order‐proximity loss coming from the Competitor‐network Laplacian Eigenmap, and the Reconstructed‐feature Xᵢ loss is our second‐order‐proximity (GCN autoencoder) loss. We introduce a balancing hyper‐parameter λ because:\n\n1. The two losses pull the embedding in different directions.  \n   –  The Laplacian Eigenmap term (Mask₁) tries to make known competitors close in embedding space and non-competitors far apart.  \n   –  The reconstruction term tries to preserve each node’s original attributes (and supply‐chain structure) in the embedding.  \n\n2. If we weight one loss too heavily:  \n   –  λ→0 (only reconstruction): the model learns to reconstruct features but ignores the sparse competitor links and cannot distinguish competitors from non-competitors.  \n   –  λ→∞ (only Laplacian Eigenmap): the model overfits the few known competitor edges and may lose all information about the node’s attributes and supply‐chain context.  \n\n3. By tuning λ, we find the sweet spot where the embedding simultaneously  \n   –  Respects the (scarce) ground‐truth competitor signals, and  \n   –  Retains the rich attribute/supply‐chain structure so that we generalize to uncover missing competitor relationships."
    },
    {
        "question": "How does Laplacian smoothing propagate attributes through the directed supply chain network?",
        "relevant_section_ids": [
            "2.2.2"
        ],
        "relevant_context": [
            "Since each node has associated attributes, GCN is a straightforward option to utilize and learn graph structure and attributes simultaneously.",
            "GCN is naturally designed for undirected graphs, and we change the GCN’s propagation function Φ to Φ′, to apply it into a directed supply‐chain graph (schlichtkrull2018modeling; shi2019skeleton). By changing the normalization function, the propagation rule of GCN can be rewritten as Equation 4, where A is the adjacency matrix, I is the identity matrix, D is the degree matrix, W is a layer‐specific trainable weight matrix, σ denotes an activation function, and H^(l) is the hidden representation at layer l."
        ],
        "final_answer": "Laplacian smoothing in the directed supply chain network is implemented by a directed graph‐convolutional propagation rule: at each layer, a node’s new representation is computed by taking its own features plus those of its in‐ and out‐neighbors (via A+I), normalizing by the directed degree matrix, multiplying by a trainable weight matrix, and applying a nonlinearity. This spreads (smooths) node attributes along the directed supply‐chain edges.",
        "relevant_elements": [
            "Laplacian smoothing",
            "Supply Chain"
        ],
        "id": 1099,
        "masked_question": "How does [mask1] propagate attributes through the directed [mask2] network?",
        "masked_number": 2,
        "masked_elements": [
            "Laplacian smoothing",
            "Supply Chain"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.02692v1_figure_1.png",
        "paperid": "2411.02692v1",
        "paper_path": "./MISS-QA/papers/2411.02692v1.json",
        "figure_id": "2411.02692v1_figure_1.png",
        "caption": "Figure 1. The Model Framework for JPEC",
        "qtype": "Implementation_Details",
        "response": "First, let me restate the masked question in full:  \n\n“How does Laplacian smoothing propagate attributes through the directed supply-chain network?”  \n\n**Chain-of-thought**  \n1.  In the diagram (Figure 1) the red‐boxed term is “Laplacian Smoothing,” and the blue‐boxed network is the directed supply-chain graph.  \n2.  Section 2.2.2 tells us that to get our node embeddings \\(Z\\) we run a GCN over the directed supply-chain edges.  A standard GCN layer on an undirected graph is in fact a Laplacian smoothing step, and here we simply adapt it to a directed graph by tweaking the normalization.  \n3.  Concretely, if \\(A\\) is the (directed) supply-chain adjacency matrix and we add self‐loops to get \\(\\tilde A=A+I\\), and let \\(\\tilde D\\) be its out-degree diagonal, then one layer of our encoder is  \n   \n   \\[\n     H^{(l+1)} \\;=\\;\\sigma\\!\\bigl(\\tilde D^{-1}\\,\\tilde A\\,H^{(l)}\\,W^{(l)}\\bigr)\n   \\]  \n\n   with \\(H^{(0)}=X\\) (the original node attribute matrix).  \n4.  That multiplication \\(\\tilde D^{-1}\\tilde A\\,H^{(l)}\\) is exactly a Laplacian smoothing step: each node replaces its own features by a weighted average of its out-neighbors’ features (plus itself, from the self‐loop).  Stacking \\(L\\) of these layers diffuses (i.e.\\ “smooths”) attributes along all \\(L\\)-hop directed supply-chain paths.  \n5.  In this way, attribute information is propagated forward (and also backward if you include the transpose of \\(A\\) or use in- and out-degrees separately) through the directed edges, so that by the end of the encoder each node’s embedding contains a smoothed mixture of its upstream and downstream partners’ features.  \n\n**Answer**  \nLaplacian smoothing in our directed-GCN encoder works by taking each node’s feature vector and replacing it with a normalized sum of its own features plus those of its directed neighbors.  Formally, if \\(\\tilde A=A+I\\) is the adjacency of the supply-chain graph with self-loops and \\(\\tilde D\\) its out-degree diagonal, then one layer is  \n\\[\n   H^{(l+1)}=\\sigma\\bigl(\\tilde D^{-1}\\,\\tilde A\\,H^{(l)}\\,W^{(l)}\\bigr).\n\\]\nThis repeatedly averages (smooths) attributes over one-hop directed supply-chain links, so that after \\(L\\) layers each node’s embedding has “picked up” information from its \\(L\\)-hop upstream and downstream partners."
    },
    {
        "question": "How does the Doppler Driven Block utilize Reshape and 2DConv+ReLU operations for refined motion features?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Doppler Driven Block"
        ],
        "id": 1101,
        "masked_question": "How does the [mask1] utilize Reshape and 2DConv+ReLU operations for refined motion features?",
        "masked_number": 1,
        "masked_elements": [
            "Doppler Driven Block"
        ],
        "figure_path": "./MISS-QA/figures/0_2411.12676v1_figure_1.png",
        "paperid": "2411.12676v1",
        "paper_path": "./MISS-QA/papers/2411.12676v1.json",
        "figure_id": "2411.12676v1_figure_1.png",
        "caption": "Figure 1: Overall flow chart of IE-PONet Model Structure.",
        "qtype": "Implementation_Details",
        "response": "In the red‐boxed “Doppler Driven Block” the authors basically take the little 1×1 two‐dimensional feature tensor coming out of their front‐end, reorganize it, run a couple of cheap 2D conv+ReLU layers over it to pick up fine motions, and then put it back into its original shape for the 3D part of the network.  Concretely:  \n1.  Reshape – they first reshape the flat C×H×W tensor into a 2D map whose one axis “unrolls” the temporal (or channel) dimension against spatial locations.  \n2.  2DConv+ReLU – a small stack of 2D convolutional filters (green blocks in the diagram) interleaved with ReLUs then sweep over this pseudo‐image.  Because time (or channels) and space are now laid out next to each other in that map, those 2D filters act like learnable Doppler filters, nailing down subtle velocity and direction cues.  \n3.  Reshape – finally they reshape the output back into the original spatiotemporal tensor shape so that these newly “Doppler‐enhanced” features can be fed into the downstream C3D layers.  \n\nBy doing this cheap 2D conv on a cleverly reshaped feature map the network refines its motion descriptors before moving on to the heavier 3D convolutions."
    },
    {
        "question": "How does the C3D Block process concatenated pose and Doppler-driven feature maps for action classification?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "C3D Block",
            "Doppler Driven Block"
        ],
        "id": 1102,
        "masked_question": "How does the [mask1] process concatenated pose and [mask2] feature maps for action classification?",
        "masked_number": 2,
        "masked_elements": [
            "C3D Block",
            "Doppler Driven Block"
        ],
        "figure_path": "./MISS-QA/figures/1_2411.12676v1_figure_1.png",
        "paperid": "2411.12676v1",
        "paper_path": "./MISS-QA/papers/2411.12676v1.json",
        "figure_id": "2411.12676v1_figure_1.png",
        "caption": "Figure 1: Overall flow chart of IE-PONet Model Structure.",
        "qtype": "Implementation_Details",
        "response": "Here’s what’s happening in the red and blue boxes and how they talk to one another at classification time:\n\n1. Two parallel “feature streams” are produced from the same chunk of video frames:  \n   – The blue-boxed Doppler-Driven Block takes the 1×1 2D-conv features, reshapes them into a “pseudo-image,” runs a small 2D-conv+ReLU bottleneck, then reshapes back into a time-aware feature map.  \n   – The 2D Pose Extraction Block (OpenPose) produces a second stream of per-frame key-point heatmaps (i.e. the “pose” features).\n\n2. Mask1 (the red-boxed “C3D Block”) simply concatenates these two streams channel-wise into one 4D tensor of size (T × H × W × Ctotal).\n\n3. It then applies a stack of genuine 3D convolutions (and poolings) across the time (T), height (H), and width (W) dimensions, thereby jointly learning the spatio-temporal patterns in the fused pose + Doppler features.\n\n4. The final 3D‐CNN output is flattened (or global-pooled) and passed through a small fully-connected/softmax head to yield the final action-class scores.\n\nIn short: the C3D block just glues the pose heatmaps and Doppler feature maps together along the channel axis and then runs 3D convolutions over them to produce an action prediction."
    },
    {
        "question": "How do Doppler Driven Block reshaping operations interface with 1x1 convolution feature extraction efficiency?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "Doppler Driven Block",
            "1x1, 2D Conv feature extract"
        ],
        "id": 1103,
        "masked_question": "How do [mask1] reshaping operations interface with 1x1 convolution feature extraction efficiency?",
        "masked_number": 1,
        "masked_elements": [
            "Doppler Driven Block"
        ],
        "figure_path": "./MISS-QA/figures/2_2411.12676v1_figure_1.png",
        "paperid": "2411.12676v1",
        "paper_path": "./MISS-QA/papers/2411.12676v1.json",
        "figure_id": "2411.12676v1_figure_1.png",
        "caption": "Figure 1: Overall flow chart of IE-PONet Model Structure.",
        "qtype": "Literature_Background",
        "response": "The little “book‐ends” you see around the 2D Conv + ReLU in the red box are nothing mystical – they are simply tensor reshaping operations that let a vanilla 1×1 convolution do all of the heavy lifting, both in the spatial and (collapsed) temporal dimensions, while keeping the computation cost down.  In practice what happens is:\n\n 1.  After the per-frame 1×1, 2D convolution pulls out a small set of channel features, we “reshape” the 4-D tensor (batch, time, height, width, channels) into a 3-D (batch, height·width, time·channels) arrangement or vice-versa.  \n 2.  The 2D (effectively 1×1 in space but full in channel) convolution then mixes information across what used to be separate frames and channels in one very cheap pass.  \n 3.  Finally, the second reshape restores the original time×height×width×channel structure so that downstream blocks see the data in its normal format.  \n\nBecause reshaping is essentially free (no multiplications) and the 1×1 kernels are tiny, you end up with a module that can learn “Doppler-style” cross–frame interactions without any of the expense of a full 3D convolution."
    },
    {
        "question": "How does C3D Block's stacked convolution compare with the sequential fusion in the 2D Pose Extraction Block for temporal modeling?",
        "relevant_section_ids": [],
        "relevant_context": [],
        "final_answer": "Unanswerable",
        "relevant_elements": [
            "C3D Block",
            "2D Pose Extraction Block"
        ],
        "id": 1104,
        "masked_question": "How does [mask1]'s stacked convolution compare with the sequential fusion in the [mask2] for temporal modeling?",
        "masked_number": 2,
        "masked_elements": [
            "C3D Block",
            "2D Pose Extraction Block"
        ],
        "figure_path": "./MISS-QA/figures/3_2411.12676v1_figure_1.png",
        "paperid": "2411.12676v1",
        "paper_path": "./MISS-QA/papers/2411.12676v1.json",
        "figure_id": "2411.12676v1_figure_1.png",
        "caption": "Figure 1: Overall flow chart of IE-PONet Model Structure.",
        "qtype": "Literature_Background",
        "response": "In the IE-PONet diagram the red‐boxed “C3D Block” and the blue‐boxed “Doppler Driven Block” both try to give you time-aware features, but they do it in quite different ways:\n\n1. Mask 1 (the red C3D Block)  \n   • Uses genuine 3 D convolution kernels.  Each convolution is a little cuboid that spans H×W×T, so as you stack them you immediately pick up motion cues over several consecutive frames in one shot.  In other words you learn spatial and temporal structure jointly with volumetric filters.  \n\n2. Mask 2 (the blue Doppler Driven/“sequential fusion” block)  \n   • Keeps everything as 2 D convolutions but fuses time in a stepwise fashion.  You “reshape” your H×W×(T·C) tensor into something like (T×H×W) with channels C, run a 2 DConv+ReLU, re‐reshape back, and then repeat.  Each pass folds in a little more temporal context, but you never turn your kernels into full 3 D cuboids.  \n\nSummary:  \n– C3D’s stacked 3 D convolutions give you large temporal receptive fields in one go (heavy but very direct).  \n– The sequential 2 D‐conv + reshape fusions gradually build up a temporal view by repeatedly mixing frames (lighter weight, more iterative)."
    }
]